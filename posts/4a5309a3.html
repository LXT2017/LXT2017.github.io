<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="baidu-site-verification" content="codeva-2tBONWXoFH" />
  <meta name="msvalidate.01" content="DEF5F2F984531D10266A543EDD015EB1" />
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="https://blog.shawncoding.top">
  <title>Spark3学习笔记 | 星星的猫(&gt;^ω^&lt;)喵</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Spark3学习笔记 一、Spark 基础 1、Spark概述 1.1 Spark简介 Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。在 FullStack 理想的指引下，Spark 中的 Spark SQL 、SparkStreaming 、MLLib 、GraphX 、R 五大子框架和库之间可以无缝地共享数据和操作， 这不仅打造了 Spark 在当今大数据计算领域其他计算">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark3学习笔记">
<meta property="og:url" content="https://blog.shawncoding.top/posts/4a5309a3.html">
<meta property="og:site_name" content="星星的猫(&gt;^ω^&lt;)喵">
<meta property="og:description" content="Spark3学习笔记 一、Spark 基础 1、Spark概述 1.1 Spark简介 Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。在 FullStack 理想的指引下，Spark 中的 Spark SQL 、SparkStreaming 、MLLib 、GraphX 、R 五大子框架和库之间可以无缝地共享数据和操作， 这不仅打造了 Spark 在当今大数据计算领域其他计算">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://qnypic.shawncoding.top/blog/202402291816075.png">
<meta property="og:image" content="http://qnypic.shawncoding.top/blog/202402291816201.png">
<meta property="og:image" content="http://qnypic.shawncoding.top/blog/202402291816202.png">
<meta property="og:image" content="http://qnypic.shawncoding.top/blog/202402291816203.png">
<meta property="og:image" content="http://qnypic.shawncoding.top/blog/202402291816204.png">
<meta property="og:image" content="http://qnypic.shawncoding.top/blog/202402291816205.jpg">
<meta property="og:image" content="http://qnypic.shawncoding.top/blog/202402291816206.jpg">
<meta property="og:image" content="http://qnypic.shawncoding.top/blog/202402291816207.png">
<meta property="og:image" content="http://qnypic.shawncoding.top/blog/202402291816209.png">
<meta property="og:image" content="http://qnypic.shawncoding.top/blog/202402291816210.png">
<meta property="og:image" content="http://qnypic.shawncoding.top/blog/202402291816211.png">
<meta property="article:published_time" content="2024-02-06T07:04:33.000Z">
<meta property="article:modified_time" content="2024-02-29T12:03:29.810Z">
<meta property="article:author" content="Shawn">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://qnypic.shawncoding.top/blog/202402291816075.png">
  
    <link rel="alternative" href="/atom.xml" title="星星的猫(&gt;^ω^&lt;)喵" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/1.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
<link rel="stylesheet" type="text/css" href="/./search.css">
  
  <link rel="stylesheet" type="text/css" href="/./avatarrotation.css">
  
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  <!-- Global site tag (gtag.js) - Google Analytics -->

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-178745185-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-178745185-1');
    </script>

<!-- End Google Analytics -->
  
<script>
var _hmt = _hmt || [];
(function() {
	var hm = document.createElement("script");
	hm.src = "https://hm.baidu.com/hm.js?bfc79b86aa87f7cc5e5cb4920188a950";
	var s = document.getElementsByTagName("script")[0]; 
	s.parentNode.insertBefore(hm, s);
})();
</script>

  <link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css">
  <!--百度自动推送-->

    <!-- 开启百度站长平台自动推送https://ziyuan.baidu.com/linksubmit/index，
    https://ziyuan.baidu.com/college/courseinfo?id=267&page=2#h2_article_title19-->
    <script>
      (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
          bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
          bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
      })();
    </script>
    
<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>

    <div class="mymenucontainer" onclick="myFunction(this)">
      <div class="bar1"></div>
      <div class="bar2"></div>
      <div class="bar3"></div>
    </div>

    <div class="left-col" q-class="show:isShow">
      
<!-- <div class="overlay" style="background: url('https://w.wallhaven.cc/full/k9/wallhaven-k9zqjd.png') no-repeat center bottom;background-size: cover;opacity: 1"></div> -->
<div class="overlay" ></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/img/1.png" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/">Shawn</a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/categories">分类</a></li>
	        
				<li><a href="/archives">归档</a></li>
	        
				<li><a href="/photos/index.html">相册</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">友链</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
            
		</nav>
		<ul style="font-size:12px; font-style:oblique; font-weight:100; color:#1D94CE; margin-top:-15px" >
			总文章数：200
		</ul>
		
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/LXT2017" title="github"><i class="icon-github"></i></a>
		        
					<a class="csdn" target="_blank" href="https://blog.csdn.net/lemon_TT" title="csdn"><i class="icon-csdn"></i></a>
		        
					<a class="rss" target="_blank" href="/atom.xml" title="rss"><i class="icon-rss"></i></a>
		        
					<a class="mail" target="_blank" href="mailto:shawn955@163.com" title="mail"><i class="icon-mail"></i></a>
		        
			</div>

			
			<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=244 height=100 src="//music.163.com/outchain/player?type=2&id=550082870&auto=0&height=66"></iframe>
			

			
		</nav>

	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse" style="background: rgba(255, 255, 255, 0.4)">
    <!-- <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse"> -->
      
<nav id="mobile-nav">
  	<!-- <div class="overlay js-overlay" style="background: #4d4d4d"></div> -->
  	<div class="overlay js-overlay"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/img/1.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author">Shawn</h1>
			</hgroup>
			
			
			
				
			
				
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/LXT2017" title="github"><i class="icon-github"></i></a>
			        
						<a class="csdn" target="_blank" href="https://blog.csdn.net/lemon_TT" title="csdn"><i class="icon-csdn"></i></a>
			        
						<a class="rss" target="_blank" href="/atom.xml" title="rss"><i class="icon-rss"></i></a>
			        
						<a class="mail" target="_blank" href="mailto:shawn955@163.com" title="mail"><i class="icon-mail"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 80%">
				
				
					<li style="width: 25%"><a href="/">主页</a></li>
		        
					<li style="width: 25%"><a href="/categories">分类</a></li>
		        
					<li style="width: 25%"><a href="/archives">归档</a></li>
		        
					<li style="width: 25%"><a href="/photos/index.html">相册</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>



            
      <div class="page-header" style="">
          
          <span>🍻  
              <span id="jinrishici-sentence" title="今日诗词">正在加载今日诗词....</span>
          </span>
          <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>

          
          <script type="text/javascript" src="/js/search.js"></script>
          <span id="local-search" class="local-search local-search-plugin" style="">
            <input type="search" placeholder="站内搜索" id="local-search-input" class="local-search-input-cls" style="">
            <i id="local-search-icon-search" class="icon" aria-hidden="true" title="站内搜索">🔍</i>
            <div id="local-search-result" class="local-search-result-cls"></div>
          </span>

          <script type="text/javascript" src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
          <script>
              if ($('.local-search').size()) {
                $.getScript('/js/search.js', function() {
                  searchFunc("/search.xml", 'local-search-input', 'local-search-result');
                });
              }
          </script>
          
      </div>
      



      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            <article id="post-Spark3学习笔记" class="article article-type-post " itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
        
        <a href="/posts/4a5309a3.html" class="archive-article-date">
  	<time datetime="2024-02-06T07:04:33.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2024-02-06</time>
  	
</a>
        

        <!-- 开始添加字数统计-->
            
        <div style="margin-top:10px;">
    <span class="post-time">
      <span class="post-meta-item-icon">
        <i class="fa fa-keyboard-o"></i>
        <span class="post-meta-item-text">  字数统计: </span>
        <span class="post-count">23.5k字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
        <i class="fa fa-hourglass-half"></i>
        <span class="post-meta-item-text">  阅读时长≈</span>
        <span class="post-count">98分</span>
      </span>
    </span>
</div>
        
        <!-- 添加完成 -->

      </header>
    
    <div class="article-entry" itemprop="articleBody">

      

      
      <center><p>
  
    <h1 class="article-title_code_ant" itemprop="name">
      Spark3学习笔记
    </h1>
  
</p></center>

      
        <h1>Spark3学习笔记</h1>
<h1>一、Spark 基础</h1>
<h2 id="1、Spark概述">1、Spark概述</h2>
<h3 id="1-1-Spark简介">1.1 Spark简介</h3>
<p>Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。在 FullStack 理想的指引下，<strong>Spark 中的 Spark SQL 、SparkStreaming 、MLLib 、GraphX 、R 五大子框架和库之间可以无缝地共享数据和操作</strong>， 这不仅打造了 Spark 在当今大数据计算领域其他计算框架都无可匹敌的优势， 而且使得 Spark 正在加速成为大数据处理中心首选通用计算平台。</p>
<a id="more"></a>
<ul>
<li><strong>Spark Core</strong>：实现了 Spark 的基本功能，包含 RDD、任务调度、内存管理、错误恢复、与存储系统交互等模块</li>
<li><strong>Spark SQL</strong>：Spark 用来操作结构化数据的程序包。通过 Spark SQL，我们可以使用 SQL 操作数据</li>
<li><strong>Spark Streaming</strong>：Spark 提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API</li>
<li><strong>Spark MLlib</strong>：提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据导入等额外的支持功能</li>
<li><strong>GraphX(图计算)</strong>：Spark 中用于图计算的 API，性能良好，拥有丰富的功能和运算符，能在海量数据上自如地运行复杂的图算法</li>
<li><strong>集群管理器</strong>：Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算</li>
<li><strong>Structured Streaming</strong>：处理结构化流,统一了离线和实时的 API</li>
</ul>
<h3 id="1-2-Spark-VS-Hadoop">1.2 Spark VS Hadoop</h3>
<table>
<thead>
<tr>
<th></th>
<th><strong>Hadoop</strong></th>
<th><strong>Spark</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>类型</strong></td>
<td>分布式基础平台, 包含计算, 存储, 调度</td>
<td>分布式计算工具</td>
</tr>
<tr>
<td><strong>场景</strong></td>
<td>大规模数据集上的批处理</td>
<td>迭代计算, 交互式计算, 流计算</td>
</tr>
<tr>
<td><strong>价格</strong></td>
<td>对机器要求低, 便宜</td>
<td>对内存有要求, 相对较贵</td>
</tr>
<tr>
<td><strong>编程范式</strong></td>
<td>Map+Reduce, API 较为底层, 算法适应性差</td>
<td>RDD 组成 DAG 有向无环图, API 较为顶层, 方便使用</td>
</tr>
<tr>
<td><strong>数据存储结构</strong></td>
<td>MapReduce 中间计算结果存在 HDFS 磁盘上, 延迟大</td>
<td>RDD 中间运算结果存在内存中 , 延迟小</td>
</tr>
<tr>
<td><strong>运行方式</strong></td>
<td>Task 以进程方式维护, 任务启动慢</td>
<td>Task 以线程方式维护, 任务启动快</td>
</tr>
</tbody>
</table>
<p>💖 注意：尽管 Spark 相对于 Hadoop 而言具有较大优势，但 Spark 并不能完全替代 Hadoop，Spark 主要用于替代 Hadoop 中的 MapReduce 计算模型。存储依然可以使用 HDFS，但是中间结果可以存放在内存中；调度可以使用 Spark 内置的，也可以使用更成熟的调度系统 YARN 等。  实际上，Spark 已经很好地融入了 Hadoop 生态圈，并成为其中的重要一员，它可以借助于 YARN 实现资源调度管理，借助于 HDFS 实现分布式存储。  此外，Hadoop 可以使用廉价的、异构的机器来做分布式存储与计算，但是，Spark 对硬件的要求稍高一些，对内存与 CPU 有一定的要求。</p>
<h3 id="1-3-Spark特点">1.3 Spark特点</h3>
<ul>
<li>快。与 Hadoop 的 MapReduce 相比，<strong>Spark 基于内存的运算要快 100 倍以上，基于硬盘的运算也要快 10 倍以上</strong>。Spark 实现了高效的 DAG 执行引擎，可以通过基于内存来高效处理数据流。</li>
<li>易用。<strong>Spark 支持 Java、Python、R 和 Scala 的 API，还支持超过 80 种高级算法，使用户可以快速构建不同的应用</strong>。而且 Spark 支持交互式的 Python 和 Scala 的 shell，可以非常方便地在这些 shell 中使用 Spark 集群来验证解决问题的方法。</li>
<li>通用。Spark 提供了统一的解决方案。<strong>Spark 可以用于批处理、交互式查询(Spark SQL)、实时流处理(Spark Streaming)、机器学习(Spark MLlib)和图计算(GraphX)</strong>。这些不同类型的处理都可以在同一个应用中无缝使用。Spark 统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。</li>
<li>兼容性。<strong>Spark 可以非常方便地与其他的开源产品进行融合</strong>。比如，Spark 可以使用 Hadoop 的 YARN 和 Apache Mesos 作为它的资源管理和调度器，并且可以处理所有 Hadoop 支持的数据，包括 HDFS、HBase 和 Cassandra 等。这对于已经部署 Hadoop 集群的用户特别重要，因为不需要做任何数据迁移就可以使用 Spark 的强大处理能力。Spark 也可以不依赖于第三方的资源管理和调度器，它实现了 Standalone 作为其内置的资源管理和调度框架，这样进一步降低了 Spark 的使用门槛，使得所有人都可以非常容易地部署和使用 Spark。此外，Spark 还提供了在 EC2 上部署 Standalone 的 Spark 集群的工具。</li>
</ul>
<h3 id="1-4-Spark入门Demo">1.4 Spark入门Demo</h3>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03_WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparConf)</span><br><span class="line">        wordcount91011(sc)</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// groupBy</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount1</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> group: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">String</span>])] = words.groupBy(word=&gt;word)</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = group.mapValues(iter=&gt;iter.size)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// groupByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount2</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> group: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = wordOne.groupByKey()</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = group.mapValues(iter=&gt;iter.size)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reduceByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount3</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.reduceByKey(_+_)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// aggregateByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount4</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.aggregateByKey(<span class="number">0</span>)(_+_, _+_)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// foldByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount5</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.foldByKey(<span class="number">0</span>)(_+_)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// combineByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount6</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.combineByKey(</span><br><span class="line">            v=&gt;v,</span><br><span class="line">            (x:<span class="type">Int</span>, y) =&gt; x + y,</span><br><span class="line">            (x:<span class="type">Int</span>, y:<span class="type">Int</span>) =&gt; x + y</span><br><span class="line">        )</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// countByKey</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount7</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordOne = words.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: collection.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = wordOne.countByKey()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// countByValue</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount8</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCount: collection.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = words.countByValue()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reduce, aggregate, fold</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordcount91011</span></span>(sc : <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"Hello Scala"</span>, <span class="string">"Hello Spark"</span>))</span><br><span class="line">        <span class="keyword">val</span> words = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 【（word, count）,(word, count)】</span></span><br><span class="line">        <span class="comment">// word =&gt; Map[(word,1)]</span></span><br><span class="line">        <span class="keyword">val</span> mapWord = words.map(</span><br><span class="line">            word =&gt; &#123;</span><br><span class="line">                mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]((word,<span class="number">1</span>))</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">       <span class="keyword">val</span> wordCount = mapWord.reduce(</span><br><span class="line">            (map1, map2) =&gt; &#123;</span><br><span class="line">                map2.foreach&#123;</span><br><span class="line">                    <span class="keyword">case</span> (word, count) =&gt; &#123;</span><br><span class="line">                        <span class="keyword">val</span> newCount = map1.getOrElse(word, <span class="number">0</span>L) + count</span><br><span class="line">                        map1.update(word, newCount)</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                map1</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        println(wordCount)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="2、Spark-运行模式">2、Spark 运行模式</h2>
<h3 id="2-1-概述">2.1 概述</h3>
<ul>
<li>local 本地模式(单机)–学习测试使用，分为 local 单线程和 local-cluster 多线程</li>
<li>standalone 独立集群模式–学习测试使用，典型的 Mater/slave 模式</li>
<li>standalone-HA 高可用模式–生产环境使用，基于 standalone 模式，使用 zk 搭建高可用，避免 Master 是有单点故障的</li>
<li>**on yarn 集群模式–生产环境使用，**运行在 yarn 集群之上，由 yarn 负责资源管理，Spark 负责任务调度和计算。好处：计算资源按需伸缩，集群利用率高，共享底层存储，避免数据跨集群迁移</li>
<li>on mesos 集群模式–国内使用较少，运行在 mesos 资源管理器框架之上，由 mesos 负责资源管理，Spark 负责任务调度和计算</li>
<li>on cloud 集群模式–中小公司未来会更多的使用云服务，比如 AWS 的 EC2，使用这个模式能很方便的访问 Amazon 的 S3</li>
</ul>
<h3 id="2-2-Local模式">2.2 Local模式</h3>
<p>Local 模式，就是不需要其他任何节点资源就可以在本地执行 Spark 代码的环境，一般用于教学，调试，演示等， 之前在 IDEA 中运行代码的环境我们称之为开发环境，不太一样</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里我hadoop环境为3.1.3</span></span><br><span class="line">wget https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz</span><br><span class="line">tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module</span><br><span class="line"><span class="built_in">cd</span> /opt/module</span><br><span class="line">mv spark-3.0.0-bin-hadoop3.2 spark</span><br><span class="line"><span class="comment"># 然后进入spark目录</span></span><br><span class="line">bin/spark-shell</span><br><span class="line"><span class="comment"># 启动后可以进入Web界面进行监控 http://hadoop102:4040/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先在data下创建文本在进入</span></span><br><span class="line">sc.textFile(<span class="string">"data/word.txt"</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">:quit</span><br><span class="line"></span><br><span class="line"><span class="comment"># ================提交应用========</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master <span class="built_in">local</span>[2] \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br><span class="line"><span class="comment"># --class 表示要执行程序的主类，此处可以更换为咱们自己写的应用程序</span></span><br><span class="line"><span class="comment"># --master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟CPU 核数量</span></span><br><span class="line"><span class="comment"># spark-examples_2.12-3.0.0.jar 运行的应用类所在的 jar 包，实际使用时，可以设定为咱们自己打的 jar 包</span></span><br><span class="line"><span class="comment"># 数字 10 表示程序的入口参数，用于设定当前应用的任务数量</span></span><br></pre></td></tr></table></figure>
<h3 id="2-3-Standalone-模式">2.3 Standalone 模式</h3>
<p>local 本地模式毕竟只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行，这里我们来看看只使用 Spark 自身节点运行的集群模式，也就是我们所谓的独立部署（Standalone）模式。Spark 的 Standalone 模式体现了经典的master-slave 模式</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里我准备三台机器，hadoop102为master，103，104为slave</span></span><br><span class="line"><span class="built_in">cd</span> /opt/module/spark/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1)进入解压缩后路径的 conf 目录，修改 slaves.template 文件名为 slaves</span></span><br><span class="line">mv slaves.template slaves</span><br><span class="line"><span class="comment"># 2)修改 slaves 文件，添加work 节点</span></span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br><span class="line"><span class="comment"># 3)修改 spark-env.sh.template 文件名为 spark-env.sh</span></span><br><span class="line">mv spark-env.sh.template spark-env.sh</span><br><span class="line"><span class="comment"># 4)修改 spark-env.sh 文件，添加 JAVA_HOME 环境变量和集群对应的 master 节点</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_121</span><br><span class="line">SPARK_MASTER_HOST=hadoop102</span><br><span class="line">SPARK_MASTER_PORT=7077</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意：7077 端口，相当于 hadoop 内部通信的 8020 端口，此处的端口需要确认自己的 Hadoop配置</span></span><br><span class="line"><span class="comment"># 5)分发 spark-standalone 目录</span></span><br><span class="line">xsync spark</span><br><span class="line"></span><br><span class="line"><span class="comment"># =======集群的启动=====</span></span><br><span class="line">sbin/start-all.sh</span><br><span class="line"><span class="comment"># 查看是否启动，worker</span></span><br><span class="line">jpsall</span><br><span class="line"><span class="comment"># 查看 Master 资源监控Web UI 界面: http://hadoop102:8081</span></span><br><span class="line"><span class="comment"># =============提交应用===============</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br><span class="line"><span class="comment"># master spark://hadoop102:7077 独立部署模式，连接到Spark 集群</span></span><br></pre></td></tr></table></figure>
<p>在提交应用中，一般会同时一些提交参数</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class &lt;main-class&gt;</span><br><span class="line">--master &lt;master-url&gt; \</span><br><span class="line">... <span class="comment"># other options</span></span><br><span class="line">&lt;application-jar&gt; \ [application-arguments]</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>解释</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>–class</td>
<td>Spark 程序中包含主函数的类</td>
</tr>
<tr>
<td>–master</td>
<td>Spark 程序运行的模式(环境)</td>
</tr>
<tr>
<td>–executor-memory 1G</td>
<td>指定每个 executor 可用内存为 1G</td>
</tr>
<tr>
<td>–total-executor-cores 2</td>
<td>指定所有executor 使用的cpu 核数为 2 个</td>
</tr>
<tr>
<td>–executor-cores</td>
<td>指定每个executor 使用的cpu 核数</td>
</tr>
<tr>
<td>application-jar</td>
<td>打包好的应用 jar，包含依赖。这个 URL 在集群中全局可见。 比如 hdfs:// 共享存储系统，如果是file:// path， 那么所有的节点的path 都包含同样的 jar</td>
</tr>
<tr>
<td>application-arguments</td>
<td>传给 main()方法的参数</td>
</tr>
</tbody>
</table>
<p>接下来配置历史服务，由于 spark-shell 停止掉后，集群监控 Hadoop02:8081 页面就看不到历史任务的运行情况，所以开发时都配置历史服务器记录任务运行情况</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1)修改 spark-defaults.conf.template 文件名为 spark-defaults.conf</span></span><br><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br><span class="line"><span class="comment"># 2)修改 spark-default.conf 文件，配置日志存储路径,如果hadoop是单体就是具体的机器名</span></span><br><span class="line">spark.eventLog.enabled <span class="literal">true</span></span><br><span class="line">spark.eventLog.dir hdfs://mycluster/directory</span><br><span class="line"><span class="comment"># 注意：需要启动 hadoop 集群，HDFS 上的directory 目录需要提前存在</span></span><br><span class="line">sbin/start-dfs.sh</span><br><span class="line">hadoop fs -mkdir /directory</span><br><span class="line"><span class="comment"># 3)修改 spark-env.sh 文件, 添加日志配置</span></span><br><span class="line"><span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">"</span></span><br><span class="line"><span class="string">-Dspark.history.ui.port=18080</span></span><br><span class="line"><span class="string">-Dspark.history.fs.logDirectory=hdfs://mycluster/directory</span></span><br><span class="line"><span class="string">-Dspark.history.retainedApplications=30"</span></span><br><span class="line"><span class="comment"># 参数 1 含义：WEB UI 访问的端口号为 18080</span></span><br><span class="line"><span class="comment"># 参数 2 含义：指定历史服务器日志存储路径</span></span><br><span class="line"><span class="comment"># 参数 3 含义：指定保存Application 历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数</span></span><br><span class="line"><span class="comment"># 4)分发配置文件</span></span><br><span class="line">xsync conf</span><br><span class="line"><span class="comment"># 重新启动集群和历史服务</span></span><br><span class="line">sbin/start-all.sh</span><br><span class="line">sbin/start-history-server.sh</span><br><span class="line"><span class="comment"># 6)重新执行任务</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br><span class="line"><span class="comment"># 查看历史服务：http://hadoop102:18080</span></span><br><span class="line"><span class="comment"># 这里我不知道为什么，使用集群就报错无法解析mycluster，所以我用了hadoop102:8020</span></span><br></pre></td></tr></table></figure>
<h3 id="2-4-配置高可用（-Standalone-HA）">2.4 配置高可用（ Standalone +HA）</h3>
<p>所谓的高可用是因为当前集群中的 Master 节点只有一个，所以会存在单点故障问题。所以为了解决单点故障问题，需要在集群中配置多个 Master 节点，一旦处于活动状态的 Master 发生故障时，由备用 Master 提供服务，保证作业可以继续执行。这里的高可用一般采用Zookeeper 设置</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里使用了zk脚本，设置两个master，三台机器都要配置好zk</span></span><br><span class="line">zk.sh start</span><br><span class="line"><span class="comment"># 停止集群</span></span><br><span class="line">sbin/stop-all.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 spark-env.sh 文件添加如下配置</span></span><br><span class="line"><span class="comment"># 注 释 如 下 内 容 ： </span></span><br><span class="line"><span class="comment">#SPARK_MASTER_HOST=hadoop102</span></span><br><span class="line"><span class="comment">#SPARK_MASTER_PORT=7077</span></span><br><span class="line"><span class="comment"># 添加如下内容:</span></span><br><span class="line"><span class="comment">#Master 监控页面默认访问端口为 8080，但是可能会和 Zookeeper 冲突，所以改成 8989，也可以自定义，访问 UI 监控页面时请注意</span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8989</span><br><span class="line"><span class="built_in">export</span> SPARK_DAEMON_JAVA_OPTS=<span class="string">"</span></span><br><span class="line"><span class="string">-Dspark.deploy.recoveryMode=ZOOKEEPER</span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.url=hadoop102,hadoop103,hadoop104</span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.dir=/spark"</span></span><br><span class="line"><span class="comment"># 分发配置文件</span></span><br><span class="line">xsync conf/</span><br><span class="line">sbin/start-all.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 hadoop103的单独 Master 节点，此时 hadoop103节点 Master 状态处于备用状态</span></span><br><span class="line">sbin/start-master.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交应用</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077,hadoop103:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br></pre></td></tr></table></figure>
<h3 id="2-5-Yarn-模式">2.5 Yarn 模式</h3>
<p>独立部署（Standalone）模式由 Spark 自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是你也要记住，Spark 主要是计算框架，而不是资源调度框架，所以本身提供的资源调度并不是它的强项，所以还是和其他专业的资源调度框架集成会更靠谱一些。所以接下来我们来学习在强大的Yarn 环境下 Spark 是如何工作的（其实是因为在国内工作中，Yarn 使用的非常多）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改 hadoop 配置文件/opt/ha/hadoop-3.1.3/etc/hadoop/yarn-site.xml,  并分发</span></span><br><span class="line">vim /opt/ha/hadoop-3.1.3/etc/hadoop/yarn-site.xml</span><br><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是 <span class="literal">true</span> --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是 <span class="literal">true</span> --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 conf/spark-env.sh，添加 JAVA_HOME 和YARN_CONF_DIR 配置</span></span><br><span class="line">mv spark-env.sh.template spark-env.sh</span><br><span class="line"><span class="comment"># 添加以下内容，hadoop天自己的，我这里是ha高可用地址，其他例如YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_121</span><br><span class="line">YARN_CONF_DIR=/opt/ha/hadoop-3.1.3/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 之前要启动 HDFS 以及 YARN 集群</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br></pre></td></tr></table></figure>
<p>配置历史服务器，之前一样</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改 spark-defaults.conf.template 文件名为 spark-defaults.conf</span></span><br><span class="line"><span class="comment"># 其他一样，主要修改 spark-defaults.conf</span></span><br><span class="line">spark.yarn.historyServer.address=hadoop102:18080</span><br><span class="line">spark.history.ui.port=18080</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启history，主要在yarn调度界面就可以直接从历史记录跳转到spark的历史记录了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面会把结果显示在控制台</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br></pre></td></tr></table></figure>
<h3 id="2-6-K8S-Mesos-模式">2.6 K8S &amp; Mesos 模式</h3>
<p>Mesos 是Apache 下的开源分布式资源管理框架，它被称为是分布式系统的内核,在Twitter 得到广泛使用,管理着 Twitter 超过 30,0000 台服务器上的应用部署，但是在国内，依然使用着传统的Hadoop 大数据框架，所以国内使用 Mesos 框架的并不多</p>
<p>容器化部署是目前业界很流行的一项技术，基于Docker 镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是Kubernetes（k8s），而 Spark 也在最近的版本中支持了k8s 部署模式。可以参考：<a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" target="_blank" rel="noopener" title="https://spark.apache.org/docs/latest/running-on-kubernetes.html">https://spark.apache.org/docs/latest/running-on-kubernetes.html</a></p>
<h3 id="2-7-Windows-模式">2.7 Windows 模式</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://archive.apache.org/dist/spark/spark-3.0.0/</span></span><br><span class="line"><span class="comment"># 将文件 spark-3.0.0-bin-hadoop3.2.tgz 解压缩到无中文无空格的路径中</span></span><br><span class="line"><span class="comment"># 执行解压缩文件路径下 bin 目录中的 spark-shell.cmd 文件，启动 Spark 本地环境</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 bin 目录中创建 input 目录，并添加word.txt 文件, 在命令行中输入脚本代码</span></span><br><span class="line"><span class="comment"># 命令行提交应用</span></span><br><span class="line">spark-submit --class org.apache.spark.examples.SparkPi --master <span class="built_in">local</span>[2] ../examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br></pre></td></tr></table></figure>
<h3 id="2-8-部署模式对比">2.8 部署模式对比</h3>
<table>
<thead>
<tr>
<th><strong>模式</strong></th>
<th><strong>Spark 安装机器数</strong></th>
<th><strong>需启动的进程</strong></th>
<th><strong>所属者</strong></th>
<th><strong>应用场景</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Local</td>
<td>1</td>
<td>无</td>
<td>Spark</td>
<td>测试</td>
</tr>
<tr>
<td>Standalone</td>
<td>3</td>
<td>Master 及 Worker</td>
<td>Spark</td>
<td>单独部署</td>
</tr>
<tr>
<td>Yarn</td>
<td>1</td>
<td>Yarn 及 HDFS</td>
<td>Hadoop</td>
<td>混合部署</td>
</tr>
</tbody>
</table>
<h3 id="2-9-端口号">2.9 端口号</h3>
<ul>
<li>Spark 查看当前 Spark-shell 运行任务情况端口号：4040（计算）</li>
<li>Spark Master 内部通信服务端口号：7077</li>
<li>Standalone 模式下，Spark Master Web 端口号：8080（资源）</li>
<li>Spark 历史服务器端口号：18080</li>
<li>Hadoop YARN 任务运行情况查看端口号：8088</li>
</ul>
<h2 id="3、Spark-运行架构">3、Spark 运行架构</h2>
<h3 id="3-1-运行架构">3.1 运行架构</h3>
<p>Spark 框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。如下图所示，它展示了一个 Spark 执行时的基本结构。图形中的Driver 表示 master，负责管理整个集群中的作业任务调度。图形中的Executor 则是 slave，负责实际执行任务</p>
<p><img src="http://qnypic.shawncoding.top/blog/202402291816075.png" alt></p>
<h3 id="3-2-核心组件">3.2 核心组件</h3>
<p><strong>Driver</strong></p>
<p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。Driver 在Spark 作业执行时主要负责：</p>
<ul>
<li>将用户程序转化为作业（job）</li>
<li>在 Executor 之间调度任务(task)</li>
<li>跟踪Executor 的执行情况</li>
<li>通过UI 展示查询运行情况</li>
</ul>
<p>实际上，我们无法准确地描述Driver 的定义，因为在整个的编程过程中没有看到任何有关Driver 的字眼。所以简单理解，所谓的 Driver 就是驱使整个应用运行起来的程序，也称之为Driver 类。</p>
<p><strong>Executor</strong></p>
<p>Spark Executor 是集群中工作节点（Worker）中的一个 JVM 进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立。Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行。Executor 有两个核心功能：</p>
<ul>
<li>负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程</li>
<li>它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算</li>
</ul>
<p><strong>Master</strong> <strong>&amp;</strong> <strong>Worker</strong></p>
<p>Spark 集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master 和 Worker，这里的 Master 是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于 Yarn 环境中的 RM, 而Worker 呢，也是进程，一个 Worker 运行在集群中的一台服务器上，由 Master 分配资源对数据进行并行的处理和计算，类似于 Yarn 环境中 NM。</p>
<p><strong>ApplicationMaster</strong></p>
<p>Hadoop 用户向 YARN 集群提交应用程序时,提交程序中应该包ApplicationMaster，用于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。说的简单点就是，ResourceManager（资源）和Driver（计算）之间的解耦合靠的就是ApplicationMaster</p>
<h3 id="3-3-核心概念">3.3 核心概念</h3>
<p>Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量</p>
<table>
<thead>
<tr>
<th><strong>名称</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>–num-executors</td>
<td>配置 Executor 的数量</td>
</tr>
<tr>
<td>–executor-memory</td>
<td>配置每个 Executor 的内存大小</td>
</tr>
<tr>
<td>–executor-cores</td>
<td>配置每个 Executor 的虚拟 CPU core 数量</td>
</tr>
</tbody>
</table>
<p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，记住，这里是并行，而不是并发。这里我们<strong>将整个集群并行执行任务的数量称之为并行度</strong>。</p>
<p>针对<strong>有向无环图（ DAG）</strong>，是由 Spark 程序直接映射成的数据流的高级抽象模型</p>
<h3 id="3-4-提交流程">3.4 提交流程</h3>
<p>开发人员根据需求写的应用程序通过 Spark 客户端提交给 Spark 运行环境执行计算的流程。在不同的部署环境中，这个提交过程基本相同，但是又有细微的区别，Spark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式：Client 和 Cluster。<strong>两种模式主要区别在于：Driver 程序的运行节点位置</strong></p>
<p><img src="http://qnypic.shawncoding.top/blog/202402291816201.png" alt></p>
<p><strong>Yarn Client 模式</strong></p>
<p>Client 模式将用于监控和调度的Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试</p>
<ul>
<li>Driver 在任务提交的本地机器上运行</li>
<li>Driver 启动后会和ResourceManager 通讯申请启动ApplicationMaster</li>
<li>ResourceManager 分配 container，在合适的NodeManager 上启动ApplicationMaster，负责向ResourceManager 申请 Executor 内存</li>
<li>ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后ApplicationMaster 在资源分配指定的NodeManager 上启动 Executor 进程</li>
<li>Executor 进程启动后会向Driver 反向注册，Executor 全部注册完成后Driver 开始执行main 函数</li>
<li>之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个stage 生成对应的TaskSet，之后将 task 分发到各个Executor 上执行</li>
</ul>
<p><strong>Yarn Cluster 模式(重要)</strong></p>
<p>Cluster 模式将用于监控和调度的 Driver 模块启动在Yarn 集群资源中执行。一般应用于实际生产环境</p>
<ul>
<li>在 YARN Cluster 模式下，任务提交后会和ResourceManager 通讯申请启动ApplicationMaster</li>
<li>随后ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 就是Driver</li>
<li>Driver 启动后向 ResourceManager 申请Executor 内存，ResourceManager 接到ApplicationMaster 的资源申请后会分配container，然后在合适的NodeManager 上启动Executor 进程</li>
<li>Executor 进程启动后会向Driver 反向注册，Executor 全部注册完成后Driver 开始执行main 函数，</li>
<li>之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个stage 生成对应的TaskSet，之后将 task 分发到各个Executor 上执行</li>
</ul>
<h1>二、Spark核心编程(Core)</h1>
<h2 id="1、概述">1、概述</h2>
<p>Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：</p>
<ul>
<li>RDD : 弹性分布式数据集</li>
<li>累加器：分布式共享只写变量</li>
<li>广播变量：分布式共享只读变量</li>
</ul>
<h2 id="2、RDD详解">2、RDD详解</h2>
<h3 id="2-1-RDD概述">2.1 RDD概述</h3>
<p>RDD（Resilient Distributed Dataset） 提供了一个抽象的数据模型，让我们不必担心底层数据的分布式特性，只需将具体的应用逻辑表达为一系列转换操作(函数)，不同 RDD 之间的转换操作之间还可以形成依赖关系，进而实现管道化，从而避免了中间结果的存储，大大降低了数据复制、磁盘 IO 和序列化开销，并且还提供了更多的 API(map/reduec/filter/groupBy)</p>
<p>RDD叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。</p>
<ul>
<li>弹性
<ul>
<li>存储的弹性：内存与磁盘的自动切换；</li>
<li>容错的弹性：数据丢失可以自动恢复；</li>
<li>计算的弹性：计算出错重试机制；</li>
<li>分片的弹性：可根据需要重新分片。</li>
</ul>
</li>
<li>分布式：数据存储在大数据集群不同节点上</li>
<li>数据集：RDD 封装了计算逻辑，并不保存数据</li>
<li>数据抽象：RDD 是一个抽象类，需要子类具体实现</li>
<li>不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD 里面封装计算逻辑</li>
<li>可分区、并行计算</li>
</ul>
<h3 id="2-2-RDD主要属性">2.2 RDD主要属性</h3>
<p>在源码中可以看到有对 RDD 介绍的注释</p>
<ul>
<li><em>A list of partitions</em> ： 一组分片(Partition)/一个分区(Partition)列表，即数据集的基本组成单位。 对于 RDD 来说，每个分片都会被一个计算任务处理，分片数决定并行度。 用户可以在创建 RDD 时指定 RDD 的分片个数，如果没有指定，那么就会采用默认值</li>
<li><em>A function for computing each split</em> ： 一个函数会被作用在每一个分区。 Spark 中 RDD 的计算是以分片为单位的，compute 函数会被作用到每个分区上</li>
<li><em>A list of dependencies on other RDDs</em> ： 一个 RDD 会依赖于其他多个 RDD。 RDD 的每次转换都会生成一个新的 RDD，所以 RDD 之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark 可以通过这个依赖关系重新计算丢失的分区数据，而不是对 RDD 的所有分区进行重新计算。(Spark 的容错机制)</li>
<li><em>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</em>： 可选项，对于 KV 类型的 RDD 会有一个 Partitioner，即 RDD 的分区函数，默认为 HashPartitioner。</li>
<li><em>Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</em>： 可选项,一个列表，存储存取每个 Partition 的优先位置(preferred location)。 对于一个 HDFS 文件来说，这个列表保存的就是每个 Partition 所在的块的位置。按照&quot;移动数据不如移动计算&quot;的理念，Spark 在进行任务调度的时候，会尽可能选择那些存有数据的 worker 节点来进行任务计算。</li>
</ul>
<h3 id="2-3-执行原理">2.3 执行原理</h3>
<p>Spark 框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。RDD 是 Spark 框架中用于数据处理的核心模型，接下来看看在 Yarn 环境中RDD 的工作原理:</p>
<ul>
<li>启动 Yarn 集群环境</li>
<li>Spark 通过申请资源创建调度节点和计算节点</li>
<li>Spark 框架根据需求将计算逻辑根据分区划分成不同的任务</li>
<li>调度节点将任务根据计算节点状态发送到对应的计算节点进行计算</li>
</ul>
<p>RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给Executor 节点执行计算</p>
<h2 id="3、RDD-API">3、RDD-API</h2>
<h3 id="3-1-RDD-的创建方式">3.1 RDD 的创建方式</h3>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// =================1)从集合（内存）中创建 RDD========</span></span><br><span class="line"><span class="comment">// TODO 准备环境</span></span><br><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RDD"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从内存中创建RDD，将内存中集合的数据作为处理的数据源</span></span><br><span class="line"><span class="keyword">val</span> seq = <span class="type">Seq</span>[<span class="type">Int</span>](<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment">// parallelize : 并行</span></span><br><span class="line"><span class="comment">//val rdd: RDD[Int] = sc.parallelize(seq)</span></span><br><span class="line"><span class="comment">// makeRDD方法在底层实现时其实就是调用了rdd对象的parallelize方法。</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(seq)</span><br><span class="line">rdd.collect().foreach(println)</span><br><span class="line"><span class="comment">// TODO 关闭环境</span></span><br><span class="line">sc.stop()</span><br><span class="line"></span><br><span class="line"><span class="comment">// ====================2)从外部存储（文件）创建RDD=========</span></span><br><span class="line"><span class="comment">// TODO 准备环境</span></span><br><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RDD"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// TODO 创建RDD</span></span><br><span class="line"><span class="comment">// 从文件中创建RDD，将文件中的数据作为处理的数据源</span></span><br><span class="line"><span class="comment">// path路径默认以当前环境的根路径为基准。可以写绝对路径，也可以写相对路径</span></span><br><span class="line"><span class="comment">//sc.textFile("D:\\mineworkspace\\idea\\classes\\atguigu-classes\\datas\\1.txt")</span></span><br><span class="line"><span class="comment">//val rdd: RDD[String] = sc.textFile("datas/1.txt")</span></span><br><span class="line"><span class="comment">// path路径可以是文件的具体路径，也可以目录名称</span></span><br><span class="line"><span class="comment">//val rdd = sc.textFile("datas")</span></span><br><span class="line"><span class="comment">// path路径还可以使用通配符 *</span></span><br><span class="line"><span class="comment">//val rdd = sc.textFile("datas/1*.txt")</span></span><br><span class="line"><span class="comment">// 读取的结果表示为元组，第一个元素表示文件路径，第二个元素表示文件内容</span></span><br><span class="line"><span class="comment">// val rdd = sc.wholeTextFiles("datas")</span></span><br><span class="line"><span class="comment">// path还可以是分布式存储系统路径：HDFS</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">"hdfs://hadoop102:8020/test.txt"</span>)</span><br><span class="line">rdd.collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// TODO 关闭环境</span></span><br><span class="line">sc.stop()</span><br><span class="line"></span><br><span class="line"><span class="comment">// ==============3)从其他 RDD 创建=========</span></span><br><span class="line"><span class="comment">//主要是通过一个RDD 运算完后，再产生新的RDD</span></span><br><span class="line"><span class="comment">// ===============直接创建 RDD（new）=========</span></span><br><span class="line"><span class="comment">// 使用 new 的方式直接构造RDD，一般由Spark 框架自身使用</span></span><br></pre></td></tr></table></figure>
<h3 id="3-2-RDD-并行度与分区">3.2 RDD 并行度与分区</h3>
<p>默认情况下，Spark 可以将一个作业切分多个任务后，发送给 Executor 节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建RDD 时指定</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO 准备环境</span></span><br><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RDD"</span>)</span><br><span class="line">sparkConf.set(<span class="string">"spark.default.parallelism"</span>, <span class="string">"5"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// TODO 创建RDD</span></span><br><span class="line"><span class="comment">// RDD的并行度 &amp; 分区</span></span><br><span class="line"><span class="comment">// makeRDD方法可以传递第二个参数，这个参数表示分区的数量</span></span><br><span class="line"><span class="comment">// 第二个参数可以不传递的，那么makeRDD方法会使用默认值 ： defaultParallelism（默认并行度）</span></span><br><span class="line"><span class="comment">//     scheduler.conf.getInt("spark.default.parallelism", totalCores)</span></span><br><span class="line"><span class="comment">//    spark在默认情况下，从配置对象中获取配置参数：spark.default.parallelism</span></span><br><span class="line"><span class="comment">//    如果获取不到，那么使用totalCores属性，这个属性取值为当前运行环境的最大可用核数</span></span><br><span class="line"><span class="comment">//val rdd = sc.makeRDD(List(1,2,3,4),2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Spark读取文件，底层其实使用的就是Hadoop的读取方式</span></span><br><span class="line"><span class="comment">// 分区数量的计算方式：</span></span><br><span class="line"><span class="comment">//    totalSize = 7</span></span><br><span class="line"><span class="comment">//    goalSize =  7 / 2 = 3（byte）</span></span><br><span class="line"><span class="comment">//    7 / 3 = 2...1 (1.1) + 1 = 3(分区)</span></span><br><span class="line"><span class="comment">// 1. 数据以行为单位进行读取</span></span><br><span class="line"><span class="comment">//    spark读取文件，采用的是hadoop的方式读取，所以一行一行读取，和字节数没有关系</span></span><br><span class="line"><span class="comment">// 2. 数据读取时以偏移量为单位,偏移量不会被重复读取</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">   1@@   =&gt; 012</span></span><br><span class="line"><span class="comment">   2@@   =&gt; 345</span></span><br><span class="line"><span class="comment">   3     =&gt; 6</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 3. 数据分区的偏移量范围的计算</span></span><br><span class="line"><span class="comment">// 0 =&gt; [0, 3]  =&gt; 12</span></span><br><span class="line"><span class="comment">// 1 =&gt; [3, 6]  =&gt; 3</span></span><br><span class="line"><span class="comment">// 2 =&gt; [6, 7]  =&gt;</span></span><br><span class="line"><span class="comment">// val rdd = sc.textFile("datas/1.txt", 2)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将处理的数据保存成分区文件</span></span><br><span class="line">rdd.saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// TODO 关闭环境</span></span><br><span class="line">sc.stop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// =======================================</span></span><br><span class="line"><span class="comment">// 读取内存数据时，数据可以按照并行度的设定进行数据的分区操作，数据分区规则的Spark 核心源码如下</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positions</span></span>(length: <span class="type">Long</span>, numSlices: <span class="type">Int</span>): <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line"> (<span class="number">0</span> until numSlices).iterator.map &#123; i =&gt;</span><br><span class="line"> <span class="keyword">val</span> start = ((i * length) / numSlices).toInt</span><br><span class="line"> <span class="keyword">val</span> end = (((i + <span class="number">1</span>) * length) / numSlices).toInt</span><br><span class="line"> (start, end)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ====================================</span></span><br><span class="line"><span class="comment">// 读取文本时的分类规则，和内存有所不同</span></span><br><span class="line">public <span class="type">InputSplit</span>[] getSplits(<span class="type">JobConf</span> job, int numSplits)</span><br><span class="line">    <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line"></span><br><span class="line">    long totalSize = <span class="number">0</span>;                           <span class="comment">// compute total size</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">FileStatus</span> file: files) &#123;                <span class="comment">// check we have valid files</span></span><br><span class="line">      <span class="keyword">if</span> (file.isDirectory()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">"Not a file: "</span>+ file.getPath());</span><br><span class="line">      &#125;</span><br><span class="line">      totalSize += file.getLen();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    long goalSize = totalSize / (numSplits == <span class="number">0</span> ? <span class="number">1</span> : numSplits);</span><br><span class="line">    long minSize = <span class="type">Math</span>.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.</span><br><span class="line">      <span class="type">FileInputFormat</span>.<span class="type">SPLIT_MINSIZE</span>, <span class="number">1</span>), minSplitSize);</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">FileStatus</span> file: files) &#123;</span><br><span class="line">      ...</span><br><span class="line">        <span class="keyword">if</span> (isSplitable(fs, path)) &#123;</span><br><span class="line">          long blockSize = file.getBlockSize();</span><br><span class="line">          long splitSize = computeSplitSize(goalSize, minSize, blockSize);</span><br><span class="line">      ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span> long computeSplitSize(long goalSize, long minSize,long blockSize) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="type">Math</span>.max(minSize, <span class="type">Math</span>.min(goalSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-RDD-转换算子">3.3 RDD 转换算子</h3>
<p>RDD 的算子分为两类:</p>
<ul>
<li>Transformation_转换操作:<strong>返回一个新的 RDD</strong></li>
<li>Action动作操作:<strong>返回值不是 RDD(无返回值或返回其他的)</strong></li>
</ul>
<blockquote>
<p>❣️ 注意:<br>
1、RDD 不实际存储真正要计算的数据，而是记录了数据的位置在哪里，数据的转换关系(调用了什么方法，传入什么函数)。<br>
2、RDD 中的所有转换都是惰性求值/延迟执行的，也就是说并不会直接计算。只有当发生一个要求返回结果给 Driver 的 Action 动作时，这些转换才会真正运行。<br>
3、之所以使用惰性求值/延迟执行，是因为这样可以在 Action 时对 RDD 操作形成 DAG 有向无环图进行 Stage 的划分和并行优化，这种设计让 Spark 更加有效率地运行。</p>
</blockquote>
<p><strong>Transformation 转换算子</strong></p>
<table>
<thead>
<tr>
<th><strong>转换算子</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>map</strong>(func)</td>
<td>返回一个新的 RDD，该 RDD 由每一个输入元素经过 func 函数转换后组成</td>
</tr>
<tr>
<td><strong>filter</strong>(func)</td>
<td>返回一个新的 RDD，该 RDD 由经过 func 函数计算后返回值为 true 的输入元素组成；;当数据进行筛选过滤后，分区不变，但是分区内的数据可能不均衡，生产环境下，可能会出现数据倾斜</td>
</tr>
<tr>
<td><strong>flatMap</strong>(func)</td>
<td>类似于 map，但是每一个输入元素可以被映射为 0 或多个输出元素(所以 func 应该返回一个序列，而不是单一元素，)</td>
</tr>
<tr>
<td><strong>mapPartitions</strong>(func)</td>
<td>类似于 map，但独立地在 RDD 的每一个分片上运行，因此在类型为 T 的 RDD 上运行时，func 的函数类型必须是 Iterator[T] =&gt; Iterator[U]，性能高但容易内存溢出，内存少用map</td>
</tr>
<tr>
<td><strong>mapPartitionsWithIndex</strong>(func)</td>
<td>类似于 mapPartitions，但 func 带有一个整数参数表示分片的索引值，因此在类型为 T 的 RDD 上运行时，func 的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]</td>
</tr>
<tr>
<td>glom(func)</td>
<td>将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变</td>
</tr>
<tr>
<td>groupBy(func)</td>
<td>将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为 shuffle。 groupBy[K](f: T =&gt; K)</td>
</tr>
<tr>
<td>sample(withReplacement, fraction, seed)</td>
<td>根据 fraction 指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed 用于指定随机数生成器种子</td>
</tr>
<tr>
<td><strong>union</strong>(otherDataset)</td>
<td>对源 RDD 和参数 RDD 求并集后返回一个新的 RDD</td>
</tr>
<tr>
<td>intersection(otherDataset)</td>
<td>对源 RDD 和参数 RDD 求交集后返回一个新的 RDD；subtract为差集；zip为拉链；sliding为滑窗</td>
</tr>
<tr>
<td><strong>distinct</strong>([numTasks]))</td>
<td>对源 RDD 进行去重后返回一个新的 RDD</td>
</tr>
<tr>
<td>partitionBy(partitioner)</td>
<td>将数据按照指定 Partitioner 重新进行分区。Spark 默认的分区器是 HashPartitioner</td>
</tr>
<tr>
<td><strong>groupByKey</strong>([numTasks])</td>
<td>在一个(K,V)的 RDD 上调用,将数据源的数据根据 key 对 value 进行分组(相同的key把value放一起，group by的话还是会包括key,value)，返回一个(K, Iterator[V])的 RDD，推荐使用</td>
</tr>
<tr>
<td><strong>reduceByKey</strong>(func, [numTasks])</td>
<td>在一个(K,V)的 RDD 上调用，返回一个(K,V)的 RDD，使用指定的 reduce 函数，将相同 key 的值聚合到一起，与 groupByKey 类似，reduce 任务的个数可以通过第二个可选的参数来设置(有效减少落盘)</td>
</tr>
<tr>
<td>aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])</td>
<td>对 PairRDD 中相同的 Key 值进行聚合操作，在聚合过程中同样使用了一个中立的初始值。和 aggregate 函数类似，aggregateByKey 返回值的类型不需要和 RDD 中 value 的类型一致(第一个函数是分区内，第二个是分区间)</td>
</tr>
<tr>
<td>foldByKey(zeroValue)(func)</td>
<td>当分区内计算规则和分区间计算规则相同时，aggregateByKey 就可以简化为 foldByKey</td>
</tr>
<tr>
<td><strong>combineByKey</strong>(func1,func2,func3)</td>
<td>最通用的对 key-value 型 rdd 进行聚集操作的聚集函（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致。参数一将相同key的第一个数据进行结构的转换实现操作，参数二表示分区内的计算规则，参数三表示分区间的计算规则;上面几个底层调用都是这个函数</td>
</tr>
<tr>
<td><strong>sortByKey</strong>([ascending], [numTasks])</td>
<td>在一个(K,V)的 RDD 上调用，K 必须实现 Ordered 接口，返回一个按照 key 进行排序的(K,V)的 RDD</td>
</tr>
<tr>
<td>sortBy(func,[ascending], [numTasks])</td>
<td>与 sortByKey 类似，但是更灵活，存在shuffle，默认为升序排列。排序后新产生的 RDD 的分区数与原 RDD 的分区数一致</td>
</tr>
<tr>
<td><strong>join</strong>(otherDataset, [numTasks])</td>
<td>在类型为(K,V)和(K,W)的 RDD 上调用，返回一个相同 key 对应的所有元素对在一起的(K,(V,W))的 RDD，还有leftOuterJoin函数</td>
</tr>
<tr>
<td>cogroup(otherDataset, [numTasks])</td>
<td>在类型为(K,V)和(K,W)的 RDD 上调用，返回一个(K,(Iterable,Iterable))类型的 RDD</td>
</tr>
<tr>
<td>cartesian(otherDataset)</td>
<td>笛卡尔积</td>
</tr>
<tr>
<td>pipe(command, [envVars])</td>
<td>对 rdd 进行管道操作</td>
</tr>
<tr>
<td><strong>coalesce</strong>(numPartitions,isShuffle)</td>
<td>减少 RDD 的分区数到指定值。在过滤大量数据之后，可以执行此操作，默认是不shuffle</td>
</tr>
<tr>
<td><strong>repartition</strong>(numPartitions)</td>
<td>重新给 RDD 分区，一般用来扩大分区</td>
</tr>
</tbody>
</table>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 一个举例，统计出每一个省份每个广告被点击数量排行的 Top3</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"Operator"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 案例实操</span></span><br><span class="line">    <span class="comment">// 1. 获取原始数据：时间戳，省份，城市，用户，广告</span></span><br><span class="line">    <span class="keyword">val</span> dataRDD = sc.textFile(<span class="string">"datas/agent.log"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 将原始数据进行结构的转换。方便统计</span></span><br><span class="line">    <span class="comment">//    时间戳，省份，城市，用户，广告</span></span><br><span class="line">    <span class="comment">//    =&gt;</span></span><br><span class="line">    <span class="comment">//    ( ( 省份，广告 ), 1 )</span></span><br><span class="line">    <span class="keyword">val</span> mapRDD = dataRDD.map(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">" "</span>)</span><br><span class="line">            (( datas(<span class="number">1</span>), datas(<span class="number">4</span>) ), <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 将转换结构后的数据，进行分组聚合</span></span><br><span class="line">    <span class="comment">//    ( ( 省份，广告 ), 1 ) =&gt; ( ( 省份，广告 ), sum )</span></span><br><span class="line">    <span class="keyword">val</span> reduceRDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = mapRDD.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 将聚合的结果进行结构的转换</span></span><br><span class="line">    <span class="comment">//    ( ( 省份，广告 ), sum ) =&gt; ( 省份, ( 广告, sum ) )</span></span><br><span class="line">    <span class="keyword">val</span> newMapRDD = reduceRDD.map&#123;</span><br><span class="line">        <span class="keyword">case</span> ( (prv, ad), sum ) =&gt; &#123;</span><br><span class="line">            (prv, (ad, sum))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 将转换结构后的数据根据省份进行分组</span></span><br><span class="line">    <span class="comment">//    ( 省份, 【( 广告A, sumA )，( 广告B, sumB )】 )</span></span><br><span class="line">    <span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = newMapRDD.groupByKey()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6. 将分组后的数据组内排序（降序），取前3名</span></span><br><span class="line">    <span class="keyword">val</span> resultRDD = groupRDD.mapValues(</span><br><span class="line">        iter =&gt; &#123;</span><br><span class="line">            iter.toList.sortBy(_._2)(<span class="type">Ordering</span>.<span class="type">Int</span>.reverse).take(<span class="number">3</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 7. 采集数据打印在控制台</span></span><br><span class="line">    resultRDD.collect().foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-4-RDD-行动算子">3.4 RDD 行动算子</h3>
<p><strong>动作算子</strong></p>
<table>
<thead>
<tr>
<th><strong>动作算子</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>reduce(func)</td>
<td>通过 func 函数聚集 RDD 中的所有元素，这个功能必须是可交换且可并联的；聚集 RDD 中的所有元素，先聚合分区内数据，再聚合分区间数据</td>
</tr>
<tr>
<td>collect()</td>
<td>在驱动程序中，以数组的形式返回数据集的所有元素</td>
</tr>
<tr>
<td>count()</td>
<td>返回 RDD 的元素个数</td>
</tr>
<tr>
<td>first()</td>
<td>返回 RDD 的第一个元素(类似于 take(1))</td>
</tr>
<tr>
<td>take(n)</td>
<td>返回一个由数据集的前 n 个元素组成的数组</td>
</tr>
<tr>
<td>takeSample(withReplacement,num, [seed])</td>
<td>返回一个数组，该数组由从数据集中随机采样的 num 个元素组成，可以选择是否用随机数替换不足的部分，seed 用于指定随机数生成器种子</td>
</tr>
<tr>
<td>takeOrdered(n, [ordering])</td>
<td>返回自然顺序或者自定义顺序的前 n 个元素</td>
</tr>
<tr>
<td>aggregate(zeroValue)(U,T)</td>
<td>分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合</td>
</tr>
<tr>
<td>fold(zeroValue)(U,T)</td>
<td>折叠操作，aggregate 的简化版操作</td>
</tr>
<tr>
<td><strong>countByKey</strong>()</td>
<td>针对(K,V)类型的 RDD，返回一个(K,Int)的 map，表示每一个 key 对应的元素个数</td>
</tr>
<tr>
<td><strong>saveAsTextFile</strong>(path)</td>
<td>将数据集的元素以 textfile 的形式保存到 HDFS 文件系统或者其他支持的文件系统，对于每个元素，Spark 将会调用 toString 方法，将它装换为文件中的文本</td>
</tr>
<tr>
<td><strong>saveAsSequenceFile</strong>(path)</td>
<td>将数据集中的元素以 Hadoop sequencefile 的格式保存到指定的目录下，可以使 HDFS 或者其他 Hadoop 支持的文件系统，必须键值对</td>
</tr>
<tr>
<td>saveAsObjectFile(path)</td>
<td>将数据集的元素，以 Java 序列化的方式保存到指定的目录下</td>
</tr>
<tr>
<td>foreach(func)</td>
<td>在数据集的每一个元素上，运行函数 func 进行更新；rdd.collect().foreach(println)是driver端执行，而rdd.foreach(println)是在excutor端执行</td>
</tr>
<tr>
<td><strong>foreachPartition</strong>(func)</td>
<td>在数据集的每一个分区上，运行函数 func</td>
</tr>
</tbody>
</table>
<p><em><strong>统计操作</strong></em></p>
<table>
<thead>
<tr>
<th><strong>算子</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>count</td>
<td>个数</td>
</tr>
<tr>
<td>mean</td>
<td>均值</td>
</tr>
<tr>
<td>sum</td>
<td>求和</td>
</tr>
<tr>
<td>max</td>
<td>最大值</td>
</tr>
<tr>
<td>min</td>
<td>最小值</td>
</tr>
<tr>
<td><em>variance</em></td>
<td>方差</td>
</tr>
<tr>
<td>sampleVariance</td>
<td>从采样中计算方差</td>
</tr>
<tr>
<td><em>stdev</em></td>
<td>标准差:衡量数据的离散程度</td>
</tr>
<tr>
<td>sampleStdev</td>
<td>采样的标准差</td>
</tr>
<tr>
<td>stats</td>
<td>查看统计结果</td>
</tr>
</tbody>
</table>
<h3 id="3-5-RDD-的持久化-缓存">3.5 RDD 的持久化/缓存</h3>
<p>RDD 通过 Cache 或者 Persist 方法将前面的计算结果缓存，默认情况下会把数据以缓存在 JVM 的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的 action 算子时，该 RDD 将会被缓存在计算节点的内存中，并供后面重用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cache 操作会增加血缘关系，不改变原有的血缘关系</span></span><br><span class="line">println(wordToOneRdd.toDebugString)</span><br><span class="line"><span class="comment">// 数据缓存</span></span><br><span class="line">wordToOneRdd.cache()</span><br><span class="line"><span class="comment">// 可以更改存储级别，持久化操作必须在行动算子执行时完成的</span></span><br><span class="line"><span class="comment">//mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 默认的存储级别都是仅在内存存储一份，Spark 的存储级别还有好多种，存储级别在 object StorageLevel 中定义的</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>持久化级别</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MORY_ONLY(默认)</strong></td>
<td>将 RDD 以非序列化的 Java 对象存储在 JVM 中。 如果没有足够的内存存储 RDD，则某些分区将不会被缓存，每次需要时都会重新计算。 这是默认级别</td>
</tr>
<tr>
<td><strong>MORY_AND_DISK(开发中可以使用这个)</strong></td>
<td>将 RDD 以非序列化的 Java 对象存储在 JVM 中。如果数据在内存中放不下，则溢写到磁盘上．需要时则会从磁盘上读取</td>
</tr>
<tr>
<td>MEMORY_ONLY_SER (Java and Scala)</td>
<td>将 RDD 以序列化的 Java 对象(每个分区一个字节数组)的方式存储．这通常比非序列化对象(deserialized objects)更具空间效率，特别是在使用快速序列化的情况下，但是这种方式读取数据会消耗更多的 CPU</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_SER (Java and Scala)</td>
<td>与 MEMORY_ONLY_SER 类似，但如果数据在内存中放不下，则溢写到磁盘上，而不是每次需要重新计算它们</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>将 RDD 分区存储在磁盘上</td>
</tr>
<tr>
<td>MEMORY_ONLY_2, MEMORY_AND_DISK_2 等</td>
<td>与上面的储存级别相同，只不过将持久化数据存为两份，备份每个分区存储在两个集群节点上</td>
</tr>
<tr>
<td>OFF_HEAP(实验中)</td>
<td>与 MEMORY_ONLY_SER 类似，但将数据存储在堆外内存中。 (即不是直接存储在 JVM 内存中)</td>
</tr>
</tbody>
</table>
<p>RDD 持久化/缓存的目的是为了提高后续操作的速度；缓存的级别有很多，默认只存在内存中,开发中使用 memory_and_disk；只有执行 action 操作的时候才会真正将 RDD 数据进行持久化/缓存；实际开发中如果某一个 RDD 后续会被频繁的使用，可以将该 RDD 进行持久化/缓存</p>
<h3 id="3-6-RDD-容错机制-Checkpoint">3.6 RDD 容错机制 Checkpoint</h3>
<p>所谓的检查点其实就是通过将 RDD 中间结果写入磁盘由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销。对 RDD 进行 checkpoint 操作并不会马上被执行，必须执行 Action 操作才能触发</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cache : 将数据临时存储在内存中进行数据重用</span></span><br><span class="line"><span class="comment">//         会在血缘关系中添加新的依赖。一旦，出现问题，可以重头读取数据</span></span><br><span class="line"><span class="comment">// persist : 将数据临时存储在磁盘文件中进行数据重用</span></span><br><span class="line"><span class="comment">//           涉及到磁盘IO，性能较低，但是数据安全</span></span><br><span class="line"><span class="comment">//           如果作业执行完毕，临时保存的数据文件就会丢失</span></span><br><span class="line"><span class="comment">// checkpoint : 将数据长久地保存在磁盘文件中进行数据重用</span></span><br><span class="line"><span class="comment">//           涉及到磁盘IO，性能较低，但是数据安全</span></span><br><span class="line"><span class="comment">//           为了保证数据安全，所以一般情况下，会独立执行作业</span></span><br><span class="line"><span class="comment">//           为了能够提高效率，一般情况下，是需要和cache联合使用</span></span><br><span class="line"><span class="comment">//           执行过程中，会切断血缘关系。重新建立新的血缘关系</span></span><br><span class="line"><span class="comment">//           checkpoint等同于改变数据源</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">// 设置检查点路径</span></span><br><span class="line">sc.setCheckpointDir(<span class="string">"./checkpoint1"</span>)</span><br><span class="line"><span class="comment">//SparkContext.setCheckpointDir("目录") //HDFS的目录</span></span><br><span class="line"><span class="comment">// 创建一个 RDD，读取指定位置文件:hello atguigu atguigu</span></span><br><span class="line"><span class="keyword">val</span> lineRdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"input/1.txt"</span>)</span><br><span class="line"><span class="comment">// 业务逻辑</span></span><br><span class="line"><span class="keyword">val</span> wordRdd: <span class="type">RDD</span>[<span class="type">String</span>] = lineRdd.flatMap(line =&gt; line.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> wordToOneRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = wordRdd.map &#123;</span><br><span class="line">  word =&gt; &#123;</span><br><span class="line">    (word, <span class="type">System</span>.currentTimeMillis())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 增加缓存,避免再重新跑一个 job 做 checkpoint</span></span><br><span class="line">wordToOneRdd.cache()</span><br><span class="line"><span class="comment">// 数据检查点：针对 wordToOneRdd 做检查点计算</span></span><br><span class="line">wordToOneRdd.checkpoint()</span><br><span class="line"><span class="comment">// 触发执行逻辑</span></span><br><span class="line">wordToOneRdd.collect().foreach(println)</span><br></pre></td></tr></table></figure>
<p><strong>缓存和检查点区别</strong></p>
<ul>
<li>Cache 缓存只是将数据保存起来，不切断血缘依赖。Checkpoint 检查点切断血缘依赖</li>
<li>Cache 缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint 的数据通常存储在 HDFS 等容错、高可用的文件系统，可靠性高</li>
<li>建议对 checkpoint()的 RDD 使用 Cache 缓存，这样 checkpoint 的 job 只需从 Cache 缓存中读取数据即可，否则需要再从头计算一次 RDD</li>
</ul>
<h3 id="3-7-RDD-文件读取与保存">3.7 RDD 文件读取与保存</h3>
<p>Spark 的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。文件格式分为：text 文件、csv 文件、sequence 文件以及 Object 文件；文件系统分为：本地文件系统、HDFS、HBASE 以及数据库</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取输入文件</span></span><br><span class="line"><span class="keyword">val</span> inputRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"input/1.txt"</span>)</span><br><span class="line"><span class="comment">// 保存数据</span></span><br><span class="line">inputRDD.saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//quenceFile 文件是 Hadoop 用来存储二进制形式的 key-value 对而设计的一种平面文件(Flat File)</span></span><br><span class="line"><span class="comment">// 保存数据为 SequenceFile</span></span><br><span class="line">dataRDD.saveAsSequenceFile(<span class="string">"output"</span>)</span><br><span class="line"><span class="comment">// 读取 SequenceFile 文件</span></span><br><span class="line">sc.sequenceFile[<span class="type">Int</span>,<span class="type">Int</span>](<span class="string">"output"</span>).collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//对象文件是将对象序列化后保存的文件，采用 Java 的序列化机制</span></span><br><span class="line">dataRDD.saveAsObjectFile(<span class="string">"output"</span>)</span><br><span class="line"><span class="comment">// 读取数据</span></span><br><span class="line">sc.objectFile[<span class="type">Int</span>](<span class="string">"output"</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure>
<h2 id="4、RDD其他概念">4、RDD其他概念</h2>
<h3 id="4-1-RDD序列化">4.1 RDD序列化</h3>
<p><strong>闭包检查</strong></p>
<p>从计算的角度, 算子以外的代码都是在Driver 端执行, 算子里面的代码都是在Executor 端执行。那么在 scala 的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给Executor 端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。Scala2.12 版本后闭包编译方式发生了改变</p>
<p><strong>序列化方法和属性</strong></p>
<p>从计算的角度, 算子以外的代码都是在Driver 端执行, 算子里面的代码都是在Executor</p>
<p><strong>Kryo 序列化框架</strong></p>
<blockquote>
<p>参考地址: <a href="https://github.com/EsotericSoftware/kryo" target="_blank" rel="noopener" title="https://github.com/EsotericSoftware/kryo">https://github.com/EsotericSoftware/kryo</a></p>
</blockquote>
<p>Java 的序列化能够序列化任何的类。但是比较重（字节多），序列化后，对象的提交也比较大。Spark 出于性能的考虑，Spark2.0 开始支持另外一种Kryo 序列化机制。Kryo 速度是 Serializable 的 10 倍。当 RDD 在 Shuffle 数据的时候，简单数据类型、数组和字符串类型已经在 Spark 内部使用 Kryo 来序列化。注意：即使使用Kryo 序列化，也要继承Serializable 接口</p>
<h3 id="4-2-RDD-依赖关系">4.2 RDD 依赖关系</h3>
<p>RDD 的Lineage 会记录RDD 的元数据信息和转换行为，当该RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区，RDD存在<em>两种依赖关系类型</em>：  <strong>宽依赖</strong>(wide dependency/shuffle dependency) <strong>窄依赖</strong>(narrow dependency)</p>
<ul>
<li>窄依赖:父 RDD 的一个分区只会被子 RDD 的一个分区依赖；窄依赖的多个分区可以并行计算；  窄依赖的一个分区的数据如果丢失只需要重新计算对应的分区的数据就可以了</li>
<li>宽依赖:父 RDD 的一个分区会被子 RDD 的多个分区依赖(涉及到 shuffle)；对于宽依赖,必须等到上一阶段计算完成才能计算下一阶段</li>
</ul>
<p><img src="http://qnypic.shawncoding.top/blog/202402291816202.png" alt></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 查看血缘关系</span></span><br><span class="line">println(fileRDD.toDebugString)</span><br><span class="line"><span class="comment">//查看依赖</span></span><br><span class="line">println(wordRDD.dependencies)</span><br></pre></td></tr></table></figure>
<h3 id="4-3-DAG-的生成和划分-Stage">4.3 DAG 的生成和划分 Stage</h3>
<blockquote>
<p>DAG(Directed Acyclic Graph 有向无环图)指的是数据转换执行的过程，有方向，无闭环(其实就是 RDD 执行的流程)；  原始的 RDD 通过一系列的转换操作就形成了 DAG 有向无环图，任务执行时，可以按照 DAG 的描述，执行真正的计算(数据被操作的一个过程)</p>
</blockquote>
<p>对于DAG的边界，开始通过 <code>SparkContext</code> 创建的 RDD；触发 Action结束，一旦触发 Action 就形成了一个完整的 DAG。RDD 任务切分中间分为：Application、Job、Stage 和 Task，<strong>注意：Application-&gt;Job-&gt;Stage-&gt;Task 每一层都是 1 对 n 的关系</strong></p>
<ul>
<li>Application：初始化一个 SparkContext 即生成一个 Application；</li>
<li>Job：一个 Action 算子就会生成一个 Job；</li>
<li>Stage：Stage 等于宽依赖(ShuffleDependency)的个数加 1；</li>
<li>Task：一个 Stage 阶段中，最后一个 RDD 的分区个数就是 Task 的个数</li>
</ul>
<p><img src="http://qnypic.shawncoding.top/blog/202402291816203.png" alt></p>
<p><strong>一个 Spark 程序可以有多个 DAG(有几个 Action，就有几个 DAG，图最后只有一个 Action那么就是一个 DAG)</strong>。一个 DAG 可以有多个 Stage(根据宽依赖/shuffle 进行划分)。<strong>同一个 Stage 可以有多个 Task 并行执行</strong>(<strong>task 数=分区数</strong>，如图Stage1 中有三个分区 P1、P2、P3，对应的也有三个 Task)。可以看到这个 DAG 中只 reduceByKey 操作是一个宽依赖，Spark 内核会以此为边界将其前后划分成不同的 Stage。同时我们可以注意到，在图中 Stage1 中，<strong>从 textFile 到 flatMap 到 map 都是窄依赖，这几步操作可以形成一个流水线操作，通过 flatMap 操作生成的 partition 可以不用等待整个 RDD 计算结束，而是继续进行 map 操作，这样大大提高了计算的效率</strong></p>
<p><img src="http://qnypic.shawncoding.top/blog/202402291816204.png" alt></p>
<ul>
<li><strong>为什么要划分 Stage? --并行计算</strong></li>
</ul>
<p>一个复杂的业务逻辑如果有 shuffle，那么就意味着前面阶段产生结果后，才能执行下一个阶段，即下一个阶段的计算要依赖上一个阶段的数据。那么我们按照 shuffle 进行划分(也就是按照宽依赖就行划分)，就可以将一个 DAG 划分成多个 Stage/阶段，在同一个 Stage 中，会有多个算子操作，可以形成一个 pipeline 流水线，流水线内的多个平行的分区可以并行执行</p>
<ul>
<li><strong>如何划分 DAG 的 stage？</strong></li>
</ul>
<p>对于窄依赖，partition 的转换处理在 stage 中完成计算，不划分(将窄依赖尽量放在在同一个 stage 中，可以实现流水线计算)。对于宽依赖，由于有 shuffle 的存在，只能在父 RDD 处理完成后，才能开始接下来的计算，也就是说需要要划分 stage</p>
<p>**总结：**Spark 会根据 shuffle/宽依赖使用回溯算法来对 DAG 进行 Stage 划分，从后往前，遇到宽依赖就断开，遇到窄依赖就把当前的 RDD 加入到当前的 stage/阶段中</p>
<h3 id="4-4-RDD-分区器">4.4 RDD 分区器</h3>
<p>Spark 目前支持 Hash 分区和 Range 分区，和用户自定义分区。<strong>Hash 分区为当前的默认分区</strong>。分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分区，进而决定了 Reduce 的个数。</p>
<ul>
<li>Hash 分区：对于给定的 key，计算其 hashCode,并除以分区个数取余</li>
<li>Range 分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义分区</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark01_RDD_Part</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparConf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(</span><br><span class="line">            (<span class="string">"nba"</span>, <span class="string">"xxxxxxxxx"</span>),</span><br><span class="line">            (<span class="string">"cba"</span>, <span class="string">"xxxxxxxxx"</span>),</span><br><span class="line">            (<span class="string">"wnba"</span>, <span class="string">"xxxxxxxxx"</span>),</span><br><span class="line">            (<span class="string">"nba"</span>, <span class="string">"xxxxxxxxx"</span>),</span><br><span class="line">        ),<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">val</span> partRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = rdd.partitionBy( <span class="keyword">new</span> <span class="type">MyPartitioner</span> )</span><br><span class="line"></span><br><span class="line">        partRDD.saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line"></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 自定义分区器</span></span><br><span class="line"><span class="comment">      * 1. 继承Partitioner</span></span><br><span class="line"><span class="comment">      * 2. 重写方法</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span></span>&#123;</span><br><span class="line">        <span class="comment">// 分区数量</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据数据的key值返回数据所在的分区索引（从0开始）</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">            key <span class="keyword">match</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">"nba"</span> =&gt; <span class="number">0</span></span><br><span class="line">                <span class="keyword">case</span> <span class="string">"wnba"</span> =&gt; <span class="number">1</span></span><br><span class="line">                <span class="keyword">case</span> _ =&gt; <span class="number">2</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="5、RDD累加器">5、RDD累加器</h2>
<blockquote>
<p>累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回 Driver 端进行 merge</p>
</blockquote>
<p>系统累加器</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 系统累加器,不用累加器会发现连累加都无法正常使用</span></span><br><span class="line"><span class="comment">// 获取系统累加器</span></span><br><span class="line"><span class="comment">// Spark默认就提供了简单数据聚合的累加器</span></span><br><span class="line"><span class="keyword">val</span> sumAcc = sc.longAccumulator(<span class="string">"sum"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//sc.doubleAccumulator</span></span><br><span class="line"><span class="comment">//sc.collectionAccumulator</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mapRDD = rdd.map(</span><br><span class="line">    num =&gt; &#123;</span><br><span class="line">        <span class="comment">// 使用累加器</span></span><br><span class="line">        sumAcc.add(num)</span><br><span class="line">        num</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取累加器的值</span></span><br><span class="line"><span class="comment">// 少加：转换算子中调用累加器，如果没有行动算子的话，那么不会执行</span></span><br><span class="line"><span class="comment">// 多加：转换算子中调用累加器，如果没有行动算子的话，那么不会执行</span></span><br><span class="line"><span class="comment">// 一般情况下，累加器会放置在行动算子进行操作</span></span><br><span class="line">mapRDD.collect()</span><br><span class="line">mapRDD.collect()</span><br><span class="line">println(sumAcc.value)</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure>
<p>自定义累加器</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark04_Acc_WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"Acc"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparConf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"hello"</span>, <span class="string">"spark"</span>, <span class="string">"hello"</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 累加器 : WordCount</span></span><br><span class="line">        <span class="comment">// 创建累加器对象</span></span><br><span class="line">        <span class="keyword">val</span> wcAcc = <span class="keyword">new</span> <span class="type">MyAccumulator</span>()</span><br><span class="line">        <span class="comment">// 向Spark进行注册</span></span><br><span class="line">        sc.register(wcAcc, <span class="string">"wordCountAcc"</span>)</span><br><span class="line"></span><br><span class="line">        rdd.foreach(</span><br><span class="line">            word =&gt; &#123;</span><br><span class="line">                <span class="comment">// 数据的累加（使用累加器）</span></span><br><span class="line">                wcAcc.add(word)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取累加器累加的结果</span></span><br><span class="line">        println(wcAcc.value)</span><br><span class="line"></span><br><span class="line">        sc.stop()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">      自定义数据累加器：WordCount</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">      1. 继承AccumulatorV2, 定义泛型</span></span><br><span class="line"><span class="comment">         IN : 累加器输入的数据类型 String</span></span><br><span class="line"><span class="comment">         OUT : 累加器返回的数据类型 mutable.Map[String, Long]</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">      2. 重写方法（6）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]] </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">var</span> wcMap = mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断是否初始状态</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">            wcMap.isEmpty</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]] = &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">MyAccumulator</span>()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            wcMap.clear()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取累加器需要计算的值</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(word: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> newCnt = wcMap.getOrElse(word, <span class="number">0</span>L) + <span class="number">1</span></span><br><span class="line">            wcMap.update(word, newCnt)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Driver合并多个累加器</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> map1 = <span class="keyword">this</span>.wcMap</span><br><span class="line">            <span class="keyword">val</span> map2 = other.value</span><br><span class="line"></span><br><span class="line">            map2.foreach&#123;</span><br><span class="line">                <span class="keyword">case</span> ( word, count ) =&gt; &#123;</span><br><span class="line">                    <span class="keyword">val</span> newCount = map1.getOrElse(word, <span class="number">0</span>L) + count</span><br><span class="line">                    map1.update(word, newCount)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 累加器结果</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = &#123;</span><br><span class="line">            wcMap</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="6、RDD广播变量">6、RDD广播变量</h2>
<p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BroadcastVariablesTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"wc"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    sc.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//不使用广播变量</span></span><br><span class="line">    <span class="keyword">val</span> kvFruit: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">"apple"</span>),(<span class="number">2</span>,<span class="string">"orange"</span>),(<span class="number">3</span>,<span class="string">"banana"</span>),(<span class="number">4</span>,<span class="string">"grape"</span>)))</span><br><span class="line">    <span class="keyword">val</span> fruitMap: collection.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">String</span>] =kvFruit.collectAsMap</span><br><span class="line">    <span class="comment">//scala.collection.Map[Int,String] = Map(2 -&gt; orange, 4 -&gt; grape, 1 -&gt; apple, 3 -&gt; banana)</span></span><br><span class="line">    <span class="keyword">val</span> fruitIds: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.parallelize(<span class="type">List</span>(<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">    <span class="comment">//根据水果编号取水果名称</span></span><br><span class="line">    <span class="keyword">val</span> fruitNames: <span class="type">RDD</span>[<span class="type">String</span>] = fruitIds.map(x=&gt;fruitMap(x))</span><br><span class="line">    fruitNames.foreach(println)</span><br><span class="line">    <span class="comment">//注意:以上代码看似一点问题没有,但是考虑到数据量如果较大,且Task数较多,</span></span><br><span class="line">    <span class="comment">//那么会导致,被各个Task共用到的fruitMap会被多次传输</span></span><br><span class="line">    <span class="comment">//应该要减少fruitMap的传输,一台机器上一个,被该台机器中的Task共用即可</span></span><br><span class="line">    <span class="comment">//如何做到?---使用广播变量</span></span><br><span class="line">    <span class="comment">//注意:广播变量的值不能被修改,如需修改可以将数据存到外部数据源,如MySQL、Redis</span></span><br><span class="line">    println(<span class="string">"====================="</span>)</span><br><span class="line">    <span class="keyword">val</span> <span class="type">BroadcastFruitMap</span>: <span class="type">Broadcast</span>[collection.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">String</span>]] = sc.broadcast(fruitMap)</span><br><span class="line">    <span class="keyword">val</span> fruitNames2: <span class="type">RDD</span>[<span class="type">String</span>] = fruitIds.map(x=&gt;<span class="type">BroadcastFruitMap</span>.value(x))</span><br><span class="line">    fruitNames2.foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>三、Spark SQL</h1>
<h2 id="1、SparkSQL-概述">1、SparkSQL 概述</h2>
<blockquote>
<p>Spark SQL 是Spark 用于结构化数据(structured data)处理的 Spark 模块</p>
</blockquote>
<h3 id="1-1-SparkSQL的发展">1.1 SparkSQL的发展</h3>
<p><img src="http://qnypic.shawncoding.top/blog/202402291816205.jpg" alt></p>
<ul>
<li>
<p><strong>Hive</strong></p>
<p>Hive 实现了 SQL on Hadoop，使用 MapReduce 执行任务 简化了 MapReduce 任务。但Hive 的查询延迟比较高，原因是使用 MapReduce 做计算。</p>
</li>
<li>
<p><strong>Shark</strong></p>
<p>Shark 改写 Hive 的物理执行计划， 使用 Spark 代替 MapReduce 物理引擎 使用列式内存存储。以上两点使得 Shark 的查询效率很高。但是Shark 执行计划的生成严重依赖 Hive，想要增加新的优化非常困难；Hive 是进程级别的并行，Spark 是线程级别的并行，所以 Hive 中很多线程不安全的代码不适用于 Spark；由于以上问题，Shark 维护了 Hive 的一个分支，并且无法合并进主线，难以为继；在 2014 年 7 月 1 日的 Spark Summit 上，Databricks 宣布终止对 Shark 的开发，将重点放到 Spark SQL 上，但也因此发展出两个支线：SparkSQL 和 Hive on Spark</p>
<p>其中 SparkSQL 作为 Spark 生态的一员继续发展，而不再受限于 Hive，只是兼容 Hive；而Hive on Spark 是一个Hive 的发展计划，该计划将 Spark 作为Hive 的底层引擎之一，也就是说，Hive 将不再受限于一个引擎，可以采用 Map-Reduce、Tez、Spark 等引擎</p>
</li>
<li>
<p><strong>SparkSQL-DataFrame</strong></p>
<p>Spark SQL 执行计划和优化交给优化器 Catalyst；内建了一套简单的 SQL 解析器，可以不使用 HQL；还引入和 DataFrame 这样的 DSL API，完全可以不依赖任何 Hive 的组件。但是对于初期版本的 SparkSQL，依然有挺多问题，例如只能支持 SQL 的使用，不能很好的兼容命令式，入口不够统一等。</p>
</li>
<li>
<p><strong>SparkSQL-Dataset</strong></p>
<p>SparkSQL 在 1.6 时代，增加了一个新的 API，叫做 Dataset，Dataset 统一和结合了 SQL 的访问和命令式 API 的使用，这是一个划时代的进步。在 Dataset 中可以轻易的做到使用 SQL 查询并且筛选数据，然后使用命令式 API 进行探索式分析</p>
</li>
</ul>
<h3 id="1-2-Hive-and-SparkSQL">1.2 Hive and SparkSQL</h3>
<ul>
<li>Hive 是将 SQL 转为 MapReduce。</li>
<li>SparkSQL 可以理解成是将 SQL 解析成：“RDD + 优化” 再执行</li>
</ul>
<h3 id="1-3-SparkSQL-特点">1.3 SparkSQL 特点</h3>
<ul>
<li>易整合，无缝的整合了 SQL 查询和 Spark 编程</li>
<li>统一的数据访问，使用相同的方式连接不同的数据源</li>
<li>兼容 Hive，在已有的仓库上直接运行 SQL 或者 HiveQL</li>
<li>标准数据连接，通过 JDBC 或者 ODBC 来连接</li>
</ul>
<h3 id="1-4-DataFrame简介">1.4 DataFrame简介</h3>
<p>在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。<strong>DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame 所表示的二维表数据集的每一列都带有名称和类型</strong>。这使得 Spark SQL 得以洞察更多的结构信息，从而对藏于 DataFrame 背后的数据源以及作用于 DataFrame 之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观 RDD，由于无从得知所存数据元素的具体内部结构，Spark Core 只能在 stage 层面进行简单、通用的流水线优化。</p>
<p>DataFrame 的前身是 SchemaRDD，从 Spark 1.3.0 开始 SchemaRDD 更名为 DataFrame，并不再直接继承自 RDD，而是自己实现了 RDD 的绝大多数功能。同时，与Hive 类似，DataFrame 也支持嵌套数据类型（struct、array 和 map）。从 API 易用性的角度上看，DataFrame API 提供的是一套高层的关系操作，比函数式的 RDD API 要更加友好，门槛更低</p>
<p><strong>总结：DataFrame 就是一个分布式的表</strong>；<strong>DataFrame = RDD - 泛型 + SQL 的操作 + 优化</strong></p>
<h3 id="1-5-DataSet">1.5 DataSet</h3>
<p>DataSet 是分布式数据集合。DataSet 是Spark 1.6 中添加的一个新抽象，是DataFrame的一个扩展。它提供了RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及SparkSQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter等等）</p>
<ul>
<li>DataSet 是DataFrame API 的一个扩展，是SparkSQL 最新的数据抽象</li>
<li>用户友好的 API 风格，既具有类型安全检查也具有DataFrame 的查询优化特性；</li>
<li>用样例类来对DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet 中的字段名称；</li>
<li>DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]</li>
<li>DataFrame 是DataSet 的特列，DataFrame=DataSet[Row] ，所以可以通过 as 方法将DataFrame 转换为DataSet。Row 是一个类型，跟 Car、Person 这些的类型一样，所有的表结构信息都用 Row 来表示。获取数据时需要指定顺序</li>
</ul>
<h3 id="1-6-RDD、DataFrame、DataSet-的区别">1.6 RDD、DataFrame、DataSet 的区别</h3>
<p><img src="http://qnypic.shawncoding.top/blog/202402291816206.jpg" alt></p>
<ul>
<li>RDD[Person]：以 Person 为类型参数，但不了解 其内部结构</li>
<li>DataFrame：提供了详细的结构信息 schema 列的名称和类型。这样看起来就像一张表了</li>
<li>DataSet[Person]：不光有 schema 信息，还有类型信息</li>
</ul>
<p><strong>总结</strong></p>
<ul>
<li><strong>DataFrame = RDD - 泛型 + Schema + SQL + 优化</strong></li>
<li><strong>DataSet = DataFrame + 泛型</strong></li>
<li><strong>DataSet = RDD + Schema + SQL + 优化</strong></li>
</ul>
<h2 id="2、SparkSQL-核心编程">2、SparkSQL 核心编程</h2>
<h3 id="2-1-概述-v2">2.1 概述</h3>
<ul>
<li>
<p>在 spark2.0 版本之前</p>
<p>SQLContext 是创建 DataFrame 和执行 SQL 的入口。HiveContext 通过 hive sql 语句操作 hive 表数据，兼容 hive 操作，hiveContext 继承自 SQLContext。</p>
</li>
<li>
<p>在 spark2.0 之后</p>
<p>这些都统一于 SparkSession，SparkSession 封装了 SqlContext 及 HiveContext；实现了 SQLContext 及 HiveContext 所有功能；通过 SparkSession 还可以获取到 SparkConetxt</p>
</li>
</ul>
<h3 id="2-2-创建-DataFrame-DataSet">2.2 创建 DataFrame/DataSet</h3>
<p><strong>读取文本文件</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =====读取文本文件</span></span><br><span class="line"><span class="comment"># 在本地创建一个文件，有 id、name、age 三列，用空格分隔，然后上传到 hdfs 上</span></span><br><span class="line">vim /root/person.txt</span><br><span class="line">1 zhangsan 20</span><br><span class="line">2 lisi 29</span><br><span class="line">3 wangwu 25</span><br><span class="line">4 zhaoliu 30</span><br><span class="line">5 tianqi 35</span><br><span class="line">6 kobe 40</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开 spark-shell</span></span><br><span class="line"><span class="comment"># 创建 RDD</span></span><br><span class="line">spark/bin/spark-shell</span><br><span class="line"><span class="comment"># RDD[Array[String]]</span></span><br><span class="line">val lineRDD= sc.textFile(<span class="string">"hdfs://node1:8020/person.txt"</span>).map(_.split(<span class="string">" "</span>)) </span><br><span class="line"><span class="comment"># 定义 case class(相当于表的 schema)</span></span><br><span class="line"><span class="keyword">case</span> class Person(id:Int, name:String, age:Int)</span><br><span class="line"><span class="comment"># 将 RDD 和 case class 关联</span></span><br><span class="line"><span class="comment"># RDD[Person]</span></span><br><span class="line">val personRDD = lineRDD.map(x =&gt; Person(x(0).toInt, x(1), x(2).toInt))</span><br><span class="line"><span class="comment"># 将 RDD 转换成 DataFrame</span></span><br><span class="line"><span class="comment"># DataFrame</span></span><br><span class="line">val personDF = personRDD.toDF</span><br><span class="line"><span class="comment"># 查看数据和 schema</span></span><br><span class="line">personDF.show</span><br><span class="line">personDF.printSchema</span><br><span class="line"><span class="comment"># 注册表</span></span><br><span class="line">personDF.createOrReplaceTempView(<span class="string">"t_person"</span>)</span><br><span class="line"><span class="comment"># 执行 SQL</span></span><br><span class="line">spark.sql(<span class="string">"select id,name from t_person where id &gt; 3"</span>).show</span><br><span class="line"><span class="comment"># 也可以通过 SparkSession 构建 DataFrame</span></span><br><span class="line">val dataFrame=spark.read.text(<span class="string">"hdfs://node1:8020/person.txt"</span>)</span><br><span class="line"><span class="comment">#注意：直接读取的文本文件没有完整schema信息</span></span><br><span class="line">dataFrame.show</span><br><span class="line">dataFrame.printSchema</span><br></pre></td></tr></table></figure>
<p><strong>读取 json 文件</strong>和<strong>读取 parquet 文件</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val jsonDF= spark.read.json(<span class="string">"file:///resources/people.json"</span>)</span><br><span class="line"><span class="comment"># 接下来就可以使用 DataFrame 的函数操作</span></span><br><span class="line">jsonDF.show</span><br><span class="line"><span class="comment"># 注意：直接读取 json 文件有 schema 信息，因为 json 文件本身含有 Schema 信息，SparkSQL 可以自动解析</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取 parquet 文件</span></span><br><span class="line">val parquetDF=spark.read.parquet(<span class="string">"file:///resources/users.parquet"</span>)</span><br><span class="line">parquetDF.show</span><br><span class="line"><span class="comment"># 注意：直接读取 parquet 文件有 schema 信息，因为 parquet 文件中保存了列的信息</span></span><br></pre></td></tr></table></figure>
<h3 id="2-3-两种查询风格：DSL-和-SQL">2.3 两种查询风格：DSL 和 SQL</h3>
<p>首先进行准备工作,先读取文件并转换为 DataFrame 或 DataSet</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val lineRDD= sc.textFile("hdfs://node1:8020/person.txt").map(_.split(" "))</span><br><span class="line">case class Person(id:Int, name:String, age:Int)</span><br><span class="line">val personRDD = lineRDD.map(x =&gt; Person(x(0).toInt, x(1), x(2).toInt))</span><br><span class="line">val personDF = personRDD.toDF</span><br><span class="line">personDF.show</span><br><span class="line">//val personDS = personRDD.toDS</span><br><span class="line">//personDS.show</span><br></pre></td></tr></table></figure>
<p><strong>DSL 风格</strong>:SparkSQL 提供了一个领域特定语言(DSL)以方便操作结构化数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">// 查看 name 字段的数据</span><br><span class="line">personDF.select(personDF.col(<span class="string">"name"</span>)).show</span><br><span class="line">personDF.select(personDF(<span class="string">"name"</span>)).show</span><br><span class="line">personDF.select(<span class="keyword">col</span>(<span class="string">"name"</span>)).show</span><br><span class="line">personDF.select(<span class="string">"name"</span>).show</span><br><span class="line"></span><br><span class="line">// 查看 <span class="keyword">name</span> 和 age 字段数据</span><br><span class="line">personDF.select(<span class="string">"name"</span>, <span class="string">"age"</span>).show</span><br><span class="line"></span><br><span class="line">// 查询所有的 <span class="keyword">name</span> 和 age，并将 age+<span class="number">1</span></span><br><span class="line">personDF.select(personDF.col(<span class="string">"name"</span>), personDF.col(<span class="string">"age"</span>) + <span class="number">1</span>).show</span><br><span class="line">personDF.select(personDF(<span class="string">"name"</span>), personDF(<span class="string">"age"</span>) + <span class="number">1</span>).show</span><br><span class="line">personDF.select(<span class="keyword">col</span>(<span class="string">"name"</span>), <span class="keyword">col</span>(<span class="string">"age"</span>) + <span class="number">1</span>).show</span><br><span class="line">personDF.select(<span class="string">"name"</span>,<span class="string">"age"</span>).show</span><br><span class="line">//personDF.select(<span class="string">"name"</span>, <span class="string">"age"</span>+<span class="number">1</span>).show</span><br><span class="line">personDF.select($<span class="string">"name"</span>,$<span class="string">"age"</span>,$<span class="string">"age"</span>+<span class="number">1</span>).show</span><br><span class="line"></span><br><span class="line">// 过滤 age 大于等于 <span class="number">25</span> 的，使用 filter 方法过滤</span><br><span class="line">personDF.filter(<span class="keyword">col</span>(<span class="string">"age"</span>) &gt;= <span class="number">25</span>).show</span><br><span class="line">personDF.filter($<span class="string">"age"</span> &gt;<span class="number">25</span>).show</span><br><span class="line"></span><br><span class="line">// 统计年龄大于 <span class="number">30</span> 的人数</span><br><span class="line">personDF.filter(<span class="keyword">col</span>(<span class="string">"age"</span>)&gt;<span class="number">30</span>).count()</span><br><span class="line">personDF.filter($<span class="string">"age"</span> &gt;<span class="number">30</span>).count()</span><br><span class="line"></span><br><span class="line">// 按年龄进行分组并统计相同年龄的人数</span><br><span class="line">personDF.groupBy(<span class="string">"age"</span>).count().show</span><br></pre></td></tr></table></figure>
<p><strong>SQL 风格</strong>：DataFrame 的一个强大之处就是我们可以将它看作是一个关系型数据表，然后可以通过在程序中使用 spark.sql() 来执行 SQL 查询，结果将作为一个 DataFrame 返回</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// 如果想使用 SQL 风格的语法，需要将 DataFrame 注册成表,采用如下的方式</span><br><span class="line">personDF.createOrReplaceTempView("t_person")</span><br><span class="line">spark.sql("<span class="keyword">select</span> * <span class="keyword">from</span> t_person<span class="string">").show</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 显示表的描述信息</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">desc</span> t_person<span class="string">").show</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 查询年龄最大的前两名</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> * <span class="keyword">from</span> t_person <span class="keyword">order</span> <span class="keyword">by</span> age <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">2</span><span class="string">").show</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 查询年龄大于 30 的人的信息</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> * <span class="keyword">from</span> t_person <span class="keyword">where</span> age &gt; <span class="number">30</span> <span class="string">").show</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 使用 SQL 风格完成 DSL 中的需求</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> <span class="keyword">name</span>, age + <span class="number">1</span> <span class="keyword">from</span> t_person<span class="string">").show</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> <span class="keyword">name</span>, age <span class="keyword">from</span> t_person <span class="keyword">where</span> age &gt; <span class="number">25</span><span class="string">").show</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> <span class="keyword">count</span>(age) <span class="keyword">from</span> t_person <span class="keyword">where</span> age &gt; <span class="number">30</span><span class="string">").show</span></span><br><span class="line"><span class="string">spark.sql("</span><span class="keyword">select</span> age, <span class="keyword">count</span>(age) <span class="keyword">from</span> t_person <span class="keyword">group</span> <span class="keyword">by</span> age<span class="string">").show</span></span><br></pre></td></tr></table></figure>
<p><strong>总结</strong>：</p>
<ul>
<li><strong>DataFrame 和 DataSet 都可以通过 RDD 来进行创建</strong>；</li>
<li><strong>也可以通过读取普通文本创建–注意:直接读取没有完整的约束,需要通过 RDD+Schema</strong>；</li>
<li><strong>通过 josn/parquet 会有完整的约束</strong>；</li>
<li><strong>不管是 DataFrame 还是 DataSet 都可以注册成表，之后就可以使用 SQL 进行查询了! 也可以使用 DSL</strong>!</li>
</ul>
<h3 id="2-4-RDD、DataFrame、DataSet-三者的关系">2.4 RDD、DataFrame、DataSet 三者的关系</h3>
<p><img src="http://qnypic.shawncoding.top/blog/202402291816207.png" alt></p>
<p><strong>共性：</strong></p>
<ul>
<li>RDD、DataFrame、DataSet 全都是 spark 平台下的分布式弹性数据集，为处理超大型数据提供便利;</li>
<li>三者都有惰性机制，在进行创建、转换，如 map 方法时，不会立即执行，只有在遇到Action 如 foreach 时，三者才会开始遍历运算;</li>
<li>三者有许多共同的函数，如 filter，排序等;</li>
<li>在对DataFrame 和Dataset 进行操作许多操作都需要这个包:<code>import spark.implicits._</code>（在创建好 SparkSession 对象后尽量直接导入）</li>
<li>三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</li>
<li>三者都有 partition 的概念</li>
<li>DataFrame 和DataSet 均可使用模式匹配获取各个字段的值和类型</li>
</ul>
<p><strong>不同：</strong></p>
<ul>
<li>RDD，RDD 一般和 spark mllib 同时使用；RDD 不支持 sparksql 操作</li>
<li>DataFrame，与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值；DataFrame 与DataSet 一般不与 spark mllib 同时使用；DataFrame 与DataSet 均支持 SparkSQL 的操作，比如 select，groupby 之类，还能注册临时表/视窗，进行 sql 语句操作；DataFrame 与DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表头，这样每一列的字段名一目了然(后面专门讲解)</li>
<li>DataSet，Dataset 和DataFrame 拥有完全相同的成员函数，区别只是每一行的数据类型不同；DataFrame 其实就是DataSet 的一个特例 <code>type DataFrame = Dataset[Row]</code>；DataFrame 也可以叫Dataset[Row],每一行的类型是 Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者共性中的第七条提到的模式匹配拿出特定字段。而Dataset 中，每一行是什么类型是不一定的，在自定义了 case class 之后可以很自由的获得每一行的信息</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">object Spark01_SparkSQL_Basic &#123;</span><br><span class="line"></span><br><span class="line">    <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">        val sparkConf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">        val spark = SparkSession.builder().config(sparkConf).getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RDD &lt;=&gt; DataFrame</span></span><br><span class="line">        val rdd = spark.sparkContext.makeRDD(List((<span class="number">1</span>, <span class="string">"zhangsan"</span>, <span class="number">30</span>), (<span class="number">2</span>, <span class="string">"lisi"</span>, <span class="number">40</span>)))</span><br><span class="line">        val df: DataFrame = rdd.toDF(<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"age"</span>)</span><br><span class="line">        val rowRDD: RDD[Row] = df.rdd</span><br><span class="line"></span><br><span class="line">        <span class="comment">// DataFrame &lt;=&gt; DataSet</span></span><br><span class="line">        val ds: Dataset[User] = df.as[User]</span><br><span class="line">        val df1: DataFrame = ds.toDF()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RDD &lt;=&gt; DataSet</span></span><br><span class="line">        val ds1: Dataset[User] = rdd.map &#123;</span><br><span class="line">            <span class="keyword">case</span> (id, name, age) =&gt; &#123;</span><br><span class="line">                User(id, name, age)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;.toDS()</span><br><span class="line">        val userRDD: RDD[User] = ds1.rdd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 关闭环境</span></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">case</span> class <span class="title">User</span><span class="params">( id:Int, name:String, age:Int )</span></span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="3、Spark实战">3、Spark实战</h2>
<h3 id="3-1-Spark-SQL-完成-WordCount">3.1 Spark SQL 完成 WordCount</h3>
<p>引入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>SQL风格</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1.创建SparkSession</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[*]"</span>).appName(<span class="string">"SparkSQL"</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line">    sc.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line">    <span class="comment">//2.读取文件</span></span><br><span class="line">    <span class="keyword">val</span> fileDF: <span class="type">DataFrame</span> = spark.read.text(<span class="string">"D:\\data\\words.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> fileDS: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(<span class="string">"D:\\data\\words.txt"</span>)</span><br><span class="line">    <span class="comment">//fileDF.show()</span></span><br><span class="line">    <span class="comment">//fileDS.show()</span></span><br><span class="line">    <span class="comment">//3.对每一行按照空格进行切分并压平</span></span><br><span class="line">    <span class="comment">//fileDF.flatMap(_.split(" ")) //注意:错误,因为DF没有泛型,不知道_是String</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">Dataset</span>[<span class="type">String</span>] = fileDS.flatMap(_.split(<span class="string">" "</span>))<span class="comment">//注意:正确,因为DS有泛型,知道_是String</span></span><br><span class="line">    <span class="comment">//wordDS.show()</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    +-----+</span></span><br><span class="line"><span class="comment">    |value|</span></span><br><span class="line"><span class="comment">    +-----+</span></span><br><span class="line"><span class="comment">    |hello|</span></span><br><span class="line"><span class="comment">    |   me|</span></span><br><span class="line"><span class="comment">    |hello|</span></span><br><span class="line"><span class="comment">    |  you|</span></span><br><span class="line"><span class="comment">      ...</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//4.对上面的数据进行WordCount</span></span><br><span class="line">    wordDS.createOrReplaceTempView(<span class="string">"t_word"</span>)</span><br><span class="line">    <span class="keyword">val</span> sql =</span><br><span class="line">      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        |select value ,count(value) as count</span></span><br><span class="line"><span class="string">        |from t_word</span></span><br><span class="line"><span class="string">        |group by value</span></span><br><span class="line"><span class="string">        |order by count desc</span></span><br><span class="line"><span class="string">      "</span><span class="string">""</span>.stripMargin</span><br><span class="line">    spark.sql(sql).show()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>DSL 风格</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1.创建SparkSession</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[*]"</span>).appName(<span class="string">"SparkSQL"</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line">    sc.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line">    <span class="comment">//2.读取文件</span></span><br><span class="line">    <span class="keyword">val</span> fileDF: <span class="type">DataFrame</span> = spark.read.text(<span class="string">"D:\\data\\words.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> fileDS: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(<span class="string">"D:\\data\\words.txt"</span>)</span><br><span class="line">    <span class="comment">//fileDF.show()</span></span><br><span class="line">    <span class="comment">//fileDS.show()</span></span><br><span class="line">    <span class="comment">//3.对每一行按照空格进行切分并压平</span></span><br><span class="line">    <span class="comment">//fileDF.flatMap(_.split(" ")) //注意:错误,因为DF没有泛型,不知道_是String</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> wordDS: <span class="type">Dataset</span>[<span class="type">String</span>] = fileDS.flatMap(_.split(<span class="string">" "</span>))<span class="comment">//注意:正确,因为DS有泛型,知道_是String</span></span><br><span class="line">    <span class="comment">//wordDS.show()</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    +-----+</span></span><br><span class="line"><span class="comment">    |value|</span></span><br><span class="line"><span class="comment">    +-----+</span></span><br><span class="line"><span class="comment">    |hello|</span></span><br><span class="line"><span class="comment">    |   me|</span></span><br><span class="line"><span class="comment">    |hello|</span></span><br><span class="line"><span class="comment">    |  you|</span></span><br><span class="line"><span class="comment">      ...</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//4.对上面的数据进行WordCount</span></span><br><span class="line">    wordDS.groupBy(<span class="string">"value"</span>).count().orderBy($<span class="string">"count"</span>.desc).show()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-用户自定义函数">3.2 用户自定义函数</h3>
<p>用户可以通过 spark.udf 功能添加自定义函数，实现自定义功能</p>
<p>对于UDF，见如下基础功能</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// 1)创建DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">"datas/user.json"</span>)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"user"</span>)</span><br><span class="line">    <span class="comment">//2)注册UDF</span></span><br><span class="line">    spark.udf.register(<span class="string">"prefixName"</span>, (name:<span class="type">String</span>) =&gt; &#123;</span><br><span class="line">        <span class="string">"Name: "</span> + name</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//应用UDF</span></span><br><span class="line">    spark.sql(<span class="string">"select age, prefixName(username) from user"</span>).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>UDAF是User-Defined Aggregation Functions（用户自定义聚合函数），强类型的Dataset 和弱类型的 DataFrame 都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。通过继承 UserDefinedAggregateFunction 来实现用户自定义弱类型聚合函数。从Spark3.0 版本后，UserDefinedAggregateFunction 已经不推荐使用了。可以<strong>统一采用强类型聚合函数Aggregator</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03_SparkSQL_UDAF1</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> df = spark.read.json(<span class="string">"datas/user.json"</span>)</span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"user"</span>)</span><br><span class="line">        spark.udf.register(<span class="string">"ageAvg"</span>, functions.udaf(<span class="keyword">new</span> <span class="type">MyAvgUDAF</span>()))</span><br><span class="line">        spark.sql(<span class="string">"select ageAvg(age) from user"</span>).show</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 早期版本中，spark不能在sql中使用强类型UDAF操作</span></span><br><span class="line">        <span class="comment">// SQL &amp; DSL</span></span><br><span class="line">        <span class="comment">// 早期的UDAF强类型聚合函数使用DSL语法操作</span></span><br><span class="line">        <span class="comment">// val ds: Dataset[User] = df.as[User]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将UDAF函数转换为查询的列对象，注意下面的in要修改成User</span></span><br><span class="line">        <span class="comment">// val udafCol: TypedColumn[User, Long] = new MyAvgUDAF().toColumn</span></span><br><span class="line">        <span class="comment">// ds.select(udafCol).show</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// TODO 关闭环境</span></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     自定义聚合函数类：计算年龄的平均值</span></span><br><span class="line"><span class="comment">     1. 继承org.apache.spark.sql.expressions.Aggregator, 定义泛型</span></span><br><span class="line"><span class="comment">         IN : 输入的数据类型 Long</span></span><br><span class="line"><span class="comment">         BUF : 缓冲区的数据类型 Buff</span></span><br><span class="line"><span class="comment">         OUT : 输出的数据类型 Long</span></span><br><span class="line"><span class="comment">     2. 重写方法(6)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Buff</span>(<span class="params"> var total:<span class="type">Long</span>, var count:<span class="type">Long</span> </span>)</span></span><br><span class="line"><span class="class">    <span class="title">class</span> <span class="title">MyAvgUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Long</span>, <span class="type">Buff</span>, <span class="type">Long</span>]</span>&#123;</span><br><span class="line">        <span class="comment">// z &amp; zero : 初始值或零值</span></span><br><span class="line">        <span class="comment">// 缓冲区的初始化</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Buff</span> = &#123;</span><br><span class="line">            <span class="type">Buff</span>(<span class="number">0</span>L,<span class="number">0</span>L)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据输入的数据更新缓冲区的数据</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buff: <span class="type">Buff</span>, in: <span class="type">Long</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">            buff.total = buff.total + in</span><br><span class="line">            buff.count = buff.count + <span class="number">1</span></span><br><span class="line">            buff</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 合并缓冲区</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buff1: <span class="type">Buff</span>, buff2: <span class="type">Buff</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">            buff1.total = buff1.total + buff2.total</span><br><span class="line">            buff1.count = buff1.count + buff2.count</span><br><span class="line">            buff1</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//计算结果</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(buff: <span class="type">Buff</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">            buff.total / buff.count</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 缓冲区的编码操作</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Buff</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出的编码操作</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Long</span>] = <span class="type">Encoders</span>.scalaLong</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="4、数据的加载和保存">4、数据的加载和保存</h2>
<h3 id="4-1-通用的加载和保存方式">4.1 通用的加载和保存方式</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ===============加载数据============</span></span><br><span class="line"><span class="comment"># spark.read.load 是加载数据的通用方法</span></span><br><span class="line"><span class="comment"># 如果读取不同格式的数据，可以对不同的数据格式进行设定</span></span><br><span class="line">spark.read.format(<span class="string">"…"</span>)[.option(<span class="string">"…"</span>)].load(<span class="string">"…"</span>)</span><br><span class="line"><span class="comment"># format("…")：指定加载的数据类型，包括"csv"、"jdbc"、"json"、"orc"、"parquet"和"textFile"</span></span><br><span class="line"><span class="comment"># load("…")：在"csv"、"jdbc"、"json"、"orc"、"parquet"和"textFile"格式下需要传入加载数据的路径</span></span><br><span class="line"><span class="comment"># option("…")：在"jdbc"格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable</span></span><br><span class="line"><span class="comment"># 接在文件上进行查询: 文件格式.`文件路径`</span></span><br><span class="line">spark.sql(<span class="string">"select * from json.`/opt/module/data/user.json`"</span>).show</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==============保存数据==========</span></span><br><span class="line"><span class="comment"># df.write.save 是保存数据的通用方法</span></span><br><span class="line">df.write.format(<span class="string">"…"</span>)[.option(<span class="string">"…"</span>)].save(<span class="string">"…"</span>)</span><br><span class="line"><span class="comment"># format("…")：指定保存的数据类型，包括"csv"、"jdbc"、"json"、"orc"、"parquet"和"textFile"</span></span><br><span class="line"><span class="comment"># save ("…")：在"csv"、"orc"、"parquet"和"textFile"格式下需要传入保存数据的路径</span></span><br><span class="line"><span class="comment"># option("…")：在"jdbc"格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable保存操作可以使用 SaveMode, 用来指明如何处理数据，使用 mode()方法来设置。</span></span><br><span class="line"><span class="comment"># 有一点很重要: 这些 SaveMode 都是没有加锁的, 也不是原子操作</span></span><br><span class="line">df.write.mode(<span class="string">"append"</span>).json(<span class="string">"/opt/module/data/output"</span>)</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th><strong>Scala/Java</strong></th>
<th><strong>Any Language</strong></th>
<th><strong>Meaning</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>SaveMode.ErrorIfExists(default)</td>
<td>“error”(default)</td>
<td>如果文件已经存在则抛出异常</td>
</tr>
<tr>
<td>SaveMode.Append</td>
<td>“append”</td>
<td>如果文件已经存在则追加</td>
</tr>
<tr>
<td>SaveMode.Overwrite</td>
<td>“overwrite”</td>
<td>如果文件已经存在则覆盖</td>
</tr>
<tr>
<td>SaveMode.Ignore</td>
<td>“ignore”</td>
<td>如果文件已经存在则忽略</td>
</tr>
</tbody>
</table>
<h3 id="4-2-Parquet">4.2 Parquet</h3>
<p>Spark SQL 的默认数据源为 Parquet 格式。Parquet 是一种能够有效存储嵌套数据的列式存储格式。数据源为 Parquet 文件时，Spark SQL 可以方便的执行所有的操作，不需要使用 format。<strong>修改配置项spark.sql.sources.default，可修改默认数据源格式</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1)加载数据</span></span><br><span class="line">val df = spark.read.load(<span class="string">"examples/src/main/resources/users.parquet"</span>)</span><br><span class="line">df.show</span><br><span class="line"><span class="comment"># 2)保存数据</span></span><br><span class="line">var df = spark.read.json(<span class="string">"/opt/module/data/input/people.json"</span>)</span><br><span class="line"><span class="comment"># 保存为 parquet 格式</span></span><br><span class="line">df.write.mode(<span class="string">"append"</span>).save(<span class="string">"/opt/module/data/output"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="4-3-JSON">4.3 JSON</h3>
<p>Spark SQL 能够自动推测 JSON 数据集的结构，并将它加载为一个Dataset[Row]. 可以通过 SparkSession.read.json()去加载 JSON 文件。注意：Spark 读取的 JSON 文件不是传统的JSON 文件，<strong>每一行都应该是一个 JSON 串</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#json格式如下</span></span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"Michael"</span>&#125;</span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"Andy"</span>， <span class="string">"age"</span>:30&#125;</span><br><span class="line">[&#123;<span class="string">"name"</span>:<span class="string">"Justin"</span>， <span class="string">"age"</span>:19&#125;,&#123;<span class="string">"name"</span>:<span class="string">"Justin"</span>， <span class="string">"age"</span>:19&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入隐式转换</span></span><br><span class="line">import spark.implicits._</span><br><span class="line"><span class="comment"># 加载 JSON 文件</span></span><br><span class="line">val path = <span class="string">"/opt/module/spark-local/people.json"</span></span><br><span class="line">val peopleDF = spark.read.json(path)</span><br><span class="line"><span class="comment"># 创建临时表</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"><span class="comment"># 查询</span></span><br><span class="line">val teenagerNamesDF = spark.sql(<span class="string">"SELECT  name  FROM  people  WHERE  age  BETWEEN  13 AND 19"</span>)</span><br><span class="line">teenagerNamesDF.show()</span><br></pre></td></tr></table></figure>
<h3 id="4-4-CSV">4.4 CSV</h3>
<p>Spark SQL 可以配置 CSV 文件的列表信息，读取CSV 文件,CSV 文件的第一行设置为数据列</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">"csv"</span>).option(<span class="string">"sep"</span>, <span class="string">";"</span>).option(<span class="string">"inferSchema"</span>,<span class="string">"true"</span>).option(<span class="string">"header"</span>, <span class="string">"true"</span>).load(<span class="string">"data/user.csv"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="4-5-MySQL">4.5 MySQL</h3>
<p>Spark SQL 可以通过 JDBC 从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame 一系列的计算后，还可以将数据再写回关系型数据库中。如果使用spark-shell 操作，可在启动shell 时指定相关的数据库驱动路径或者将相关的数据库驱动放到 spark 的类路径下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure>
<p>演示在Idea 中通过 JDBC 对 Mysql 进行操作，导入依赖关系</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark04_SparkSQL_JDBC</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 读取MySQL数据</span></span><br><span class="line">        <span class="keyword">val</span> df = spark.read</span><br><span class="line">                .format(<span class="string">"jdbc"</span>)</span><br><span class="line">                .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://hadoop102:3306/spark-sql"</span>)</span><br><span class="line">                .option(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">                .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">                .option(<span class="string">"password"</span>, <span class="string">"123123"</span>)</span><br><span class="line">                .option(<span class="string">"dbtable"</span>, <span class="string">"user"</span>)</span><br><span class="line">                .load()</span><br><span class="line">        <span class="comment">//df.show</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 保存数据</span></span><br><span class="line">        df.write</span><br><span class="line">                .format(<span class="string">"jdbc"</span>)</span><br><span class="line">                .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://hadoop102:3306/spark-sql"</span>)</span><br><span class="line">                .option(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">                .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">                .option(<span class="string">"password"</span>, <span class="string">"123123"</span>)</span><br><span class="line">                .option(<span class="string">"dbtable"</span>, <span class="string">"user1"</span>)</span><br><span class="line">                .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">                .save()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 关闭环境</span></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-6-Hive">4.6 Hive</h3>
<p>Apache Hive 是 Hadoop 上的 SQL 引擎，Spark SQL 编译时可以包含 Hive  支持，也可以不包含。包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、UDF (用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。</p>
<p>若要把 Spark SQL 连接到一个部署好的 Hive  上，必须把 <code>hive-site.xml</code> 复制到Spark 的配置文件目录中(<code>$SPARK_HOME/conf</code>)。即使没有部署好 Hive，Spark SQL 也可以运行。 需要注意的是，如果你没有部署好 Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 <code>metastore_db</code>。此外，如果你尝试使用 HiveQL 中的<code>CREATE TABLE</code> (并非 <code>CREATE EXTERNAL TABLE</code>)语句来创建表，这些表会被放在你默认的文件系统中的<code> /user/hive/warehouse</code> 目录中(如果你的 classpath 中有配好的<code>hdfs-site.xml</code>，<strong>默认的文件系统就是 HDFS</strong>，否则就是本地文件系统)。<strong>spark-shell 默认是Hive 支持的；代码中是默认不支持的</strong>，需要手动指定（加一个参数即可）</p>
<p><strong>内嵌的</strong> <strong>HIVE</strong>，如果使用 Spark 内嵌的 Hive, 则什么都不用做, 直接使用即可。Hive 的元数据存储在 derby 中, 默认仓库地址:$<code>SPARK_HOME/spark-warehouse</code>,在实际使用中, 几乎没有任何人会使用内置的 Hive</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"show tables"</span>).show</span><br><span class="line">spark.sql(<span class="string">"create table aa(id int)"</span>)</span><br><span class="line">spark.sql(<span class="string">"load data local inpath 'input/ids.txt' into table aa"</span>)</span><br><span class="line">spark.sql(<span class="string">"select * from aa"</span>).show</span><br></pre></td></tr></table></figure>
<p><strong>外部的</strong> <strong>HIVE</strong>，如果想连接外部已经部署好的Hive，需要通过以下几个步骤：</p>
<ul>
<li>Spark 要接管 Hive 需要把<code>hive-site.xml</code> 拷贝到conf/目录下</li>
<li>把 Mysql 的驱动 copy 到 jars/目录下</li>
<li>如果访问不到 hdfs，则需要把<code>core-site.xml</code> 和 <code>hdfs-site.xml</code> 拷贝到 <code>conf/</code>目录下</li>
<li>重启 spark-shell</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"show tables"</span>).show</span><br><span class="line"><span class="comment"># Spark SQL CLI 可以很方便的在本地运行Hive 元数据服务以及从命令行执行查询任务。在</span></span><br><span class="line"><span class="comment"># Spark 目录下执行如下命令启动 Spark SQL CLI，直接执行 SQL 语句，类似一Hive 窗口</span></span><br><span class="line">bin/spark-sql</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark Thrift Server 是Spark 社区基于HiveServer2 实现的一个Thrift 服务。旨在无缝兼容HiveServer2。</span></span><br><span class="line"><span class="comment"># 因为 Spark Thrift Server 的接口和协议都和HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用hive 的 beeline 访问Spark Thrift Server 执行相关语句。</span></span><br><span class="line"><span class="comment"># Spark Thrift Server 的目的也只是取代HiveServer2，因此它依旧可以和 Hive Metastore 进行交互，获取到hive 的元数据</span></span><br><span class="line">sbin/start-thriftserver.sh</span><br><span class="line"><span class="comment"># 使用 beeline 连接 Thrift Server</span></span><br><span class="line">bin/beeline -u jdbc:hive2://Hadoop102:10000 -n root</span><br></pre></td></tr></table></figure>
<p>最后是IDEA操作Hive，首先导入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>然后将<code>hive-site.xml</code> 文件拷贝到项目的 resources 目录中，代码实现，注意：在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址：<code>config(&quot;spark.sql.warehouse.dir&quot;, &quot;hdfs://hadoop102:8020/user/hive/warehouse&quot;)</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark05_SparkSQL_Hive</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 权限问题，此处的 root 改为你们自己的 hadoop 用户名称</span></span><br><span class="line">        <span class="type">System</span>.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"atguigu"</span>)</span><br><span class="line">        <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().enableHiveSupport().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用SparkSQL连接外置的Hive</span></span><br><span class="line">        <span class="comment">// 1. 拷贝Hive-size.xml文件到classpath下</span></span><br><span class="line">        <span class="comment">// 2. 启用Hive的支持</span></span><br><span class="line">        <span class="comment">// 3. 增加对应的依赖关系（包含MySQL驱动）</span></span><br><span class="line">        spark.sql(<span class="string">"show tables"</span>).show</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 关闭环境</span></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>四、Spark Streaming</h1>
<h2 id="1、SparkStreaming-概述">1、SparkStreaming 概述</h2>
<h3 id="1-1-简介">1.1 简介</h3>
<p>Spark Streaming 是一个基于 Spark Core 之上的<strong>实时计算框架</strong>，可以从很多数据源消费数据并对数据进行实时的处理，具有高吞吐量和容错能力强等特点。</p>
<p><img src="http://qnypic.shawncoding.top/blog/202402291816209.png" alt></p>
<p>和 Spark 基于 RDD 的概念很相似，Spark Streaming 使用离散化流(discretized stream)作为抽象表示，叫作 DStream。DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而 DStream 是由这些 RDD 所组成的序列(因此得名“离散化”)。所以简单来将，DStream 就是对 RDD 在实时数据处理场景的一种封装</p>
<h3 id="1-2-特点">1.2 特点</h3>
<ul>
<li>易用，可以像编写离线批处理一样去编写流式程序，支持 java/scala/python 语言</li>
<li>容错，SparkStreaming 在没有额外代码和配置的情况下可以恢复丢失的工作</li>
<li><strong>易整合到 Spark 体系</strong>，流式处理与批处理和交互式查询相结合</li>
</ul>
<h3 id="1-3-整体流程">1.3 整体流程</h3>
<p><img src="http://qnypic.shawncoding.top/blog/202402291816210.png" alt></p>
<p>Spark Streaming 中，会有一个接收器组件 Receiver，作为一个长期运行的 task 跑在一个 Executor 上。Receiver 接收外部的数据流形成 input DStream。</p>
<p>DStream 会被按照时间间隔划分成一批一批的 RDD，当批处理间隔缩短到秒级时，便可以用于处理实时数据流。时间间隔的大小可以由参数指定，一般设在 500 毫秒到几秒之间。对 DStream 进行操作就是对 RDD 进行操作，计算处理的结果可以传给外部系统。Spark Streaming 接受到实时数据后，给数据分批次，然后传给 Spark Engine 处理最后生成该批次的结果。</p>
<h3 id="1-4-数据抽象">1.4 数据抽象</h3>
<p>这里使用 netcat 工具向 9999 端口不断的发送数据，通过 SparkStreaming 读取端口数据并统计不同单词出现的次数，首先添加依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>编写demo代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming01_WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建环境对象</span></span><br><span class="line">        <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">        <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 逻辑处理</span></span><br><span class="line">        <span class="comment">// 获取端口数据</span></span><br><span class="line">        <span class="keyword">val</span> lines: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> wordToOne = words.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> wordToCount: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">        wordToCount.print()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 由于SparkStreaming采集器是长期执行的任务，所以不能直接关闭</span></span><br><span class="line">        <span class="comment">// 如果main方法执行完毕，应用程序也会自动结束。所以不能让main执行完毕</span></span><br><span class="line">        <span class="comment">//ssc.stop()</span></span><br><span class="line">        <span class="comment">// 1. 启动采集器</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        <span class="comment">// 2. 等待采集器的关闭</span></span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 启动程序并通过netcat 发送数据</span></span><br><span class="line"><span class="comment">// nc -lk 9999 </span></span><br><span class="line"><span class="comment">// hello spark</span></span><br></pre></td></tr></table></figure>
<p>Spark Streaming 的基础抽象是 DStream(Discretized Stream，离散化数据流，连续不断的数据流)，代表持续性的数据流和经过各种 Spark 算子操作后的结果数据流</p>
<ul>
<li>DStream 本质上就是一系列时间上连续的 RDD</li>
<li>对 DStream 的数据的进行操作也是按照 RDD 为单位来进行的</li>
</ul>
<p><img src="http://qnypic.shawncoding.top/blog/202402291816211.png" alt></p>
<ul>
<li>容错性，底层 RDD 之间存在依赖关系，DStream 直接也有依赖关系，RDD 具有容错性，那么 DStream 也具有容错性</li>
<li>准实时性/近实时性，Spark Streaming 将流式计算分解成多个 Spark Job，对于每一时间段数据的处理都会经过 Spark DAG 图分解以及 Spark 的任务集的调度过程。对于目前版本的 Spark Streaming 而言，其最小的 Batch Size 的选取在 0.5~5 秒钟之间</li>
</ul>
<h2 id="2、DStream-相关操作">2、DStream 相关操作</h2>
<h3 id="2-1-DStream-创建">2.1  DStream 创建</h3>
<p><strong>RDD队列</strong></p>
<p>测试过程中，可以通过使用 ssc.queueStream(queueOfRDDs)来创建 DStream，每一个推送到这个队列中的RDD，都会作为一个DStream 处理</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming02_Queue</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 创建环境对象</span></span><br><span class="line">        <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">        <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rddQueue = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[<span class="type">RDD</span>[<span class="type">Int</span>]]()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> inputStream = ssc.queueStream(rddQueue,oneAtATime = <span class="literal">false</span>)</span><br><span class="line">        <span class="keyword">val</span> mappedStream = inputStream.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> reducedStream = mappedStream.reduceByKey(_ + _)</span><br><span class="line">        reducedStream.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">5</span>) &#123;</span><br><span class="line">            rddQueue += ssc.sparkContext.makeRDD(<span class="number">1</span> to <span class="number">300</span>, <span class="number">10</span>)</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">2000</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>自定义数据源</strong></p>
<p>需要继承Receiver，并实现 onStart、onStop 方法来自定义数据源采集</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming03_DIY</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> messageDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.receiverStream(<span class="keyword">new</span> <span class="type">MyReceiver</span>())</span><br><span class="line">        messageDS.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    自定义数据采集器</span></span><br><span class="line"><span class="comment">    1. 继承Receiver，定义泛型, 传递参数</span></span><br><span class="line"><span class="comment">    2. 重写方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyReceiver</span> <span class="keyword">extends</span> <span class="title">Receiver</span>[<span class="type">String</span>](<span class="params"><span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span></span>) </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">var</span> flg = <span class="literal">true</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">Thread</span>(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">                <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">                    <span class="keyword">while</span> ( flg ) &#123;</span><br><span class="line">                        <span class="keyword">val</span> message = <span class="string">"采集的数据为："</span> + <span class="keyword">new</span> <span class="type">Random</span>().nextInt(<span class="number">10</span>).toString</span><br><span class="line">                        store(message)</span><br><span class="line">                        <span class="type">Thread</span>.sleep(<span class="number">500</span>)</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;).start()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            flg = <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Kafka 数据源（面试、开发重点）</strong></p>
<p>ReceiverAPI：需要一个专门的Executor 去接收数据，然后发送给其他的 Executor 做计算。存在的问题，接收数据的Executor 和计算的Executor 速度会有所不同，特别在接收数据的Executor 速度大于计算的Executor 速度，会导致计算数据的节点内存溢出。早期版本中提供此方式，当前版本不适用</p>
<p>DirectAPI：是由计算的Executor 来主动消费Kafka 的数据，速度由自身控制，这里只讲这个模式，首先导入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.fasterxml.jackson.core<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jackson-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>编写代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming04_Kafka</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"hadoop102:9092,hadoop103:9092,hadoop104:9092"</span>,</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"atguigu"</span>,</span><br><span class="line">            <span class="string">"key.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>,</span><br><span class="line">            <span class="string">"value.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> kafkaDataDS: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">            ssc,</span><br><span class="line">            <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">            <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"atguiguNew"</span>), kafkaPara)</span><br><span class="line">        )</span><br><span class="line">        kafkaDataDS.map(_.value()).print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查看Kafka 消费进度</span></span><br><span class="line"><span class="comment">// bin/kafka-consumer-groups.sh --describe --bootstrap-server hadoop102:9092 --group atguigu</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-DStream-转换">2.2 DStream 转换</h3>
<p><strong>无状态转化操作</strong></p>
<p>无状态转化操作就是把简单的RDD 转化操作应用到每个批次上，也就是转化DStream 中的每一个RDD，即每个批次的处理不依赖于之前批次的数据</p>
<table>
<thead>
<tr>
<th><strong>Transformation</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>map(func)</td>
<td>对 DStream 中的各个元素进行 func 函数操作，然后返回一个新的 DStream</td>
</tr>
<tr>
<td>flatMap(func)</td>
<td>与 map 方法类似，只不过各个输入项可以被输出为零个或多个输出项</td>
</tr>
<tr>
<td>filter(func)</td>
<td>过滤出所有函数 func 返回值为 true 的 DStream 元素并返回一个新的 DStream</td>
</tr>
<tr>
<td>union(otherStream)</td>
<td>将源 DStream 和输入参数为 otherDStream 的元素合并，并返回一个新的 DStream</td>
</tr>
<tr>
<td>reduceByKey(func, [numTasks])</td>
<td>利用 func 函数对源 DStream 中的 key 进行聚合操作，然后返回新的(K，V)对构成的 DStream</td>
</tr>
<tr>
<td>join(otherStream, [numTasks])</td>
<td>输入为(K,V)、(K,W)类型的 DStream，返回一个新的(K，(V，W)类型的 DStream</td>
</tr>
<tr>
<td><strong>transform(func)</strong></td>
<td>通过 RDD-to-RDD 函数作用于 DStream 中的各个 RDD，可以是任意的 RDD 操作，从而返回一个新的 RDD</td>
</tr>
</tbody>
</table>
<p>Transform示例</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming06_State_Transform</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// transform方法可以将底层RDD获取到后进行操作</span></span><br><span class="line">        <span class="comment">// 1. DStream功能不完善</span></span><br><span class="line">        <span class="comment">// 2. 需要代码周期性的执行</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// Code : Driver端</span></span><br><span class="line">        <span class="keyword">val</span> newDS: <span class="type">DStream</span>[<span class="type">String</span>] = lines.transform(</span><br><span class="line">            rdd =&gt; &#123;</span><br><span class="line">                <span class="comment">// Code : Driver端，（周期性执行）</span></span><br><span class="line">                rdd.map(</span><br><span class="line">                    str =&gt; &#123;</span><br><span class="line">                        <span class="comment">// Code : Executor端</span></span><br><span class="line">                        str</span><br><span class="line">                    &#125;</span><br><span class="line">                )</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">// Code : Driver端</span></span><br><span class="line">        <span class="keyword">val</span> newDS1: <span class="type">DStream</span>[<span class="type">String</span>] = lines.map(</span><br><span class="line">            data =&gt; &#123;</span><br><span class="line">                <span class="comment">// Code : Executor端</span></span><br><span class="line">                data</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>有状态转化操作</strong></p>
<p>有状态转换包括基于追踪状态变化的转换(updateStateByKey)和滑动窗口的转换</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming05_State</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        ssc.checkpoint(<span class="string">"cp"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 无状态数据操作，只对当前的采集周期内的数据进行处理</span></span><br><span class="line">        <span class="comment">// 在某些场合下，需要保留数据统计结果（状态），实现数据的汇总</span></span><br><span class="line">        <span class="comment">// 使用有状态操作时，需要设定检查点路径</span></span><br><span class="line">        <span class="keyword">val</span> datas = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> wordToOne = datas.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">//val wordToCount = wordToOne.reduceByKey(_+_)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// updateStateByKey：根据key对数据的状态进行更新</span></span><br><span class="line">        <span class="comment">// 传递的参数中含有两个值</span></span><br><span class="line">        <span class="comment">// 第一个值表示相同的key的value数据</span></span><br><span class="line">        <span class="comment">// 第二个值表示缓存区相同key的value数据</span></span><br><span class="line">        <span class="keyword">val</span> state = wordToOne.updateStateByKey(</span><br><span class="line">            ( seq:<span class="type">Seq</span>[<span class="type">Int</span>], buff:<span class="type">Option</span>[<span class="type">Int</span>] ) =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> newCount = buff.getOrElse(<span class="number">0</span>) + seq.sum</span><br><span class="line">                <span class="type">Option</span>(newCount)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        state.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Window Operations 可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming 的允许状态。所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长。注意：这两者都必须为采集周期大小的整数倍</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming06_State_Window</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">        <span class="keyword">val</span> wordToOne = lines.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 窗口的范围应该是采集周期的整数倍</span></span><br><span class="line">        <span class="comment">// 窗口可以滑动的，但是默认情况下，一个采集周期进行滑动</span></span><br><span class="line">        <span class="comment">// 这样的话，可能会出现重复数据的计算，为了避免这种情况，可以改变滑动的滑动（步长）</span></span><br><span class="line">        <span class="keyword">val</span> windowDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.window(<span class="type">Seconds</span>(<span class="number">6</span>), <span class="type">Seconds</span>(<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> wordToCount = windowDS.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">        wordToCount.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关于Window 的操作还有如下方法：</p>
<ul>
<li>window(windowLength, slideInterval): 基于对源DStream 窗化的批次进行计算返回一个新的Dstream；</li>
<li>countByWindow(windowLength, slideInterval): 返回一个滑动窗口计数流中的元素个数；</li>
<li>reduceByWindow(func, windowLength, slideInterval): 通过使用自定义函数整合滑动区间流元素来创建一个新的单元素流；</li>
<li>reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]): 当在一个(K,V) 对的DStream 上调用此函数，会返回一个新(K,V)对的 DStream，此处通过对滑动窗口中批次数据使用 reduce 函数来整合每个 key 的 value 值。</li>
<li>reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]): 这个函数是上述函数的变化版本，每个窗口的 reduce 值都是通过用前一个窗的 reduce 值来递增计算。通过 reduce 进入到滑动窗口数据并&quot;反向 reduce&quot;离开窗口的旧数据来实现这个操作。一个例子是随着窗口滑动对keys 的&quot;加&quot;“减&quot;计数。通过前边介绍可以想到，这个函数只适用于&quot;可逆的 reduce 函数”，也就是这些 reduce 函数有相应的&quot;反 reduce&quot;函数(以参数 invFunc 形式传入)。如前述函数，reduce 任务的数量通过可选参数来配置</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sc.checkpoint(<span class="string">"cp"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"><span class="keyword">val</span> wordToOne = lines.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// reduceByKeyAndWindow : 当窗口范围比较大，但是滑动幅度比较小，那么可以采用增加数据和删除数据的方式</span></span><br><span class="line"><span class="comment">// 无需重复计算，提升性能。</span></span><br><span class="line"><span class="keyword">val</span> windowDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] =</span><br><span class="line">    wordToOne.reduceByKeyAndWindow(</span><br><span class="line">        (x:<span class="type">Int</span>, y:<span class="type">Int</span>) =&gt; &#123; x + y&#125;,</span><br><span class="line">        (x:<span class="type">Int</span>, y:<span class="type">Int</span>) =&gt; &#123;x - y&#125;,</span><br><span class="line">        <span class="type">Seconds</span>(<span class="number">9</span>), <span class="type">Seconds</span>(<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<h3 id="2-3-DStream-输出">2.3 DStream 输出</h3>
<p>Output Operations 可以将 DStream 的数据输出到外部的数据库或文件系统。当某个 Output Operations 被调用时，spark streaming 程序才会开始真正的计算过程(与 RDD 的 Action 类似)</p>
<table>
<thead>
<tr>
<th><strong>Output Operation</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>print()</td>
<td>打印到控制台</td>
</tr>
<tr>
<td>saveAsTextFiles(prefix, [suffix])</td>
<td>保存流的内容为文本文件，文件名为&quot;prefix-TIME_IN_MS[.suffix]&quot;</td>
</tr>
<tr>
<td>saveAsObjectFiles(prefix,[suffix])</td>
<td>保存流的内容为 SequenceFile，文件名为 “prefix-TIME_IN_MS[.suffix]”</td>
</tr>
<tr>
<td>saveAsHadoopFiles(prefix,[suffix])</td>
<td>保存流的内容为 hadoop 文件，文件名为&quot;prefix-TIME_IN_MS[.suffix]&quot;</td>
</tr>
<tr>
<td>foreachRDD(func)</td>
<td>对 Dstream 里面的每个 RDD 执行 func；注意连接不能写在 driver 层面（序列化）； 如果写在 foreach 则每个 RDD 中的每一条数据都创建，得不偿失；增加 foreachPartition，在分区创建（获取）</td>
</tr>
</tbody>
</table>
<h3 id="2-4-优雅关闭">2.4 优雅关闭</h3>
<p>流式任务需要 7*24 小时执行，但是有时涉及到升级代码需要主动停止程序，但是分布式程序，没办法做到一个个进程去杀死，所有配置优雅的关闭就显得至关重要了。使用外部文件系统来控制内部程序关闭</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming08_Close</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">           线程的关闭：</span></span><br><span class="line"><span class="comment">           val thread = new Thread()</span></span><br><span class="line"><span class="comment">           thread.start()</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">           thread.stop(); // 强制关闭</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">        <span class="keyword">val</span> wordToOne = lines.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        wordToOne.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果想要关闭采集器，那么需要创建新的线程</span></span><br><span class="line">        <span class="comment">// 而且需要在第三方程序中增加关闭状态</span></span><br><span class="line">        <span class="keyword">new</span> <span class="type">Thread</span>(</span><br><span class="line">            <span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">                <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">                    <span class="comment">// 优雅地关闭</span></span><br><span class="line">                    <span class="comment">// 计算节点不在接收新的数据，而是将现有的数据处理完毕，然后关闭</span></span><br><span class="line">                    <span class="comment">// Mysql : Table(stopSpark) =&gt; Row =&gt; data</span></span><br><span class="line">                    <span class="comment">// Redis : Data（K-V）</span></span><br><span class="line">                    <span class="comment">// ZK    : /stopSpark</span></span><br><span class="line">                    <span class="comment">// HDFS  : /stopSpark</span></span><br><span class="line">                    <span class="comment">/*</span></span><br><span class="line"><span class="comment">                    while ( true ) &#123;</span></span><br><span class="line"><span class="comment">                        if (true) &#123;</span></span><br><span class="line"><span class="comment">                            // 获取SparkStreaming状态</span></span><br><span class="line"><span class="comment">                            val state: StreamingContextState = ssc.getState()</span></span><br><span class="line"><span class="comment">                            if ( state == StreamingContextState.ACTIVE ) &#123;</span></span><br><span class="line"><span class="comment">                                ssc.stop(true, true)</span></span><br><span class="line"><span class="comment">                            &#125;</span></span><br><span class="line"><span class="comment">                        &#125;</span></span><br><span class="line"><span class="comment">                        Thread.sleep(5000)</span></span><br><span class="line"><span class="comment">                    &#125;</span></span><br><span class="line"><span class="comment">                     */</span></span><br><span class="line"></span><br><span class="line">                    <span class="type">Thread</span>.sleep(<span class="number">5000</span>)</span><br><span class="line">                    <span class="keyword">val</span> state: <span class="type">StreamingContextState</span> = ssc.getState()</span><br><span class="line">                    <span class="keyword">if</span> ( state == <span class="type">StreamingContextState</span>.<span class="type">ACTIVE</span> ) &#123;</span><br><span class="line">                        ssc.stop(<span class="literal">true</span>, <span class="literal">true</span>)</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="type">System</span>.exit(<span class="number">0</span>)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        ).start()</span><br><span class="line"></span><br><span class="line">        ssc.awaitTermination() <span class="comment">// block 阻塞main线程</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于优雅恢复，停止前可以保存检查点</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming09_Resume</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getActiveOrCreate(<span class="string">"cp"</span>, ()=&gt;&#123;</span><br><span class="line">            <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">            <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">            <span class="keyword">val</span> wordToOne = lines.map((_,<span class="number">1</span>))</span><br><span class="line">            wordToOne.print()</span><br><span class="line">            ssc</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        ssc.checkpoint(<span class="string">"cp"</span>)</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination() <span class="comment">// block 阻塞main线程</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      

      


      
        <div class="page-reward">
          <a href="javascript:;" class="page-reward-btn tooltip-top">
            <div class="tooltip tooltip-east">
            <span class="tooltip-item">
              赏
            </span>
            <span class="tooltip-content">
              <span class="tooltip-text">
                <span class="tooltip-inner">
                  <p class="reward-p"><i class="icon icon-quo-left"></i>谢谢你请我吃糖果<i class="icon icon-quo-right"></i></p>
                  <div class="reward-box">
                    
                    <div class="reward-box-item">
                      <img class="reward-img" src="/img/alipay.png">
                      <span class="reward-type">支付宝</span>
                    </div>
                    
                    
                  </div>
                </span>
              </span>
            </span>
          </div>
          </a>
        </div>
      
    </div>

    <!-- 添加本文结束标记-->
    
    <div style="text-align:center;color: #00BFFF;font-size:14px;">
         -------------本文结束^-^感谢您的阅读-------------
         </div>
    
    <!-- 添加完成 -->


      <!-- 《添加版权声明 -->
      
          <!-- 《添加版权声明 -->
<!--添加版权声明https://github.com/JoeyBling/hexo-theme-yilia-plus/commit/c1215e132f6d5621c5fea83d3c4f7ccbcca074a3-->


<!-- #版权基础设定：0-关闭声明； 1-文章对应的md文件里有declare: true属性，才有版权声明； 2-所有文章均有版权声明 -->

  <div class="declare">
    <strong class="author">本文作者：</strong>
    
      Shawn
    
    <br>
    <strong class="create-time">发布时间：</strong>
    2024-02-06
    <br>
    <strong class="update-time">最后更新：</strong>
    2024-02-29
    <br>
    <strong class="article-titles">本文标题：</strong>
    <a href="https://blog.shawncoding.top/posts/4a5309a3.html" title="Spark3学习笔记" target="_blank">Spark3学习笔记</a>
    <br>
    <strong class="article-url">本文链接：</strong>
    <a href="https://blog.shawncoding.top/posts/4a5309a3.html" title="Spark3学习笔记" target="_blank">https://blog.shawncoding.top/posts/4a5309a3.html</a>
    <br>
    <strong class="copyright">版权声明：</strong>
    本作品采用
    <a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" title="知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议">CC BY-NC-SA 4.0</a>
    许可协议进行许可。转载请注明出处！
    
      <br>
      <a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"/></a>
    
  </div>

<!-- 添加版权声明》 -->
      
      <!-- 添加版权声明》 -->

    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">大数据</a>
        		</li>
      		
		</ul>
	</div>

      
      <!-- 文末访问量统计（开始） -->
      <!--<span id="busuanzi_container_page_pv">
          |阅读量:<span id="busuanzi_value_page_pv"></span>次
      </span>-->
      <!-- 文末访问量统计（结束） -->

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/大数据//" class="article-tag-list-link color4">大数据</a>
        		</li>
      		
		</ul>
	</div>


     

      
        
<div class="share-btn share-icons tooltip-left">
  <div class="tooltip tooltip-east">
    <span class="tooltip-item">
      <a href="javascript:;" class="share-sns share-outer">
        <i class="icon icon-share"></i>
      </a>
    </span>
    <span class="tooltip-content">
      <div class="share-wrap">
        <div class="share-icons">
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="icon icon-weibo"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="icon icon-weixin"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="icon icon-qq"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="icon icon-douban"></i>
          </a>
          <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a>
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="icon icon-facebook"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="icon icon-twitter"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="icon icon-google"></i>
          </a>
        </div>
      </div>
    </span>
  </div>
</div>

<div class="page-modal wx-share js-wx-box">
    <a class="close js-modal-close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//pan.baidu.com/share/qrcode?url=https://blog.shawncoding.top/posts/4a5309a3.html" alt="微信分享二维码">
    </div>
</div>

<div class="mask js-mask"></div>
      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

  
<nav id="article-nav">
  
    <a href="/posts/15b652da.html" id="article-nav-newer" class="article-nav-link-wrap">
      <i class="icon-circle-left"></i>
      <div class="article-nav-title">
        
          Zookeeper3.5.7基础学习
        
      </div>
    </a>
  
  
    <a href="/posts/28ae1688.html" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Spark3内核源码与优化</div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>


<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
        <div class="toc-container tooltip-left">
            <i class="icon-font icon-category"></i>
            <div class="tooltip tooltip-east">
                <span class="tooltip-item">
                </span>
                <span class="tooltip-content">
                    <div class="toc-article">
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#null"><span class="toc-number">1.</span> <span class="toc-text">Spark3学习笔记</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#null"><span class="toc-number">2.</span> <span class="toc-text">一、Spark 基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、Spark概述"><span class="toc-number">2.1.</span> <span class="toc-text">1、Spark概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Spark简介"><span class="toc-number">2.1.1.</span> <span class="toc-text">1.1 Spark简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Spark-VS-Hadoop"><span class="toc-number">2.1.2.</span> <span class="toc-text">1.2 Spark VS Hadoop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-Spark特点"><span class="toc-number">2.1.3.</span> <span class="toc-text">1.3 Spark特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-Spark入门Demo"><span class="toc-number">2.1.4.</span> <span class="toc-text">1.4 Spark入门Demo</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、Spark-运行模式"><span class="toc-number">2.2.</span> <span class="toc-text">2、Spark 运行模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-概述"><span class="toc-number">2.2.1.</span> <span class="toc-text">2.1 概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Local模式"><span class="toc-number">2.2.2.</span> <span class="toc-text">2.2 Local模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Standalone-模式"><span class="toc-number">2.2.3.</span> <span class="toc-text">2.3 Standalone 模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-配置高可用（-Standalone-HA）"><span class="toc-number">2.2.4.</span> <span class="toc-text">2.4 配置高可用（ Standalone +HA）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-Yarn-模式"><span class="toc-number">2.2.5.</span> <span class="toc-text">2.5 Yarn 模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-K8S-Mesos-模式"><span class="toc-number">2.2.6.</span> <span class="toc-text">2.6 K8S &amp; Mesos 模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-Windows-模式"><span class="toc-number">2.2.7.</span> <span class="toc-text">2.7 Windows 模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-8-部署模式对比"><span class="toc-number">2.2.8.</span> <span class="toc-text">2.8 部署模式对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-9-端口号"><span class="toc-number">2.2.9.</span> <span class="toc-text">2.9 端口号</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、Spark-运行架构"><span class="toc-number">2.3.</span> <span class="toc-text">3、Spark 运行架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-运行架构"><span class="toc-number">2.3.1.</span> <span class="toc-text">3.1 运行架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-核心组件"><span class="toc-number">2.3.2.</span> <span class="toc-text">3.2 核心组件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-核心概念"><span class="toc-number">2.3.3.</span> <span class="toc-text">3.3 核心概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-提交流程"><span class="toc-number">2.3.4.</span> <span class="toc-text">3.4 提交流程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#null"><span class="toc-number">3.</span> <span class="toc-text">二、Spark核心编程(Core)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、概述"><span class="toc-number">3.1.</span> <span class="toc-text">1、概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、RDD详解"><span class="toc-number">3.2.</span> <span class="toc-text">2、RDD详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-RDD概述"><span class="toc-number">3.2.1.</span> <span class="toc-text">2.1 RDD概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-RDD主要属性"><span class="toc-number">3.2.2.</span> <span class="toc-text">2.2 RDD主要属性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-执行原理"><span class="toc-number">3.2.3.</span> <span class="toc-text">2.3 执行原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、RDD-API"><span class="toc-number">3.3.</span> <span class="toc-text">3、RDD-API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-RDD-的创建方式"><span class="toc-number">3.3.1.</span> <span class="toc-text">3.1 RDD 的创建方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-RDD-并行度与分区"><span class="toc-number">3.3.2.</span> <span class="toc-text">3.2 RDD 并行度与分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-RDD-转换算子"><span class="toc-number">3.3.3.</span> <span class="toc-text">3.3 RDD 转换算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-RDD-行动算子"><span class="toc-number">3.3.4.</span> <span class="toc-text">3.4 RDD 行动算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-RDD-的持久化-缓存"><span class="toc-number">3.3.5.</span> <span class="toc-text">3.5 RDD 的持久化&#x2F;缓存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-RDD-容错机制-Checkpoint"><span class="toc-number">3.3.6.</span> <span class="toc-text">3.6 RDD 容错机制 Checkpoint</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-7-RDD-文件读取与保存"><span class="toc-number">3.3.7.</span> <span class="toc-text">3.7 RDD 文件读取与保存</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、RDD其他概念"><span class="toc-number">3.4.</span> <span class="toc-text">4、RDD其他概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-RDD序列化"><span class="toc-number">3.4.1.</span> <span class="toc-text">4.1 RDD序列化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-RDD-依赖关系"><span class="toc-number">3.4.2.</span> <span class="toc-text">4.2 RDD 依赖关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-DAG-的生成和划分-Stage"><span class="toc-number">3.4.3.</span> <span class="toc-text">4.3 DAG 的生成和划分 Stage</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-RDD-分区器"><span class="toc-number">3.4.4.</span> <span class="toc-text">4.4 RDD 分区器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5、RDD累加器"><span class="toc-number">3.5.</span> <span class="toc-text">5、RDD累加器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6、RDD广播变量"><span class="toc-number">3.6.</span> <span class="toc-text">6、RDD广播变量</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#null"><span class="toc-number">4.</span> <span class="toc-text">三、Spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、SparkSQL-概述"><span class="toc-number">4.1.</span> <span class="toc-text">1、SparkSQL 概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-SparkSQL的发展"><span class="toc-number">4.1.1.</span> <span class="toc-text">1.1 SparkSQL的发展</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Hive-and-SparkSQL"><span class="toc-number">4.1.2.</span> <span class="toc-text">1.2 Hive and SparkSQL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-SparkSQL-特点"><span class="toc-number">4.1.3.</span> <span class="toc-text">1.3 SparkSQL 特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-DataFrame简介"><span class="toc-number">4.1.4.</span> <span class="toc-text">1.4 DataFrame简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-DataSet"><span class="toc-number">4.1.5.</span> <span class="toc-text">1.5 DataSet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-RDD、DataFrame、DataSet-的区别"><span class="toc-number">4.1.6.</span> <span class="toc-text">1.6 RDD、DataFrame、DataSet 的区别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、SparkSQL-核心编程"><span class="toc-number">4.2.</span> <span class="toc-text">2、SparkSQL 核心编程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-概述-v2"><span class="toc-number">4.2.1.</span> <span class="toc-text">2.1 概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-创建-DataFrame-DataSet"><span class="toc-number">4.2.2.</span> <span class="toc-text">2.2 创建 DataFrame&#x2F;DataSet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-两种查询风格：DSL-和-SQL"><span class="toc-number">4.2.3.</span> <span class="toc-text">2.3 两种查询风格：DSL 和 SQL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-RDD、DataFrame、DataSet-三者的关系"><span class="toc-number">4.2.4.</span> <span class="toc-text">2.4 RDD、DataFrame、DataSet 三者的关系</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、Spark实战"><span class="toc-number">4.3.</span> <span class="toc-text">3、Spark实战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Spark-SQL-完成-WordCount"><span class="toc-number">4.3.1.</span> <span class="toc-text">3.1 Spark SQL 完成 WordCount</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-用户自定义函数"><span class="toc-number">4.3.2.</span> <span class="toc-text">3.2 用户自定义函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、数据的加载和保存"><span class="toc-number">4.4.</span> <span class="toc-text">4、数据的加载和保存</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-通用的加载和保存方式"><span class="toc-number">4.4.1.</span> <span class="toc-text">4.1 通用的加载和保存方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Parquet"><span class="toc-number">4.4.2.</span> <span class="toc-text">4.2 Parquet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-JSON"><span class="toc-number">4.4.3.</span> <span class="toc-text">4.3 JSON</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-CSV"><span class="toc-number">4.4.4.</span> <span class="toc-text">4.4 CSV</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-MySQL"><span class="toc-number">4.4.5.</span> <span class="toc-text">4.5 MySQL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-Hive"><span class="toc-number">4.4.6.</span> <span class="toc-text">4.6 Hive</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#null"><span class="toc-number">5.</span> <span class="toc-text">四、Spark Streaming</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、SparkStreaming-概述"><span class="toc-number">5.1.</span> <span class="toc-text">1、SparkStreaming 概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-简介"><span class="toc-number">5.1.1.</span> <span class="toc-text">1.1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-特点"><span class="toc-number">5.1.2.</span> <span class="toc-text">1.2 特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-整体流程"><span class="toc-number">5.1.3.</span> <span class="toc-text">1.3 整体流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-数据抽象"><span class="toc-number">5.1.4.</span> <span class="toc-text">1.4 数据抽象</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、DStream-相关操作"><span class="toc-number">5.2.</span> <span class="toc-text">2、DStream 相关操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-DStream-创建"><span class="toc-number">5.2.1.</span> <span class="toc-text">2.1  DStream 创建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-DStream-转换"><span class="toc-number">5.2.2.</span> <span class="toc-text">2.2 DStream 转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-DStream-输出"><span class="toc-number">5.2.3.</span> <span class="toc-text">2.3 DStream 输出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-优雅关闭"><span class="toc-number">5.2.4.</span> <span class="toc-text">2.4 优雅关闭</span></a></li></ol></li></ol></li></ol>
                    </div>
                </span>
            </div>
        </div>
        
    </div>
</aside>

<!-- 评论 -->

  
    <section id="comments" class="comments">
       <div id="vcomment" class="comment"></div> 
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>
<script>
   var notify = 'false' == true ? true : false;
   var verify = 'false' == true ? true : false;
    window.onload = function() {
        new Valine({
            el: '.comment',
            notify: notify,
            verify: verify,
            app_id: "hRDQtFPNrfKmHhxhBlFlXkGq-gzGzoHsz",
            app_key: "jSCOVLxSm31BVC1D8RrKWn1d",
            placeholder: "😉看了这么久，有啥想说的吗？🤔",
            avatar:"retro"
        });
    }
</script>
  </section>

  
  
  

  

  

  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
        <div class="footer-left">
            <!--如果有设置theme.since，并且时间小于当前时间（年份），形式为：@ 2018 - 2019(+作者/标题)；否则默认为当前年份(+作者/标题)。 -->
            
                &copy; 2020-2024
            
            Shawn
        </div>


<!-- 《网站备案号 -->
    
        <div class="BeiAn-info">
            <!-- <img src="https://gitee.com/LXT2017/Picbed/raw/blogimg/noteimg/china.png" title="备案管理系统"> -->
            <img src="https://picblog.shawncoding.top/background/china.png" title="备案管理系统">
            <a href="https://beian.miit.gov.cn/" style="color:#f72b07" target="_blank" title="备案号">
                粤ICP备00000000号
            </a>
        </div>
    
    <!-- 网站备案号》 -->

<div class="footer-right">
        <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
    <script>
        var now = new Date(); 
        function createtime() { 
            var grt= new Date("1/1/2020 00:00:00");//此处修改你的建站时间或者网站上线时间 
            now.setTime(now.getTime()+250); 
            days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
            hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
            if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
            mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
            seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
            snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
            document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; 
            document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
        } 
        setInterval("createtime()",250);
    </script>


    	
    </div>
</div>
</div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">上一页</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">下一页</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>

    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">友链</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Spring基础</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">安卓</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">python</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">SpringCloud</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">虚拟机</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">C&C++</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">docker</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">SQL</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">linux基础</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">中间件</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">IDETools</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">MyBatis</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Java基础</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">SpringBoot</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Linux运维</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">K8S</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">虚拟化与云计算</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">网络</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">ROS</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Python</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">源码分析</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">虚拟化基础</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">前端</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Github</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">linux运维</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">vue</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">大数据</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">运维相关</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="https://lxt2017.github.io/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>Github</a>
            </li>
          
            <li class="search-li">
              <a href="https://blog.csdn.net/lemon_TT" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>CSDN博客</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.yansheng.xyz/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>荷塘月色</a>
            </li>
          
            <li class="search-li">
              <a href="https://tding.top/archives/9a232bbe.html" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>小丁的个人博客</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
          <div class="aboutme-wrap">
        <div style="display:;">
          <script type="text/javascript" src="https://api.lwl12.com/hitokoto/v1?encode=js&charset=utf-8"></script>
          <p style="margin:0 20px 0 20px;">
            <span id="lwlhitokoto">
              <script>
                lwlhitokoto();
              </script>
            </span>
          </p>
          <p id="js-aboutme" style="margin:0 20px 0 20px;">&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;心怀天下&lt;br/&gt;只认真生活&lt;br/&gt;自信微笑面对未来</P>
        </div>
      </div>
        
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
  <!-- 雪花特效3 -->
  
    <script type="text/javascript" src="/js/snow3.js"></script>
  
<!-- 代码块复制功能 -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.js"></script>
<script type="text/javascript" src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script type="text/javascript" src="/js/clipboard_use.js"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hibiki.model.json"},"display":{"position":"right","width":120,"height":300},"mobile":{"show":false},"log":false});</script></body>

<!-- 《添加折叠按钮脚本 -->
<script>
    var hide = false;
    function myFunction(x) {
        x.classList.toggle("change");
        if(hide == false){
            $(".left-col").css('display', 'none');
            $(".mid-col").css("left", 6);
            $(".tools-col").css('display', 'none');
            $(".tools-col.hide").css('display', 'none');
            hide = true;

            $(".mymenucontainer").css('left', '0');

        }else{
            $(".left-col").css('display', '');
            $(".mid-col").css("left", 300);
            $(".tools-col").css('display', '');
            $(".tools-col.hide").css('display', '');
            hide = false;

            $(".mymenucontainer").css('left', '260px');
        }
    }
</script>
<script>
    // 移动设备侦测
    var isMobile = {
      Android: function () {
        return navigator.userAgent.match(/Android/i);
      },
      BlackBerry: function () {
        return navigator.userAgent.match(/BlackBerry/i);
      },
      iOS: function () {
        return navigator.userAgent.match(/iPhone|iPad|iPod/i);
      },
      Opera: function () {
        return navigator.userAgent.match(/Opera Mini/i);
      },
      Windows: function () {
        return navigator.userAgent.match(/IEMobile/i);
      },
      any: function () {
        return (isMobile.Android() || isMobile.BlackBerry() || isMobile.iOS() || isMobile.Opera() || isMobile.Windows());
      }
    };

    if(isMobile.any()){
        //手机端取消搜索功能
        $('.local-search').css("display","none");
    }

    if ($('.local-search').size() && !isMobile.any()) {
      $.getScript('/js/search.js', function() {
        searchFunc("/search.xml", 'local-search-input', 'local-search-result');
      });
    }
</script>
<!-- 添加折叠按钮脚本》 -->
</html>
<script type="text/javascript" src="/js/src/love.js"></script>